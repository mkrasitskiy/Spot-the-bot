Grandes Ideas De La Ciencia

ISAAC ASIMOV

GRANDES IDEAS DE LACIENCIA

Segunda reimpresión en "El Libro de Bolsillo": 1987

© 1969 by Isaac Asimov

© Ed. cast.: Alianza Editorial, S. A., Madrid, 1983, 1984, 1987

Calle Milán, 38, 28043 Madrid; teléf. 200 00 45

Papel fabricado por Sniace, S. A.

Printed in Spain

A Eric Berger,

que siempre ha cooperado

1. Tales y la Ciencia

¿De qué está compuesto el universo?

Esa pregunta, tan importante, se la planteó hacia el año 600 A. C. el pensador griego Tales, y dio una solución falsa: «Todas las cosas son agua».

La idea, además de incorrecta, tampoco era original del todo. Pero aún así es uno de los enunciados más importantes en la historia de la ciencia, porque sin él -u otro equivalente- no habría ni siquiera lo que hoy entendemos por «ciencia».

La importancia de la solución que dio Tales se nos hará clara si examinamos cómo llegó a ella. A nadie le sorprenderá saber que este hombre que dijo que todas las cosas eran agua vivía en un puerto de mar. Mileto, que así se llamaba la ciudad, estaba situada en la costa oriental del Mar Egeo, que hoy pertenece a Turquía. Mileto ya no existe, pero en el año 600 A. C. era la ciudad más próspera del mundo de habla griega.

Al borde del litoral

No es impensable que Tales cavilase sobre la naturaleza del universo al borde del mar, con la mirada fija en el Egeo. Sabía que éste se abría hacia el sur en otro mar más grande, al que hoy llamamos Mediterráneo, y que se extendía cientos de millas hacia el Oeste. El Mediterráneo pasaba por un angosto estrecho (el de Gibraltar), vigilado por dos peñones rocosos que los griegos llamaban las Columnas de Hércules.

Más allá de las Columnas de Hércules había un océano (el Atlántico), y los griegos creían que esta masa de agua circundaba los continentes de la Tierra por todas partes.

El continente, la tierra firme, tenía, según Tales, la forma de un disco de algunos miles de millas de diámetro, flotando en medio de un océano infinito. Pero tampoco ignoraba que el continente propiamente dicho estaba surcado por las aguas. Había ríos que lo cruzaban, lagos diseminados aquí y allá y manantiales que surgían de sus entrañas. El agua se secaba y desaparecía en el aire, para convertirse luego otra vez en agua y caer en forma de lluvia. Había agua arriba, abajo y por todas partes.

¿Tierra compuesta de agua?

Según él, los mismos cuerpos sólidos de la tierra firme estaban compuestos de agua, como creía haber comprobado de joven con sus propios ojos: viajando por Egipto había visto crecer el río Nilo; al retirarse las aguas, quedaba atrás un suelo fértil y rico. Y en el norte de Egipto, allí donde el Nilo moría en el mar, había una región de suelo blando formado por las aguas de las crecidas. (Esta zona tenía forma triangular, como la letra «delta» del alfabeto griego, por lo cual recibía el nombre de «delta del Nilo».)

Al hilo de todos estos pensamientos Tales llegó a una conclusión que le parecía lógica: «Todo es agua». Ni qué decir tiene que estaba equivocado. El aire no es agua, y aunque el vapor de agua puede mezclarse con el aire, no por eso se transforma en él. Tampoco la tierra firme es agua; los ríos pueden arrastrar partículas de tierra desde las montañas a la planicie, pero esas partículas no son de agua.

Tales «versus» Babilonia

La idea de Tales, ya lo dijimos, no era del todo suya, pues tuvo su origen en Babilonia, otro de los países que había visitado de joven. La antigua civilización de Babilonia había llegado a importantes conclusiones en materia de astronomía y matemáticas, y estos resultados tuvieron por fuerza que fascinar a un pensador tan serio como Tales. Los babilonios creían que la tierra firme era un disco situado en un manantial de agua dulce, la cual afloraba aquí y allá a la superficie formando ríos, lagos y fuentes; y que alrededor de la tierra había agua salada por todas partes.

Cualquiera diría que la idea era la misma que la de Tales, y que éste no hacía más que repetir las teorías babilónicas. ¡No del todo! Los babilonios, a diferencia de Tales, concebían el agua no como tal, sino como una colección de seres sobrenaturales. El agua dulce era el dios Apsu, el agua salada la diosa Tiamat, y entre ambos engendraron muchos otros dioses y diosas. (Los griegos tenían una idea parecida, pues pensaban que Okeanos, el dios del océano, era el padre de los dioses.)

Según la mitología babilónica, entre Tiamat y sus descendientes hubo una guerra en la que, tras gigantesca batalla, Marduk, uno de los nuevos dioses, mató a Tiamat y la escindió en dos. Con una de las mitades hizo el cielo, con la otra la tierra firme.

Esa era la respuesta que daban los babilonios a la pregunta «¿de qué está compuesto el universo?». Tales se acercó a la misma solución desde un ángulo diferente. Su imagen del universo era distinta porque prescindía de dioses, diosas y grandes batallas entre seres sobrenaturales. Se limitó a decir: «Todas las cosas son agua».

Tales tenía discípulos en Mileto y en ciudades vecinas de la costa egea. Doce de ellas componían una región que se llamaba Jonia, por la cual Tales y sus discípulos recibieron el nombre de «escuela jónica» Los jonios persistieron en su empeño de explicar el universo sin recurrir a seres divinos, iniciando así una tradición que ha perdurado hasta nuestros días.

La importancia de la tradición jónica

¿Por qué fue tan importante el interpretar el universo sin recurrir a divinidades? La ciencia ¿podría haber surgido sin esa tradición?

Imaginemos que el universo es producto de los dioses, que lo tienen a su merced y pueden hacer con él lo que se les antoje. Si tal diosa está enojada porque el templo erguido en su honor no es suficientemente grandioso, envía una plaga. Si un guerrero se halla en mal trance y reza al dios X y le promete sacrificarle reses, éste puede enviar una nube que le oculte de sus enemigos. No hay manera de prever el curso del universo: todo depende del capricho de los dioses.

En la teoría de Tales y de sus discípulos no había divinidades que se inmiscuyeran en los designios del universo. El universo obraba exclusivamente de acuerdo con su propia naturaleza. Las plagas y las nubes eran producto de causas naturales solamente y no aparecían mientras no se hallaran presentes éstas últimas. La escuela de Tales llegó así a un supuesto básico: El universo se conduce de acuerdo con ciertas «leyes de la naturaleza» que no pueden alterarse.

Este universo ¿es mejor que aquel otro que se mueve al son de las veleidades divinas? Si los dioses hacen y deshacen a su antojo, ¿quién es capaz de predecir lo que sucederá mañana? Bastaría que el «dios del Sol» estuviese enojado para que, a lo peor, no amaneciera el día siguiente. Mientras los hombres tuvieron fijada la mente en lo sobrenatural no vieron razón alguna para tratar de descifrar los designios del universo, prefiriendo idear modos y maneras de agradar a los dioses o de aplacarlos cuando se desataba su ira. Lo importante era construir templos y altares, inventar rezos y rituales de sacrificio, fabricar ídolos y hacer magia.

Y lo malo es que nada podía descalificar este sistema. Porque supongamos que, pese a todo el ritual, sobrevenía la sequía o se desataba la plaga. Lo único que significaba aquello es que los curanderos habían incurrido en error u omitido algún rito; lo que tenían que hacer era volver a intentarlo, sacrificar más reses y rezar con más fruición.

En cambio, si la hipótesis de Tales y de sus discípulos era correcta -si el universo funcionaba de acuerdo con leyes naturales que no variaban-, entonces sí que merecía la pena estudiar el universo, observar cómo se mueven las estrellas y cómo se desplazan las nubes, cómo cae la lluvia y cómo crecen las plantas, y además en la seguridad de que estas observaciones serían válidas siempre y de que no se verían alteradas inopinadamente por la voluntad de ningún dios. Y entonces sería posible establecer una serie de leyes elementales que describiesen la naturaleza general de las observaciones. La primera hipótesis de Tales condujo así a una segunda: la razón humana es capaz de esclarecer la naturaleza de las leyes que gobiernan el universo.

La idea de ciencia

Estos dos supuestos -el de que existen leyes de la naturaleza y el de que el hombre puede esclarecerlas mediante la razón- constituyen la «idea de ciencia». Pero ¡ojo!, son sólo eso, supuestos, y no pueden demostrarse; lo cual no es óbice para que desde Tales siempre haya habido hombres que han creído obstinadamente en ellos.

La idea de ciencia estuvo a punto de desvanecerse en Europa tras la caída del Imperio Romano; pero no llegó a morir. Luego, en el siglo XVI, adquirió enorme empuje. Y hoy día, en la segunda mitad del siglo XX, se halla en pleno apogeo.

El universo, todo hay que decirlo, es mucho más complejo de lo que Tales se imaginaba. Pero, aun así, hay leyes de la naturaleza que pueden expresarse con gran simplicidad y que son, según los conocimientos actuales, inmutables. La más importante de ellas quizá sea el «principio de conservación de la energía», que, expresado con pocas palabras, afirma lo siguiente: «La energía total del universo es constante».

Una cierta incertidumbre

La ciencia ha comprobado que el conocimiento tiene también sus límites. El físico alemán Werner Heisenberg elaboró en la década de los veinte un principio que se conoce por «principio de incertidumbre» y que afirma que es imposible determinar con exactitud la posición y la velocidad de un objeto en un instante dado. Se puede hallar una u otra con la precisión que se quiera, pero no ambas al mismo tiempo. ¿Hay que entender que el segundo supuesto de la ciencia es falso, que el hombre no puede adquirir conocimiento con el cual descifrar el enigma del universo?

En absoluto, porque el principio de incertidumbre es, de suyo, una ley natural. La exactitud con la que podemos medir el universo tiene sus límites, nadie lo niega; pero la razón puede discernir esos límites, y la cabal comprensión de la incertidumbre permite conocer muchas cosas que, de otro modo, serían inexplicables. Así pues, la gran idea de Tales, la «idea de ciencia», es igual de válida hoy que hace unos 2.500 años, cuando la propuso el griego de Mileto.

2. Pitágoras y el número

No mucho después de la época en que Tales cavilaba sobre los misterios del universo, hace unos 2.500 años, había otro sabio griego que jugaba con cuerdas. Pitágoras, al igual que Tales, vivía en una ciudad costera, Crotona, en el sur de Italia; y lo mismo que él, no era precisamente un hombre del montón.

Las cuerdas con las que jugaba Pitágoras no eran cuerdas comunes y corrientes, sino recias, como las que se utilizaban en los instrumentos musicales del tipo de la lira. Pitágoras se había procurado cuerdas de diferentes longitudes, las había tensado y las pulsaba ahora una a una para producir distintas notas musicales.

Números musicales

Finalmente halló dos cuerdas que daban notas separadas por una octava; es decir, si una daba el do bajo, la otra daba el do agudo. Lo que cautivó a Pitágoras es que la cuerda que daba el do bajo era exactamente dos veces más larga que la del do agudo. La razón de longitudes de las dos cuerdas era de 2 a 1.

Volvió a experimentar y obtuvo otras dos cuerdas cuyas notas diferían en una «quinta»; una de las notas era un do, por ejemplo, y la otra un sol. La cuerda que producía la nota más baja era ahora exactamente vez y media más larga que la otra. La razón de las longitudes era de 3 a 2.

Como es lógico, los músicos griegos y de otros países sabían también fabricar cuerdas que diesen ciertas notas y las utilizaban en instrumentos musicales. Pero Pitágoras fue, que se sepa, el primer hombre en estudiar, no la música, sino el juego de longitudes que producía la música.

¿Por qué eran precisamente estas proporciones de números sencillos -2 a 1, 3 a 2, 4 a 3- las que originaban sonidos especialmente agradables? Cuando se elegían cuerdas cuyas longitudes guardaban proporciones menos simples -23 a 13, por ejemplo- la combinación de sonidos no era grata al oído.

Puede ser, quién sabe, que a Pitágoras se le ocurriera aquí una idea luminosa: que los números no eran simples herramientas para contar y medir, sino que gobernaban la música y hasta el universo entero.

Si los números eran tan importantes, valía la pena estudiarlos en sí mismos. Había que empezar a pensar, por ejemplo, en el número 2 a secas, no en dos hombres o dos manzanas. El número 2 era divisible por 2; era un número par. El número 3 no se podía dividir exactamente por 2; era un número impar. ¿Qué propiedades compartían todos los números pares? ¿Y los impares? Cabía empezar por el hecho de que la suma de dos números pares o de dos impares es siempre un número par, y la de un par y un impar es siempre impar.

O imaginemos que dibujásemos cada número como una colección de puntos. El 6 vendría representado por seis puntos; el 23, por veintitrés, etc. Espaciando regularmente los puntos se comprueba que ciertos números, conocidos por números triangulares, se pueden representar mediante triángulos equiláteros. Otros, llamados cuadrados, se pueden disponer en formaciones cuadradas.

Números triangulares

Pitágoras sabía que no todos los números de puntos se podían disponer en triángulo. De los que sí admitían esta formación, el más pequeño era el conjunto de un solo punto, equivalente al número triangular 1.

Para construir triángulos más grandes bastaba con ir añadiendo filas adicionales que corrieran paralelas a uno de los lados del triángulo. Colocando dos puntos más a un lado del triángulo de 1 punto se obtenía el triángulo de tres puntos, que representa el número 3. Y el triángulo de seis, que representa el número 6, se obtiene al añadir tres puntos más al triángulo de tres.

Los siguientes triángulos de la serie estaban constituidos por diez puntos (el triángulo de seis, más cuatro puntos), quince puntos (diez más cinco), veintiuno (quince más seis), etc. La serie de números triangulares era, por tanto, 1, 3, 6, 10, 15, 21,…

Al formar la serie de triángulos a base de añadir puntos, Pitágoras se percató de un hecho interesante, y es que para pasar de un triángulo al siguiente había que añadir siempre un punto más que la vez anterior (la letra cursiva así lo indica en los dos párrafos anteriores).

Dicho con otras palabras, era posible construir los triángulos, o los números triangulares, mediante una sucesión de sumas de números consecutivos: 1=1; 3=1 +2; 6=1 + 2 + 3; 10 =1 + 2 + 3 +4; 15 = 1+2 + 3+4 + 5; 21 = 1+2 + 3+4 + 5 + 6; etcétera.

Números cuadrados

Si el triángulo tiene tres lados, el cuadrado tiene cuatro (y cuatro ángulos rectos, de 90 grados), por lo cual era de esperar que la sucesión de los números cuadrados fuese muy distinta de la de los triangulares. Ahora bien, un solo punto aislado encajaba igual de bien en un cuadrado que en un triángulo, de manera que la sucesión de cuadrados empezaba también por el número 1.

Los siguientes cuadrados se podían formar colocando orlas de puntos adicionales a lo largo de dos lados adyacentes del cuadrado anterior. Añadiendo tres puntos al cuadrado de uno se formaba un cuadrado de cuatro puntos, que representaba el número 4. Y el de nueve se obtenía de forma análoga, orlando con cinco puntos más el cuadrado de cuatro.

La secuencia proseguía con cuadrados de dieciséis puntos (el cuadrado de nueve, más siete puntos), veinticinco puntos (dieciséis más nueve), treinta y seis (veinticinco más once), etc. El resultado era la sucesión de números cuadrados: 1, 4, 9, 16, 25, 36,…

Como los triángulos crecían de manera regular, no le cogió de sorpresa a Pitágoras el que los cuadrados hicieran lo propio. El número de puntos añadidos a cada nuevo cuadrado era siempre un número impar, y siempre era dos puntos mayor que el número añadido la vez anterior. (Las cursivas vuelven a indicarlo.)

Dicho de otro modo, los números cuadrados podían formarse mediante una sucesión de sumas de números impares consecutivos: 1 = 1; 4 = 1 + 3; 9=1 + 3 + 5; 16 = 1 + 3 + 5 + 7; 25 = 1 + 3 + 5 + 7 + 9; etcétera.

Los cuadrados también se podían construir a base de sumar dos números triangulares consecutivos: 4=1+3; 9 = 3 + 6; 16 = 6+10; 25=10+15;… O multiplicando un número por sí mismo: 1 = 1x1; 4 = 2x2; 9 = 3x3;…

Este último método es una manera especialmente importante de formar cuadrados. Puesto que 9 = 3x3, decimos que 9 es el cuadrado de 3; y lo mismo para 16, el cuadrado de 4, o para 25, el cuadrado de 5, etc. Por otro lado, decimos que el número más pequeño -el que multiplicamos por sí mismo- es la raíz cuadrada de su producto: 3 es la raíz cuadrada de 9, 4 la de 16, etcétera.

Triángulos rectángulos

El interés de Pitágoras por los números cuadrados le llevó a estudiar los triángulos rectángulos, es decir, los triángulos que tienen un ángulo recto. Un ángulo recto está formado por dos lados perpendiculares, lo que quiere decir que si colocamos uno de ellos en posición perfectamente horizontal, el otro quedará perfectamente vertical. El triángulo rectángulo queda formado al añadir un tercer lado que va desde el extremo de uno de los lados del ángulo recto hasta el extremo del otro. Este tercer lado, llamado «hipotenusa», es siempre más largo que cualquiera de los otros dos, que se llaman «catetos».

Imaginemos que Pitágoras trazase un triángulo rectángulo al azar y midiese la longitud de los lados. Dividiendo uno de ellos en un número entero de unidades, lo normal es que los otros dos no contuvieran un número entero de las mismas unidades.

Pero había excepciones. Volvamos a imaginarnos a Pitágoras ante un triángulo cuyos catetos midiesen exactamente tres y cuatro unidades, respectivamente. La hipotenusa tendría entonces exactamente cinco unidades.

Los números 3, 4 y 5 ¿por qué formaban un triángulo rectángulo? Los números 1, 2 y 3 no lo formaban, ni tampoco los números 2, 3 y 4; de hecho, casi ningún trío de números elegidos al azar.

Supongamos ahora que Pitágoras se fijara en los cuadrados de los números: en lugar de 3, 4 y 5 tendría ahora 9, 16 y 25. Pues bien, lo interesante es que 9+16=25. La suma de los cuadrados de los catetos de este triángulo rectángulo resultaba ser igual al cuadrado de la hipotenusa.

Pitágoras fue más lejos y observó que la diferencia entre dos números cuadrados sucesivos era siempre un número impar: 4-1 = 3; 9-4 = 5; 16-9 = 7; 25 – 16 = 9; etc. Cada cierto tiempo, esta diferencia impar era a su vez un cuadrado, como en 25- 16 = 9 (que es lo mismo que 9 + 16 = 25). Cuando ocurría esto, volvía a ser posible construir un triángulo rectángulo con números enteros.

Puede ser, por ejemplo, que Pitágoras restase 144 de 169, que son dos cuadrados sucesivos: 169 – 144 = 25. Las raíces cuadradas de estos números resultan ser 13, 12 y 5, porque 169 = 13 X 13; 144 = 12 X 12 y 25 = 5 X 5. Por consiguiente, se podía formar un triángulo rectángulo con catetos de cinco y doce unidades, respectivamente, e hipotenusa de trece unidades.

El teorema de Pitágoras

Pitágoras tenía ahora gran número de triángulos rectángulos en los que el cuadrado de la hipotenusa era igual a la suma de los cuadrados de los catetos. No tardó en demostrar que esta propiedad era cierta para todos los triángulos rectángulos.

Los egipcios, los babilonios y los chinos sabían ya, cientos de años antes que Pitágoras, que esa relación se cumplía para el triángulo de 3, 4 y 5. Y es incluso probable que los babilonios supiesen a ciencia cierta que era válida para todos los triángulos rectángulos. Pero, que sepamos, fue Pitágoras el primero que lo demostró.

El enunciado que dio es: En cualquier triángulo rectángulo la suma de los cuadrados de los catetos es igual al cuadrado de la hipotenusa. Como fue él quien primero lo demostró, se conoce con el nombre de «teorema de Pitágoras». Veamos cómo lo hizo.

Prueba de deducción

Para ello tenemos que volver a Tales de Mileto, el pensador griego de que hablamos en el Capítulo 1. Dice la tradición que Pitágoras fue discípulo suyo.

Tales había elaborado un pulcro sistema para demostrar razonadamente la verdad de enunciados o teoremas matemáticos. El punto de arranque eran los «axiomas» o enunciados cuya verdad no se ponía en duda. A partir de los axiomas se llegaba a una determinada conclusión; aceptada ésta, se podía obtener una segunda, y así sucesivamente. Pitágoras utilizó el sistema de Tales -llamado «deducción»-, para demostrar el teorema que lleva su nombre. Y es un método que se ha aplicado desde entonces hasta nuestros días.

Puede que no fuese realmente Tales quien inventara el sistema de demostración por deducción; es posible que lo aprendiera de los babilonios y que el nombre del verdadero inventor permanezca en la penumbra. Pero aunque Tales fuese el inventor de la deducción matemática, fue Pitágoras quien le dio fama.

El nacimiento de la geometría

Las enseñanzas de Pitágoras, y sobre todo su gran éxito al hallar una prueba deductiva del famoso teorema, fueron fuente de inspiración para los griegos, que prosiguieron trabajando en esta línea. En los 300 años siguientes erigieron una compleja estructura de pruebas matemáticas que se refieren principalmente a líneas y formas. Este sistema se llama «geometría» (véase el Capítulo 3).

En los miles de años que han transcurrido desde los griegos ha progresado mucho la ciencia. Pero, por mucho que el hombre moderno haya logrado en el terreno de las matemáticas y penetrado en sus misterios, todo reposa sobre dos pilares: primero, el estudio de las propiedades de los números, y segundo, el uso del método de deducción. Lo primero nació con Pitágoras y lo segundo lo divulgó él.

Lo que Pitágoras había arrancado de sus cuerdas no fueron sólo notas musicales: era también el vasto mundo de las matemáticas.

3. Arquímedes y la matemática aplicada

Cualquiera diría que un aristócrata de una de las ciudades más grandes y opulentas de la Grecia antigua tenía cosas mejores que hacer que estudiar el funcionamiento de las palancas. Nuestro aristócrata, a lo que se ve, pensaba lo mismo, porque se avergonzaba de cultivar aficiones tan «plebeyas».

Nos referimos a Arquímedes, natural de Siracusa, ciudad situada en la costa oriental de Sicilia. Arquímedes nació hacia el año 287 a. C, era hijo de un distinguido astrónomo y probablemente pariente de Herón II, rey de Siracusa.

Un inventor de artilugios

El sentir general en los tiempos de Arquímedes era que las personas de bien no debían ocuparse de artilugios mecánicos, que asuntos como esos sólo convenían a esclavos y trabajadores manuales. Pero Arquímedes no lo podía remediar. La maquinaria le interesaba, y a lo largo de su vida inventó multitud de artilugios de uso bélico y pacífico.

Tampoco es cierto que cediera del todo a intereses tan «bajos», porque nunca se atrevió a dejar testimonio escrito de sus artilugios mecánicos; le daba vergüenza. Sólo tenemos noticia de ellos a través del relato inexacto y quizá exagerado, de terceros. La única salvedad es la descripción que hizo el propio Arquímedes de un dispositivo que imitaba los movimientos celestes del Sol, la Luna y los planetas; pero no es menos cierto que era un instrumento destinado a la ciencia de la astronomía y no a burdas faenas mecánicas.

¿Ingeniería o matemáticas?

Las máquinas no eran la única afición de Arquímedes. En sus años jóvenes había estado en Alejandría (Egipto), la sede del gran Museo. El Museo era algo así como una gran universidad adonde acudían todos los eruditos griegos para estudiar y enseñar. Arquímedes había sido allí discípulo del gran matemático Conón de Samos, a quien superó luego en este campo, pues inventó una forma de cálculo dos mil años antes de que los matemáticos modernos elaboraran luego los detalles.

A Arquímedes, como decimos, le interesaban las matemáticas y también la ingeniería; y en aquel tiempo tenían muy poco en común estos dos campos.

Es muy cierto que los ingenieros griegos y los de épocas anteriores, como los babilonios y egipcios, tuvieron por fuerza que utilizar las matemáticas para realizar sus proyectos. Los egipcios habían construido grandes pirámides que ya eran históricas en tiempos de Arquímedes; con instrumentos tosquísimos arrastraban bloques inmensos de granito a kilómetros y kilómetros de distancia, para luego izarlos a alturas nada desdeñables.

También los babilonios habían erigido estructuras imponentes, y los propios griegos no se quedaron atrás. El ingeniero griego Eupalino, por citar un caso, construyó un túnel en la isla de Samos tres siglos antes de Arquímedes. A ambos lados de una montaña puso a trabajar a dos equipos de zapadores, y cuando se reunieron a mitad de camino las paredes del túnel coincidían casi exactamente.

Para realizar estas obras y otras de parecido calibre, los ingenieros de Egipto, Babilonia y Grecia tuvieron que utilizar, repetimos, las matemáticas. Tenían que entender qué relación guardaban las líneas entre sí y cómo el tamaño de una parte de una estructura determinaba el tamaño de otra.

Arquímedes, sin embargo, no estaba familiarizado con estas matemáticas, sino con otra modalidad, abstracta, que los griegos habían comenzado a desarrollar en tiempos de Eupalino.

Pitágoras había divulgado el sistema de deducción matemática (véase el capítulo 2), en el cual se partía de un puñado de nociones elementales, aceptadas por todos, para llegar a conclusiones más complicadas a base de proceder, paso a paso, según los principios deductivos.

Un teorema magnífico

Otros matemáticos griegos siguieron los pasos de Pitágoras y construyeron poco a poco un hermoso sistema de teoremas (de enunciados matemáticos) relativos a ángulos, líneas paralelas, triángulos, cuadrados, círculos y otras figuras. Aprendieron a demostrar que dos figuras tenían igual área o ángulos iguales o ambas cosas a la vez, y descubrieron cómo determinar números, tamaños y áreas.

Sin negar que la maravillosa estructura de la matemática griega sobrepasaba con mucho el sistema matemático de anteriores civilizaciones, hay que decir también que era completamente teórico. Los círculos y triángulos eran imaginarios, construidos con líneas infinitamente delgadas y perfectamente rectas o que se curvaban con absoluta suavidad. La matemática no tenía uso práctico.

La siguiente historia lo ilustra muy bien. Un siglo antes de que naciera Arquímedes, el filósofo Platón fundó una academia en Atenas, donde enseñaba matemáticas. Un día, durante una demostración matemática, cierto estudiante le preguntó: «Pero maestro, ¿qué uso práctico tiene esto?». Platón, indignado, ordenó a un esclavo que le diera una moneda pequeña para hacerle así sentir que su estudio tenía uso práctico; y luego lo expulsó de la academia.

Una figura importante en la historia de las matemáticas griegas fue Euclides, y discípulo de él fue Conón de Samos, maestro de Arquímedes. Poco antes de nacer éste, Euclides compiló en Alejandría todas las deducciones obtenidas por pensadores anteriores y las organizó en un bello sistema, demostración por demostración, empezando por un puñado de «axiomas» o enunciados aceptados con carácter general. Los axiomas eran tan evidentes, según los griegos, que no requerían demostración. Ejemplos de axiomas son «la línea recta es la distancia más corta entre dos puntos» y «el todo es igual a la suma de sus partes».

Todo teoría, nada de práctica

El libro de Euclides era de factura tan primorosa, que desde entonces ha sido un texto básico. Sin embargo, en toda su magnífica estructura no había indicio de que ninguna de sus conclusiones tuviera que ver con las labores cotidianas de los mortales. La aplicación más intensa que los griegos dieron a las matemáticas fue el cálculo de los movimientos de los planetas y la teoría de la armonía. Al fin y al cabo, la astronomía y la música eran ocupaciones aptas para aristócratas.

Arquímedes sobresalía, pues, en dos mundos: uno práctico, el de la ingeniería, sin las brillantes matemáticas de los griegos, y otro, el de las matemáticas griegas, que carecían de uso práctico. Sus aptitudes ofrecían excelente oportunidad para combinar ambos mundos. Pero ¿cómo hacerlo?

Un dispositivo maravilloso

Existe una herramienta que se llama «pie de cabra», un dispositivo mecánico elemental ¡pero maravilloso! Sin su ayuda hacen falta muchos brazos para levantar un bloque de piedra grande. Pero basta colocar el pie de cabra debajo del bloque y apoyarlo en un saliente (una roca más pequeña, por ejemplo) para que pueda moverlo fácilmente una sola persona.

Los pies de cabra, espeques y dispositivos parecidos son tipos de palancas. Cualquier objeto relativamente largo y rígido, un palo, un listón o una barra, sirve de palanca. Es un dispositivo tan sencillo que lo debió de usar ya el hombre prehistórico. Pero ni él ni los sapientísimos filósofos griegos sabían cómo funcionaba. El gran Aristóteles, que fue discípulo de Platón, observó que los dos extremos de la palanca, al empujar hacia arriba y abajo respectivamente, describían una circunferencia en el aire. Aristóteles concluyó que la palanca poseía propiedades maravillosas, pues la forma del círculo era tenida por perfecta.

Arquímedes había experimentado con palancas y sabía que la explicación de Aristóteles era incorrecta. En uno de los experimentos había equilibrado una larga palanca apoyada sobre un fulcro. Si colocaba peso en un solo brazo de la barra, ese extremo bajaba. Poniendo peso a ambos lados del punto de apoyo se podía volver a equilibrar. Cuando los pesos eran iguales, ocupaban en el equilibrio posiciones distintas de las ocupadas cuando eran desiguales.

El lenguaje de las matemáticas

Arquímedes comprobó que las palancas se comportaban con gran regularidad. ¿Por qué no utilizar las matemáticas para explicar ese comportamiento regular? De acuerdo con los principios de la deducción matemática tendría que empezar por un axioma, es decir, por algún enunciado incuestionable.

El axioma que utilizó descansaba en el principal resultado de sus experimentos con palancas. Decía así: Pesos iguales a distancias iguales del punto de apoyo equilibran la palanca. Pesos iguales a distancias desiguales del punto de apoyo hacen que el lado que soporta el peso más distante descienda.

Arquímedes aplicó luego el método de deducción matemática para obtener conclusiones basadas en este axioma y descubrió que los factores más importantes en el funcionamiento de cualquier palanca son la magnitud de los pesos o fuerzas que actúan sobre ella y sus distancias al punto de apoyo.

Supongamos que una palanca está equilibrada por pesos desiguales a ambos lados del punto de apoyo. Según los hallazgos de Arquímedes, estos pesos desiguales han de hallarse a distancias diferentes del fulcro. La distancia del peso menor ha de ser más grande para compensar su menor fuerza. Así, un peso de diez kilos a veinte centímetros del apoyo equilibra cien kilos colocados a dos centímetros. La pesa de diez kilos es diez veces más ligera, por lo cual su distancia es diez veces mayor.

Eso explica por qué un solo hombre puede levantar un bloque inmenso de piedra con una palanca. Al colocar el punto de apoyo muy cerca de la mole consigue que su exigua fuerza, aplicada lejos de aquél, equilibre el enorme peso del bloque, que actúa muy cerca del fulcro.

Arquímedes se dio cuenta de que aplicando la fuerza de un hombre a gran distancia del punto de apoyo podían levantarse pesos descomunales, y a él se le atribuye la frase: «Dadme un punto de apoyo y moveré el mundo».

Pero no hacía falta que le dieran nada, porque su trabajo sobre la palanca ya había conmovido el mundo. Arquímedes fue el primero en aplicar la matemática griega a la ingeniería. De un solo golpe había inaugurado la matemática aplicada y fundado la ciencia de la mecánica, encendiendo así la mecha de una revolución científica que explotaría dieciocho siglos más tarde.

4. Galileo y la experimentación

Entre los asistentes a la misa celebrada en la catedral de Pisa, aquel domingo de 1581, se hallaba un joven de diecisiete años. Era devotamente religioso y no hay por qué dudar que intentaba concentrarse en sus oraciones; pero le distraía un candelero que pendía del techo cerca de él. Había corriente y el candelero oscilaba de acá para allá.

En su movimiento de vaivén, unas veces corto y otras de vuelo más amplio, el joven observó algo curioso: el candelero parecía batir tiempos ¡guales, fuese el vuelo corto o largo. ¡Qué raro! ¡Cualquiera diría que tenía que tardar más en recorrer el arco más grande!

A estas alturas el joven, cuyo nombre era Galileo, tenía que haberse olvidado por completo de la misa. Sus ojos estaban clavados en el candelero oscilante y los dedos de su mano derecha palpaban la muñeca contraria. Mientras la música de órgano flotaba alrededor de él, contó el número de pulsos: tantos para esta oscilación, tantos otros para la siguiente, etc. El número de pulsos era siempre el mismo, independientemente de que la oscilación fuese amplia o corta. O lo que es lo mismo, el candelero tardaba exactamente igual en recorrer un arco pequeño que uno grande.

Galileo no veía el momento de que acabara la misa. Cuando por fin terminó, corrió a casa y ató diferentes pesas en el extremo de varias cuerdas. Cronometrando las oscilaciones comprobó que un peso suspendido de una cuerda larga tardaba más tiempo en ir y venir que un peso colgado de una cuerda corta. Sin embargo, al estudiar cada peso por separado, comprobó que siempre tardaba lo mismo en una oscilación, fuese ésta amplia o breve. ¡Galileo había descubierto el principio del péndulo!

Pero había conseguido algo más: hincar el diente a un problema que había traído de cabeza a los sabios durante dos mil años: el problema de los objetos en movimiento.

Viejas teorías

Los antiguos habían observado que las cosas vivas podían moverse ellas mismas y mover también objetos inertes, mientras que las cosas inertes eran, por lo general, incapaces de moverse a menos que un ser animado las impulsara. Había, sin embargo, excepciones que no pasaron inadvertidas: el mar, el viento, el Sol y la Luna se movían sin ayuda de las cosas vivientes, y otro movimiento que no dependía del mundo de lo vivo era el de los cuerpos en caída libre.

El filósofo griego Aristóteles pensaba que el movimiento de caída era propio de todas las cosas pesadas y creía que cuanto más pesado era el objeto, más deprisa caía: un guijarro caería más aprisa que una hoja, y la piedra grande descendería más rápidamente que la pequeña.

Un siglo después Arquímedes aplicó las matemáticas a situaciones físicas, pero de carácter puramente estático, sin movimiento (véase el capítulo 3). Un ejemplo es el de la palanca en equilibrio. El problema del movimiento rápido desbordaba incluso un talento como el suyo. En los dieciocho siglos siguientes nadie desafió las ideas de Aristóteles sobre el movimiento, y la física quedó empantanada.

Cómo retardar la caída

Hacia 1589 había terminado Galileo su formación universitaria y era ya famoso por su labor en el campo de la mecánica. Al igual que Arquímedes, había aplicado las matemáticas a situaciones estáticas, inmóviles; pero su espíritu anhelaba volver sobre el problema del movimiento.

Toda su preocupación era hallar la manera de retardar la caída de los cuerpos para así poder experimentar con ellos y estudiar detenidamente su movimiento. (Lo que hace el científico en un experimento es establecer condiciones especiales que le ayuden a estudiar y observar los fenómenos con mayor sencillez que en la naturaleza.)

Galileo se acordó entonces del péndulo. Al desplazar un peso suspendido de una cuerda y soltarlo, comienza a caer. La cuerda a la que está atado le impide, sin embargo, descender en línea recta, obligándole a hacerlo oblicuamente y con suficiente lentitud como para poder cronometrarlo.

Como decimos, el péndulo, a diferencia de un cuerpo en caída libre, no cae en línea recta, lo cual introducía ciertas complicaciones. La cuestión era cómo montar un experimento en el que la caída fuese oblicua y en línea recta.

¡Estaba claro! Bastaba con colocar un tablero de madera inclinado, que llevara en el centro un surco largo, recto y bien pulido. Una bola que ruede por el surco se mueve en línea recta. Y si se coloca la tabla en posición casi horizontal, las bolas rodarán muy despacio, permitiendo así estudiar su movimiento.

Galileo dejó rodar por el surco bolas de diferentes pesos y cronometró su descenso por el número de gotas de agua que caían a través de un agujero practicado en el fondo de un recipiente. Comprobó que, exceptuando objetos muy ligeros, el peso no influía para nada: todas las bolas cubrían la longitud del surco en el mismo tiempo.

Aristóteles, superado

Según Galileo, todos los objetos, al caer, se veían obligados a apartar el aire de su camino. Los objetos muy ligeros sólo podían hacerlo con dificultad y eran retardados por la resistencia del aire. Los más pesados apartaban el aire fácilmente y no sufrían ningún retardo. En el vacío, donde la resistencia del aire es nula, la pluma y el copo de nieve tenían que caer tan aprisa como las bolas de plomo.

Aristóteles había afirmado que la velocidad de caída de los objetos dependía de su peso. Galileo demostró que eso sólo era cierto en casos excepcionales, concretamente para objetos muy ligeros, y que la causa estribaba en la resistencia del aire. Galileo tenía razón; Aristóteles estaba equivocado.

Galileo subdividió luego la ranura en tramos iguales mediante marcas laterales y comprobó que cualquier bola, al rodar hacia abajo, tardaba en recorrer cada tramo menos tiempo que el anterior. Estaba claro que los objetos aceleraban al caer, es decir se movían cada vez más deprisa por unidad de tiempo.

Galileo logró establecer relaciones matemáticas sencillas para calcular la aceleración de la caída de un cuerpo. Aplicó, pues, las matemáticas a los cuerpos en movimiento, igual que Arquímedes las aplicara antes a los cuerpos en reposo.

Con esta aplicación, y con los conocimientos que había adquirido en los experimentos con bolas rodantes, llegó a resultados asombrosos. Calculó exactamente, por ejemplo, el movimiento de una bala después de salir del cañón.

Galileo no fue el primero en experimentar, pero sus espectaculares resultados en el problema de la caída de los cuerpos ayudaron a difundir la experimentación en el mundo de la ciencia. Los científicos no se contentaban ya con razonar a partir de axiomas, sino que empezaron a diseñar experimentos y hacer medidas. Y podían utilizar los experimentos para comprobar sus inferencias y para construir nuevos razonamientos. Por eso fechamos en 1589 los inicios de la ciencia experimental.

Ahora bien, para que la ciencia experimental cuajara hacían falta mediciones exactas del cambio en general, y concretamente del paso del tiempo.

La humanidad sabía, desde tiempos muy antiguos, cómo medir unidades grandes de tiempo a través de los cambios astronómicos. La marcha sostenida de las estaciones marcaba el año, el cambio constante de las fases de la Luna determinaba el mes y la rotación continua de la Tierra señalaba el día.

Para unidades de tiempo menores que el día había que recurrir a métodos menos exactos. El reloj mecánico había entrado en uso en la Edad Media. Las manillas daban vueltas a la esfera movidas por ruedas dentadas, que a su vez eran gobernadas por pesas suspendidas. A medida que éstas caían, hacían girar las ruedas.

Sin embargo, era difícil regular la caída de las pesas y hacer que las ruedas giraran suave y uniformemente. Estos relojes siempre adelantaban o atrasaban, y ninguno tenía una precisión superior a una hora.

La revolución en la medida del tiempo

Lo que hacía falta era un movimiento muy constante que regulara las ruedas dentadas. En 1656 (catorce años después de morir Galileo), Christian Huygens, un científico holandés, se acordó del péndulo.

El péndulo bate a intervalos regulares. Acoplándolo a un reloj para que gobierne los engranajes se consigue que éstos adquieran un movimiento tan uniforme como el de la oscilación del péndulo.

Huygens inventó así el reloj de péndulo, basado en un principio descubierto por el joven Galileo. El reloj de Huygens fue el primer cronómetro de precisión que tuvo la humanidad y una bendición para la ciencia experimental.

5. Demócrito y los átomos

Le llamaban el «filósofo risueño» por su eterna y amarga sonrisa ante la necedad humana.

Su nombre era Demócrito y nació hacia el año 470 a. C. en la ciudad griega de Abdera. Sus conciudadanos puede que tomaran esa actitud suya por síntoma de locura, porque dice la leyenda que le tenían por lunático y que llegaron a recabar la ayuda de doctores para que le curaran.

Demócrito parecía albergar, desde luego, ideas muy peregrinas. Le preocupaba, por ejemplo, hasta dónde se podía dividir una gota de agua. Uno podía ir obteniendo gotas cada vez más pequeñas hasta casi perderlas de vista. Pero ¿había algún límite? ¿Se llegaba alguna vez hasta un punto en que fuese imposible seguir dividiendo?

¿El final de la escisión?

Leucipo, maestro de Demócrito, había intuido que esa escisión tenía un límite. Demócrito hizo suya esta idea y anunció finalmente su convicción de que cualquier sustancia podía dividirse hasta allí y no más. El trozo más pequeño o partícula de cualquier clase de sustancia era indivisible, y a esa partícula mínima la llamó átomos, que en griego quiere decir «indivisible». Según Demócrito, el universo estaba constituido por esas partículas diminutas e indivisibles. En el universo no había otra cosa que partículas y espacio vacío entre ellas.

Según él, había distintos tipos de partículas que, al combinarse en diferentes ordenaciones, formaban las diversas sustancias. Si la sustancia hierro se aherrumbraba -es decir, se convertía en la sustancia herrumbre- era porque las distintas clases de partículas que había en el hierro se reordenaban. Si el mineral se convertía en cobre, otro tanto de lo mismo; e igual para la madera al arder y convertirse en ceniza.

La mayoría de los filósofos griegos se rieron de Demócrito. ¿Cómo iba a existir algo que fuera indivisible? Cualquier partícula, o bien ocupaba espacio, o no lo ocupaba. En el primer caso tenía que dejarse escindir, y cada una de las nuevas partículas ocuparía menos espacio que la original. Y en el segundo caso, si era indivisible, no podía ocupar espacio, por lo cual no era nada; y las sustancias ¿cómo podían estar hechas de la nada?

En cualquier caso, dictaminaron los filósofos, la idea del átomos era absurda. No es extraño que las gentes miraran a Demócrito de reojo y pensaran que estaba loco. Ni siquiera juzgaron conveniente confeccionar, muchos ejemplares de sus escritos. Demócrito escribió más de setenta obras; ninguna se conserva.

Hubo algunos filósofos, para ser exactos, en quienes sí prendió la idea de las partículas indivisibles. Uno de ellos fue Epicuro, otro filósofo, que fundó una escuela en Atenas, en el año 306 a. C, casi un siglo después de morir Demócrito. Epicuro era un maestro de gran renombre y tenía numerosos discípulos. Su estilo filosófico, el epicureismo, retuvo su importancia durante siglos. Parte de esta filosofía eran las teorías de Demócrito sobre las partículas.

Aun así, Epicuro no logró convencer a sus coetáneos, y sus seguidores permanecieron en minoría. Lo mismo que en el caso de Demócrito, ninguna de las muchas obras de Epicuro ha logrado sobrevivir hasta nuestros días.

Hacia el año 60 a. C. ocurrió algo afortunado, y es que el poeta romano Lucrecio, interesado por la filosofía epicúrea, escribió un largo poema, de título Sobre la naturaleza de las cosas, en el que describía el universo como si estuviera compuesto de las partículas indivisibles de Demócrito. La obra gozó de gran popularidad, y se confeccionaron ejemplares bastantes para que sobreviviera a los tiempos antiguos y medievales. Fue a través de este libro como el mundo tuvo noticia puntual de las teorías de Demócrito.

En los tiempos antiguos, los libros se copiaban a mano y eran caros. Incluso de las grandes obras se podían confeccionar solamente unos cuantos ejemplares, asequibles tan sólo a las economías más saneadas. La invención de la imprenta hacia el año 1450 d. C. supuso un gran cambio, porque permitía tirar miles de ejemplares a precios más moderados. Uno de los primeros libros que se imprimieron fue Sobre la naturaleza de las cosas, de Lucrecio.

De Gassendi a Boyle

Así fue como hasta los sabios más menesterosos de los tiempos modernos tuvieron acceso a las teorías de Demócrito. En algunos, como Pierre Gassendi, filósofo francés del siglo XVII, dejaron huella indeleble. Gassendi se convirtió en epicúreo convencido y defendió a capa y espada la teoría de las partículas indivisibles.

Uno de los discípulos de Gassendi era el inglés Robert Boyle, quien en 1660 estudió el aire y se preguntó por qué se podía comprimir, haciendo que ocupara menos y menos espacio.

Boyle supuso que el aire estaba compuesto de partículas minúsculas que dejaban grandes vanos entre ellas. Comprimir el aire equivaldría a juntar más las partículas, dejando menos espacio vacío. La idea tenía sentido.

Por otro lado, el agua podría consistir en partículas muy juntas, tan juntas que estaban en contacto. Por eso, razonó Boyle, el agua no se puede comprimir más, mientras que, al separar las partículas, el agua se convertía en vapor, sustancia tenue parecida al aire.

Boyle se convirtió así en nuevo seguidor de Demócrito. Como vemos, durante dos mil años hubo una cadena ininterrumpida de partidarios de la teoría de las partículas indivisibles: Demócrito, Epicuro, Lucrecio, Gassendi y Boyle. La mayoría, sin embargo, jamás aceptó sus ideas. «¿Qué? ¿Una partícula que no puede dividirse en otras menores? ¡Absurdo!»

Vigilantes del peso

Pero llegó el siglo XVIII y los químicos empezaron a reconsiderar la manera en que se formaban los compuestos químicos. Sabían que eran producto de la combinación de otras sustancias: el cobre, el oxígeno y el carbono, pongamos por caso, se unían para formar el compuesto llamado carbonato cúprico. Pero por primera vez en la historia se hizo el intento de medir los pesos relativos de las sustancias componentes.

Joseph Louis Proust, químico francés, realizó mediciones muy cuidadosas hacia finales de siglo. Comprobó, por ejemplo, que siempre que el cobre, el oxígeno y el carbono formaban carbonato de cobre, se combinaban en las mismas proporciones de peso: cinco unidades de cobre por cuatro de oxígeno por una de carbono. Dicho de otro modo, si Proust usaba cinco onzas de cobre para formar el compuesto, tenía que usar cuatro de oxígeno y una de carbono.

Y aquello no era como hacer un bizcocho, donde uno puede echar una pizca más de harina o quitar un poco de leche. La «receta» del carbonato de cobre era inmutable; hiciese uno lo que hiciese la proporción era siempre 5:4:1, y punto.

Proust ensayó con otras sustancias y constató el mismo hecho: la receta inflexible. En 1779 anunció sus resultados, de los cuales proviene lo que hoy conocemos por «ley de Proust» o «ley de las proporciones fijas».

¡Qué extraño!, pensó el químico inglés John Dalton cuando supo de los resultados de Proust. «¿Por qué ha de ser así?»

Dalton pensó en la posibilidad de las partículas indivisibles. ¿No sería que la partícula de oxígeno pesa siempre cuatro veces más que la de carbono, y la de cobre cinco veces más que ésta? Al formar carbonato de cobre por combinación de una partícula de cobre, otra de oxígeno y otra de carbono, la proporción de pesos sería entonces 5:4:1.

Para alterar ligeramente la proporción del carbonato de cobre habría que quitar un trozo a una de las tres partículas; pero Proust y otros químicos habían demostrado que las proporciones de un compuesto no podían alterarse, lo cual quería decir que era imposible romper las partículas. Dalton concluyó que eran indivisibles, como pensaba Demócrito.

Dalton, buscando nuevas pruebas, halló compuestos diferentes que, sin embargo, estaban constituidos por las mismas sustancias; lo que difería era la proporción en que entraba cada una de ellas. El anhídrido carbónico, pongamos por caso, estaba compuesto por carbono y oxígeno en la proporción, por pesos, de 3 unidades del primero por 8 del segundo. El monóxido de carbono también constaba de carbono y oxígeno, pero en la proporción de 3 a 4.

He aquí algo interesante. El número de unidades de peso de carbono era el mismo en ambas proporciones: tres unidades en el monóxido y tres unidades en el anhídrido. Podría ser, por tanto, que en cada uno de los dos compuestos hubiese una partícula de carbono que pesara tres unidades.

Al mismo tiempo, las ocho unidades de oxígeno en la proporción del anhídrido carbónico doblaban exactamente las cuatro unidades en la proporción del monóxido. Dalton pensó: si la partícula de oxígeno pesara cuatro unidades, entonces el monóxido de carbono estaría compuesto, en parte, por una partícula de oxígeno y el anhídrido por dos.

Puede que Dalton se acordara entonces del carbonato de cobre. La proporción de pesos del carbono y el oxígeno eran allí de 1 a 4 (que es lo mismo que 3 a 12). La proporción podía explicarse si uno suponía que el carbonato de cobre estaba compuesto de una partícula de carbono y tres de oxígeno. Siempre se podía arbitrar un sistema que hiciese aparecer números enteros de partículas, nunca fracciones.

Dalton anunció su teoría de las partículas indivisibles hacía el año 1803, pero ahora en forma algo diferente. Ya no era cuestión de creérsela o no. A sus espaldas tenía todo un siglo de experimentación química.

Átomos por experimento

El cambio que introdujo Galileo en la ciencia demostró su valor (véase el capítulo 4). Los argumentos teóricos por sí solos nunca habían convencido a la humanidad de la existencia real de partículas indivisibles; los argumentos, más los resultados experimentales, surtieron casi de inmediato el efecto apetecido.

Dalton reconoció que su teoría tenía sus orígenes en el filósofo risueño, y para demostrarlo utilizó humildemente la palabra átomos de Demócrito (que en castellano es átomo). Dalton dejó establecida así la teoría atómica.

Este hecho revolucionó la química. Hacia 1900, los físicos utilizaron métodos hasta entonces insólitos para descubrir que el átomo estaba constituido por partículas aún más pequeñas, lo cual revolucionó a su vez la física. Y cuando se extrajo energía del interior del átomo para producir energía atómica, lo que se revolucionó fue el curso de la historia humana.

6. Lavoisier y los gases

Cuesta creer que el aire sea realmente algo. No se puede ver y normalmente tampoco se deja sentir; y, sin embargo, está ahí. Cuando cobra suficiente velocidad, sopla un viento huracanado que es capaz de hacer naufragar barcos y tronchar árboles. Su presencia resulta entonces innegable.

El aire ¿es la única sustancia invisible? Los alquimistas de la Edad Media pensaban que sí, pues las pompas o vapores incoloros que emanaban sus pócimas recibían el nombre de «aires».

Si los alquimistas vivieran hoy día, no tomaríamos en serio muchos de sus hallazgos. Al fin y al cabo, la alquimia era una falsa ciencia, más interesada en convertir metales en oro que en contribuir al conocimiento de la materia. Con todo, hubo alquimistas de talento que observaron y estudiaron el comportamiento de los metales y otras sustancias con las que trabajaban e hicieron importantes aportaciones a la química moderna.

Un alquimista de talento

Uno de estos alquimistas brillantes fue Jan Baptista van Helmont. A decir verdad era médico y tenía la alquimia como afición. Pues bien, corría el año 1630 aproximadamente y el tal van Helmont estaba muy descontento con la idea de que todos los vapores incoloros fuesen aire. Los «aires» que veía borbotear de sus mixturas no parecían aire ni nada que se le pareciera.

Al echar, por ejemplo, trocitos de plata en un corrosivo muy fuerte llamado ácido nítrico, la plata se disolvía y un vapor rojo borboteaba y dibujaba rizos por encima de la superficie del líquido. ¿Era aquello aire? ¿Quién había visto jamás aire rojo? ¿Quién había oído jamás hablar de un aire que podía verse?

Van Helmont echó luego caliza sobre vinagre y observó de nuevo una serie de pompas que ascendían a la superficie. Al menos esta vez eran incoloras y tenían todo el aspecto de ser burbujas de aire. Pero al colocar una vela encendida sobre la superficie del líquido, la llama se apagaba. ¿Qué clase de aire era aquél en el que no podía arder una vela? Esos mismos vapores ignífugos emanaban del jugo de fruta en fermentación y de las ascuas de madera.

Los así llamados aires obtenidos por van Helmont y otros alquimistas no eran realmente aire. Pero se parecían tanto que engañaron a todos… menos a van Helmont, quien concluyó que el aire era sólo un ejemplo de un grupo de sustancias similares.

Estas sustancias eran más difíciles de estudiar que los materiales corrientes, que uno podía ver y sentir fácilmente; tenían formas definidas y ocupaban cantidades fijas de espacio; se daban en trozos o en cantidades: un terrón de azúcar, medio vaso de agua. Las sustancias aéreas, por el contrario, parecían esparcirse uniformemente por doquier y carecían de estructura.

Del «caos» al «gas»

Este nuevo grupo de sustancias necesitaba un nombre. Van Helmont conocía el mito griego según el cual el universo fue en su origen materia tenue e informe que llenaba todo el espacio. Los griegos llamaban a esta materia primigenia caos. ¡Una buena palabra! Pero van Helmont era flamenco -vivía en lo que hoy es Bélgica- y escribió la palabra tal y como la pronunciaba:

«gas».

Van Helmont fue el primero en darse cuenta de que el aire era sólo uno de tantos gases. A ese gas rojo que observó lo llamamos hoy dióxido de nitrógeno, y al gas que apagaba la llama, anhídrico carbónico.

A van Helmont no le fue fácil estudiar los gases, porque tan pronto como surgían se mezclaban con el aire y desaparecían. Unos cien años más tarde, el inglés Stephen Hales, que era pastor protestante, inventó un método para impedir esa difusión.

Hales dispuso las cosas de manera que las burbujas de gas se formaran en un matraz cuya única salida era un tubo acodado que conducía hasta la boca de otro matraz en posición invertida y lleno de agua. Las burbujas salían por el tubo y subían por el segundo matraz, desplazando el agua. Al final tenía un recipiente lleno de un gas determinado con el que podía experimentar.

La nueva bebida de Priestley

Había gases que, para desesperación de los químicos, no podían recogerse en un matraz lleno de agua porque se disolvían en este líquido. Joseph Priestley, otro pastor inglés, sustituyó hacia 1770 el agua por mercurio. Los gases no se disuelven en mercurio, por lo cual el método servía para recoger cualquier gas.

Priestley obtuvo los dos gases de van Helmont con ayuda del mercurio. El que más le interesaba era el dióxido de carbono, así que, tras obtenerlo con mercurio, disolvió un poco en agua y comprobó que la bebida resultante tenía un sabor agradable. Había inventado el agua de soda.

Priestley recogió también los gases amoníaco, cloruro de hidrógeno y dióxido de azufre y descubrió el oxígeno. Evidentemente, existían docenas de gases distintos.

Una cuestión candente

Hacia la misma época en que Priestley descubría gases, en los años 70 del siglo XVIII, el químico francés Antoine-Laurent Lavoisier estaba enfrascado en el problema de la combustión. La combustión -es decir, el proceso de arder u oxidarse una sustancia en el aire- era algo que nadie terminaba de comprender.

Lavoisier no fue, claro está, el primero en estudiar la combustión; pero tenía una ventaja sobre sus predecesores, y es que creía firmemente que las mediciones precisas eran parte esencial de un experimento. La idea de tomar medidas cuidadosas tampoco era nueva, pues la introdujo doscientos años antes Galileo (véase el capítulo 4); pero fue Lavoisier quien la extendió a la química.

Lavoisier, como decimos, no se limitaba a observar la combustión de una sustancia y examinar las cenizas residuales; ni a observar solamente la oxidación de los metales y examinar la herrumbre, esa sustancia escamosa y pulverulenta que se formaba en la superficie. Antes de arder o aherrumbrarse la sustancia, la pesaba con todo cuidado; y al final del proceso volvía a pesarla.

Estas mediciones no hicieron más que aumentar la confusión al principio. La madera ardía, y la ceniza residual era mucho más ligera que aquélla. Una vela se consumía y desaparecía por completo; no dejaba ni rastro. Lavoisier y varios amigos suyos compraron un pequeño diamante y lo calentaron hasta que ardió; y tampoco dejó rastro alguno. La combustión de un metal ¿destruía parte o la totalidad de su sustancia?

Por otro lado, Lavoisier comprobó que cuando un metal se oxidaba, la herrumbre era más pesada que el metal original. Parecía como si un material sólido, sin saber de dónde venía, se agregara al metal. ¿Por qué la oxidación añadía materia, mientras que la combustión parecía destruirla?

Un problema de peso

Los químicos anteriores no habían perdido el sueño por cuestiones de esta índole, porque no tenían la costumbre de pesar las sustancias. ¿Qué más daba un poco más o un poco menos de peso?

A Lavoisier sí le importaba. ¿No sería que el material quemado se disipaba en el aire? Si las sustancias formaban gases al arder, ¿no se mezclarían éstos con el aire y desaparecerían?

Van Helmont había demostrado que la combustión de la madera producía dióxido de carbono. Lavoisier había obtenido el mismo gas en la combustión del diamante. Una cosa era cierta, por tanto: que la combustión podía producir gas. Pero ¿cuánto? ¿En cantidad suficiente para compensar la pérdida de peso?

Lavoisier pensó que podría ser así. Veinte años atrás, Joseph Black, un químico escocés, había calentado caliza (carbonato de calcio) y comprobado que liberaba dióxido de carbono. La caliza perdió peso, pero el peso del gas producido compensaba exactamente la pérdida.

«Bien», pensó Lavoisier, «supongamos que una sustancia, al arder, pierde peso porque libera un gas. ¿Qué ocurre entonces con los metales? ¿Ganan peso cuando se aherrumbran porque se combinan con un gas?».

El trabajo de Black volvió a dar una pista. Black había hecho burbujear dióxido de carbono a través de agua de cal (una solución de hidróxido de calcio), y el gas y el hidróxido se habían combinado para formar caliza en polvo. Si el hidróxido de calcio podía combinarse con un gas y formar otra sustancia -pensó Lavoisier- es posible que los metales hagan lo propio.

Dejar el aire afuera

Lavoisier tenía, pues, buenas razones para sospechar que detrás de los cambios de peso que se producían en la combustión estaban los gases. Mas ¿cómo probar su sospecha? No bastaba con pesar las cenizas y la herrumbre; había que pesar también los gases.

El problema era la ancha capa de aire que rodea a la Tierra, tanto a la hora de pesar los gases que escapaban de un objeto en combustión como a la hora de medir la cantidad de gas que abandonaba el aire para combinarse con un metal, porque en este segundo caso no pasaría mucho tiempo sin que el espacio dejado por el gas lo ocupara una cantidad parecida de aire.

Lavoisier cayó en la cuenta de que la solución consistía en encerrar los gases y dejar afuera todo el aire, menos una cantidad determinada. Ambas cosas podía conseguirlas si preveía que las reacciones químicas ocurrieran en un recipiente sellado. Los gases liberados en la combustión de una sustancia quedarían capturados entonces dentro del recipiente; y los necesarios para formar la herrumbre sólo podían provenir del aire retenido dentro del mismo.

Sopesar la evidencia

Lavoisier comenzó por pesar con todo cuidado el recipiente estanco, junto con la sustancia sólida y el aire retenido dentro. Luego calentó aquélla enfocando la luz solar por medio de una gran lupa o encendiendo un fuego debajo. Una vez que la sustancia se había quemado o aherrumbrado, volvió a pesar el recipiente junto con su contenido.

El proceso lo repitió con diversas sustancias, y en todos los casos, independientemente de qué fuese lo que se quemara o aherrumbrara, el recipiente sellado no mostró cambios de peso.

Imaginemos, por ejemplo, un trozo de madera reducido a cenizas por combustión. Las cenizas, como es lógico, pesaban menos que la madera, pero la diferencia de peso quedaba compensada por el del gas liberado, de manera que, a fin de cuentas, el peso del recipiente no variaba.

Lo mismo con la oxidación. El trozo de hierro absorbía gas del aire retenido en el recipiente y se transformaba en herrumbre. La herrumbre era más pesada que el hierro, pero la ganancia quedaba exactamente compensada por la pérdida de peso del aire, de modo que, al final, el peso del recipiente tampoco variaba.

Los experimentos y mediciones de Lavoisier ejercieron gran influencia en el desarrollo de la química. Constituyeron los cimientos para su interpretación de la combustión (que es la que seguimos aceptando hoy) y le llevaron a inferir que la materia ni se crea ni se destruye, sino sólo cambia de una forma a otra (de sólido a gas, por ejemplo).

Este es el famoso «principio de conservación de la materia». Y esta idea de que la materia es indestructible ayudó a aceptar, treinta años más tarde, la teoría de que la materia se compone de átomos indestructibles (véase el capítulo 5).

Tanto el principio de conservación de la materia como la teoría atómica han sufrido retoques y mejoras en el siglo XX. Pero, a grandes rasgos, constituyen la sólida plataforma sobre la que se alza la química moderna. En reconocimiento a su contribución a esta tarea, Lavoisier lleva el título de «padre de la química moderna».

7. Newton y la inercia

Es natural pensar que el universo se compone de dos partes, los cielos y la tierra; y, según el filósofo griego Aristóteles, esas dos partes parecían comportarse de manera completamente diferente.

Aristóteles observó que aquí abajo, en la tierra, todo cambia o se desintegra: los hombres envejecen y mueren, los edificios se deterioran y derrumban, el mar se encrespa y luego se calma, los vientos llevan y traen las nubes, el fuego prende y luego se apaga, y la Tierra misma tiembla con los terremotos.

En los cielos, por el contrario, parecían reinar sólo la serenidad y la inmutabilidad. El Sol salía y se ponía puntualmente y su luz jamás subía ni bajaba de brillo. La Luna desgranaba sus fases en orden regular, y las estrellas brillaban sin desmayo.

Aristóteles concluyó que las dos partes del universo funcionaban de acuerdo con reglas o «leyes naturales» de distinta especie. Había una ley natural para los objetos de la Tierra y otra para los objetos celestes. Estos dos conjuntos diferentes de leyes naturales parecían retener su validez al aplicarlas al movimiento. Una piedra soltada en el aire caía derecha hacia abajo. Y en un día sin viento, el humo subía recto hacia lo alto. Todos los movimientos terrestres, librados a su suerte, parecían avanzar o hacia arriba o hacia abajo.

No así en el cielo. El Sol y la Luna y las estrellas no caían hacia la Tierra ni se alejaban de ella. Aristóteles creía que se movían en círculos suaves y uniformes alrededor de nuestro planeta.

Había otra diferencia, y es que en la Tierra los objetos en movimiento terminaban por pararse. La piedra caía al suelo y se detenía. Una pelota podía botar varias veces, pero muy pronto quedaba en reposo. Y lo mismo con un bloque de madera que deslizara pendiente abajo, o con una vagoneta sobre ruedas, o con una piedra lanzada. Inclusive un caballo al galope acababa por cansarse y pararse.

Aristóteles pensaba, por tanto, que el estado natural de las cosas en la Tierra era el reposo. Cualquier objeto en movimiento regresaba a ese estado natural de reposo lo antes posible. En el cielo, por el contrario, la Luna, el Sol y las estrellas jamás hacían un alto y se movían siempre con la misma rapidez.

De Galileo a Newton

Las ideas aristotélicas sobre el movimiento de los objetos fueron lo mejor que pudo ofrecer la mente humana durante casi dos mil años. Luego vino Galileo con otras mejores (véase capítulo 4).

Allí donde Aristóteles creía que los objetos pesados caen más rápidamente que los ligeros, Galileo mostró que todos los objetos caen con la misma velocidad. Aristóteles tenía razón en lo que se refiere a objetos muy ligeros: era cierto que caían más despacio. Pero Galileo explicó por qué: al ser tan ligeros, no podían abrirse paso a través del aire; en el vacío, por el contrario, caería igual de aprisa un trozo de plomo que el objeto más ligero, pues éste no se vería ya retardado por la resistencia del aire.

Unos cuarenta años después de la muerte de Galileo, el científico inglés Isaac Newton estudió la idea de que la resistencia del aire influía sobre los objetos en movimiento y logró descubrir otras formas de interferir con éste.

Cuando una piedra caía y golpeaba la tierra, su movimiento cesaba porque el suelo se cruzaba en su camino. Y cuando una roca rodaba por una carretera irregular, el suelo seguía cruzándose en su camino: la roca se paraba debido al rozamiento entre la superficie áspera de la carretera y las desigualdades de la suya propia.

Cuando la roca bajaba por una carretera lisa y pavimentada, el rozamiento era menor y la roca llegaba más lejos antes de pararse. Y sobre una superficie helada la distancia cubierta era aún mayor.

Newton pensó: ¿Qué ocurriría si un objeto en movimiento no hiciese contacto con nada, si no hubiese barreras, ni rozamiento ni resistencia del aire? Dicho de otro modo, ¿qué pasaría si el objeto se mueve a través de un enorme vacío?

En ese caso no habría nada que lo detuviera, lo retardara o lo desviara de su trayectoria. El objeto seguiría moviéndose para siempre a la misma velocidad y en la misma dirección.

Newton concluyó, por tanto, que el estado natural de un objeto en la Tierra no era necesariamente el reposo; esa era sólo una posibilidad.

Sus conclusiones las resumió en un enunciado que puede expresarse así: Cualquier objeto en reposo, abandonado completamente a su suerte, permanecerá para siempre en reposo. Cualquier objeto en movimiento, abandonado completamente a su suerte, se moverá a la misma velocidad y en línea recta indefinidamente.

Este enunciado es la primera ley de Newton del movimiento.

Según Newton, los objetos tendían a permanecer en reposo o en movimiento. Era como si fuesen demasiado «perezosos» para cambiar de estado. Por eso, la primera ley de Newton se denomina a veces la ley de «inercia». («Inertia», en latín, quiere decir «ocio», «pereza».)

A poco que uno recapacite verá que los objetos tienen cantidades de inercia (de resistencia al cambio) muy variables. Basta dar una patadita a un balón de playa para mandarlo muy lejos, mientras que para mover una bala de cañón hay que empujar con todas nuestras fuerzas, y aun así se moverá muy despacio.

Una vez en movimiento, también es grande la diferencia en la facilidad con que dejan detenerse. Un balón de playa que viene lanzado hacia nosotros lo podemos parar de un manotazo. Una bala de cañón, a la misma velocidad, más vale dejarla pasar, porque nos arrancaría la mano y ni se enteraría.

La bala de cañón es mucho más reacia a cambiar su estado de movimiento que un balón de playa. Tiene mucha más inercia. Newton sugirió que la masa de un objeto es la cantidad de inercia del objeto. Una bala de cañón tiene más masa que un balón de playa.

La bala de cañón tiene también más peso que el balón. Los objetos pesados tienen en general gran masa, mientras que los ligeros tienen poca. Pero el peso no es lo mismo que la masa. En la Luna, por ejemplo, el peso de cualquier objeto es sólo un sexto de su peso en la Tierra, pero su masa es la misma. El movimiento de una bala de cañón en la Luna sería tan difícil de iniciar y tan peligroso de detener como en la Tierra; y, sin embargo, la bala nos parecería sorprendentemente ligera al levantarla.

Para hacer que un objeto se mueva más rápidamente, más lentamente o abandone su trayectoria, hay que tirar de él o empujarlo. Un tirón o un empujón recibe el nombre de «fuerza». Y la razón (por unidad de tiempo) a la que un cuerpo aviva o retarda su paso o cambia de dirección es su «aceleración».

La segunda ley del movimiento que enunció Newton cabe expresarla así: la aceleración de cualquier cuerpo es igual a la fuerza aplicada a él, dividida por la masa del cuerpo. Dicho de otro modo, un objeto, al empujarlo o tirar de él, tiende a acelerar o retardar su movimiento o a cambiar de dirección. Cuanto mayor es la fuerza, tanto más cambiará de velocidad o de dirección. Por otro lado, la masa del objeto -la cantidad de inercia que posee- actúa en contra de esa aceleración. Un empujón fuerte hará que el balón de playa se mueva mucho más deprisa porque posee poca masa; pero la misma fuerza, aplicada a la bala de cañón (que tiene mucha más masa), apenas afectará su movimiento.

De la manzana a la Luna

Newton propuso luego una tercera ley del movimiento, que puede enunciarse de la siguiente manera: Si un cuerpo ejerce una fuerza sobre un segundo cuerpo, éste ejerce sobre el primero una fuerza igual pero de sentido contrario. Es decir, que si un libro aprieta hacia abajo sobre una mesa, la mesa tiene que estar empujando el libro hacia arriba en la misma cuantía. Por eso el libro se queda donde está, sin desplomarse a través del tablero ni saltar a los aires.

Las tres leyes del movimiento sirven para explicar casi todos los movimientos y fuerzas de la Tierra. ¿Sirven también para explicar los de los cielos, que son tan distintos?

Los objetos celestes se mueven en el vacío, pero no en línea recta. La Luna, pongamos por caso, sigue una trayectoria curva alrededor de la Tierra. Lo cual no contradice la primera ley de Newton, porque la Luna no está «librada completamente a su suerte». No se mueve en línea recta porque sufre continuamente un tirón lateral en dirección a la Tierra.

Para que la Luna se viera solicitada de este modo era necesario -por la segunda Ley de Newton- que existiera una fuerza aplicada a ella, una fuerza ejercida siempre en dirección a la Tierra.

La Tierra ejerce, sin duda, una fuerza sobre los cuerpos terrestres y hace que las manzanas caigan, por ejemplo. Es la fuerza de la gravedad. ¿Era esta fuerza la misma que actuaba sobre la Luna? Newton aplicó sus tres leyes del movimiento a nuestro satélite y demostró que su trayectoria quedaba explicada admirablemente con sólo suponer que sobre ella actuaba la misma fuerza gravitatoria que hacía caer a las manzanas.

Pero la cosa no paraba ahí, porque cualquier objeto del universo establece una fuerza de gravitación; y es la gravitación del Sol, por ejemplo, la que hace que la Tierra gire y gire alrededor del astro central.

Newton aplicó sus tres leyes para demostrar que la magnitud de la fuerza de gravitación entre dos cuerpos cualesquiera del universo dependía de las masas de los cuerpos y de la distancia entre ellos. Cuanto mayores las masas, mayor la fuerza. Y cuanto mayor la distancia mutua, menor la atracción entre los cuerpos. Newton había descubierto la ley de la gravitación universal.

Esta ley consiguió dos cosas importantes. En primer lugar explicaba el movimiento de los cuerpos celestes hasta casi sus últimos detalles; explicaba asimismo por qué la Tierra cabeceaba muy lentamente sobre su eje; y más tarde sirvió para explicar la rotación mutua de parejas de estrellas (binarias), alejadas billones de kilómetros de nosotros.

En segundo lugar, y quizá sea esto lo más importante, Newton demostró que Aristóteles se había equivocado al pensar que existían dos conjuntos de leyes naturales, uno para los cielos y otro para la Tierra. Las tres leyes del movimiento explicaban igual de bien la caída de una manzana o el rebote de una pelota que la trayectoria de la Luna. Newton demostró así que los. cielos y la Tierra eran parte del mismo universo.

8. Faraday y los campos

Imaginemos una barra de hierro, de pie sobre uno de sus extremos, con una cuerda atada cerca del borde superior. ¿Podemos tumbarla?

Por supuesto que sí. Basta con empujarla con un dedo o agarrar la cuerda y tirar. El tirón o el empujón es una fuerza. En casi todos los casos la fuerza sólo actúa cuando los dos objetos se tocan.

Al empujar la barra, el dedo la toca. Al tirar, los dedos tocan la cuerda y ésta toca la barra. Alguien podría decir que si soplamos con fuerza en dirección a la barra, la podemos tumbar sin tocarla. Pero lo que hacemos es empujar moléculas de aire, que son las que tocan y empujan la barra.

Las tres leyes newtonianas del movimiento explicaban el comportamiento de estas fuerzas (véase el capítulo 7) y servían también para explicar los principios en que se basaban máquinas en las que las palancas, las poleas y los engranajes actuaban tirando y empujando. En este tipo de máquinas los objetos ejercían fuerzas sobre otros objetos por contacto.

Un universo «mecánico»

Los científicos de principios del siglo XVIII pensaban que el universo entero funcionaba a base de estas fuerzas de contacto: era lo que se llama una visión mecanicista del universo.

¿Podían existir fuerzas sin contacto? Sin duda: una de ellas era la fuerza de gravitación explicada por el propio Newton. La Tierra tiraba de la Luna y la mantenía en su órbita, pero no la tocaba en absoluto. Entre ambos cuerpos no mediaba absolutamente nada, ni siquiera aire; pero aun así, ambas estaban ligadas por la gran fuerza gravitatoria.

Otra clase de fuerza sin contacto cabe observarla si volvemos por un momento a nuestra barra de hierro colocada de pie. Lo único que necesitamos es un pequeño imán. Lo acercamos a la punta superior de la barra y ésta se inclina hacia el imán y cae. El imán no necesita tocar para nada la barra, ni tampoco es el aire el causante del fenómeno, porque exactamente lo mismo ocurre en el vacío.

Si dejamos que un imán largo y fino oscile en cualquier dirección, acabará por apuntar hacia el Norte y el Sur. O dicho de otro modo, el imán se convierte en brújula, en una brújula como las que utilizaron los navegantes europeos para explorar los océanos desde mediados del siglo XIV aproximadamente.

El extremo del imán que apunta al Norte se llama polo norte; el otro es el polo sur. Si se acerca el polo norte de un imán al polo sur de otro, se establece una fuerte atracción entre ambos, que tenderán a unirse. Y si se hace lo mismo con polos iguales -norte y norte o sur y sur-, ambos se repelen y separan.

Este tipo de fuerza sin contacto se llama «acción a distancia» y trajo de cabeza a los científicos desde el principio. Incluso Tales (véase el capítulo 1) quedó atónito cuando observó por primera vez que cierto mineral negro atraía al hierro a distancia, y exclamó: «¡Este mineral tiene que tener vida!».

No había tal, claro; se trataba simplemente del mineral magnetita. ¿Pero cómo iban a explicar si no los científicos la misteriosa fuerza de un imán, una fuerza que era capaz de atraer y tumbar una barra de hierro sin tocarla? La acción de una brújula era aún más misteriosa. La aguja apuntaba siempre hacia el Norte y hacia el Sur porque era atraída por las lejanas regiones polares de la Tierra. ¡He aquí una acción a distancias realmente grandes! ¡Una fuerza que podía encontrar una aguja magnética en un pajar!

El científico inglés Michael Faraday abordó en 1831 el problema de esa misteriosa fuerza. Colocó dos imanes sobre una mesa de madera, con el polo norte de uno mirando hacia el polo sur del otro. Los imanes estaban suficientemente cerca como para atraerse, pero no tanto como para llegar a juntarse; la atracción a esa distancia no era suficiente para superar el rozamiento con la mesa. Faraday sabía, sin embargo, que la fuerza estaba ahí, porque si dejaba caer limaduras de hierro entre los dos imanes, aquéllas se movían hacia los polos y se quedaban pegadas a ellos.

Faraday modificó luego el experimento: colocó un trozo de papel recio sobre los dos imanes y esparció por encima las limaduras. El rozamiento de las limaduras contra el papel las retenía e impedía que migraran hacia los imanes.

«Alineamiento» magnético

Faraday dio luego un ligero golpecito al papel para que las limaduras se movieran un poco, y al punto giraron como diminutas agujas magnéticas y quedaron señalando hacia uno u otro imán.

Las limaduras parecían alinearse realmente según curvas que iban del polo de uno de los imanes al polo del otro. Faraday lo estudió detenidamente. Las líneas situadas exactamente entre los dos polos eran rectas. A orillas del vano entre los dos imanes seguían alineándose las limaduras, pero ahora trazaban una curva. Cuanto más fuera estaban las limaduras, más curvada era la línea que dibujaban.

Faraday cayó en la cuenta. ¡Ya lo tenía! Entre el polo norte de un imán y su propio polo sur o el de otro imán corrían líneas magnéticas de fuerza que llegaban muy lejos de los polos.

Quiere decirse que el imán no actuaba ni mucho menos por acción a distancia, sino que atraía o empujaba a un objeto cuando sus líneas de fuerza se aproximaban a él. Las líneas de fuerza de un imán o tocaban el objeto, o se acercaban a las líneas de fuerza que salían de éste.

Los científicos pensaron más tarde que probablemente era lo mismo que sucedía con otros tipos de acción a distancia. Alrededor de la Tierra y de la Luna, por ejemplo, tenía que haber líneas gravitatorias de fuerza, cuyo contacto es el que permite que se atraigan los dos cuerpos. Y, por otro lado, los cuerpos eléctricamente cargados también repelían y atraían a otros objetos, de manera que existían asimismo líneas eléctricas de fuerza.

Nuevos generadores

Faraday no tardó en demostrar que cuando ciertos objetos (no cualesquiera) se mueven a través de líneas magnéticas de fuerza se establece una corriente eléctrica en ellos.

Hasta entonces la corriente eléctrica sólo se podía obtener con baterías, que son recipientes cerrados en cuyo interior reaccionan ciertas sustancias químicas. La electricidad generada con baterías era bastante cara. El nuevo descubrimiento de Faraday permitía generarla con una máquina de vapor que moviera ciertos objetos a través de líneas magnéticas de fuerza. La electricidad obtenida con estos generadores de vapor era muy barata y podía producirse en grandes cantidades. Cabe decir, pues, que fueron las líneas magnéticas de fuerza las que electrificaron el mundo en el siglo XX.

Faraday era un genio autodidacta. Sólo cursó estudios primarios y no sabía matemáticas, por lo cual no pudo describir cuantitativamente la distribución de las líneas de fuerza alrededor de un imán. Tuvo que limitarse a reproducirla con limaduras de hierro.

Sin embargo, el problema lo abordó hacia 1860 un matemático escocés que se llamaba James Clerk Maxwell. Maxwell obtuvo un conjunto de ecuaciones matemáticas que describían cómo la intensidad de la fuerza variaba al alejarse cada vez más del imán en cualquier dirección.

La fuerza que rodea un imán se denomina «campo». El campo de cualquier imán llena el universo entero; lo que ocurre es que se debilita rápidamente con la distancia, de manera que sólo puede medirse muy cerca del imán. A Maxwell se le ocurrió trazar una línea que pasara por todas las partes del campo que tenían una determinada intensidad. El resultado eran las líneas de fuerza de las que había hablado Faraday. Las ecuaciones de Maxwell permitieron, pues, manejar con precisión las líneas de fuerza de Faraday.

Maxwell demostró también que los campos magnéticos y los eléctricos coexistían siempre y que había que hablar, por tanto, de un campo electromagnético. En ciertas condiciones podía propagarse desde el centro de este campo, y en todas direcciones, un conjunto de «ondas». Era la radiación electromagnética. Según los cálculos matemáticos de Maxwell, esa radiación tenía que viajar a la velocidad de la luz. Parecía, pues, que la propia luz era una radiación electromagnética.

Años después de morir Maxwell se demostró que sus teorías eran correctas y se descubrieron nuevos tipos de radiación electromagnética, como las ondas de radio y los rayos X. Maxwell lo había predicho, pero no llegó a verlo confirmado experimentalmente.

En 1905, el científico suizo-alemán Albert Einstein comenzó a remodelar la imagen del universo: abandonó la visión mecanicista nacida con las leyes del movimiento de Newton, y explicó el universo sobre la base de la idea de campo.

Los dos campos que se conocían por entonces eran el gravitatorio y el electromagnético. Einstein trató de hallar un único conjunto de ecuaciones matemáticas que describiera ambos campos; pero fracasó. Desde entonces se han descubierto dos nuevos campos que tienen que ver con las minúsculas partículas que componen el núcleo del átomo. Son lo que se conoce por «campos nucleares».

La acción electromagnética

Todo lo que antes solía tenerse por fuerzas de «tirar y empujar» se considera ahora como la interacción de campos.

El contorno de un átomo está ocupado por electrones. Cuando dos átomos se aproximan entre sí, los campos electromagnéticos que rodean a estos electrones se empujan mutuamente. Los átomos propiamente dichos se separan sin haber llegado a tocarse.

Así pues, cuando empujamos una barca o tiramos de una cuerda no tocamos en realidad nada sólido. Lo único que hacemos es aprovecharnos de estos diminutos campos electromagnéticos. La Luna gira alrededor de la Tierra y ésta alrededor del Sol debido a los campos gravitatorios que rodean a estos cuerpos. Y las bombas atómicas explosionan a causa de procesos que se operan en los campos nucleares.

La nueva imagen del universo, la imagen basada en los campos, ha permitido a los científicos hacer avances que habrían sido imposibles en tiempos de la visión mecanicista. Y lo cierto es que esta nueva visión tiene su origen en la idea de Faraday de que las líneas magnéticas de fuerza pueden empujar un objeto o tirar de él.

9. Rumford y el calor

No es fácil sentir demasiada simpatía por Benjamin Thompson, una de esas personas astutas cuya primera v única preocupación son ellas mismas. Cuando sólo tenía diecinueve años escapó de la pobreza de su infancia casándose con una rica viuda que casi le doblaba en edad.

Thompson nació en Woburn, Massachusetts, en 1753. En aquellos días, Massachusetts y los demás estados norteamericanos eran todavía colonias británicas. Pocos años después de casarse Thompson estalló la Revolución Americana, y esta vez marró el pronóstico y apuntó por el perdedor. Se enroló en el ejército británico en Boston y fue espía contra los patriotas coloniales.

Cuando los británicos abandonaron Boston se llevaron a Thompson consigo. Sin grandes remordimientos dejó atrás a su mujer y a sus hijos y jamás regresó.

En Europa ofreció sus servicios a cualquier gobierno que accedió a pagar el precio que pedía, y con todos tuvo líos por aceptar sobornos, vender secretos y tener, en general, una conducta inmoral y deshonesta. Thompson salió en 179O de Inglaterra para el continente europeo. Entró al servicio del Estado de Baviera (que hoy pertenece a Alemania, pero que en aquel entonces era nación independiente) y allí le otorgaron el título de conde. Thompson adoptó el nombre de conde de Rumford, pues «Rumford» era como se llamaba originalmente la ciudad de Concord (New Hampshire) donde se casó con su primera mujer. Así fue como Benjamín Thompson ha pasado a la historia con el nombre de Rumford.

Una mente científica

Una cosa sí puede decirse a favor de Rumford, y es que tenía una sed inagotable de conocimiento. Desde niño hizo gala de una mente activa y despierta que penetraba hasta el meollo mismo de los problemas.

A lo largo de su vida hizo muchos experimentos de interés y llegó a numerosas conclusiones importantes. La más señalada tuvo como escenario Baviera, donde estuvo al frente de una fábrica de cañones. Los cañones se hacían vertiendo el metal en moldes y taladrando luego la pieza para formar el alma. Esta última operación se efectuaba con una taladradora rápida.

Como es lógico, el cañón y el taladro se calentaban y había que estar echando constantemente agua fría por encima para refrigerarlos. Al ver salir el calor, la mente incansable de Rumford se puso en funcionamiento.

Antes de nada, ¿qué era el calor? Los científicos de aquella época, entre ellos el gran químico francés Lavoisier, creían que el calor era un fluido ingrávido que llamaban calórico. Al introducir más calórico en una sustancia ésta se calentaba, hasta que finalmente el calórico rebosaba y fluía en todas direcciones. Por eso, la calidez de un objeto al rojo vivo se dejaba sentir a gran distancia. El calor del Sol, por ejemplo, se notaba a 150 millones de kilómetros. Al poner en contacto un objeto caliente con otro frío, el calórico fluía desde el primero al segundo. Ese flujo hacía que el objeto caliente se enfriara y que el frío se calentara.

La teoría funcionaba bastante bien, y muy pocos científicos la ponían en duda. Uno de los que sí dudó fue Rumford, preguntándose por qué el calórico salía del cañón. Los partidarios de la teoría del calórico contestaron que era porque el taladro rompía en pedazos el metal, dejando que el calórico contenido en éste fluyese hacia afuera, como el agua de un jarrón roto.

Rumford, escéptico, revolvió entre los taladros y halló uno completamente romo y desgastado. «Utilizad éste», dijo. Los obreros objetaron que no servía, que estaba gastado; pero Rumford repitió la orden en tono más firme y aquéllos se apresuraron a cumplirla.

El taladro giró en vano, sin hacer mella en el metal; pero en cambio producía aún más calor que uno nuevo. Imagínense la extrañeza de los obreros al ver el gesto complacido del conde.

Rumford vio claro que el calórico no se desprendía por la rotura del metal, y que quizá no procediese siquiera de éste. El metal estaba inicialmente frío, por lo cual no podía contener mucho calórico; y, aun así, parecía que el calórico fluía en cantidades ilimitadas.

Rumford, para medir el calórico que salía del cañón, observó cuánto se calentaba el agua utilizada para refrigerar el taladro y el cañón, y llegó a la conclusión de que si todo ese calórico se reintegrara al metal, el cañón se fundiría.

Partículas en movimiento

Rumford llegó al convencimiento de que el calor no era un fluido, sino una forma de movimiento. A medida que el taladro rozaba contra el metal, su movimiento se convertía en rápidos y pequeñísimos movimientos de las partículas que constituían el bronce. Igual daba que el taladro cortara o no el metal; el calor provenía de esos pequeñísimos y rápidos movimientos de las partículas, y, como es natural, seguía produciéndose mientras girara el taladro. La producción de calor no tenía nada que ver con ningún calórico que pudiera haber o dejar de haber en el metal.

El trabajo de Rumford quedó ignorado durante los cincuenta años siguientes. Los científicos se contentaban con la idea del calórico y con inventar teorías que explicaran cómo fluía de un cuerpo a otro. La razón, o parte de la razón, es que vacilaban en aceptar la idea de diminutas partículas que experimentaban un movimiento rápido y pequeñísimo que nadie podía ver.

Sin embargo, unos diez años después de los trabajos de Rumford, John Dalton enunció su teoría atómica (véase el capítulo 5). Poco a poco, los científicos iban aceptando la existencia de los átomos. ¿No sería, entonces, que las pequeñas partículas móviles de Rumford fuesen átomos o moléculas (grupos de átomos)?

Podía ser. Pero ¿cómo imaginar el movimiento de billones y billones de moléculas invisibles? ¿Se movían todas al unísono, o unas para un lado y otras para otro, según una ley fija? ¿O tendrían acaso un movimiento aleatorio, al azar, con direcciones y velocidades arbitrarias, sin poder decir en qué dirección y con qué velocidad se movía cualquiera de ellas?

El matemático suizo Daniel Bernouilli, a principios del siglo XVIII, algunas décadas antes de los trabajos de Rumford, había intentado estudiar el problema del movimiento aleatorio de partículas en gases. Esto fue mucho antes de que los científicos aceptaran la teoría atómica y, por otro lado, las matemáticas de Bernouilli no tenían tampoco la exactitud que requería el caso. Aun así, fue un intento válido.

En los años 60 del siglo XIX entró en escena James Clerk Maxwell (véase el capítulo 8). Maxwell partió del supuesto de que las moléculas que componían los gases tenían movimientos aleatorios, y mediante agudos análisis matemáticos demostró que el movimiento aleatorio proporcionaba una bella explicación del comportamiento de los gases.

Maxwell mostró cómo las partículas del gas, moviéndose al azar, creaban una presión contra las paredes del recipiente que lo contenía. Además, esa presión variaba al comprimir las partículas o al dejar que se expandieran. Esta explicación del comportamiento de los gases se conoce por la teoría cinética de los gases («cinética» proviene de una palabra griega que significa «movimiento»).

Maxwell suele compartir la paternidad de esta teoría con el físico austríaco Ludwig Boltzmann. Los dos, cada uno por su lado, elaboraron la teoría casi al mismo tiempo.

La solución de Maxwell

Una de las importantes leyes del comportamiento de los gases afirma que un gas se expande al subir la temperatura y se contrae al disminuir ésta. Según la teoría del calórico, la explicación de este fenómeno era simple: al calentarse un gas, entra calórico en él; como el calórico ocupa espacio, el gas se expande; al enfriarse el gas, sale el calórico y aquél se contrae.

¿Qué tenía que decir Maxwell a esto? Por fuerza tuvo que pensar en el experimento de Rumford. El calor es una forma de movimiento. Al calentar un gas, sus moléculas se mueven más deprisa y empujan a las vecinas hacia afuera. El gas se expande. Al disminuir la temperatura, ocurre lo contrario y el gas se contrae.

Maxwell halló una ecuación que especificaba la gama de velocidades que debían tener las moléculas gaseosas a una temperatura dada. Algunas se movían despacio y otras deprisa; pero la mayoría tendrían una velocidad intermedia. De entre todas estas velocidades había una que era máximamente probable a una temperatura dada. Al subir la temperatura, aumentaba también esa? velocidad más probable.

Esta teoría cinética del calor era aplicable tanto a líquidos y sólidos como a gases. En un sólido, por ejemplo, las moléculas no volaban de acá para allá como proyectiles, que es lo que sucedía en un gas; pero en cambio podían vibrar en torno a un punto fijo. La velocidad de esta vibración, lo mismo que las moléculas proyectiles de los gases, obedecían a las ecuaciones de Maxwel.

Una explicación mejor

Todas las propiedades del calor podían ser exploradas igual de bien por la teoría cinética que por la del calórico. Pero aquélla daba fácilmente cuenta de algunas propiedades (como las descritas por Rumford) que la teoría del calórico no había conseguido explicar bien.

La teoría del calórico describía la transferencia de calor como un flujo de calórico desde el objeto caliente al frío. Según la teoría cinética, la transferencia de calor era resultado del movimiento de moléculas. Al poner en contacto un cuerpo caliente con otro frío, sus moléculas, animadas de rápido movimiento, chocaban con las del objeto frío, que se movían más lentamente. Como consecuencia de ello, las moléculas rápidas perdían velocidad y las lentas se aceleraban un poco, con lo cual «fluía» calor del cuerpo caliente al frío.

La concepción del calor como una forma de movimiento es otra de las grandes ideas de la ciencia. Maxwell le dio mayor realce aún mostrando cómo utilizar el movimiento aleatorio para explicar ciertas leyes muy concretas de la naturaleza cuyo efecto era totalmente predecible y nada aleatorio.

La idea de Maxwell fue luego ampliada notablemente, y los científicos dan hoy por supuesto que el comportamiento aleatorio de átomos y moléculas pueden producir resultados muy asombrosos. Cabe, inclusive, que la vida misma fuese creada a partir de la materia inerte en los océanos mediante movimientos aleatorios de átomos y moléculas.

10. Joule y la energía

Desde los tiempos prehistóricos el hombre se dio cuenta de que el movimiento puede realizar trabajo y hacer esfuerzos. Colocamos una piedra sobre una nuez y no pasa nada; pero le comunicamos un rápido movimiento hacia abajo y la nuez se casca. Una flecha en reposo es casi inofensiva, pero lanzada en rápido movimiento puede perforar la gruesa piel de un animal. Y muchos habrán visto esas demoledoras que pulverizan muros de ladrillo con un enorme péndulo de acero.

La capacidad de realizar trabajo se llama «energía». Los objetos en movimiento poseen energía de movimiento o «energía cinética».

Cuando Newton enunció sus leyes del movimiento en los años 80 del siglo XVII, dijo que cualquier objeto en movimiento continuaría moviéndose a la misma velocidad a menos que una fuerza exterior actuara sobre él (véase el capítulo 7). Dicho de otro modo, la energía cinética de un objeto tenía que permanecer constante.

Ahora bien, en el mundo real operan siempre fuerzas exteriores sobre los objetos en movimiento, y la energía cinética da la sensación de que desaparece. Una pelota que rueda por el suelo pierde velocidad y se para. Una canica bota varias veces y luego se detiene. Y los meteoritos cruzan por el aire y son detenidos por la Tierra.

¿Qué ocurre con la energía cinética en todos estos casos? Parte de ella, pero no toda, puede convertirse en trabajo. En efecto, la canica que rebota o la pelota que rueda puede que no realicen ningún trabajo, y aun así su energía cinética desaparece.

La respuesta: el calor

El meteorito nos da una pista, porque crea gran cantidad de calor al atravesar la atmósfera, hasta el punto de ponerse incandescente.

Aquí entra en escena el científico inglés Prescott Joule. Poco apto -por culpa de una infancia enfermiza- para llevar una vida activa, se refugió en el mundo de los libros y descubrió su interés por la ciencia. Por fortuna era hijo de un rico cervecero que podía permitirse el lujo de darle los mejores tutores. Joule llegó a heredar la cervecería, pero siempre le interesó más la ciencia que el mundo de los negocios.

El interés de Joule giraba en torno al problema de la conexión entre la energía y el calor, y seguramente no desconocía la idea de Rumford de que el calor era una forma de movimiento. Según éste, el calor consistía en el rápido movimiento de partículas diminutas de materia (véase el capítulo 9).

De ser así, pensó Joule, la energía cinética no desaparecía para nada. El movimiento de una pelota al rodar producía rozamiento contra el suelo; el rozamiento producía calor; por consiguiente, el movimiento de la pelota al rodar se convertía lentamente en el movimiento de millones y millones de partículas: las partículas de la pelota y las del suelo sobre el que rodaba.

El calor sería entonces otra forma de energía en movimiento, pensó Joule. La energía cinética ordinaria se convertía en energía térmica sin pérdida de ninguna clase. Quizá ocurriera lo mismo con otras formas de energía. La idea no parecía descabellada. La electricidad y el magnetismo podían realizar trabajo, y lo mismo las reacciones entre sustancias químicas.

Así pues, existían la energía eléctrica, la magnética y la química. Todas ellas podían convertirse en calor. El magnetismo, por ejemplo, podía producir una corriente eléctrica que a su vez era capaz de calentar un alambre. Y al arder el carbón, la reacción química entre éste y el aire generaba gran cantidad de calor.

El calor, se dijo Joule, debía ser otra forma más de energía, igual que las anteriores. Por consiguiente, una cantidad dada de energía debería producir siempre la misma cantidad de calor. En 1840, cuando sólo tenía 22 años, comenzó a hacer mediciones muy precisas con el fin de comprobar esa posibilidad.

Uno de los experimentos consistió en agitar agua o mercurio con ruedas de paletas y medir la energía invertida por éstas y el aumento de temperatura en el líquido. Otro, en comprimir aire y medir luego la energía invertida en la compresión y el calor generado en el aire. Un tercero, en inyectar agua a través de tubos delgados. Otro más, en generar corriente eléctrica en una espira de alambre, haciéndola rotar entre los polos de un imán, o bien en hacer pasar una corriente por un cable sin la presencia del imán. En todos los casos Joule midió la energía consumida y el calor generado. '

Ni siquiera durante su luna de miel pudo resistir la tentación de hacer un paréntesis para medir la temperatura en la parte superior e inferior de una cascada, con el fin de ver cuánto calor había generado la energía del agua al caer.

Hacia 1847 Joule estaba ya convencido de que una cantidad dada de energía de cualquier tipo producía siempre la misma cantidad de calor. (La energía se puede medir en ergios y el calor en calorías.) Joule demostró que siempre que se consumían unos 41.800.000 ergios de energía de cualquier tipo, se producía 1 caloría. Esta relación entre energía y calor se denomina «equivalente mecánico del calor». Más tarde se introdujo en honor de Joule otra unidad de energía llamada «joule» o «julio». El julio es igual a 10 millones de ergios, y una caloría equivale a 4'18 julios.

Un auditorio reacio

A Joule no le fue fácil anunciar su descubrimiento, porque no era ni profesor ni miembro de ninguna sociedad erudita. Era simplemente cervecero, y los científicos de la época no le prestaron oídos. Finalmente decidió dar una conferencia pública en Manchester y convenció a un periódico de la ciudad para que publicara el texto íntegro.

Meses después logró pronunciar la misma conferencia ante un auditorio de científicos, que, sin embargo, le dispensaron fría acogida. Y habrían pasado por alto el meollo de la cuestión de no ser porque uno de los asistentes, el joven William Thompson, se levantó e hizo algunas observaciones a favor de Joule. Los comentarios de Thompson fueron tan inteligentes y agudos que el auditorio no tuvo más remedio que darse por enterado. (Thompson se convirtió con el tiempo en uno de los grandes científicos del siglo XIX, y es más conocido por el título de Lord Kelvin.)

Quedó así establecido que cualquier forma de energía podía convertirse en una cantidad fija y limitada de calor. Pero el propio calor era una forma de energía. ¿Sería que ésta no se puede destruir ni crear, sino sólo transformar de una modalidad a otra?

Un mérito mal atribuido

Esa idea se le ocurrió al científico alemán Julius Robert Mayer en 1842. Pero por aquel entonces estaba todavía inédita la labor de Joule, y Mayer disponía de muy pocas mediciones. La idea de Mayer parecía como sacada de la manga y nadie le prestó atención.

Hermann Ludwig Ferdinand von Helmholtz, otro científico alemán, lanzó la misma idea en 1847, al parecer sin conocimiento de los trabajos de Mayer. Para entonces ya se habían publicado los trabajos de Joule; los científicos estaban por fin dispuestos a escuchar y a calibrar la importancia del hallazgo.

Es Helmholtz, por tanto, a quien suele atribuirse la paternidad del así llamado «principio de conservación de la energía», que en su formulación más simple dice lo siguiente: la energía total del universo es constante.

Mayer trató de recordar al mundo que eso mismo lo había dicho él en 1842; pero todos lo habían olvidado o ni siquiera lo habían oído, de modo que el pobre Mayer fue acusado de querer adornarse con plumas ajenas. Su desesperación llegó hasta tal punto que intentó suicidarse tirándose por una ventana. Se recuperó, sin embargo, y vivió en la oscuridad otros treinta años. No fue hasta el final de sus días cuando se comprendió la importancia de este hombre.

El principio de conservación de la energía recibe a menudo el nombre de «primer principio de la termodinámica». Desde la primera parte del siglo XIX, los científicos venían investigando el flujo de calor de un objeto a otro, estudio que lleva el nombre de «termodinámica» (del griego «movimiento del calor»). Una vez aceptado el principio de conservación de la energía, hubo que tenerlo en cuenta en todos los estudios de termodinámica.

La máquina de Carnot

Hacia la época en que fue establecido este principio, los estudiosos de la termodinámica ya habían caído en la cuenta de que la energía no siempre se podía convertir íntegramente en trabajo. Parte de ella se esfumaba invariablemente en calor, hiciese uno lo que hiciese por impedirlo.

El primero en demostrar esto mediante cuidadosos análisis científicos fue el joven físico francés Nicholas Leonard Sadi Carnot. En 1824 publicó un librito sobre la máquina de vapor en el cual exponía argumentos encaminados a demostrar que la energía térmica producida por una máquina de vapor no podía generar más que una cierta cantidad de trabajo. Esta cantidad de trabajo dependía de la diferencia de temperatura entre la parte más caliente de la máquina de vapor y la más fría. Si la máquina entera estuviese a una misma temperatura, no produciría trabajo, por mucho calor que acumulara.

Cuando Helmholtz anunció el principio de conservación de la energía, los científicos se acordaron de las pruebas de Carnot relativas a la limitación del trabajo que se podía obtener con una máquina de vapor. ¿Por qué ese trabajo era normalmente mucho menor que la energía producida por la máquina? Que las diferencias de temperatura influían en el trabajo obtenido lo había demostrado Carnot convenientemente; pero ¿por qué?

La razón de Clausius

La formulación matemática del fenómeno fue elaborada en 1850 por el físico alemán Rudolf Julius Emmanuel Clausius, quien lo hizo con ayuda del concepto de temperatura absoluta o temperatura por encima del cero absoluto. En el cero absoluto, es decir, a -273 grados centígrados, no hay calor ninguno.

Clausius comprobó que si dividía energía térmica total de un sistema por su temperatura absoluta, obtenía una razón que aumentaba siempre en cualquier proceso natural, ya fuese la combustión de carbón en el sistema de una máquina de vapor o la explosión de hidrógeno y helio en el «sistema» del Sol. Cuanto más rápidamente aumentaba esa razón, menor era el trabajo que se podía extraer del calor. Hacia 1865 Clausius llamó «entropía» a esta razón.

La entropía aumenta en cualquier proceso natural. Crece, por ejemplo, cuando un objeto caliente se enfría, cuando el agua cae ladera abajo, cuando el hierro se oxida, cuando la carne se descompone, etc. El hecho de que la entropía crece siempre se conoce hoy por el «segundo principio de la termodinámica», que puede expresarse con mayor sencillez de la manera siguiente: La entropía total del universo no cesa de aumentar.

Los principios primero y segundo de la termodinámica son quizás los enunciados más fundamentales que jamás hayan establecido los científicos. Nadie ha encontrado jamás excepción alguna, y quizá nadie la encuentre nunca. Por lo que sabemos hoy día, son leyes que se aplican al universo entero, desde los grupos más grandes de estrellas a las partículas subatómicas más pequeñas.

Pese a las revoluciones científicas que ha experimentado el pensamiento científico en el siglo presente, los principios de la termodinámica se han mantenido firmes y siguen siendo sólidos pilares de la ciencia física.

11. Planck y los cuantos

A mediados del siglo XIX la ciencia descubrió que la luz proporcionaba a cada elemento químico una especie de «huellas digitales». Veamos cómo puede utilizarse la luz para distinguir un elemento de otro.

Si se calienta un elemento hasta la incandescencia, la luz que emite estará constituida por ondas de diversas longitudes. El grupo de longitudes de onda que produce el elemento difiere del de cualquier otro elemento.

Cada longitud de onda produce un efecto diferente en el ojo y es percibida, por tanto, como un color distinto de los demás. Supongamos que la luz de un elemento dado es descompuesta en sus diversas ondas. Este grupo de longitudes de onda, que es característico del elemento, se manifiesta entonces en la forma de un patrón de colores también singular. Pero ¿cómo se puede desglosar la luz de un elemento incandescente en ondas elementales?

Una manera consiste en hacer pasar la luz por una rendija y luego por un trozo triangular de vidrio que se denomina prisma. El prisma refracta cada onda en medida diferente, según su longitud, y forma así imágenes de la rendija en los colores que se hallan asociados con las longitudes de onda del elemento. El resultado es un «espectro» de rayas de color cuya combinación difiere de la de cualquier otro elemento.

Este procedimiento lo elaboró con detalle el físico alemán Gustav Robert Kirchhoff en 1859. Kirchhoff y el químico alemán Robert Wilhelm von Bunsen inventaron el espectroscopio -el instrumento descrito anteriormente- y lo emplearon para estudiar los espectros de diversos elementos. Y, de paso, descubrieron dos elementos nuevos al hallar combinaciones de rayas que no coincidían con las de ningún elemento conocido.

Otros científicos detectaron más tarde la huella de elementos terrestres en los espectros del Sol y las estrellas. Por otro lado, el elemento helio fue descubierto en el Sol en 1868, mucho antes de ser detectado en la Tierra. Estos estudios de los espectros demostraron finalmente que la materia que constituye el universo es en todas partes la misma.

El hallazgo más importante de Kirchhoff fue éste: que cuando un elemento es calentado hasta emitir luz de ciertas longitudes de onda, al enfriarse tiende a absorber esas mismas longitudes de onda.

El concepto de cuerpo negro

Un objeto que absorbiera toda la luz que incide sobre él no reflejaría ninguna y, por consiguiente, parecería negro. Un objeto de estas características cabría llamarlo «cuerpo negro».

¿Qué ocurriría al calentar hasta la incandescencia un cuerpo negro? Según el hallazgo de Kirchhoff debería emitir luz de todas las longitudes de onda posibles, pues con anterioridad las ha absorbido todas. Ahora bien, existen muchas más longitudes de onda en el extremo ultravioleta invisible del espectro electromagnético (el sistema de todas las posibles longitudes de onda) que en todo el espectro visible (las longitudes de onda que producen la luz visible). Por consiguiente, si un cuerpo negro es capaz de radiar luz de todas las longitudes de onda, la mayor parte de la luz provendría del extremo violeta y ultravioleta del espectro.

Lord Rayleigh, un físico inglés, halló en la última década del siglo pasado una ecuación basada en el comportamiento que se le atribuía por entonces a la luz. Sus resultados parecían demostrar que cuanto más corta era la longitud de onda, más luz debería emitirse. Las longitudes de onda más cortas de la luz estaban en el extremo violeta y ultravioleta del espectro, por lo cual la luz debería ser emitida por el cuerpo negro en un violento estallido de luz violeta y ultravioleta: una «catástrofe violeta».

Pero esa catástrofe violeta jamás había sido observada. ¿Por qué? Quizá porque ningún objeto ordinario absorbía realmente toda la luz incidente sobre él. De ser así, no podría llamarse cuerpo negro a ningún objeto, aunque los físicos trabajasen en la teoría con ese concepto. Quizá, si existiese realmente un verdadero cuerpo negro, podría observarse la catástrofe violeta.

Hacia la época en que Rayleigh estableció su ecuación, el físico alemán Wilhelm Wien creyó haber averiguado cómo fabricar un cuerpo negro. Para ello construyó una cámara provista de un pequeño agujero. Según él, la luz de cualquier longitud de onda, al entrar por el orificio, sería absorbida por las paredes rugosas de la cámara; y si parte de la luz era reflejada, chocaría contra otra de las paredes y sería absorbida allí.

Es decir, que una vez que la luz entraba en la cámara, no sobrevivía para salir de nuevo por el orificio. El agujero sería un absorbente total y actuaría por tanto como un verdadero cuerpo negro. Calentando entonces la cámara hasta poner el interior incandescente, la luz radiada hacia afuera a través del agujero sería radiación del cuerpo negro.

Por desgracia, la luz no radiaba en la forma de una catástrofe violenta. Wien estudió la radiación emergente y comprobó que se hacía más intensa al acortarse las longitudes de onda (tal y como predecía la ecuación de Rayleigh). Siempre había alguna longitud de onda para la cual la radiación alcanzaba intensidad máxima. Pero después, y a pesar de que la longitud de onda seguía decreciendo, disminuía la intensidad de la radiación. Cuanto más calentaba Wien la cámara, más corta era la longitud de onda a partir de la cual se iniciaba el descenso en la intensidad de radiación; pero en ningún caso se producía la catástrofe violeta.

Wien intentó hallar una ecuación que describiera cómo su «cuerpo negro» radiaba las longitudes de onda largas y cortas, pero los resultados fueron insatisfactorios.

El problema fue abordado en 1899 por otro físico alemán, Max Planck. Planck pensó que la luz quizá era radiada sólo en porciones discretas. Como no sabía qué tamaño podrían tener estas porciones, las llamó quanta (en singular quantum), que en latín significa «¿cuánto?».

Hasta entonces se creía que todas las formas de energía, entre ellas la luz, existían en cantidades tan pequeñas como uno quisiera imaginar. Lo que Planck sugería ahora era lo contrario, que la energía, al igual que la materia, existía exclusivamente en la forma de partículas de tamaño discreto y que no podían existir porciones de energía más pequeñas que lo que él llamó «cuantos». Los cuantos eran, por consiguiente, «paquetes» de energía, lo mismo que los átomos y las moléculas eran «paquetes» de materia.

Planck supuso además que el tamaño del cuanto de energía variaba con la longitud de onda de la luz: cuanto más corta la longitud de onda, más grande el cuanto. Aplicó esta idea al problema del cuerpo negro y supuso que éste radiaba ondas luminosas en la forma de cuantos. Al cuerpo negro le sería fácil reunir suficiente energía para formar cuantos pequeños; por eso, radiaría fácilmente longitudes de onda largas, que son las que requieren cuantos más modestos. Las longitudes de onda cortas, por el contrario, no podrían ser radiadas a menos que se acumularan cuantos mayores, que serían más difíciles de reunir.

Es como si nos encontráramos en unos grandes almacenes y nos dijeran que podíamos comprar lo que quisiéramos, con tal de pagar en monedas. Comprar un artículo de una peseta no plantearía problemas; pero en cambio sería gravoso (en los dos sentidos de la palabra) adquirir algo por valor de diez mil pesetas, porque lo más probable es que no pudiéramos acarrear el peso de tantas monedas.

Planck logró hallar una ecuación que describía la radiación del cuerpo negro en el lenguaje de los cuantos. La ecuación concordaba con la observación de Wien de que había una longitud de onda para la cual la radiación alcanzaba máxima intensidad. Para longitudes de ondas más cortas que ella, el cuerpo negro se las vería y desearía para producir los grandes cuantos que requería el caso.

Es cierto que calentando la cámara del cuerpo negro a temperaturas más altas habría más energía disponible, con lo cual se podrían producir longitudes de onda más cortas, compuestas de cuantos más grandes. Pero, aun así, siempre habría una longitud de onda que fuese demasiado corta, incluso para un cuerpo negro fuertemente calentado; y entonces sería imposible emitir los grandes cuantos que eran necesarios. Por consiguiente, nunca podría haber una catástrofe violeta, que sería como decir que siempre habría un artículo demasiado caro para la cantidad de monedas que pudiésemos acarrear.

La «teoría de los cuantos» o «teoría cuántica» de Planck fue publicada en 1900, y al principio no despertó demasiada expectación. Pero ésta se estaba ya gestando, porque los físicos empezaban ya por entonces a estudiar el peculiar comportamiento de las partículas menores que los átomos (partículas subatómicas).

Parte de este comportamiento era inexplicable con los conocimientos existentes. Por ejemplo, cuando la luz incidía sobre ciertos metales ¿por qué las partículas subatómicas llamadas «electrones» se comportaban como lo hacían? La luz era capaz de arrancar electrones de los átomos situados en la superficie del metal. Pero estos electrones sólo eran emitidos si la longitud de onda de la luz incidente era más corta que cierto valor, y este valor crítico dependía de la naturaleza del metal. ¿Cómo podía explicarse este fenómeno, llamado el «efecto fotoeléctrico»?

Albert Einstein halló en 1905 la explicación del efecto fotoeléctrico, y para ello utilizó la teoría cuántica. Según él, cuando sobre un metal incidían longitudes de onda largas, los cuantos de estas longitudes de onda eran demasiado pequeños para arrancar ningún electrón. Sin embargo, al decrecer cada vez más la longitud de onda, llegaba un momento en que los cuantos eran suficientemente grandes para llevarse por delante a los electrones.

Einstein explicó así por qué los electrones no salían despedidos de la superficie del metal hasta que la longitud de onda de la luz incidente era más corta que cierta magnitud crítica.

La solución al problema del efecto fotoeléctrico fue una gran victoria para la teoría cuántica, y tanto Einstein como Planck obtuvieron el Premio Nobel por su labor.

La teoría cuántica demostró de nuevo su valía en la investigación sobre la estructura del átomo. Los físicos estaban de acuerdo en que el átomo consistía en un núcleo central relativamente pesado, alrededor del cual se movían uno o más electrones en trayectorias circulares llamadas órbitas. Según las teorías físicas de la época, los electrones, al girar en su órbita, tenían que radiar luz, perder energía y precipitarse finalmente hacia el núcleo del átomo, cuando lo cierto es que los electrones giraban y giraban alrededor del núcleo sin chocar contra él. Era evidente que las teorías al uso no podían explicar el movimiento de los electrones.

En 1913, el físico danés Niels Bohr aplicó la teoría cuántica a la estructura atómica. Bohr afirmó que un electrón sólo podía emitir energía en cantidades fijas y discretas, es decir en cuantos enteros. Al emitir energía, el electrón ocupaba una nueva órbita, más próxima al núcleo del átomo. De manera análoga, el electrón sólo podía absorber cuantos enteros, ocupando entonces nuevas órbitas más alejadas del núcleo. El electrón no podía jamás precipitarse hacia el núcleo, porque nunca podría acercarse a él más allá de la órbita más cercana permitida por su estado de energía.

Soluciones y comprensión

Estudiando las distintas órbitas permitidas, los físicos lograron comprender por qué cada elemento radiaba sólo ciertas longitudes de onda luminosas y por qué la luz absorbida era siempre igual a la emitida. Así quedó explicada, por fin, la regla de Kirchhoff, que era la que había desencadenado toda esta revolución.

Posteriormente, el físico austríaco Erwin Schrödinger elaboró en 1927 las matemáticas del átomo en el marco de la mecánica cuántica. La explicación de Schrödinger tenía en cuenta prácticamente todos los aspectos del estudio del átomo, y su trabajo fue crucial para la investigación atómica. Sin él sería imposible entender siquiera cómo el átomo almacena y libera la energía.

La mecánica cuántica es hoy tan importante que el nacimiento de la física moderna se sitúa en 1900, cuando Planck publicó la teoría cuántica. La física anterior a 1900 se llama física clásica. La idea de Planck, que en sí es relativamente simple, logró cambiar por completo el rumbo de la ciencia de la materia y del movimiento.

12. Hipócrates y la Medicina

¡Qué maravilloso es el milagro de la vida y qué asombrosas son las cosas vivientes! La planta más minúscula, el animal más ínfimo parece más complejo e interesante que la masa más grande de materia inerte que podamos imaginar.

Porque, a fin de cuentas, la materia inerte no parece hacer nada la mayor parte del tiempo. O si hace algo, actúa de un modo mecánico y poco interesante. Pensemos en una piedra que yace en el camino. Si nada la molesta, seguirá allí por los siglos de los siglos. Si le damos una patada, se moverá y volverá a detenerse. Le damos más fuerte y se alejará un poco más. Si la tiramos al aire, describirá una curva de forma determinada y caerá. Y si la golpeamos con un martillo, se romperá.

Con algo de experiencia es posible predecir exactamente lo que le ocurrirá a la piedra en cualquier circunstancia. Uno puede describir sus avatares en términos de causa y efecto. Si se hace tal cosa con la piedra (causa), le ocurrirá tal otra (efecto). La creencia de que iguales causas obran más o menos los mismos efectos en todas las ocasiones conduce a la visión del universo que llamamos «mecanicismo» (véase el capítulo 8).

Un universo predecible

Incluso algo tan notable como el Sol parece salir mecánicamente todas las mañanas y ponerse mecánicamente todas las noches. Si uno lo observa con atención, aprenderá a predecir exactamente la hora a que sale y se pone todos los días del año y la trayectoria exacta que recorre en el cielo. Los antiguos hallaron reglas para predecir el movimiento del Sol y de los demás cuerpos celestes, y esas reglas jamás han sido infringidas.

El filósofo griego Tales y sus discípulos afirmaron hacia el año 600 a. C. que la «ley natural» de la causa y el efecto era todo cuanto hacía falta para comprender la naturaleza (véase el capítulo 1), y esa ley natural hacía innecesario suponer que el universo estaba regido por espíritus y demonios.

Pero ¿y los seres vivos? ¿Era válida para ellos la ley natural? ¿Acaso no se regían por sí mismos, desviándose a menudo de la ley de la causa y el efecto?

Un resultado incierto

Imaginemos que damos un empujón a un amigo. Puede ser que el pobre se caiga, o también que logre conservar el equilibrio. A renglón seguido puede que lo eche a risa, o que se acuerde de nuestros antepasados, que nos devuelva el empujón o incluso que trate de ponernos la mano encima. Pero cabe también que no haga nada, o que se vaya y nos la guarde. Dicho de otra manera, un ser viviente puede responder a una causa concreta con toda una serie de efectos. La idea de que el mundo vivo no obedece las reglas que gobiernan el mundo inanimado se llama «vitalismo».

Por otro lado, está el hecho de que hay personas que poseen aptitudes poco usuales. ¿Por qué unos saben escribir admirablemente poesía y otros no? ¿Por qué hay personas que son líderes habilísimos, o buenos oradores, o indómitos luchadores, mientras que otros no?

Frente a esto se alza otro hecho, y es que todos los hombres parecen iguales en lo fundamental. Todos tienen brazos y piernas, oídos y ojos, corazones y cerebros. ¿Qué es entonces lo que marca la diferencia entre el hombre común y el excepcional?

Los antiguos pensaban que un hombre podía salirse de lo común si estaba protegido por algún espíritu personal o ángel de la guarda. Los griegos llamaban a esos espíritus daimon, que es la raíz de la palabra «demonio». Y de alguien que trabaja infatigablemente seguimos diciendo hoy que trabaja «como un demonio».

La palabra «entusiasta», por seguir con los ejemplos, proviene de otra palabra griega que significa «poseído por un dios»; de alguien que realiza una gran obra se dice que está «inspirado», término que proviene de un verbo latino que significa «tomar aire», es decir meter dentro de uno un espíritu invisible; y la palabra «genio» se deriva de la versión latina del término griego daimon.

Como es lógico, se creía que estos espíritus y demonios trabajaban tanto para el mal como para el bien de los hombres. Cuando un hombre enfermaba, los antiguos decían que estaba poseído por un espíritu maligno, y la idea parecía especialmente certera cuando el afectado hacía y decía cosas incoherentes. Como nadie. actuaría así por propia voluntad, la gente lo atribuía al «demonio que llevaba dentro». Por eso, las sociedades primitivas trataban a veces al enfermo mental con sumo respeto y cuidado. El loco era alguien que había sido tocado por el dedo de un ser sobrenatural (y hoy seguimos utilizando la palabra «tocado» para describir a un individuo que parece no estar en sus cabales).

El «mal sagrado»

La epilepsia, que hoy sabemos que es un trastorno del cerebro, era atribuida también a la acción de un espíritu. La persona que lo sufre pierde de vez en cuando el control de su cuerpo durante algunos minutos, cayéndose al suelo, mostrando convulsiones, etc. Después recuerda muy poco de lo ocurrido. Antiguamente la gente estaba convencida de que veía entrar un demonio en el cuerpo de la persona afectada y que era él el que lo agitaba; los griegos llamaban por eso el «mal sagrado» a la epilepsia.

Mientras la manera de clasificar esta enfermedad fue tan poco científica, el método de tratamiento no podía tener otro carácter. La terapia indicada consistía en ahuyentar o exorcizar a los demonios. Las tribus primitivas siguen teniendo «brujos» y curanderos que lanzan conjuros y ejecutan ritos para que los espíritus malignos salgan de la persona enferma. Y la gente cree realmente que el enfermo sanará en el momento en que sean expulsados los malos espíritus.

El dios griego de la Medicina se llamaba Asclepio, y los sacerdotes de Asclepio eran médicos. Uno de los templos más importantes de este dios estaba en la isla de Cos, en el Mar Egeo (frente a la costa occidental de la actual Turquía). Hacia el año 400 a. C. el médico más importante en la isla de Cos era un hombre llamado Hipócrates.

Hipócrates tenía una manera de ver las cosas que era nueva para los griegos, pues creía que lo que había que hacer era tratar al paciente, y no preocuparse del demonio que hubiera o dejara de haber dentro de él. Hipócrates no fue el primero en pensar así, pues las viejas civilizaciones de Babilonia y Egipto tuvieron muchos médicos que defendían esta actitud, y dice la leyenda que Hipócrates estudió en Egipto. Pero es la obra de Hipócrates la que ha sobrevivido y su nombre el que se recuerda.

Una escuela sensata

Hipócrates fundó una escuela que pervivió durante siglos. Los doctores de esta tradición utilizaban el sentido común al tratar a los pacientes. Carecían de medicinas, instrumental y teorías modernas, pero tenían sentido común y buenas dotes de observación.

Los discípulos de Hipócrates estaban convencidos de la importancia de la limpieza, tanto en el paciente como en ellos mismos, los médicos. Eran partidarios de que el enfermo gozara de aire fresco, de un entorno agradable y tranquilo y de una dieta equilibrada a base de alimentos sencillos. Se atenían a reglas de sentido común para cortar hemorragias, limpiar y tratar las heridas, reducir fracturas e intervenciones análogas, evitando cualquier extremo y prescindiendo de ritos mágicos.

Los escritos de toda la escuela hipocrática están reunidos, sin distinción de autores, en el Corpus Hippocraticum, y es imposible saber a ciencia cierta quién escribió cada parte y cuándo. La más conocida es un juramento que tenían que prestar todos los médicos de la escuela para ingresar en la profesión y que, por defender los ideales más altos de la práctica médica, sigue utilizándose hoy como guía profesional: en algunos lugares los estudiantes de Medicina lo pronuncian al licenciarse. Sin embargo, el «Juramento hipocrático» no fue escrito por Hipócrates; la hipótesis más verosímil es que entró en uso hacia el año 200 d. C., seis siglos después de Hipócrates.

De entre los escritos hipocráticos hay un tratado que figura entre los más antiguos del Corpus y que muy probablemente es del propio Hipócrates. Se titula «Sobre el mal sagrado» y versa sobre la epilepsia.

Los demonios expulsados

Este tratado mantiene con vehemencia la inutilidad de atribuir la enfermedad a los demonios. Cada enfermedad tiene su causa natural, y compete al médico descubrirla. Conocida la causa, puede hallarse el remedio. Y esto es incluso cierto -así lo afirma el tratado- para ese mal misterioso y aterrador que se llama epilepsia. No es de ningún modo un mal sagrado, sino una enfermedad como cualquier otra.

Lo que en resumidas cuentas defiende el tratado es que la idea de causa y efecto se aplica también a las cosas vivientes, entre ellas el hombre. Como el mundo de lo vivo es tan complejo, puede que no sea fácil detectar las relaciones de causa y efecto; pero al final puede y debe hacerse.

La Medicina tuvo que luchar durante muchos siglos contra la creencia común en demonios y malos espíritus y contra el uso de ritos y conjuros mágicos con fines terapéuticos. Pero las ideas de Hipócrates no cayeron jamás en el olvido.

La doctrina de Hipócrates sobre el tratamiento de los enfermos le ha valido el nombre de «padre de la Medicina». En realidad es más que eso, pues aplicó la noción de ley natural a los seres vivos y dio así el primer gran paso contra el vitalismo. Desde el momento en que se aplicó la ley natural a la vida, los científicos pudieron empezar a estudiarla sistemáticamente. Por eso, las ideas de Hipócrates abrieron la posibilidad de una ciencia de la vida (biología), lo cual le hace acreedor a un segundo título, el de «padre de la biología».

13. Wöhler y la química orgánica

El joven químico, alemán Friedrich Wöhler sabía en 1828 qué era exactamente lo que le interesaba: estudiar los metales y minerales. Estas sustancias pertenecían a un campo, la química inorgánica, que se ocupaba de compuestos que supuestamente nada tenían que ver con la vida. Frente a ella estaba la química orgánica, que estudiaba aquellas sustancias químicas que se formaban en los tejidos de las plantas y animales vivos.

El maestro de Wöhler, el químico sueco Jöns J. Berzelius, había dividido la química en estos dos compartimentos y afirmado que las sustancias orgánicas no podían formarse a partir de sustancias inorgánicas en el laboratorio. Sólo podían formarse en los tejidos vivos, porque requerían la presencia de una «fuerza vital».

El enfoque vitalista

Berzelius, como vemos, era vitalista, partidario del «vitalismo» (véase el capítulo 12). Creía que la materia viva obedecía a leyes naturales distintas de las que regían sobre la materia inerte. Más de dos mil años antes, Hipócrates había sugerido que las leyes que regulaban ambos tipos de materia eran las mismas. Pero la idea seguía siendo difícil de digerir, porque los tejidos vivos eran muy complejos y sus funciones no eran fáciles de comprender. Muchos químicos estaban por eso convencidos de que los métodos elementales del laboratorio jamás servirían para estudiar las complejas sustancias de los organismos vivos.

Wöhler trabajaba, como decimos, con sustancias inorgánicas, sin imaginarse para nada que estaba a punto de revolucionar el campo de la química orgánica. Todo comenzó con una sustancia inorgánica llamada cianato amónico, que al calentarlo se convertía en otra sustancia. Para identificarla, Wöhler estudió sus propiedades, y tras eliminar un factor tras otro comenzó a subir de punto su estupor.

Wöhler, no queriendo dejar nada en manos del azar, repitió una y otra vez el experimento; el resultado era siempre el mismo. El cianato amónico, una sustancia inorgánica, se había transformado en urea, que era un conocido compuesto orgánico. Wöhler había hecho algo que Berzelius tenía por imposible: obtener una sustancia orgánica a partir de otra inorgánica con sólo calentarla.

El revolucionario descubrimiento de Wöhler fue una revelación; muchos otros químicos trataron de emularle y obtener compuestos orgánicos a partir de inorgánicos. El químico francés Pierre E. Berthelot formó docenas de tales compuestos en los años cincuenta del siglo pasado, al tiempo que el inglés William H. Perkin obtenía una sustancia cuyas propiedades se parecían a las de los compuestos orgánicos pero que no se daba en el reino de lo viviente. Y luego siguieron miles y miles de otros compuestos orgánicos sintéticos.

Los químicos estaban ahora en condiciones de preparar compuestos que la naturaleza sólo fabricaba en los tejidos vivos. Y además eran capaces de formar otros, de la misma clase, que los tejidos vivos ni siquiera producían.

Todos estos hechos no lograron, sin embargo, acabar con las explicaciones vitalistas. Podía ser que los químicos fuesen capaces de sintetizar sustancias formadas por los tejidos vivos -replicaron los partidarios del vitalismo-, pero cualitativamente era diferente el proceso. El tejido vivo formaba esas sustancias en condiciones de suave temperatura y a base de componentes muy delicados, mientras que los químicos tenían que utilizar mucho calor o altas presiones o bien reactivos muy fuertes.

Ahora bien, los químicos sabían cómo provocar, a la temperatura ambiente, reacciones que de ordinario sólo ocurrían con gran aporte de calor. El truco consistía en utilizar un catalizador. El polvo de platino, por ejemplo, hacía que el hidrógeno explotara en llamas al mezclarse con el aire. Sin el platino era necesario aportar calor para iniciar la reacción.

Catalizadores de la vida

Parecía claro, por tanto, que los tejidos vivos tenían que contener catalizadores, pero de un tipo distinto de los que conocía hasta entonces el hombre. Los catalizadores de los tejidos vivos eran en extremo eficientes: una porción minúscula propiciaba una gran reacción. Y también eran harto selectivos: su presencia facilitaba la transformación de ciertas sustancias, pero no afectaba para nada a otras muy similares.

Por otro lado, los biocatalizadores eran muy fáciles de inactivar. El calor, las sustancias químicas potentes o pequeñas cantidades de ciertos metales detenían su acción, normalmente para bien del organismo.

Estos catalizadores de la vida se llamaban «fermentos», y el ejemplo más conocido eran los que se contenían en las diminutas células de la levadura. Desde los albores de la historia, el hombre había utilizado fermentos para obtener vino del jugo de fruta y para fabricar pan blando y esponjoso a partir de la masa plana.

En 1752, el científico francés René A. F. de Réaumur extrajo jugos gástricos de un halcón y demostró que eran capaces de disolver la carne. Pero ¿cómo? Porque los jugos no eran, de suyo, materia viva.

Los químicos se encogieron de hombros. La respuesta parecía cosa de niños: había dos clases de fermentos. Los unos actuaban fuera de las células vivas para digerir el alimento y eran fermentos «no formes» o «desorganizados». Los otros eran fermentos «organizados» o «formes», que sólo podían actuar dentro de las células vivas. Los fermentos de la levadura, que descomponían los azúcares y almidones para formar vino o hinchar el pan, eran ejemplos de fermentos formes.

Hacia mediados de la década de 1800-1810 estaba ya desacreditado el vitalismo de viejo cuño, gracias al trabajo de Wöhler y sus sucesores. Pero en su lugar había surgido una forma nueva de la misma idea. Los nuevos vitalistas afirmaban que los procesos de la vida podían operarse únicamente como resultado de la acción de fermentos organizados, que sólo se daban dentro de las células vivas. Y sostenían que los fermentos organizados eran de suyo la «fuerza vital».

Wilhelm Kühne, otro químico alemán, insistió en 1876 en no llamar fermentos desorganizados a los jugos digestivos. La palabra «fermento» estaba tan asociada a la vida, que podría comunicar la falsa impresión de estar ocurriendo un proceso vivo fuera de las células. Kühne propuso decir que los jugos digestivos contenían enzimas. La palabra «enzima», que proviene de otra griega que significa «en la levadura», parecía apropiada, porque los jugos gástricos se comportaban hasta cierto punto como los fermentos de la levadura.

El fin del vitalismo

Era preciso poner a prueba el nuevo vitalismo. Si los fermentos actuaban sólo en las células vivas, entonces cualquier cosa que matara la célula debería destruir el fermento. Claro que, al matar las células de levadura, dejaban de fermentar. Pero podía ser que no hubiesen sido bien matadas. Normalmente se utilizaba con este fin el calor o sustancias químicas potentes. ¿Podrían sustituirse por otra cosa?

Fue a Eduard Buchner, un químico alemán, a quien se le ocurrió matar las células de levadura triturándolas con arena. Las finas y duras partículas de sílice rompían las diminutas células y las destruían; pero los fermentos contenidos en su interior quedaban a salvo del calor y de los productos químicos. ¿Quedarían, aun así, destruidos?

En 1896 Buchner molió levadura y la filtró. Estudió los jugos al microscopio y se cercioró de que no quedaba ni una sola célula viva; no era más que jugo «muerto». Luego añadió una solución de azúcar. Inmediatamente empezaron a desprenderse burbujas de anhídrido carbónico y el azúcar se convirtió lentamente en alcohol.

Los químicos sabían ahora que el jugo «muerto» era capaz de llevar a cabo un proceso que antes pensaban era imposible fuera de las células vivas. Esta vez el vitalismo quedó realmente triturado. Todos los fermentos, dentro y fuera de la célula, eran iguales. El término «enzima», que Kühne había utilizado sólo para fermentos fuera de la célula, fue aplicado a todos los fermentos sin distinción.

Así pues, a principios del siglo XX la mayoría de los químicos habían llegado a la conclusión de que dentro de las células vivas no había fuerzas misteriosas. Todos los procesos que tenían lugar en los tejidos eran ejecutados por medio de sustancias químicas ordinarias, con las que se podría trabajar en tubos de ensayo si se utilizaban métodos de laboratorio suficientemente finos.

Aislar una enzima

Quedaba aún por determinar exactamente la composición química de las enzimas; el problema era que éstas se hallaban presentes en trazas tan pequeñas que era casi imposible aislarlas e identificarlas.

El bioquímico norteamericano James B. Sumner mostró en 1926 el camino a seguir. Sumner estaba trabajando con una enzima que se hallaba presente en el jugo de judías sable trituradas. Aisló los cristales formados en el jugo y comprobó que, en solución, producían una reacción enzimática muy activa. Cualquier cosa que destruía la estructura molecular de los cristales, destruía también la reacción enzimática, y además Sumner fue incapaz de separar la acción enzimática, por un lado, y los cristales, por otro.

Finalmente llegó a la conclusión de que los cristales eran la enzima buscada, la primera que se obtenía de forma claramente visible. Pruebas ulteriores demostraron que los cristales consistían en una proteína, la ureasa. Desde entonces se han cristalizado en el laboratorio muchas enzimas, y todas, sin excepción, han resultado ser de naturaleza proteica.

Una sarta de ácidos

Las proteínas tienen una estructura molecular que no encierra ya ningún misterio hoy día. En el siglo XIX se comprobó que consistían en veinte clases diferentes de unidades menores llamadas «aminoácidos», y el químico alemán Emil Fischer mostró en 1907 cómo estaban encadenados entre sí los aminoácidos en la molécula de proteína.

Después, ya en los años cincuenta y sesenta, varios químicos, entre los que destaca el inglés Frederick Sanger, lograron descomponer moléculas de proteína y determinar exactamente qué aminoácidos ocupaban cada lugar de la cadena. Y, por otro lado, se consiguió también sintetizar artificialmente en el laboratorio moléculas sencillas de proteína.

Así es como más de un siglo y medio de infatigable labor científica vino a dar la razón a Hipócrates y a su doctrina no vitalista. Esta búsqueda de la verdad desveló los procesos vitales de la célula y demostró que los componentes celulares son sustancias químicas, no «fermentos» ni otras fuerzas vitalistas. Desde Wöhler a Sanger, los científicos han demostrado que las leyes naturales del universo gobiernan tanto la materia viva como la inerte.

14. Linneo y la clasificación

La mente científica más influyente en la historia del mundo quizá haya sido la del filósofo griego Aristóteles (384 a. C. – 322 a. C).

Aristóteles fue probablemente el alumno más famoso de la Academia de Platón en Atenas. Algunos años después de morir éste en el año 347 a. C, Aristóteles marchó al reino de Macedonia, en el norte de Grecia, donde su padre había sido médico de la corte. Allí fue durante varios años tutor del joven príncipe macedonio Alejandro, que más tarde recibiría el título de Magno.

Cuando Alejandro partió para iniciar su carrera de conquistas, Aristóteles regresó a Atenas y fundó su propia escuela. Sus enseñanzas fueron compiladas en lo que casi es una enciclopedia del saber antiguo, escrita por un solo hombre. Muchos de estos libros sobrevivieron y fueron considerados, durante casi dos mil años, como la última palabra en el pensamiento científico.

Influyente, pero equivocado

La influencia de las ideas de Aristóteles sobre los científicos posteriores no fue nada desdeñable, en particular sus teorías sobre la naturaleza del universo, el movimiento de los cuerpos, etc. (véanse los capítulos 4 y 7). Pero lo cierto es que en el campo de la ciencia física estaba, por lo general, equivocado.

Paradójicamente, sus ideas acerca de temas biológicos, que eran uno de sus puntos fuertes, ejercieron menos influencia. La ciencia natural era su campo preferido, y dedicó años al estudio de los animales marinos.

Aristóteles no se conformó con contemplar los animales y describirlos. Ayudado por su claridad de ideas y su amor por el orden, fue más lejos y clasificó los animales en grupos. Esa clasificación se llama hoy «taxonomía», que en griego significa «sistema de ordenación».

Todo el mundo tiene cierta tendencia a clasificar las cosas. Salta a la vista que los leones y los tigres se parecen bastante, que las ovejas se parecen a las cabras y que las moscas se parecen a los tábanos. Aristóteles, sin embargo, no se conformó con observaciones casuales, sino que hizo una lista de más de quinientos tipos diferentes de animales y los agrupó cuidadosamente en clases. Y además, colocó estas clases en orden, desde las más simples a las más complicadas.

Aristóteles observó que algunos animales no pertenecían a la clase a la que parecían asemejarse más. Casi todo el mundo daba por supuesto, por ejemplo, que el delfín era un pez: vivía en el agua y tenía la misma forma que los peces. Aristóteles, por el contrario, observó que el delfín respiraba aire, paría crías vivas y nutría al feto mediante un órgano llamado «placenta». El delfín se parecía en estos aspectos a las bestias cuadrúpedas de tierra firme, por lo cual lo incluyó entre los mamíferos, y no entre los peces.

Los naturalistas ignoraron esta conclusión, absolutamente correcta, durante dos mil años. Aristóteles parecía predestinado a ser creído cuando se equivocaba y descreído cuando tenía razón.

Los naturalistas que vinieron después de Aristóteles no prolongaron su labor clasificatoria de los animales. Los libros antiguos y medievales que describen animales los colocan en cualquier orden e ignoran la posibilidad de agrupar los de estructuras similares.

Los primeros intentos de clasificación después de Aristóteles no vinieron hasta principios del siglo XVI, y tampoco destacaron precisamente por su rigor. Algunos autores agrupaban juntas todas las plantas que tenían hojas estrechas, mientras que otros se atenían al criterio de que tuvieran grandes flores amarillas, por ejemplo.

El primer naturalista que hizo una labor tan meticulosa como la de Aristóteles fue el inglés John Ray. Ray viajó por Europa y estudió la fauna y la flora; y durante los treinta y cinco años que siguieron a 1667 publicó libros que describían y clasificaban las plantas y animales que había estudiado.

Comenzó por clasificar los mamíferos en dos grandes grupos: los que tenían dedos y los que tenían pezuñas; luego subdividió estas clasificaciones según el número de dedos o pezuñas, según que los dedos estuvieran armados de uñas o garras y según que un animal con pezuñas tuviera cornamenta perenne o caduca. Ray, digámoslo de una vez, restauró el sentido del orden que Aristóteles había introducido en el reino de la vida.

Una vez que Ray señaló el camino, los naturalistas no tardaron en ir más allá de Aristóteles. El joven naturalista sueco Carl von Linné publicó en 1735 un opúsculo en el que alistaba diferentes criaturas según un sistema de su invención. (Hoy se le conoce más por la versión castellanizada de su nombre, que es Linneo, o por la latina, Carolus Linnaeus.) Su trabajo estaba basado en viajes intensivos por toda Europa, incluido el norte de Escandinavia, que hasta entonces no había sido bien explorado.

Linneo describía breve y claramente cada clase o especie de planta y animal, agrupaba luego cada colección de especies similares en un género y daba finalmente a cada clase de planta o animal dos nombres latinos: el del género y el de la especie.

Un ejemplo: el gato y el león son dos especies muy parecidas, pese a que el segundo es mucho más grande y fiero que el primero; de ahí que ambos pertenezcan al mismo género, Felis (que en latín es «gato»). El segundo nombre latino sirve para distinguir el gato común del león y de otras especies del mismo género. Así, el gato es Felis domesticus, mientras que el león es Felis leo.

Análogamente, el perro y el lobo pertenecen al género Canis («perro»). El perro es Canis familiaris y el lobo Canis lupus.

Linneo dio también a los seres humanos un nombre latino. Al hombre lo colocó en el género Homo y a la especie humana la llamó Homo sapiens («hombre sabio»).

El sistema de Linneo se conoce por «nomenclatura binaria», y en realidad es muy parecido al que utilizamos para identificarnos por nombre y apellido. Dentro de una familia todos llevan el mismo apellido, pero nombres diferentes. Un hermano figurará en la guía telefónica como «García, Juan», y otro como «García, Pedro».

La labor de Linneo fue enormemente útil. Por primera vez los naturalistas de todo el mundo tenían un sistema común de denominaciones para identificar las distintas criaturas. Cuando un naturalista hablaba de Canis lupus, los demás sabían inmediatamente que se refería al lobo. Para nada importaban sus respectivas lenguas maternas ni qué nombre local tuviese el lobo en cada una de ellas. Además, sabían inmediatamente que sé refería a una clase particular de lobo, el lobo gris europeo. El americano, por ejemplo, era una especie diferente, Canis occidentalis.

Este sistema común de identificación supuso un avance muy importante. A medida que el hombre exploró la tierra y descubrió continentes fue hallando cada vez más animales. Aristóteles había registrado unos quinientos solamente, mientras que en tiempos de Linneo se conocían ya decenas de miles.

El libro de Linneo sobre la clasificación animal tenía sólo siete páginas en su primera edición; en la décima se había hinchado ya hasta las 2.500. Si los naturalistas no hubiesen adoptado un sistema de clasificación normalizado, no podrían haber estado nunca seguros de qué plantas o animales estaban estudiando los demás. El estudio de la historia natural se habría sumido en el caos.

De la clasificación por géneros y especies Linneo pasó a agrupar géneros similares en órdenes, y órdenes semejantes en clases. Linneo distinguió seis clases diferentes de animales: mamíferos, aves, reptiles, peces, insectos y gusanos.

La labor de Linneo fue proseguida por el biólogo francés Georges Cuvier. Cuvier vio que las cuatro primeras clases -mamíferos, aves, reptiles y peces- eran todas ellas vertebradas, es decir que tenían esqueletos óseos internos. A estos animales los agrupó en una clasificación aún más amplia llamada «phylum» en latín («phyla» en plural) y filum o filo en castellano.

Cuvier hizo avanzar la taxonomía en otra dirección más. Los naturalistas comenzaron a estudiar hacia el año 1800 lo que ellos llamaron «fósiles», es decir minerales con restos o huellas petrificadas de lo que parecían haber sido seres vivos. Cuvier advirtió que aunque los fósiles no se parecían demasiado a ninguna especie existente a la sazón, encajaban de algún modo en el esquema taxonómico.

Así, cuando Cuvier estudió un fósil que tenía todas las características del esqueleto de un reptil, concluyó que el animal había sido en su tiempo un miembro de la clase de los reptiles. Por su esqueleto podía afirmarse también que había poseído alas. Cuvier identificó así el primer ejemplar de un grupo extinto de reptiles voladores. Debido a que cada una de las alas iba soportada por un solo hueso largo, como los de los dedos, bautizó a la criatura con el nombre de «pterodáctilo» («ala-dedo»).

El camino a la evolución

Los discípulos y seguidores de Cuvier continuaron perfeccionando este sistema de clasificación. Linneo había agrupado a menudo los animales por su aspecto exterior. Los seguidores de Cuvier, por el contrario, comenzaron a utilizar como criterio las estructuras internas, que eran más importantes para fines de agrupamiento.

Hacia 1805 existía ya un sistema para clasificar todos los seres vivos, completando finalmente la labor que hacía tanto tiempo iniciara Aristóteles. Toda criatura, viva o extinguida, podía colocarse en una categoría concreta. Cabía quizás disentir acerca de detalles menores, pero el plan general fue aceptado por todo el mundo.

El desarrollo de la taxonomía hizo pensar a los naturalistas. El hecho de que la vida pudiera clasificarse de manera tan limpia y elegante indicaba que tenía que haber ciertos principios biológicos que valieran para todas las criaturas, por diferentes que parecieran.

La clasificación de la vida dio así lugar a la idea de que todos los seres vivientes estaban inmersos en un mismo y único fenómeno. Y este concepto conduciría, a su vez, a una de las indiscutiblemente «grandes ideas de la ciencia»: la evolución de las especies (véase el capítulo siguiente).

15. Darwin y la evolución

El ser un león o un gato o una rosa lleva consigo algo especial, algo que ningún otro animal o planta comparte con él. Cada uno de ellos es una especie única de vegetal o animal. Sólo los leones pueden parir cachorros de león, solamente los gatos pueden tener garitos, y únicamente de semillas de rosa -y no de clavel- pueden salir rosas.

Aun así, es posible que dos especies diferentes muestren semejanzas. Los leones se parecen mucho a los tigres, y los chacales a los coyotes, a pesar de que los leones sólo engendran leones y no tigres, y los chacales sólo paren chacales y no coyotes.

Y es que el reino entero de la vida puede organizarse convenientemente en grupos de criaturas semejantes (véase el capítulo 14). Cuando los científicos se percataron por primera vez de esto, muchos pensaron que no podía ser pura coincidencia. Dos especies parecidas ¿lo eran porque algunos miembros de una de ellas habían pasado a formar parte de la otra? ¿No sería que se parecían porque ambas estaban íntimamente relacionadas?

Algunos filósofos griegos habían sugerido la posibilidad de una relación entre las especies, pero la idea parecía por entonces demasiado descabellada y no tuvo ningún eco. Parecía inverosímil que algunos leones se hubiesen convertido en tigres, o viceversa, o que alguna criatura felina hubiese engendrado tanto tigres como leones. Nadie había visto jamás una cosa semejante; de haber sucedido, tenía que haber sido un proceso muy lento.

La mayoría de la gente creía, a principios de los tiempos modernos, que la Tierra tenía solamente unos seis mil años de edad: un tiempo absolutamente insuficiente para que las especies cambiaran de naturaleza. La idea fue rechazada por absurda.

Pero ¿era verdad que la Tierra sólo tenía seis mil años de edad? Los científicos que estudiaban a principios del siglo XVIII la estructura de las capas rocosas de la corteza terrestre empezaron a sospechar que esos estratos sólo podrían haberse formado al cabo de períodos muy largos de tiempo. Y hacia 1760 el naturalista francés Georges de Buffon osó sugerir que la Tierra podía tener hasta setenta y cinco mil años.

Algunos años después, en 1785, el médico escocés James Hutton llevó las cosas un poco más lejos. Hutton, que había adoptado su afición a los minerales como ocupación central de su vida, publicó un libro titulado la Teoría de la Tierra, donde reunía abundantes datos y sólidos argumentos que demostraban que nuestro planeta podía tener en realidad muchos millones de años de edad. Hutton afirmó sin ambages que no veía signos de ningún origen.

La puerta se abre

Por primera vez parecía posible hablar de la evolución de la vida. Si la Tierra tenía millones de años, había habido tiempo de sobra para que animales y plantas se hubiesen transformado lentamente en nuevas especies, tan lentamente que el hombre, en los pocos miles de años de existencia civilizada, no podía haber notado esa evolución.

Pero ¿por qué iban a cambiar las especies? ¿Y por qué en una dirección y no en otra? La primera persona que intentó contestar a esta pregunta fue el naturalista francés Jean Baptiste de Lamarck.

En 1809 presentó Lamarck su teoría de la evolución en un libro titulado Filosofía zoológica. La teoría sugería que las criaturas cambiaban porque intentaban cambiar, sin que necesariamente supiesen lo que hacían.

Según Lamarck, un antílope que se alimentara de hojas de árbol estiraría el cuello hacia arriba con todas sus fuerzas para alcanzar la máxima cantidad de pasto; y junto con el cuello estiraría también la lengua y las patas. Este estiramiento, mantenido a lo largo de toda la vida, haría que las patas, el cuello y la lengua se alargaran ligeramente.

Las crías que nacieran de este antílope heredarían este alargamiento de las proporciones corporales. La descendencia alargaría aún más el cuerpo por un proceso idéntico de estiramiento, de manera que, poco a poco, a lo largo de miles de años, el proceso llegaría a un punto en que el linaje de los antílopes se convirtiese en una nueva especie: la jirafa.

La teoría de Lamarck se basaba en el concepto de la herencia de caracteres adquiridos: los cambios que se operaban en el cuerpo de una criatura a lo largo de su vida pasaban a la descendencia. Lo malo es que la idea carecía por completo de apoyo empírico. Y cuando fue investigada se vio cada vez más claramente que no podía ser cierta. La doctrina de Lamarck tuvo que ser abandonada.

En 1831, un joven naturalista inglés llamado Charles Darwin se enroló en un barco fletado para explorar el mundo. Poco antes de zarpar había leído un libro de geología escrito por otro súbdito inglés, Charles Lyell, donde éste comentaba y explicaba las teorías de Hutton sobre la edad de la Tierra. Darwin quedó impresionado.

El periplo por costas remotas y las escalas en islas poco menos que inexploradas dieron a Darwin la oportunidad de estudiar especies aún desconocidas por los europeos. Especial interés despertó en él la vida animal de las Islas Galápagos, situadas en el Pacífico, a unos mil kilómetros de la costa de Ecuador.

Darwin observó catorce especies diferentes de pinzones en estas remotas islas. Todas ellas diferían ligeramente de las demás y también de los pinzones que vivían en la costa sudamericana. El pico de algunos de los pinzones estaba bien diseñado para comer pequeñas semillas; el de otros, para partir semillas grandes; una tercera especie estaba armada de un pico idóneo para comer insectos; y así sucesivamente.

Darwin intuyó que todos estos pinzones tenían su origen en un antepasado común. ¿Qué les había hecho cambiar? La idea que se le ocurrió era la siguiente: podía ser que algunos de ellos hubiesen nacido con ligeras modificaciones en el pico y que hubieran transmitido luego estas características innatas a la descendencia. Darwin, sin embargo, seguía albergando sus dudas, porque esos cambios accidentales ¿serían suficientes para explicar la evolución de diferentes especies?

En 1838 halló una posible solución en el libro titulado Un ensayo sobre el principio de población, publicado en 1798 por el clérigo inglés Thomas R. Malthus. Malthus mantenía allí que la población humana aumentaba siempre más deprisa que sus recursos alimenticios. Por consiguiente, el número de habitantes se vería reducido en último término por el hambre, si es que no por enfermedades o guerras.

El estilo de la Naturaleza

A Darwin le impresionaron los argumentos de Malthus, porque le hicieron ver la potentísima fuerza que podía ejercer la Naturaleza, no sólo sobre la población humana, sino sobre la población de cualquier especie.

Muchas criaturas se multiplican con gran prodigalidad, pero de la descendencia sobrevive sólo una proporción pequeña. A Darwin se le ocurrió que, hablando en términos generales, sólo salían adelante aquellos individuos que eran más eficientes en un aspecto u otro. Entre los pinzones, por poner un caso, sólo sobrevivirían aquéllos que nacieran con picos ligeramente más robustos, por ser más capaces de triturar semillas duras. Y aquellos otros que fuesen capaces de digerir de cuando en cuando un insecto tendrían probabilidades aún mayores de sobrevivir.

Generación tras generación, los pinzones que fuesen ligeramente más eficientes en cualquier aspecto sobrevivirían a expensas de los menos eficaces. Y como esa eficiencia podía darse en terrenos muy diversos, al final habría toda una serie de especies muy diferentes, cada una de ellas especializada en una función distinta.

Darwin creyó justificado afirmar que este proceso de selección natural valía, no sólo para los pinzones, sino para todas las criaturas. La selección natural determinaba qué individuos debían sobrevivir, a costa de dejar morir de hambre a aquellos otros que no gozaban de ningún rasgo de superioridad.

Darwin trabajó en su teoría de la selección natural durante años. Finalmente vertió en 1859 sus ideas en un libro titulado: Sobre el origen de las especies por medio de la selección natural, o la preservación de las razas favorecidas en la lucha por la vida.

Las ideas de Darwin levantaron al principio enconadas polémicas; pero la cantidad de evidencia acumulada a lo largo de los años ha confirmado el núcleo central de sus teorías: el lento cambio de las especies a través de la selección natural.

La idea de la evolución, que en su origen entrevieron los filósofos griegos y que finalmente dejó sentada Charles Darwin, revolucionó el pensamiento biológico en su integridad. Fue, indudablemente, la idea más importante en la historia de la biología moderna.

16. Russell y la evolución estelar

Aristóteles pensaba que la Tierra y los cielos estaban regidos por leyes diferentes (véase el capítulo 7). Allí, según él, reinaba el cambio errático: sol y tormenta, crecimiento y descomposición. Aquí, por el contrario, no había cambio: el Sol, la Luna y los planetas giraban en los cielos de forma tan mecánica que cabía predecir con gran antelación el lugar que ocuparían en cualquier instante, y las estrellas jamás se movían de su sitio.

Había objetos, para qué negarlo, que parecían estrellas fugaces. Pero según Aristóteles no caían de los cielos, eran fenómenos que ocurrían en el aire, y el aire pertenecía a la Tierra. (Hoy sabemos que las estrellas fugaces son partículas más o menos grandes que entran en la atmósfera terrestre desde el espacio exterior. La fricción producida al caer a través de la atmósfera hace que ardan y emitan luz. Así pues, Aristóteles en parte tenía razón y en parte estaba equivocado en el tema de las estrellas fugaces. Erraba al pensar que no venían de los cielos, pero estaba en lo cierto porque realmente se hacen visibles en el aire. Y es curioso que las estrellas fugaces se llaman también «meteoros», palabra que en griego quiere decir «cosas en el aire»).

En el año 134 a. C, dos siglos después de morir Aristóteles, el astrónomo griego Hiparco observó una estrella nueva en la constelación del Escorpión. ¿Qué pensar de aquello? ¿Acaso las estrellas podían «nacer»? ¿Es que, después de todo, los cielos podían cambiar?

Hiparco, en previsión de que su observación no fuese correcta y de que la estrella hubiera estado siempre allí, confeccionó un mapa de más de mil estrellas brillantes, para así ahorrar engaños a todos los futuros astrónomos. Aquel fue el primer mapa estelar, y el mejor durante los mil seiscientos años siguientes. Pero durante siglos no volvieron a registrarse nuevas estrellas.

En el año 1054 d. C. apareció un nuevo astro en la constelación del Toro, que sólo fue observado por los astrónomos chinos y japoneses. La ciencia europea pasaba por momentos bajos, tanto que ningún astrónomo reparó en el nuevo lucero, a pesar de que durante semanas lució con un brillo mayor que el de cualquier otro cuerpo celeste, exceptuando el Sol y la Luna.

En 1572 volvió a surgir un nuevo astro brillante, esta vez en la constelación de Casiopea. Para entonces la ciencia empezaba a florecer de nuevo en Europa, y los astrónomos escrutaban celosamente los cielos. Entre ellos estaba un joven danés llamado Tycho Brahe, quien observó la estrella y escribió sobre ella un libro titulado De Nova Stella («Sobre la nueva estrella»). Desde entonces las estrellas que surgen de pronto en los cielos se llaman «novas».

Ahora no había ya excusa que valiera. Aristóteles estaba confundido: los cielos no eran inmutables.

Más indicios de cambio

Pero la historia no había tocado a su fin. En 1577 apareció un cometa en los cielos y Brahe intentó calcular su distancia a la Tierra. Para ello registró su posición con referencia a las estrellas, desde dos observatoríos diferentes momentos y en lo más cercanos posibles. Los observatorios distaban entre sí un buen trecho: el uno estaba en Dinamarca y el otro en Checoslovaquia. Brahe sabía que la posición aparente del cometa tenía que variar al observarlo desde dos lugares distintos. Y cuanto más cerca estuviera de la Tierra, mayor sería la diferencia. Sin embargo, la posición aparente del cometa no variaba para nada, mientras que la de la Luna sí cambiaba. Eso quería decir que el cometa se hallaba a mayor distancia que la Luna y que, pese a su movimiento errático, formaba parte de los cielos.

El astrónomo holandés David Fabricius descubrió algunos años más tarde, en 1596, una estrella peculiar en la constelación de la Ballena. Su brillo no permanecía nunca fijo. Unas veces era muy intenso, mientras que otras se tornaba tan tenue que resultaba invisible. Era una «estrella variable» y representaba otro tipo de cambio. La estrella recibió el nombre de Mira («maravillosa»).

Y aún se observaron más cambios. En 1718, por citar otro ejemplo, el astrónomo inglés Edmund Halley demostró que la posición de algunas estrellas había variado desde tiempos de los griegos.

No cabía la menor duda de que en los cielos había toda clase de cambios. Lo que no estaba claro era si admitían alguna explicación o si sucedían simplemente al azar.

La solución de este problema no fue posible hasta que el físico alemán Gustav R. Kirchhoff inventó el espectroscopio en 1859 (véase el capítulo 11). El espectroscopio es un instrumento que descompone en un espectro de colores cualquier luz que incida en él. Cada elemento químico, al emitir luz, tiene un espectro característico. Por eso, el espectroscopio puede identificar los elementos que se hallan presentes en una fuente luminosa y ha sido utilizado para determinar la composición química del Sol y las estrellas.

Cada clase de estrella produce un «espectro luminoso» diferente. Este hecho animó al astrónomo italiano Pietro A. Secchi a dividir en 1867 las estrellas en cuatro «clases espectrales». Otros astrónomos hicieron posteriormente una subdivisión más fina, en diez clases.

Este hallazgo estaba lleno de interés, porque significaba que las estrellas podían clasificarse en grupos de acuerdo con sus propiedades, igual que las plantas y los animales podían agruparse según sus características (véase el capítulo 14).

Wilhelm Wien, un físico alemán, demostró en 1893 cómo la luz emitida por cualquier fuente variaba con su temperatura. El trabajo de Wien permitía deducir la temperatura superficial de una estrella a partir simplemente de su clase espectral. Y resultó que la temperatura estaba relacionada con el color y el tamaño de la estrella.

El astrónomo danés Ejnar Hertzsprung (en 1905) y el norteamericano Henry N. Rusell (en 1914) compararon la temperatura de diversas estrellas con su luminosidad (la cantidad de luz emitida). Hicieron un gráfico de los resultados y comprobaron que casi todas las estrellas caían sobre una línea recta, que recibió el nombre de «secuencia principal».

Por un lado había estrellas rojas y frías, cuerpos descomunales que recibieron el nombre de «gigantes rojas». Aunque cualquier zona local de su superficie era más bien tenue, la estrella en su conjunto, por poseer una superficie total enorme, emitía gran cantidad de luz.

Luego estaban las estrellas amarillas, más calientes que las gigantes rojas. Aunque más pequeñas que éstas, seguían mereciendo el nombre de gigantes, en este caso «gigantes amarillas». También había estrellas aún más pequeñas y calientes, con temperatura suficiente para exhibir un color blanco-azulado. Las estrellas blanco-azuladas parecían ser las de máxima temperatura. Las que venían después eran más pequeñas y más frías. Eran las «enanas amarillas» (como nuestro Sol) y las «enanas rojas», estrellas muy débiles y muy frías.

¿Evolución de las estrellas?

La humanidad entrevió por primera vez una pauta de continuo cambio en los cielos. Podía ser que éstos envejecieran igual que envejecía la Tierra, o que las estrellas tuvieran un ciclo vital como el de los seres vivos; cabía incluso que hubiera una evolución estelar, igual que existía una evolución de la vida sobre la Tierra.

Russell sugirió que las estrellas nacían bajo la forma de ingentes masas de gas frío y disperso que emitía un débil calor rojo. A medida que envejecían, iban contrayéndose y tornándose más calientes hasta alcanzar una temperatura máxima. A partir de ahí seguían contrayéndose, pero descendiendo ahora hacia temperaturas más bajas, hasta convertirse finalmente en rescoldos extintos. El Sol, según este esquema, se hallaría bastante más allá del ecuador de la vida.

La teoría, sin embargo, era demasiado simple. Lo cierto es que a principios del siglo XX los astrónomos no sabían aún por qué las estrellas brillaban y radiaban luz. En la década de los ochenta del siglo pasado se había sugerido que la energía de la radiación de las estrellas provenía de su lenta contracción, y que la energía gravitacional se convertía en luz (lo cual encajaba bien con la teoría de Russell). Pero la idea hubo de ser abandonada, porque el proceso anterior no podía suministrar suficiente energía.

Los científicos habían descubierto en los años noventa que el corazón del átomo, el «núcleo», albergaba una reserva de energía mucho mayor de lo que se habían imaginado. Más tarde, en los años treinta de nuestro siglo, el físico germano – norteamericano Hans A. Bethe elaboró un esquema de reacciones nucleares que podía desarrollarse en el interior del Sol y proporcionarle la energía necesaria para formar la luz.

Según la hipótesis de Bethe, estas reacciones consistían en la conversión de átomos de hidrógeno (los átomos más sencillos de todos) en átomos de helio (que son algo más complejos). La enorme reserva de hidrógeno del Sol le ha permitido brillar durante cinco mil a seis mil millones de años y le permitirá lucir todavía durante bastantes miles de millones de años más. El Sol no está, por tanto, en declive; es aún una estrella joven.

Los astrónomos han continuado estudiando la naturaleza de las reacciones nucleares que tienen lugar en el interior de las estrellas. Según se cree, a medida que el hidrógeno se convierte en helio, este elemento se acumula en el centro y forma un «núcleo de helio». Este núcleo va subiendo de temperatura con la edad de la estrella, hasta que los átomos de helio comienzan a interaccionar y formar átomos aún más complejos. Y aparte de esto, se cree que ocurren otros cambios también.

Una explosión tremenda

En último término, la reserva inicial de hidrógeno de la estrella desciende por debajo de cierto nivel. La temperatura y el brillo de la estrella cambian tan drásticamente que el astro abandona la secuencia principal. Sufre una tremenda expansión y a veces comienza a pulsar a medida que su estructura se hace más inestable.

La estrella puede entonces explosionar. En ese caso, prácticamente todo el «combustible» que queda se inflama inmediatamente y la estrella adquiere un brillo inusitado por breve tiempo. Explosiones de esta clase son las que formaron las novas observadas por Hiparco y Tycho Brahe.

Dicho con pocas palabras, los astrónomos han desarrollado la idea del cambio celeste (que tan perplejo dejó a Hiparco hace dos mil años) hasta el punto de poder discutir cómo las estrellas nacen, crecen, envejecen y mueren.

Pero los astrónomos van todavía más lejos. Algunos especulan que el universo nació en una tremenda explosión cuyos fragmentos siguen alejándose, aún hoy, unos de otros. Cada fragmento es una vasta galaxia de miles de millones de estrellas. Quizá llegue el día en que todas las galaxias se pierdan de vista, en que todas las estrellas hayan explosionado y el universo muera.

O quizá sea, como piensan algunos astrónomos, que el universo está renaciendo constantemente, que muy lentamente se forme sin cesar nueva materia y que de ella nazcan nuevas estrellas y galaxias mientras las viejas mueren.

La idea del cambio celeste nos proporciona teorías, no sólo de la evolución estelar, sino incluso de una evolución cósmica: una «gran idea de la ciencia» que es de ámbito casi demasiado amplio para abarcarla con la mente.

13/06/2008
