LA ECONOMÍA DEL HIDRÓGENO

El hidrógeno es el elemento más básico y ubicuo del universo, la materia de la que están hechos el sol y las estrellas. En este inicio del siglo XXI, los principales fabricantes de automóviles han destinado más de 2.000 millones de dólares a desarrollar prototipos alimentados con hidrógeno, y se espera que en pocos años circulen ya los primeros vehículos producidos en serie. Además, cuando millones de usuarios puedan conectarse a redes energéticas de hidrógeno de alcance local, regional y nacional, basadas en los mismos principios que hicieron posible la World Wide Web, podrán compartir la energía de igual a igual y crear un modelo descentralizado.

El hidrógeno puede paliar la dependencia del mundo respecto a las importaciones de petróleo, y contribuir a enfriar el peligroso juego geopolítico que se genera entre algunos países productores y el mundo occidental.

Título Original: The Hydrogen Economy

©2002, Rifkin, Jeremy

Corregido: Silicon, 07/09/2010

Capítulo 1

ENTRE DISTINTAS REALIDADES

En el curso de la historia, los seres humanos se han encontrado a veces atrapados entre dos formas muy distintas de percibir la realidad. Tal era el caso, sin duda, en los últimos días del siglo XVII. Los científicos y filósofos de la Ilustración —Isaac Newton, John Locke, René Descartes y otros— pusieron en cuestión algunos de los credos más sagrados del catecismo de la Iglesia entre ellos una de sus doctrinas centrales: que la Tierra ha sido creada por Dios y posee un valor intrínseco. Estos nuevos pensadores preferían una explicación más materialista de la existencia y apostaban por las matemáticas y la razón. Menos de un siglo más tarde, los renegados políticos de las colonias americanas y los insurrectos franceses derrocaron el régimen monárquico en favor de una forma de gobierno republicana y proclamaron el «derecho inalienable del hombre a la vida, la libertad, la felicidad y la propiedad». James Watt patentó su motor de vapor en vísperas de la Revolución norteamericana, con lo que se consumó la relación entre el carbón y el nuevo espíritu prometeico de la época y la humanidad dio sus primeros pasos vacilantes hacia una forma de vida industrial que, a lo largo de los dos siglos siguientes, iba a cambiar el mundo para siempre.

Hoy vivimos tiempos similares marcados por la confusión, el fracaso de las ortodoxias y la apertura hacia posibilidades nuevas y radicales. Tras dos siglos de producción y comercio industriales, las fábricas, las oficinas y las empresas comerciales están abandonando lentamente el uso de mano de obra masificada, uncida a máquinas alimentadas con combustibles fósiles. Constantemente aparecen tecnologías inteligentes nuevas y más sofisticadas que sustituyen a la mano de obra humana en todas las industrias y los campos profesionales. Estamos realizando una gran transición hacia una fuerza de trabajo más reducida y elitista que trabajará en colaboración con tecnologías informáticas y robóticas cada vez más inteligentes. En cuestión de pocas décadas, el trabajador más barato del mundo no lo será tanto como las tecnologías inteligentes que vendrán a reemplazarle, ya sea en la planta de la fábrica o en los despachos de la dirección. Hacia mediados del siglo XXI probablemente seremos capaces de producir bienes y servicios para todos los habitantes de la Tierra con sólo una pequeña parte de la fuerza de trabajo que empleamos actualmente. Esto nos obligará a replantearnos el papel que deberán desempeñar los seres humanos cuando ya no sean necesarios en el mercado como fuerza de trabajo.

La física y la química, que han dominado la época que ahora termina y han influido sobre todos los aspectos de nuestra existencia, incluidos los detalles más insignificantes de nuestro estilo de vida, están dejando paso ahora a la era de la biología. El desciframiento y la manipulación del genoma del ser humano, de los animales y de las plantas abre la puerta a una nueva era en la que la vida misma se convierte finalmente en una mercancía manipulable. La era de la biotecnología está comenzando a plantear preguntas fundamentales sobre la naturaleza humana y el espacio público se está viendo ocupado rápidamente por un amplio debate entre aquellos que contemplan la nueva época como un renacimiento biológico y los que alertan sobre la llegada de una civilización ligada a la eugenesia comercial.

La revolución de la informática y de las telecomunicaciones ha dado origen a Internet y a la World Wide Web, lo que ha significado un cambio importante en la forma en que se comunican los seres humanos. «Acceso» se ha convertido en la metáfora general para una generación de individuos que pueden conectarse entre sí a través de un «sistema nervioso central» electrónicamente mediado que se extiende por todo el globo. Esta nueva sociedad que se mueve a la velocidad de la luz está introduciendo cambios fundamentales en nuestra forma de hacer negocios. La economía de mercado, caracterizada por el intercambio de bienes y servicios entre vendedores y compradores, está demostrando ser demasiado lenta para adaptarse a la nueva velocidad a la que se mueve la vida comercial. En la era que ahora comienza, el intercambio de la propiedad en el mercado deja paso progresivamente al acceso a servicios y experiencias en el marco de una red. En una sociedad en la que el tiempo mismo es el recurso más escaso y valioso, los proveedores conservan el derecho de propiedad y los usuarios pagan por el tiempo durante el cual acceden a los bienes y servicios. La suscripción, el leasing, la multipropiedad, la licencia y el alquiler se convierten en las formas preferidas de hacer negocios. La nueva economía «temporal» se caracteriza por la caída de los costes de transacción y la reducción de los márgenes de beneficio, lo cual obliga a las empresas comerciales a introducir nuevos y radicales modelos de negocio basados en acuerdos de «ahorros compartidos» entre compañeros de red. La transformación de los intercambios de propiedad en relaciones de acceso y de los márgenes de beneficios en ahorros compartidos están comenzando a reestructurar la vida comercial en todo el mundo.

Nuestras ideas sobre en qué consiste la cultura también están cambiando de manera radical. Empresas gigantescas dedicadas a la producción de contenidos como Disney, Universal Vivendi, AOL-Time Warner y Sony rastrean recursos culturales en todo el mundo y los convierten en todo tipo de experiencias de pago. Las personas que disfrutan de mayores ingresos —el 20% que más consume en el mundo— gastan actualmente casi tanto dinero en experiencias como en bienes y servicios básicos.

Una joven generación de activistas culturales que se oponen al nuevo comercio ha declarado una guerra cada vez más cruenta contra las «marcas», la mercantilización de los estilos de vida y los nuevos tipos de entretenimientos y franquicias comerciales, todo lo cual está llevando desde su punto de vista a una homogeneización de la cultura. Los activistas argumentan que el nuevo comercio cultural globalizado constituye una amenaza para la diversidad cultural del mundo y reclaman protección para las culturas indígenas. El esfuerzo de la esfera comercial por absorber la esfera cultural y convertirse en el único árbitro de la historia de la humanidad representa un punto de inflexión importante en la relación entre el comercio y la cultura que traerá consigo profundas consecuencias a largo plazo para todas las sociedades.

La transformación de la naturaleza del trabajo, la incipiente revolución en los campos de la biotecnología y las telecomunicaciones, la progresiva temporalización de la actividad económica y la batalla global entre el comercio y la cultura están alterando profundamente tanto nuestra concepción del mundo que nos rodea como su realidad.

En la actualidad está a punto de producirse un cambio igualmente profundo en nuestra forma de emplear la energía. La era moderna ha sido posible gracias a la explotación del carbón, el petróleo y el gas natural. Todos los avances de los dos últimos siglos, sean de naturaleza comercial, política o social, están conectados, de un modo u otro, con el aumento masivo de la energía generado por la quema de combustibles fósiles.

Los antropólogos afirman que la cantidad de energía consumida per cápita en una sociedad da una buena medida de su estadio relativo de desarrollo. Durante los últimos doscientos años, la sociedad occidental ha consumido más energía per cápita que todas las demás sociedades históricas juntas. Disfrutamos actualmente de un estándar de vida sin precedentes y debemos nuestra buena fortuna a los depósitos de hidrocarburos que se generaron hace millones de años. El maná, sí, pero no procedente del cielo, sino de las profundidades de la Tierra.

Por desgracia, todo lo bueno se acaba algún día. Durante mucho tiempo hemos creído ingenuamente que las reservas de petróleo almacenadas en los rincones más oscuros del planeta, si no ilimitadas, al menos sí eran suficientes como para satisfacer todas nuestras necesidades hasta bien entrado el futuro. Cuando la producción de petróleo en Estados Unidos tocó techo en 1970 —el punto en que la mitad de las reservas recuperables habían sido explotadas— los geólogos comenzaron a inquietarse. Sin embargo, mientras el petróleo continuó fluyendo desde otras partes del mundo, el norteamericano medio no dedicó ni un minuto a pensar en el asunto. Hubo que esperar al embargo petrolero de los países árabes, tres años más tarde, para que los norteamericanos, junto con los consumidores de otros países, tomaran conciencia del problema. Hacer colas de horas en las estaciones de servicio con la esperanza de asegurarse unos litros de gasolina fue una experiencia que dio que pensar a millones de personas. Algunos críticos advirtieron entonces de que pronto nos quedaríamos sin petróleo. Pero no fue eso lo que sucedió. Estados Unidos, con la ayuda de otras naciones y de las principales compañías energéticas, inició una búsqueda frenética de nuevas fuentes de petróleo, y las encontró. Disminuyeron las colas en las estaciones de servicio y la crisis del petróleo amainó. La gasolina circulaba y era más barata que nunca. El mundo retomó su ritmo habitual.

En la actualidad, el petróleo es relativamente barato en los mercados mundiales. Nuestros expertos nos dicen que con el tiempo las reservas de petróleo —y las de gas natural— se agotarán, pero que todavía faltan treinta o cuarenta años para que llegue ese momento, tal vez incluso más; tiempo suficiente para proyectar fuentes alternativas de energía.

Pero ¿qué pasaría si de repente le dijeran que las cosas no son exactamente lo que parecían en el tema del petróleo? Imagine que se despierta un día a primera hora y se encuentra con el siguiente titular en el periódico: «LA PRODUCCIÓN MUNDIAL DE PETRÓLEO TOCA TECHO; SE ESPERA QUE EN LOS PRÓXIMOS AÑOS SE DISPAREN LOS PRECIOS EN LOS MERCADOS MUNDIALES».

Cada vez son más los expertos mundiales en geología que sostienen precisamente eso. Esta vez, avisan, nos encontramos en la antesala de una verdadera crisis petrolera y cuando ésta llegue será permanente. Así pues, ¿qué es lo que nos espera en los próximos años?

Si la producción mundial de petróleo tocara techo en algún momento de la próxima década, seguido poco tiempo después por el gas natural, ello provocaría una serie de efectos en cadena que podrían llegar a poner en jaque buena parte de nuestro estilo de vida industrial. En particular, hay dos cambios que probablemente destacarían dentro del nuevo equilibrio derivado de la crisis del petróleo.

En primer lugar, los expertos discrepan en cuanto al momento en que la producción global de petróleo tocará techo, pero todos están de acuerdo en que cuando esto ocurra prácticamente la totalidad de las reservas sin explotar se hallarán en los países musulmanes de Oriente Medio, lo cual podría cambiar el actual equilibrio de poder en el mundo. La disminución de las reservas de petróleo combinada con el aumento de la militancia entre la población musulmana más joven podría poner en peligro la estabilidad económica y política de todos los países de la Tierra. Los líderes y los analistas políticos están particularmente preocupados por la escalada del conflicto entre Israel y el pueblo palestino y por la posibilidad de que en el futuro los fundamentalistas islámicos pudieran presionar a sus gobiernos para que usaran el petróleo como un arma contra Estados Unidos y otros países occidentales en represalia por su apoyo a los israelíes.

En segundo lugar, si el descenso de la producción mundial de petróleo y gas natural sorprende al mundo, es probable que los países y las compañías energéticas busquen sustitutos entre los combustibles fósiles más sucios, como el carbón, los crudos pesados y las arenas asfálticas. Si en la actualidad se prevé un aumento de la temperatura en la Tierra de entre 1,4 °C y 5,8 °C para el siglo XXII, la utilización de combustibles más sucios significaría un aumento en las emisiones de CO2 y habría que prever un incremento aún más importante de la temperatura, con efectos todavía más devastadores sobre la biosfera terrestre de los que se habían previsto hasta ahora.

La nuestra no es la primera gran civilización de la historia que se enfrenta a una crisis energética. La energía ha desempeñado un papel importante en el auge y la caída de las civilizaciones. Muchos antropólogos e historiadores mantendrían que ha sido el factor crucial. Si hay alguna lección que aprender —y ciertamente la hay— en la respuesta que dieron otras civilizaciones a sus propias crisis energéticas, ahora sería el momento de hacer inventario, de escuchar a las Casandras. Lo cierto es que el flujo de la energía está gobernado por leyes férreas y si una civilización las transgrede, puede llegar a perecer. En último término, son las propias leyes de la termodinámica las que nos dicen cuáles son los límites superiores en la aspiración del hombre a dominar su entorno. Las sociedades que van más allá de las limitaciones que imponen sus propios regímenes energéticos corren el riesgo de experimentar un colapso.

A medida que nos acercamos a los estadios finales de la era del petróleo, Estados Unidos se encuentra en una posición cada vez más vulnerable ante las crecientes amenazas e interferencias de origen externo e interno, al igual que todos los demás países del mundo. Nuestra vulnerabilidad viene agravada por el carácter altamente centralizado y jerarquizado de la infraestructura energética, así como de la correspondiente infraestructura económica que hemos creado para gestionar un régimen energético basado en los combustibles fósiles. La era de los combustibles fósiles se caracteriza por un modelo organizativo vertical motivado por los obstáculos que plantea el control y la explotación de unas formas de energía que resultan difíciles de encontrar. Los extraordinarios costes asociados al procesamiento del carbón, el petróleo y el gas natural exigían grandes cantidades de capital de inversión y han llevado a la formación de empresas energéticas gigantescas. En la actualidad, ocho megacompañías —tanto de capital privado como público— controlan la circulación de la energía en todo el mundo. Al centralizar el poder sobre los recursos energéticos de la Tierra, las compañías energéticas han creado unas condiciones favorables para las economías a escala y para la centralización de la actividad económica en todas las demás industrias.

La quema de combustibles fósiles también ha acelerado la vida comercial. La necesidad de gestionar la mayor densidad y movilidad del comercio humano ha contribuido a asentar la formación de empresas comerciales altamente centralizadas y jerarquizadas. En la actualidad, menos de quinientas empresas globales controlan buena parte de la actividad económica del planeta. La globalización representa el estadio final de la era de los combustibles fósiles, un período en el que cada vez son menos las instituciones corporativas que gestionan tanto el flujo de energía como la actividad económica en las comunidades de todo el mundo.

La globalización es la dinámica que define nuestra época. Sus defensores la ven como el nuevo gran avance económico de la humanidad y como una forma de mejorar las vidas de las personas en todo el mundo. Sus críticos ven en ella el máximo exponente del dominio que ejercen las empresas sobre la vida de la sociedad y un instrumento dirigido a aumentar la distancia entre los que tienen dinero y los que no. Las compañías transnacionales, con la ayuda de las naciones del G-7, están presionando para modificar las regulaciones y los estatutos gubernamentales que según ellos limitan el libre comercio. Cada vez más antiglobalizadores se lanzan a la calle para protestar ante lo que consideran el desmantelamiento sistemático de las normas medioambientales y laborales diseñadas para proteger las comunidades ecológicas y humanas de la Tierra de la rapacidad de las empresas. Los trágicos acontecimientos del 11 de septiembre y sus consecuencias inmediatas han exacerbado las tensiones que rodean la globalización y han aumentado la sensación de vulnerabilidad de todos en un mundo que se presenta cada vez más incierto e inseguro.

A pesar del desacuerdo y de la polarización crecientes, se han realizado escasos esfuerzos dirigidos a analizar con seriedad los factores básicos que se encuentran detrás del proceso globalizador y las enconadas reacciones que suscita. Aunque son varias las perspectivas desde las que se puede entender la globalización, ninguna es tan importante como el equilibrio energético. A veces olvidamos que sin los combustibles fósiles la globalización habría sido imposible. La energía de los combustibles fósiles ha permitido recortar de forma radical los tiempos y las distancias en la actividad comercial, lo cual ha hecho posible que la explotación de las materias primas y la mano de obra, así como la comercialización de los servicios y productos manufacturados, tenga lugar en un mercado mundial único.

No es nada extraño que el control de las reservas energéticas de combustibles fósiles haya constituido la principal preocupación de los gobiernos y la industria durante más de un siglo. Para cinco generaciones, hablar de geopolítica ha sido sinónimo, en gran medida, de hablar de políticas relacionadas con el petróleo. Aquellos países, empresas y pueblos que han podido controlar el flujo del petróleo han disfrutado de una riqueza sin igual, mientras que aquellos a quienes se ha negado un acceso privilegiado al potencial generador de riqueza de lo que los geólogos llaman el «oro negro» se han hundido todavía más en la pobreza y han estado sometidos a una creciente explotación y marginalización.

Consideremos, por ejemplo, la subida de los precios del petróleo en los años setenta y ochenta, una de las principales causas de la escalada de la crisis de la deuda externa en los países del Tercer Mundo. Incapaces de pagar los elevados precios del petróleo en los mercados mundiales, los países en vías de desarrollo se vieron forzados a pedir miles de millones de dólares en forma de préstamos de instituciones públicas y privadas para pagar las encarecidas importaciones de petróleo y los crecientes costes de todas las actividades relacionadas con las abultadas facturas energéticas. El peso de la deuda se ha hecho mayor en los últimos años a medida que los países en vías de desarrollo han pasado a ser todavía más dependientes del petróleo extranjero para modernizar sus economías industriales y cubrir las necesidades de sus crecientes cifras de población urbana. Muchos de los países más pobres del mundo dedican actualmente más dinero a devolver deudas pasadas que a prestar servicios humanos básicos. El resultado es una espiral descendente irreversible de pobreza y desesperación. Los activistas que han participado en los recientes foros sobre el desarrollo global han coincidido en señalar la crisis de la deuda externa en el Tercer Mundo como el signo más visible de las desigualdades que genera la globalización y reclaman la cancelación de la deuda externa de los países pobres. Así pues, el régimen energético de los combustibles fósiles es tanto la fuerza vital que hace posible la globalización como uno de los principales factores responsables de la creciente distancia entre los países ricos y pobres del mundo.

En la actualidad, sin embargo, la infraestructura global creada para explotar los combustibles fósiles y para gestionar la actividad industrial está comenzando a envejecer y amenaza con desmoronarse. Las fisuras aparecen por todas partes y crece la preocupación ante la posibilidad de que dicha infraestructura no aguante mucho tiempo más. Algunos geólogos comienzan a apuntar la posibilidad de que el propio sistema entre en colapso. Sería temerario, dicen los agoreros, no prepararse para lo que pudiera venir.

Pero ¿qué significa realmente estar «preparado»? Si la era de los combustibles fósiles está llegando a su fin, ¿qué es lo que puede reemplazarlos? Nos encontramos a las puertas de un nuevo régimen energético cuya naturaleza y carácter es tan distinto del de los combustibles fósiles como lo era este del régimen energético anterior basado en la quema de madera.

El hidrógeno es el elemento más ligero, más básico y más ubicuo del universo. Cuando se utiliza como forma de energía, se convierte en «el combustible eterno». Nunca se termina y, como no contiene ni un solo átomo de carbono, no emite dióxido de carbono. El hidrógeno se encuentra repartido por todo el planeta: en el agua, en los combustibles fósiles y en los seres vivos. Sin embargo, raramente aparece en estado libre en la naturaleza, sino que tiene que ser extraído de fuentes naturales.

Hoy día ya se están sentando las bases para la economía del hidrógeno. En los próximos años, la revolución de la informática y las telecomunicaciones se fusionará con la nueva revolución de la energía del hidrógeno, una potente combinación que podría llegar a reconfigurar los fundamentos de las relaciones humanas en los siglos XXI y XXII. Si tenemos en cuenta que el hidrógeno está en todas partes y es inagotable, la posibilidad de aprovecharlo adecuadamente pondría el «poder» al alcance de todas las personas de la Tierra, lo que convertiría la energía del hidrógeno en el primer régimen energético verdaderamente democrático de la historia.

Se están comercializando pilas de combustible alimentadas con hidrógeno capaces de generar potencia, luz y calor y preparadas para ser instaladas en factorías, oficinas, edificios comerciales, hogares, coches, autobuses y camiones. La posibilidad de que el usuario disponga de una pequeña planta de energía propia —lo que se llama «generación distribuida»— amenaza la posición de dominio que han disfrutado durante largo tiempo las plantas energéticas centralizadas surgidas durante la era de los combustibles fósiles. Ahora, el usuario final no sólo consume, sino que también produce su propia energía. Cuando haya millones de pequeñas plantas energéticas conectadas en grandes redes, basadas en los mismos principios arquitectónicos y las tecnologías inteligentes que han hecho posible la World Wide Web, las personas podrán compartir e intercambiar la energía entre ellas —energía compartida «de igual a igual» [peer-to-peer]— y liberarse para siempre del dominio de las grandes compañías energéticas.

La red energética mundial del hidrógeno [hydrogen energy web, HEW] será la próxima gran revolución tecnológica, comercial y social de la historia. Sigue los pasos del desarrollo de las comunicaciones a nivel mundial en los años noventa y, al igual que éste, traerá consigo una nueva cultura del compromiso. Aunque es cierto que la HEW constituye potencialmente una revolución en el régimen energético que podría descentralizar y democratizar la energía y refundar las instituciones sociales y comerciales sobre bases radicalmente distintas, no hay ninguna garantía de que, de hecho, sea así. En este sentido, la historia de Internet y de la World Wide Web resulta instructiva. Internet trae consigo la promesa de poner nuevos instrumentos de poder al alcance de millones de personas, al darles acceso potencial a todas las demás y hacer realmente democrática la comunicación y el intercambio de información entre las personas. Los «net.activistas» de los años noventa defendían que la información debía ser libremente compartida. Aunque pronto establecieron redes comunitarias y redes libres para hacer realidad esta idea, eran demasiado pocas, demasiado débiles y demasiado carentes de contenido significativo como para resistir ante una campaña sumamente organizada y mejor financiada para controlar el nuevo medio lanzada por compañías como AOL y Microsoft. Las fuerzas comerciales han conspirado desde el primer momento para hacerse con el control absoluto de los portales del ciberespacio, para convertirse en los árbitros y los guardianes de la era de la información. La red energética del hidrógeno se enfrenta a una amenaza y un reto similares.

La posibilidad de que el hidrógeno se convierta en «la energía del pueblo» depende en gran medida de cómo sea utilizada en los primeros estadios de su desarrollo. Al igual que los «net.activistas» de la última década, está comenzando a aparecer una nueva generación de activistas de la energía que defienden la necesidad de compartir la energía del hidrógeno. Hacer realidad esta idea requerirá que las instituciones públicas y las organizaciones no lucrativas —especialmente las compañías públicas que proporcionan energía a cientos de millones de personas y los miles de cooperativas sin ánimo de lucro que agrupan a más de 750 millones de personas en todo el mundo— se pongan en acción al comienzo de la nueva revolución energética para ayudar a establecer asociaciones de generación distribuida [distributed generation associations, DGA] en todos los países.

Conectar a toda la humanidad a una red energética del hidrógeno va a requerir también la participación del sector privado. Las empresas comerciales deberán desarrollar y producir un nuevo hardware y software para la revolución de la generación distribuida y tendrán un papel importante a la hora de organizar servicios energéticos y coordinar el flujo de energía en la red energética del hidrógeno. Encontrar la combinación adecuada de intereses comerciales y no comerciales tendrá una importancia crucial a la hora de garantizar la legitimidad, la efectividad y la viabilidad a largo plazo del nuevo régimen energético.

El paso a la economía del hidrógeno puede poner fin a la dependencia del mundo respecto de las importaciones de petróleo y contribuir a rebajar la tensión del peligroso juego geopolítico que practican actualmente los militantes musulmanes (de Oriente Medio y el resto del mundo) con los poderes occidentales. Y lo que es igual de importante, destetar al mundo del régimen energético de los combustibles fósiles limitará las emisiones de CO2 a sólo el doble de los niveles preindustriales y mitigará los efectos del calentamiento global sobre la ya castigada biosfera de la Tierra.

Un régimen energético descentralizado basado en el hidrógeno ofrece al menos la esperanza de conectar a los desconectados y de acercar el poder a aquellos que no lo tienen. Cuando esto ocurra, tendremos realmente en nuestras manos la posibilidad de llevar a cabo una «reglobalización», esta vez comenzando desde abajo y con la participación de todo el mundo.

La era de los combustibles fósiles trajo consigo nuevas formas de organizar la sociedad, como la actividad industrial, el gobierno de los Estados-nación, los densos asentamientos urbanos y un estilo de vida burgués. Las grandes diferencias del hidrógeno respecto a las diversas formas de energía basadas en los hidrocarburos darán pie al surgimiento de un nuevo tipo de infraestructura energética, así como a un conjunto de instituciones económicas radicalmente distintas y a nuevos modelos de asentamiento humano, igual que sucedió en el pasado con el carbón y el motor de vapor y más tarde con el petróleo y el motor de combustión interna. Cuando todas las personas de la Tierra puedan producir su propia energía, la propia naturaleza de la vida comercial cambiará radicalmente. La actividad económica se hará más dispersa. La desconcentración del comercio, a su vez, permitirá desconcentrar los asentamientos humanos. La centralización del poder y las economías de escala que caracterizaban la era de los combustibles fósiles llevaron inevitablemente a la concentración de la población en megaciudades que consumían grandes cantidades de energía y que resultaban en último término insostenibles. La creación de redes energéticas descentralizadas de hidrógeno entre usuarios finales haría posible el establecimiento de asentamientos humanos más dispersos y más sostenibles en relación con los recursos medioambientales locales y regionales.

La red energética mundial del hidrógeno, al igual que la red mundial de comunicaciones, nos permitirá conectar entre sí a todos los seres humanos del planeta en una matriz económica y social indivisible e interdependiente. La especie humana tiene la ocasión de convertirse en una comunidad integrada de forma más completa en los ecosistemas de la Tierra. Por desgracia, nuestras ideas acerca de la seguridad personal y colectiva están todavía impregnadas de actitudes propias de la era de los combustibles fósiles. En la era del petróleo, la sensación de seguridad personal de cada individuo reflejaba los valores organizativos de la gran estructura institucional que gestionaba el flujo de la energía y la actividad económica. La autonomía y la movilidad se convirtieron incuestionablemente en las virtudes sociales de la época, tanto en la vida personal como en la institucional. En la economía del hidrógeno que está por venir, la densidad misma de las interacciones humanas, así como la rapidez de los acuerdos, darán pie a un nuevo sentido de la seguridad, ligado a la integración en diversas redes comerciales, sociales y medioambientales, así como a la interdependencia global. Nuestra seguridad individual y el bienestar de las diversas comunidades humanas, biológicas y geológicas de la Tierra serán inseparables. Terminaremos por vernos a nosotros mismos como parte de un único organismo planetario. La geopolítica de la división, tan omnipresente en la época de los combustibles fósiles, dejará paso a una nueva política de la biosfera en la era del hidrógeno.

Nos hallamos en el vértice de una nueva época histórica en la que todas las posibilidades se mantienen abiertas. El hidrógeno, la materia misma de la que están hechas las estrellas y nuestro propio Sol, está comenzando a ser controlado por el ingenio humano y aprovechado para fines humanos. Proyectar la ruta adecuada al comienzo del viaje es esencial si queremos convertir la gran promesa de una era del hidrógeno en una realidad viable para nuestros hijos y en un valioso legado para las generaciones que vendrán después de nosotros.

Capítulo 2

RESBALANDO POR LA CURVA DE CAMPANA DE HUBBERT

La crisis energética de los años setenta es un pálido recuerdo. Parece como si nos bañáramos en petróleo. En la primavera de 2002, los precios del petróleo en los mercados mundiales rondaban los 24 dólares y los países de la OPEP luchaban por mantener su cuota de mercado frente a los países productores no pertenecientes a dicha organización —entre ellos Rusia, México y Noruega—, que generan una proporción cada vez mayor del petróleo mundial. Millones de norteamericanos circulan en coches que consumen enormes cantidades de gasolina y los precios de las estaciones de servicio suben, pero a un ritmo manejable. Es raro oír hablar de la conservación de la energía —en otra época el pasatiempo nacional—, tanto en Estados Unidos como en los demás países industrializados.

Los responsables políticos nos dicen que gracias a las nuevas tecnologías en materia de exploración estamos encontrando más petróleo para reponer las reservas y que gracias a las nuevas tecnologías de perforación estamos aprovechando mejor el petróleo de los yacimientos existentes. Según la Administración de Información Energética [Energy Information Administration, EIA] del Departamento de Energía de Estados Unidos, faltan casi treinta y cinco años para que la producción de petróleo crudo barato llegue a tocar techo, tiempo suficiente para realizar la transición hacia estrategias energéticas alternativas. En resumen, el mundo se enfrentará a muchos problemas en los años venideros, y la escasez de petróleo no es uno de ellos. La base energética del estilo de vida industrial y postindustrial, nos dicen, está asegurada.

Sin embargo, en medio de esta aparente complacencia, los resultados de los nuevos estudios publicados por algunos de los principales expertos mundiales en geología ofrecen una imagen muy distinta. Sus cálculos sugieren que la producción global de petróleo crudo barato —la sangre que da vida a la economía global— podría tocar techo antes del año 2010 y no más tarde de 2020. (Se considera que se ha «tocado techo» cuando aproximadamente la mitad de las reservas recuperables estimadas [estimated ultimate recoverable, EUR] de petróleo del mundo han sido explotadas.) Aunque estos estudios nuevos y sumamente controvertidos se han publicado en las principales revistas científicas del mundo —entre ellas Science y Scientific American— y han suscitado un encendido debate en el campo de la geología del petróleo y en los consejos de dirección de algunas de las principales compañías energéticas del mundo, todavía no han sido difundidos por los medios de comunicación. Muchos de nuestros políticos y asesores políticos ignoran estos nuevos datos, y nuestros economistas y hombres de negocios están igualmente desinformados. Sin embargo, si tales estudios resultan ser correctos, nos estamos acercando a toda velocidad a una de las principales encrucijadas de la historia de la civilización humana, cuyas profundas consecuencias apenas comenzamos a vislumbrar.

Falsear las cuentas

Cuando los geólogos dicen que la producción mundial de petróleo puede tocar techo se refieren principalmente a lo que se conoce como petróleo convencional o ligero, el tipo de petróleo que brota libremente del interior de la Tierra, en el continente o en el mar, y que puede ser fácilmente transformado en gasolina y otros productos basados en el petróleo. Existen también los llamados «petróleos no convencionales» (petróleo derivado de arenas asfálticas, crudo pesado, petróleo procedente de aguas profundas o regiones polares, y petróleo de esquisto).

El petróleo se compone de material orgánico, en su mayor parte derivado de plantas planctónicas flotantes como las algas verdes, y de animales planctónicos unicelulares. Los restos orgánicos se depositaron en el fondo de los lagos y los mares, donde el estancamiento de las aguas evitó su oxidación. Una vez sepultado, el plancton se transformó en petróleo y gas por efecto del calor y la presión. Muchos de los depósitos de petróleo se formaron durante el período Jurásico, hace más de 150 millones de años, en las regiones tropicales cercanas al ecuador. Los movimientos de las placas tectónicas de los continentes desplazaron más tarde las rocas madre hacia el norte y hacia el este, hacia Oriente Medio, el Mar del Norte, Siberia y otras regiones septentrionales. Los depósitos de petróleo de Estados Unidos datan del período Pérmico (hace 230 millones de años) y el petróleo de Venezuela se formó en el Cretácico (hace 90 millones de años).

Los geólogos están de acuerdo en que hasta el momento se han extraído de la Tierra más de 875.000 millones de barriles de petróleo, casi todos en los últimos 140 años de la era industrial. El punto sobre el que no se ponen de acuerdo es en la cantidad de petróleo convencional que todavía queda por extraer (hay que decir que, a pesar de las discrepancias de los expertos sobre la cantidad restante de petróleo, sus cifras se mueven, sin embargo, dentro de un margen bastante estrecho). Parte del problema es atribuible a las diversas formas de interpretar la palabra «reservas».

En primer lugar, los geólogos y los ingenieros distinguen entre reservas y recursos. «Reservas» se refiere a la cantidad conocida de petróleo presente en yacimientos que pueden ser explotados con las actuales tecnologías, dentro de un futuro previsible y a un coste razonable desde el punto de vista comercial. «Recursos» se refiere a las estimaciones teóricas sobre la cantidad total de petróleo que puede existir en una región, incluidas las reservas cuya extracción o procesamiento no es económicamente viable con las tecnologías actuales o con las actuales condiciones de mercado. Para complicar aún más las cosas, la industria también utiliza otros términos para definir las «reservas», como por ejemplo: «activas» e «inactivas», «probables», «posibles», «inferidas», «identificadas» y «no descubiertas».8

El veterano geólogo Jean H. Laherrère afirma que esta proliferación de términos descriptivos para referirse a las reservas es intencionada y tiene como objetivo permitir a los países y las empresas disfrazar las cifras, una especie de contabilidad geológica creativa con fines políticos o comerciales. Laherrère culpa de ello a «los intereses ocultos que utilizan definiciones poco rigurosas para proponer cifras que se ajusten a sus objetivos políticos: el petróleo es dinero y las reservas son, por así decirlo, petróleo en el banco, en este caso un banco situado en las profundidades de la Tierra y donde no hay auditores que puedan comprobar las cuentas».

El petróleo de esquisto es un buen ejemplo de estas prácticas de «contabilidad creativa». El gobierno de Estados Unidos cita a menudo el hecho de que hay suficientes «recursos» de petróleo de esquisto como para producir dos billones de barriles de petróleo, lo cual hace pensar que Estados Unidos dispone de abundantes «reservas» de combustibles fósiles para conservar su independencia energética. Sin embargo, el petróleo de esquisto aún no ha sido explotado comercialmente, pues no es viable económicamente con las técnicas actuales de extracción y refinado. Así pues, el petróleo de esquisto es ciertamente un recurso, pero no cumple las condiciones para ser considerado una reserva.

A efectos de esta discusión, hay tres datos que resultan cruciales: la producción acumulada, es decir, cuánto petróleo convencional se ha producido globalmente hasta la fecha; una estimación de las reservas globales de petróleo; y una proyección del petróleo recuperable que puede quedar por descubrir. Estos tres factores sumados constituyen la totalidad del petróleo recuperable.

Tal como se ha señalado antes, el petróleo existe en cuencas donde se acumularon y preservaron los materiales orgánicos. Tales cuencas se hallan en aguas marinas poco profundas y en el continente. Los geólogos han localizado seiscientas cuencas de este tipo y hay un convencimiento general de que faltan pocas por descubrir. Cuatrocientas de estas cuencas ya han sido exploradas. Las otras doscientas están ubicadas en regiones remotas como Groenlandia y en aguas profundas cerca de las costas de Brasil» África Occidental y el golfo de México y su exploración es difícil y costosa. Se han encontrado cantidades significativas de petróleo en 125 de estas cuencas. Para determinar la cantidad de petróleo convencional que todavía queda por descubrir, los geólogos empleaban tradicionalmente el siguiente método. Primero, calculaban cuánto petróleo se había encontrado en las cuatrocientas cuencas en las que ya se habían realizado perforaciones y determinaban en cuántos kilómetros cúbicos de materia sedimentaria se hallaba este petróleo. Al dividir una cifra por la otra, los geólogos establecían un promedio mundial relativo a la cantidad de petróleo que se podía esperar encontrar por kilómetro cúbico de materia sedimentaria. Entonces calculaban el volumen de materia sedimentaria que puede haber en todas las cuencas del mundo y multiplicaban esta cifra por la cantidad media de petróleo hallada por unidad cúbica. En la actualidad, los geólogos pueden realizar estimaciones mucho más precisas sobre la cantidad de petróleo que todavía queda por descubrir gracias a sofisticadas tecnologías de análisis geoquímico.

Estados Unidos (los 48 Estados contiguos), cuyas reservas recuperables se estiman aproximadamente en 195.000 millones de barriles, ha extraído ya 169.000 millones de barriles de su riqueza petrolera, lo cual deja al país con unas reservas de sólo 20.000 millones de barriles, más otros 6.000 millones aproximadamente que están todavía por descubrir. Arabia Saudí, en cambio, posee unas reservas recuperables totales de 300.000 millones de barriles. Los saudíes han producido sólo 91.000 millones de barriles, lo que deja sus reservas en 194.000 millones de barriles y unas reservas adicionales de 14.000 millones de barriles todavía por encontrar. Las reservas recuperables totales de Rusia son aproximadamente de 200.000 millones de barriles. Rusia ha extraído ya 121.000 millones de este total, lo cual deja al país con unas reservas de sólo 66.000 millones, más otros 13.000 millones de barriles todavía por encontrar. Así, mientras que Estados Unidos conserva sólo el 14% del petróleo que poseía originalmente y Rusia el 39%, Arabia Saudí todavía tiene el 70% de su petróleo bajo tierra.

Por desgracia, existe un coro cada vez más grande de escépticos según los cuales las cifras que figuran en los documentos oficiales resultan sospechosas, no porque la ciencia no sea capaz de hacer un cálculo preciso, sino más bien porque, tal como sugiere Laherrère, los países y las compañías falsean los estudios para hinchar las cifras.

Según la Oficina de Estudios Geológicos de Estados Unidos [U.S. Geological Survey, USGS], hay 3,003 billones de barriles de las reservas totales recuperables estimadas (EUR). Sin embargo, algunos de los nuevos modelos informáticos indican que las EUR sólo llegarían a dos tercios de las que pretende la USGS. Aunque las diferencias en las estimaciones no son muy significativas desde el punto de vista de la historia de la humanidad, y todavía menos desde el punto de vista de la historia geológica, sí adquieren cierta importancia en el escenario geopolítico, donde el impacto de los cambios se mide por años y décadas. Basándonos en la estimación de la USGS de los 3,003 billones de barriles de reservas recuperables totales, y suponiendo que se mantenga la tasa actual del 2% de crecimiento en la producción anual, la EIA estima que la producción global de petróleo tocará techo en 2037. Pero si los nuevos modelos son correctos, el tiempo que falta para que la producción global de petróleo toque techo se quedaría entre ocho y dieciocho años. (Incluso el análisis de la EIA, basado en las estimaciones actuales de la USGS sobre las reservas recuperables totales, reconoce que el pico en la producción global de petróleo se podría adelantar hasta 2016 si se aplican supuestos ligeramente distintos al modelo.) Aunque las diversas opiniones reflejan diferencias legítimas en la interpretación de los datos, queda abierta la cuestión de hasta qué punto ha habido presiones políticas y comerciales sobre las cifras oficiales. Veamos las pruebas.

Los datos relativos a las reservas se publican anualmente en el Oil&Gas Journal y el World Oil. Tales revistas realizan estudios en distintos países y elaboran sus estadísticas sin ningún tipo de verificación independiente. El resultado es una ensalada de cifras que a menudo resulta engañosa.

Así por ejemplo, los geólogos asignan probabilidades a sus estimaciones de reservas recuperables. El geólogo Colin Campbell cita el caso del yacimiento noruego de Oseberg. Los ingenieros del petróleo estiman que hay un 90% de probabilidades de que este yacimiento produzca más de 700 millones de barriles de petróleo recuperable y sólo un 10% de que produzca 2.500 millones de barriles más. La cifra más baja se conoce como estimación P90 y la más alta como estimación P10. En Estados Unidos, la Comisión Nacional de Valores sólo permite que las compañías se refieran a sus reservas como probadas «si el petróleo se halla cerca de una explotación activa y si existe una "certeza razonable" de que pueden ser recuperadas de forma rentable de acuerdo con los actuales precios del petróleo y empleando las tecnologías existentes». Ello significa emplear una estimación P90. Campbell y Laherrère sostienen que una estimación P90 es demasiado estricta y que a menudo infravalora la cantidad de petróleo que producirá efectivamente un yacimiento a lo largo de su vida activa. Consideran más adecuada una estimación intermedia, llamada «probada y probable» o P50. La estimación P50 «es el número de barriles de petróleo que es tan probable que salga como que no salga de un pozo a lo largo de su vida activa, suponiendo que los precios se mantengan dentro de unos márgenes limitados».

Si Estados Unidos subestima las reservas probadas al emplear una estimación P90, países como los que formaban la antigua Unión Soviética han sobreestimado constantemente las reservas probadas a partir de supuestos P10. El gobierno ruso exagera por sistema sus reservas al incluir las reservas geológicas —todo lo que pueda haber en la región—, en lugar de limitarse al concepto de las «reservas económicamente explotables». La distancia entre la realidad y la ficción puede llegar a extremos absurdos. En 1996 World Oil cifraba las reservas recuperables totales de la antigua Unión Soviética en 190.000 millones de barriles de petróleo. Ese mismo año, Oil&Gas Journal publicó que las reservas probadas en la antigua Unión Soviética eran de 57.000 millones de barriles.

Los países de la OPEP exageran sus cifras, según los críticos, para aumentar sus cuotas de producción y conseguir préstamos internacionales de instituciones como el Banco Mundial y el Fondo Monetario Internacional, o para atraer préstamos de bancos privados para el desarrollo de infraestructuras y nuevos proyectos comerciales.

Para hacerse una idea exacta de lo rápida e inexacta que puede ser la circulación de informaciones, piénsese en el hecho de que a mediados de los años ochenta se estimaba que las reservas globales probadas de petróleo crudo estaban entre los 650.000 y los 700.000 millones de barriles. En los años noventa ya se habían añadido 300.000 millones de barriles adicionales de petróleo crudo a las reservas globales probadas —un tercio de las reservas mundiales—, a pesar de que no se habían realizado descubrimientos significativos de nuevos yacimientos. Prácticamente todo el aumento provenía de los países de la OPEP. Arabia Saudí, cuyas reservas probadas habían oscilado durante años entre 163.000 y 170.000 millones de barriles, saltó de golpe a los 257.500 en un solo año, 1990. Las reservas de Kuwait aumentaron 26.100 millones de barriles —de 63.900 a 90.000 millones— entre 1984 y 1985. Las reservas probadas de Irak pasaron a ser más del doble entre 1987 y 1988, de 47.100 a 100.000 millones. Irán pasó de 48.800 millones, en 1987, a 92.900 millones doce meses más tarde. Abu Dhabi y Dubai informaron en 1988 de que sus reservas se habían triplicado. En 1988, tres de los principales países productores de petróleo pretendían haber duplicado las reservas que tenían sólo un año antes. Es cierto que las cifras anteriores eran probablemente demasiado bajas y debían ser revisadas al alza. El motivo es que cuando los yacimientos petroleros eran propiedad de las compañías extranjeras, éstas acostumbraban a silenciar los nuevos descubrimientos para evitarse impuestos. Sin embargo, los monumentales aumentos de las reservas que se anunciaron a mediados y finales de los años ochenta superan con creces lo que puede justificar la corrección de las inexactitudes previas de las compañías.

Otra prueba de la escasa fiabilidad de las cifras es que en los años noventa las compañías petrolíferas descubrieron una media de 7.000 millones de barriles de petróleo por año, pero extrajeron más de tres veces esta cantidad. Sin embargo, a lo largo de los años noventa más de la mitad de los países incluidos en el informe anual de Oil&Gas Journal pretendieron conservar, año tras año, las mismas reservas probadas que el año anterior. En 1997, cincuenta y nueve países productores de petróleo informaron de que sus reservas no habían cambiado desde el año anterior, a pesar de que los yacimientos actuales estaban siendo explotados y que se habían descubierto algunos yacimientos nuevos. En 1999, el número de países que no informaban de ningún cambio en sus reservas había aumentado a setenta.

Test de realidad

Así pues, ¿cuánto petróleo crudo barato recuperable queda todavía? La introducción de métodos sísmicos digitales en 3D, a finales de los años sesenta, ha permitido a los geólogos aumentar progresivamente su precisión a la hora de localizar nuevos yacimientos petrolíferos. La exploración global en busca de petróleo se intensificó notablemente en los años setenta y ochenta, como consecuencia de la guerra árabe-israelí y del subsiguiente embargo petrolero de la OPEP y, más tarde, por la guerra Irán-Irak, que disparó los precios del petróleo hasta los 40 dólares por barril. La preocupación creciente de Estados Unidos y otros países, así como de las compañías energéticas globales, por su dependencia del petróleo de Oriente Medio, les empujó a buscar fuentes alternativas. Las compañías petroleras iniciaron una campaña de búsqueda de nuevos yacimientos a nivel mundial. Sólo en Estados Unidos el número de pozos dedicados a la exploración y el desarrollo aumentó de 28.000 a 90.000 entre 1973 y 1981. Y sin embargo, a pesar del enorme esfuerzo realizado, las reservas probadas de los 48 Estados contiguos descendieron durante ese período de 25.000 millones de barriles en 1973 a 20.000 millones de barriles en 1986 y la producción de crudo se redujo en un 24%.

Los resultados obtenidos en el resto del mundo fueron parecidos. Los geólogos conceden actualmente que la mayor parte de los grandes yacimientos petrolíferos habían sido descubiertos ya antes de la última ola de prospecciones. El clásico argumento de los economistas, según el cual el aumento de los precios del petróleo tendría el efecto de estimular nuevas exploraciones y llevar al descubrimiento de yacimientos petrolíferos significativos, demostró ser cuestionable.

En la actualidad, hay alrededor de 1.500 yacimientos petrolíferos grandes y gigantes en el mundo. Entre todos contienen el 94% de todo el petróleo crudo conocido. Los 400 yacimientos más importantes contienen entre el 60 y el 70% del total. Desde 1980 sólo se han descubierto 41 de estos yacimientos. Según Colín Campbell, miembro del Centro de Análisis del Agotamiento del Petróleo [Oil Depletion Analysis Center, ODAC] de Londres, la conclusión final es que «a estas alturas todo el mundo ha sido objeto de una exploración exhaustiva y ha quedado claro que no quedan nuevas regiones por descubrir comparables al Mar del Norte y Alaska». La Oficina de Estudios Geológicos de Estados Unidos coincide con este diagnóstico. Según sus informes, el descubrimiento de nuevos yacimientos petrolíferos en todo el mundo alcanzó su punto culminante en 1962 y, desde entonces, no ha cesado, de bajar. La edad de oro del petróleo ya ha quedado atrás. Eso no significa que no se sigan descubriendo pequeños yacimientos, pero no serán suficientes como para compensar el descenso continuado del inventario mundial de reservas probadas.

Todos estos datos resultan más inquietantes aún si tenemos en cuenta que la demanda mundial de petróleo crudo se sitúa actualmente en los 24.000 millones de barriles por año y sigue aumentando, mientras que sólo descubrimos menos de 12.000 millones de barriles de petróleo recuperable en nuevos yacimientos en el mismo período, e incluso se prevé que esta cifra descenderá de año en año. En otras palabras, estamos consumiendo casi dos barriles de petróleo crudo convencional por cada nuevo barril que descubrimos.

Por más que los jefes de Estado, los asesores políticos y los economistas occidentales sigan hablando con entusiasmo sobre el aumento de la producción de petróleo en yacimientos situados en regiones distintas de Oriente Medio, la realidad es que el rápido aumento que ha experimentado la producción de países no pertenecientes a la OPEP en los últimos veinte años está comenzando a perder impulso. Durante los años ochenta y noventa buena parte del petróleo provenía del Mar del Norte. La Administración de Información Energética estima que la producción del Mar del Norte tocará techo a finales de 2002, con una producción de 6,77 millones de barriles. Otra región donde los geólogos y las compañías energéticas esperaban encontrar un filón de petróleo comparable a un minigolfo Pérsico era el mar Caspio. En realidad, la región contiene aproximadamente un total de 50.000 millones de barriles de reservas recuperables, más o menos lo mismo que el Mar del Norte, y se espera que su producción toque techo hacia 2010. Mientras tanto, las compañías petrolíferas rusas construyen febrilmente nuevas refinerías y oleoductos y bombean petróleo sin cesar en los mercados mundiales, lo cual está teniendo como efecto la rebaja de los precios mundiales del petróleo y el agotamiento de las reservas probadas de Rusia.

Igual que en el caso del Mar del Norte y el mar Caspio, los yacimientos petrolíferos de Alaska han sido objeto de muchas especulaciones entre los empresarios del petróleo, los economistas de recursos y los líderes políticos. La administración Bush ha puesto un interés especial en abrir el Refugio Nacional de Vida Salvaje del Ártico [Arctic National Wildlife Refuge, ANWR], un prístino hábitat natural, legalmente protegido de la explotación comercial, para realizar prospecciones petrolíferas. El petróleo adicional que podría extraerse de la región, sin embargo, es tan escaso que resulta en buena medida irrelevante para la cuestión de la independencia energética de Estados Unidos.

La Oficina de Estudios Geológicos de Estados Unidos estima que podría haber 20.700 millones de barriles de petróleo recuperable bajo la ANWR, pero de acuerdo con las tecnologías actuales la cantidad de petróleo recuperable es significativamente inferior: como máximo 7.700 millones de barriles. La cantidad de petróleo comercialmente explotable —la cantidad recuperable a 20 dólares por barril— es incluso inferior: alrededor de los 3.000 millones de barriles. Eso significa que la cantidad recuperable desde el punto de vista técnico equivale a 390 días de petróleo convencional de acuerdo con la actual tasa de consumo y que la cantidad recuperable desde el punto de vista económico equivale a tan sólo 152 días de consumo de petróleo. La Administración de Información Energética del Departamento de Energía de Estados Unidos prevé que hacia el año 2020 la ANWR podría proporcionar 1,4 millones de barriles al día. Sin embargo, si tenemos en cuenta que para el año 2020 se espera que la producción de petróleo oscile entre los 112 y los 120 millones al día, vemos que la ANWR sólo aportaría alrededor de un 1% más al suministro total de petróleo.

Franco Bernabe, anterior asesor económico de la Organización para la Cooperación y el Desarrollo Económico, con sede en París, y director ejecutivo de la compañía petrolífera italiana ENISPA, realizó recientemente un estudio sobre doscientas destacadas compañías petroleras no pertenecientes a la OPEP y descubrió que, «entre 1980 y 1997, su ratio entre reservas y producción bajó de dieciocho años a doce». Eso es sin duda una mala noticia para el futuro de la producción petrolífera de los países no pertenecientes a la OPEP. Bernabe concluye que «sólo para mantener esta ratio con el actual 2,5% de incremento anual en la producción mundial, dicho grupo se vería en la necesidad de reemplazar el 140% de sus reservas en los próximos cinco años», un objetivo virtualmente imposible.

El descenso en el número de nuevos descubrimientos y el agotamiento de las reservas probadas adquieren todavía más gravedad a la luz del aumento esperado en la demanda de petróleo para las próximas dos décadas. Se espera que la población mundial pasará de 6.200 a 7.500 millones de personas para el año 2020, por lo que la presión sobre las reservas de petróleo no hará más que intensificarse. El aumento de la población traerá consigo una aceleración del proceso de urbanización. Eso significa más petróleo para el transporte, la calefacción, la electricidad y la producción agrícola e industrial. Las necesidades energéticas de una población en pleno proceso expansivo impondrán una presión sin precedentes sobre las reservas de crudo restantes.

Resulta ilusorio siquiera pensar que la población de los países en vías de desarrollo podrá tener acceso algún día a la cantidad de petróleo per cápita de la que ha disfrutado Estados Unidos durante la edad de oro del petróleo. Si China pretendiera consumir tanto petróleo per cápita como gastamos nosotros en Estados Unidos para mantener nuestro nivel de vida, necesitaría 81 millones de barriles de petróleo al día: 10 millones de barriles más que la totalidad de la producción mundial del año 1997. Según la revista Fortune, si países como China e India se limitaran a incrementar su consumo de energía hasta el nivel per cápita de Corea del Sur, «sólo estos dos países necesitarían un total de 119 millones de barriles de petróleo al día». Esto es, casi un 50% más del total de la demanda mundial del año 2000.

China e India están a la cabeza de una gran transformación que está teniendo lugar en el mundo en vías de desarrollo. Los países más pobres han iniciado un proceso de industrialización, urbanización y modernización, en una carrera por alcanzar los niveles de vida de que disfrutan los países industrializados de Occidente. Necesitan petróleo y, como cada vez es más escaso, se van a ver enfrentados con los países industriales del Norte en una dura competición por acceder a las reservas restantes. En un artículo escrito para The Economist, el editor Edward Carr estima lo siguiente:

Hacia el año 2010 la cuota de consumo energético total correspondiente a los países ricos habrá caído, por primera vez en la era industrial, por debajo del 50% [...] El crecimiento del consumo energético de los países en vías de desarrollo entre 2000 y 2010 será mayor que el consumo actual de Europa Occidental.

Es probable que la creciente demanda de petróleo, tanto en los países industrializados como en el mundo en vías de desarrollo, se convierta en el factor más importante dentro de los conflictos geopolíticos del primer cuarto del siglo XXI. Las proyecciones de la EIA sobre la demanda global de petróleo revelan lo duros que serán los retos que nos esperan. Según esta agencia, la demanda diaria mundial de petróleo aumentará de 80 millones a 120 millones de barriles al día antes de 2020, un aumento del 50% en menos de veinte años. Será difícil encontrar y extraer 40 millones de barriles adicionales de petróleo barato por día.

Debe quedar claro que la cuestión no es si el petróleo se está agotando o no, sino si la producción de petróleo, que ha sido el lubricante de los grandes avances industriales del siglo XX, está a punto de tocar techo. Sobre este punto existen diferencias de opinión entre los expertos.

Resulta interesante señalar que la mayoría de los estudios geológicos realizados en los últimos cincuenta años han mostrado una notable coherencia en cuanto a sus estimaciones de reservas totales de petróleo recuperable. James J. Mackenzie, que fue miembro del Consejo Presidencial para la Calidad del Medio Ambiente [Council on Environmental Quality, CEQ] entre 1977 y 1981 y que actualmente participa en el Programa sobre el Clima, la Energía y la Contaminación del Instituto de Recursos Mundiales, ha escrito que «la gran mayoría de estos estudios refleja un consenso entre los expertos en petróleo según el cual las reservas totales estimadas de petróleo recuperable se sitúan entre 1,8 y 2,2 billones de barriles». El mundo ha consumido ya más de 875.000 millones de barriles del total.

Tal como se ha señalado antes, la Oficina de Estudios Geológicos de Estados Unidos sitúa las EUR de petróleo por encima de las previsiones anteriores, en 3 billones de barriles. Esta proyección tan optimista se basa, en parte, en la creencia de que la antigua Unión Soviética, Oriente Medio, los deltas del Níger y del Congo en África y la plataforma noreste de Groenlandia contienen reservas inexploradas potencialmente importantes.

John Edwards, de la Universidad de Colorado, es igualmente optimista. Sus proyecciones se basan en añadir a las estimaciones de las reservas totales recuperables las reservas de petróleo no convencional, como el crudo pesado de Venezuela y las arenas asfálticas de Canadá, así como la posible transformación en líquido del 20% de las reservas de gas natural. Basándose en una interpretación liberal de lo que forma parte de las EUR, Edwards predice que la producción global de petróleo tocará techo entre 2030 y 2040. Si sólo se considera el petróleo convencional, Edwards prevé que la producción global tocará techo entre 2020 y 2030.

Colin Campbell y Jean Laherrère se sitúan, en cambio, en el otro extremo, ya que estiman las reservas totales recuperables de petróleo en sólo 1,8 billones de barriles. Según ellos, los países productores de petróleo han exagerado groseramente las cifras de sus reservas con fines políticos, sobre todo Rusia y los países de la OPEP. Campbell y Laherrère no son los únicos que piensan así. Cada vez son más los estudios publicados por geólogos de talla mundial que presentan resultados notablemente alejados de las tesis convencionales, sobre la base de nuevos modelos informáticos.

Estos nuevos estudios sugieren que la producción global de petróleo tocará techo en algún momento entre 2010 y 2020, y algunos de los estudios estiman incluso que antes de 2010. En otras palabras, en este tiempo se habrá extraído la mitad de las reservas recuperables. Una vez que la producción toque techo, los precios del petróleo no dejarán de aumentar como resultado de la competencia de los países, las empresas y los consumidores por la mitad restante. A diferencia de la primera crisis del petróleo de los años setenta y ochenta, que fue inducida políticamente, esta vez la crisis se basará en una escasez real. Cada año que pase habrá menos crudo barato disponible en el mundo. El descenso del crudo barato, combinado con el aumento de la población humana (sobre todo en el mundo en vías de desarrollo) generará una dinámica nueva y peligrosa.

El modelo metodológico en el que se basan todas estas predicciones se conoce como la «curva de Hubbert». M. King Hubbert fue un geofísico que trabajó para la compañía Shell Oil. En 1956 publicó un artículo, que posteriormente se ha hecho famoso, en el que predecía el auge y la caída de la producción de petróleo en los 48 Estados contiguos. Basándose en la cantidad y el ritmo de la producción en el pasado, King estimó que la producción de petróleo en Estados Unidos tocaría techo entre 1965 y 1970. En el momento en que hizo su predicción, la producción de petróleo en Norteamérica estaba alcanzando cifras récord. La mayoría de los geólogos y ejecutivos de las compañías energéticas se burlaron entonces de la predicción y ridiculizaron tanto al autor como a sus tesis. Para su sorpresa, la predicción de Hubbert demostró ser cierta. La producción tocó techo en 1970 e inició un descenso continuado. Estados Unidos perdió su papel preeminente como principal productor mundial de petróleo y ese cambio ha dictado buena parte de la geopolítica mundial desde entonces.

La tesis de Hubbert es elegante por su simplicidad. Su idea era que la producción de petróleo comienza desde cero, sube, llega a su punto máximo cuando se han explotado la mitad de las reservas totales recuperables, y luego cae siguiendo una curva clásica en forma de campana. La extracción de petróleo comienza lentamente y luego se acelera con rapidez a medida que se localizan yacimientos petrolíferos importantes. Una vez se han encontrado y explotado los principales yacimientos, la producción comienza a perder impulso. Los yacimientos pequeños son más difíciles de encontrar y el petróleo que contienen es más caro de perforar y explotar. Al mismo tiempo, y a medida que se agotan los yacimientos principales, se hace cada vez más difícil sacar a la superficie el petróleo restante. El surtidor deja paso a un goteo cada vez más lento. La combinación del descenso en el ritmo de los descubrimientos y en la tasa de extracción de petróleo de los yacimientos existentes hace que finalmente la producción toque techo. El punto más alto de esta curva en forma de campana representa el punto medio en el que la mitad de las reservas recuperables totales han sido extraídas. A partir de este punto, la producción cae tan rápidamente como había subido antes, siguiendo la segunda mitad de la curva en forma de campana.

Un examen más detallado de la curva de Hubbert revela un aspecto de gran relevancia para el futuro que nos espera. Hubbert observó que se habían necesitado ciento diez años —desde 1859 hasta 1969— para producir 227.000 millones de barriles de crudo barato. La mitad de este petróleo fue extraída en los primeros cien años. Para la segunda mitad, en cambio, hicieron falta menos de diez años, entre 1959 y 1969. Usando el mismo modelo, Hubbert estimó en 1971 que el 80% central de la producción global de petróleo será extraído en un período de entre 58 y 64 años, menos tiempo del que dura una vida humana.

Las Casandras contra los optimistas

Los geólogos han combinado la curva de Hubbert con modelos matemáticos para predecir el momento en que la producción global va a tocar techo. Colin J. Campbell y Jean H. Laherrère reavivaron el debate acerca de esta cuestión en un extenso artículo publicado en Scientific American en marzo de 1998. Campbell, doctor en geología por la Universidad de Oxford, había trabajado para Texaco como geólogo de exploración y más tarde como director de estudios geológicos en Ecuador para AMOCO. Luego se convirtió en director de exploraciones en Noruega para AMOCO y en vicepresidente ejecutivo de FINA en el mismo país. Laherrère había trabajado para la compañía petrolera francesa Total, donde se encargaba de la supervisión de las técnicas de exploración en todo el mundo. Sus estudios llevaron al descubrimiento del principal yacimiento petrolífero de África. Campbell y Laherrère han estado asociados durante muchos años a Petroconsultants, una empresa consultora radicada en Ginebra, Suiza, que gestiona bases de datos industriales.

El estudio de Campbell y Laherrère se apoyaba en una base de datos de Petroconsultants que cubría 18.000 yacimientos petrolíferos de todo el mundo. De acuerdo con sus datos, en 1996 sólo había 850.000 millones de barriles de petróleo convencional en todo el mundo en reservas P50. Esta cifra es significativamente inferior a los 1,019 billones de barriles que recogía The Oil & Gas Journal y a los 1,160 billones de barriles que aparecían publicados en World Oil. Campbell y Laherrère conceden que se puedan descubrir 150.000 millones más de petróleo, con lo que estiman que a la industria petrolífera todavía le queda 1 billón de barriles por extraer. Esto representa una cantidad de petróleo ligeramente superior a los 875.000 millones de barriles que se han producido hasta ahora. La producción de petróleo de los países no pertenecientes a la OPEP tocará techo antes de 2010, mientras que los cinco principales países productores de la OPEP en Oriente Medio —Arabia Saudí, Kuwait, Irak, Irán y Abu Dhabi— alcanzarán su pico de producción alrededor de 2015. Basándose en el conjunto de los datos y en los modelos informáticos, los dos geólogos predicen que la producción global de petróleo tocará techo alrededor del año 2010.

Hay otros geólogos que concuerdan con las estimaciones de Campbell y Laherrère. L. F. (Buz) Ivanhoe es geólogo del petróleo y fue anteriormente asesor de Occidental Petroleum para la evaluación de cuencas petrolíferas en todo el mundo. Ivanhoe cree que el suministro de petróleo dejará de cubrir la demanda mundial aproximadamente en 2010 y que a partir de entonces bajará rápidamente a razón de un 3% por año. Según Ivanhoe, cuando la producción toque techo, aproximadamente en 2010, hay que esperar un repunte en el precio del crudo y otros combustibles, acompañado de una hiperinflación global. Ivanhoe advierte que un descenso de sólo el 5% en el suministro global de crudo «podría hacer que se reprodujeran las colas en las gasolineras de los años setenta [...] pero esta vez la escasez de petróleo será permanente».

Tomando como base la estimación de que quedan 1,55 billones de barriles de petróleo —una cifra un 55% más alta que la usada por Campbell y Laherrère—, un geólogo de la Universidad de Toledo, Craig Hatfield, llega, sin embargo, a una conclusión similar en cuanto al tiempo que falta para que la producción global toque techo. Hatfield parte de la base de que las reservas globales de petróleo conocidas ascienden a 1 billón de barriles, 150.000 millones de barriles más que la cantidad existente según Campbell y Laherrère. Hatfield reconoce que tal vez sea demasiado optimista y que algunas de estas reservas pueden corresponder tan sólo a cálculos inspirados políticamente. Luego estima que todavía faltan por descubrir 550.000 millones de barriles de petróleo recuperable, de nuevo una cifra significativamente superior a las estimaciones de Campbell y Laherrère. Combinando ambas cifras, Hatfield especula que todavía podría haber 1,55 billones de barriles de petróleo por extraer. Tras añadir esta cifra a los 800.000 millones de barriles ya explotados, Hatfield llega a un total de 2,350 billones de barriles de reservas de petróleo recuperable.

Suponiendo que el consumo global de petróleo siga aumentando a un ritmo del 2%, Hatfield concluye que antes de 2010 el mundo habrá consumido la mitad de las reservas totales de petróleo. La fecha se podría retrasar algunos años más, dice Hatfield, si los países de la OPEP restringen la producción petrolífera, tal como han hecho en el pasado, para mantener altos los precios en los mercados mundiales.

James J. MacKenzie dice que incluso suponiendo unas reservas totales recuperables de 2,6 billones de barriles, una cifra superior a la mayoría de las estimaciones, el pico de la producción de petróleo sólo se retrasaría hasta el año 2019.

El debate ha contado también con otras participaciones. Franco Bernabe piensa que la producción global podría tocar techo en la primera década del siglo XXI, tras lo cual se produciría una crisis petrolífera parecida a la de los años setenta. Kenneth S. Deffeyes, profesor emérito en la Universidad de Princeton y uno de los pioneros de la ingeniería petrolera por su anterior trabajo en el Laboratorio de Investigación de Shell Oil en Houston, Texas, dice que el pico en la producción global de petróleo podría producirse en 2003 o bien en 2009, dependiendo de cuál de los métodos de Hubbert —Deffeyes fue compañero de trabajo de M. King Hubbert— se utilice para el cálculo. Deffeyes basa sus cálculos en la posibilidad de que queden aproximadamente 2,1 billones de barriles de reservas de crudo recuperable. Y añade:

Ninguna iniciativa que se pueda poner en marcha hoy puede alterar sustancialmente el año en que la producción tocará techo. No hay ninguna exploración del mar Caspio, ninguna campaña de perforaciones en el mar del Sur de la China, ningún sustituto para los vehículos deportivos, ningún proyecto de energías renovables que pueda implantarse a una velocidad suficiente como para evitar una guerra de precios por las restantes reservas de petróleo.

Deffeyes, claramente preocupado por las consecuencias internacionales de un descenso temprano en la producción global de petróleo, bromea diciendo que «por lo menos, esperemos que la guerra se haga con dinero, y no con cabezas nucleares».

La Agencia Internacional de la Energía de la Organización para la Cooperación y el Desarrollo Económicos (OCDE) estima que la demanda energética mundial podría llegar a crecer hasta un 57% antes de 2020 y que la producción global de petróleo convencional tocará techo en algún momento de la segunda década del siglo XXI, entre 2010 y 2020.

Así pues, los expertos están divididos en dos grandes grupos: unos creen que todavía faltan entre veintiocho y treinta y ocho años para que la producción de petróleo convencional toque techo y otros piensan que probablemente serán muchos menos, entre ocho y dieciocho. De nuevo es importante resaltar que las proyecciones de los optimistas y los pesimistas sobre el momento en que la producción global de petróleo tocará techo no difieren más de diez o treinta años, lo que representa una brecha temporal muy pequeña dentro de la historia. Ambos grupos consideran que la época del crudo barato está llegando a su fin, aunque sus diferencias en la perspectiva temporal son cruciales a la hora de determinar las prioridades, tanto en términos de políticas energéticas como de iniciativas políticas y económicas.

Los optimistas responden a las conclusiones más pesimistas de los nuevos estudios basados en modelos informáticos con el argumento de que no es cierto que no queden yacimientos importantes de petróleo por descubrir y ponen como ejemplo el descubrimiento de un yacimiento gigante nuevo —el primero en años—, realizado por la compañía francesa Elf en la costa oriental de África, así como el descubrimiento de dos nuevos yacimientos supergigantes en Kazajstán e Irán. Los optimistas tienen la esperanza de que habrá nuevos descubrimientos que permitirán aportar 5.000 millones de barriles adicionales de petróleo por año, con lo que se reduciría a la mitad la distancia entre el suministro y la demanda en 2010. La Oficina de Estudios Geológicos de Estados Unidos, por ejemplo, estima que en la antigua Unión Soviética podría haber hasta 100.000 millones de barriles de petróleo por descubrir. La Oficina también se muestra optimista en sus previsiones de encontrar más petróleo en Oriente Medio y en zonas del Atlántico próximas a Sudáfrica y Sudamérica.

Colin Campbell considera que estas cifras son exageradamente optimistas y dice que sobre la base de todos los datos disponibles la cantidad de petróleo convencional recuperable que puede haber en la antigua Unión Soviética apenas llega a dos tercios del total que prevé la USGS.

Por lo que respecta a la posibilidad de encontrar otros yacimientos gigantes como el que se ha descubierto en la costa oriental africana, los críticos son prácticamente unánimes al considerar que es posible que queden unos pocos yacimientos importantes por descubrir, pero que es muy improbable que se localicen «megayacimientos» comparables a los de Kuwait y Arabia Saudí. «La cantidad de crudo que hay en el mundo es limitada —dice Campbell—, y la industria ha encontrado ya prácticamente el 90%.»

Más que confiar en el descubrimiento de nuevos yacimientos importantes, la mayoría de los optimistas ponen sus esperanzas en la posibilidad de que se produzcan avances tecnológicos que permitan extraer más cantidad de petróleo de los yacimientos existentes. Igual que otras personas vinculadas a la industria, Douglas Bohi, un economista que trabaja para Charles River Associates, en Washington, D.C., cree que «hay razones para esperar un aumento espectacular en la base de las reservas [de petróleo]».

Ciertamente, el incremento de las reservas mundiales de petróleo ha sido constante a lo largo de las dos últimas décadas, lo cual ha llevado a geólogos de la talla de William Fisher, de la Universidad de Texas, en Austin, a la conclusión de que «faltan treinta años, tal vez incluso cuarenta, para tocar techo». La USGS considera que la razón está de parte de los optimistas y ha revisado al alza las estimaciones de crecimiento de las reservas recuperables hasta situarlas en 612.000 millones de barriles.

La cuestión del incremento de las reservas en los yacimientos existentes guarda tanta relación con las condiciones de mercado como con las innovaciones tecnológicas. Si los precios del petróleo suben en los mercados mundiales, el desarrollo y aplicación de tecnologías de perforación nuevas y más caras se convierte en una opción viable desde el punto de vista comercial. Hay tres nuevas tecnologías que ya están contribuyendo de forma significativa al aumento de las reservas. El nuevo análisis sísmico 4D no sólo permite a los geólogos localizar los lugares donde hay petróleo, agua o gas dentro de un yacimiento determinado, sino también predecir hacia dónde se desplazarán. Las nuevas tecnologías de control y seguimiento pueden aumentar la capacidad de recuperación hasta diez o quince puntos porcentuales en algunos yacimientos, aunque la técnica sólo es aplicable en zonas donde prevalecen las rocas blandas.

Hace tiempo que los ingenieros del petróleo saben que con las técnicas convencionales de extracción, cuando el flujo de petróleo en los pozos queda reducido a un simple goteo, puede quedar todavía hasta un 60% del petróleo abajo. Para extraer este petróleo lo que hacen es inyectar gas natural, vapor o dióxido de carbono líquido en los pozos agotados. La inyección penetra en los poros de las rocas y empuja el petróleo que pudiera haber quedado hacia pozos adyacentes. También se inyecta agua bajo el petróleo para aumentar la presión y forzarlo a salir a la superficie. Tales técnicas han incrementado el factor de recuperación hasta un 10 o un 15%, aunque son caras. Las técnicas de seguimiento sísmico en 4D incrementan en un 10 o 15% el coste por barril producido. Las nuevas técnicas de inyección son todavía más caras, ya que aumentan el coste de la producción de petróleo hasta un 50 o un 100%.

Otra técnica para incrementar las tasas de recuperación es la perforación direccional y es más barata que la inyección. Los ingenieros emplean nuevos equipos sensores para medir la resistencia eléctrica de las rocas adyacentes y utilizan un nuevo tipo de perforadoras más versátiles que se pueden orientar hacia los lugares donde se encuentra el petróleo.

Los optimistas señalan que en los años sesenta sólo se recuperaba el 30% del petróleo en la mayoría de los yacimientos. La tasa de recuperación actual está entre el 40 y el 50%, y en unos pocos años podría llegar al 75% en muchos yacimientos. Los pesimistas responden diciendo que «buena parte de la tecnología tiene por objeto aumentar las tasas de producción [...] no hace demasiado por aumentar las reservas mismas». Por otro lado, las tecnologías de control, seguimiento y perforación han experimentado un desarrollo constante durante los últimos cien años y, por lo tanto, ya están contempladas en la curva de Hubbert. Y si hablamos de los principales yacimientos petrolíferos de Oriente Medio, las nuevas tecnologías sirven de bien poco, dado que no hace falta hacer demasiado para que el petróleo salga a la superficie. Según los pesimistas, las nuevas técnicas de recuperación sólo introducirán algunos cambios menores en la curva. El físico Albert Bartlett, profesor emérito de la Universidad de Colorado, en Boulder, afirma que «todas estas cosas de las que hablan los economistas apenas pueden introducir una ligera desviación en la curva». El pronóstico personal de Bartlett es que la producción global de petróleo tocará techo en 2004.

Por último, los optimistas vuelven su mirada hacia los océanos y argumentan que es probable que existan nuevos yacimientos petrolíferos a mil metros o más de profundidad. Las nuevas tecnologías permiten a los ingenieros localizar y extraer este petróleo, aunque el proceso resulta caro. Roger Anderson, investigador Doherty en el Observatorio Terrestre Lamont-Doherty de la Universidad de Columbia, ha escrito un artículo entusiasta en Scientific American sobre la exploración en aguas profundas. Anderson y otros creen que esta clase de exploraciones pueden incrementar en un 5% las reservas globales combinadas, aunque conceden que eso no representará una diferencia decisiva. «Aunque es improbable que estas técnicas puedan eliminar por completo el descenso inminente en el suministro de crudo», Anderson mantiene que «nos darán el tiempo necesario para realizar una transición ordenada hacia un mundo basado en otras fuentes energéticas».

Los pesimistas argumentan que, en último término, poco se puede hacer en el terreno de los descubrimientos o de las nuevas tecnologías que afecte significativamente a los plazos de tiempo de que disponemos. Estemos o no preparados, la producción global de petróleo tocará techo probablemente en algún momento entre 2010 y 2020. El doctor Walter Youngquist, una de las principales figuras de la geología del siglo XX, se suma al bando de los pesimistas: «Las observaciones que he realizado en aproximadamente setenta países durante cincuenta años de viajes y trabajo me dicen claramente que estamos a punto de llegar al cambio de pendiente». Youngquist advierte que «la inercia del crecimiento de la población y el consumo de recursos es tan grande que parece imposible evitar el desastre».

Las últimas reservas de petróleo

Aunque discrepen acerca del momento en que la producción global de crudo convencional tocará techo, tanto los optimistas como los pesimistas están de acuerdo en que la mayor parte de las reservas que quedan se hallan en Oriente Medio y que es sólo una cuestión de tiempo que el mundo pase a depender del golfo Pérsico para satisfacer sus crecientes necesidades de petróleo. Estados Unidos, durante mucho tiempo el principal productor de petróleo y hasta los años cincuenta el responsable de más de la mitad de la producción mundial, ha experimentado un descenso continuado en su producción desde 1970, el año en que ésta tocó techo. Desde entonces, Estados Unidos ha tenido que importar cada vez más petróleo. Actualmente, este país sigue siendo el principal consumidor de crudo. Con sólo el 5% de la población mundial, Estados Unidos consume casi el 26% del petróleo que se extrae en todo el mundo. Sólo produce el 11% del petróleo mundial y actualmente posee el 2 % de las reservas globales. La EIA predice que Estados Unidos será todavía más dependiente del petróleo extranjero en los próximos años, una perspectiva preocupante si tenemos en cuenta que las importaciones de petróleo son una partida importante dentro de la deficitaria balanza comercial estadounidense.

La mayoría de los norteamericanos se sorprenderían sin duda si supieran que el porcentaje de petróleo que importamos actualmente de la OPEP es inferior al de hace veinticinco años. En los seis primeros meses de 2001, Estados Unidos compró más petróleo a Canadá que a Arabia Saudí y sólo dos de nuestros diez principales suministradores de petróleo eran países de Oriente Medio.

A lo largo de los próximos cuatro o cinco años se espera que Estados Unidos y otros países aumenten progresivamente sus importaciones petroleras de Rusia. Las compañías petroleras rusas han invertido miles de millones de dólares en nuevas campañas de exploración y perforación, y el gobierno ruso ha contribuido a la construcción de nuevos oleoductos hacia el mar Báltico y el mar Negro. El resultado es que la producción de petróleo rusa ha aumentado hasta los 7 millones de barriles al día en 2002, lo que convierte al país, al menos temporalmente, en el principal productor de petróleo del mundo.

En el otoño de 2001, el petróleo ruso inundó los mercados mundiales, lo cual provocó un descenso tal de los precios que la OPEP amenazó abiertamente con iniciar una guerra comercial si Rusia no recortaba su producción. No obstante, a menos que la OPEP haga efectivas sus amenazas, es improbable que Rusia recorte de forma significativa su producción de petróleo. El gobierno ruso ya no está en posición de dictar los términos a las compañías petroleras de su país, que se encuentran en manos privadas, y tiene muchos intereses puestos en las exportaciones de petróleo y gas natural. Después de todo, cuatro de cada diez dólares que ingresan en las arcas públicas proceden de las exportaciones de energía. Los planes económicos del gobierno ruso se basan, por lo tanto, en mantener las exportaciones de petróleo en niveles parecidos o superiores a los actuales y a un precio de 20 dólares por barril, lo cual sitúa al gobierno ruso en una especie de círculo vicioso. Si las compañías petroleras rusas siguen inundando los mercados mundiales y los precios bajan hasta los 17 dólares o menos por barril, el gobierno ruso vería frenado su intento de modernizar la economía y mejorar las condiciones de vida en el país. Por otro lado, si se recortan significativamente las exportaciones de petróleo para mantener los precios altos, el volumen podría ser insuficiente para aportar los ingresos necesarios al tesoro público.

El presidente ruso Vladimir V. Putin es consciente de la nueva posición estratégica de la que disfruta su país gracias a las exportaciones petrolíferas. En octubre de 2001 dijo a los delegados del foro económico mundial reunidos en Moscú: «Ahora que los mercados mundiales se ven afectados por una creciente inestabilidad, Rusia sigue siendo un socio y un suministrador de petróleo fiable y seguro». A nadie se le escapó la referencia velada de Putin a la tensión y la inestabilidad en Oriente Medio, sobre todo a la luz de los ataques del 11 de septiembre contra las torres gemelas del World Trade Center.

Es probable que el nuevo estatus de Rusia en el mercado mundial del petróleo tenga una vida corta. Las reservas de petróleo de la antigua Unión Soviética no han parado de bajar durante dos décadas. En 1975 la URSS presumía de unas reservas de 83.000 millones de barriles. A mediados de los años noventa, la antigua Unión Soviética —incluyendo todas las repúblicas que formaban parte anteriormente de la URSS— reunía unas reservas inferiores a los 57.000 millones.

Durante los próximos años, el descenso de la producción petrolera en Rusia, así como en el Mar del Norte, la vertiente norte de Alaska, las costas de África occidental y otras regiones, dejará a Oriente Medio en la envidiable posición de proveedor de los últimos recursos antes de que termine la década. A pesar de las exageraciones en cuanto a las reservas, todo el mundo está de acuerdo en que dos terceras partes del petróleo que queda en el mundo se hallan en Oriente Medio. Por sí sola, Arabia Saudí posee el 26% de las reservas globales de petróleo.

La verdadera importancia del petróleo de Oriente Medio se hace inmediatamente evidente cuando examinamos la naturaleza de los yacimientos petrolíferos del golfo Pérsico. Hay más de 40.000 yacimientos petrolíferos conocidos en el mundo, pero sólo 40 yacimientos supergigantes —es decir, con más de 5.000 millones de barriles de petróleo— contienen más de la mitad de las reservas de petróleo del mundo. De estos 40 yacimientos supergigantes, 26 se hallan en el golfo Pérsico. Además, mientras que otros yacimientos gigantes, sobre todo los de Estados Unidos y Rusia, han tocado techo y se hallan en fase descendente, los yacimientos de Oriente Medio todavía están en la fase ascendente de la curva de campana. La ratio entre reserva y producción (R/P) lo dice todo. La R/P es el número de años que durarán las reservas de petróleo de acuerdo con las tasas actuales de producción. En Estados Unidos, donde se ha extraído más del 60% del petróleo recuperable, la R/P es de 10/1. En Noruega la ratio R/P es también de 10/1 y en Canadá de 8/1. En cambio, la ratio R/P en Irán es de 53/1, en Arabia Saudí de 55/1, en los Emiratos Árabes Unidos de 75/1, en Kuwait de 116/1 y en Irak de 526/1.

Durante la próxima década, la producción de petróleo gravitará de nuevo hacia Oriente Medio. De acuerdo con las previsiones de la Administración de Información Energética, el golfo Pérsico contiene dos tercios de las reservas restantes de petróleo y a lo largo de los próximos años aportará una cuota cada vez más importante de la producción mundial. Recordemos que en el momento en que se produjo la primera crisis del petróleo, en 1973, Oriente Medio era responsable del 38% de la producción mundial de crudo. En los siguientes años, la cuota de Oriente Medio en los mercados mundiales cayó hasta el 18%, como consecuencia de la reducción del consumo de los países y de las exploraciones en busca de petróleo realizadas por las compañías petroleras en otras regiones del planeta. Con el rápido agotamiento de estos yacimientos, Oriente Medio ha comenzado a recuperar posiciones en el mercado y actualmente representa una cuota del 30% del total. Esta vez, sin embargo, la cuota de mercado de Oriente Medio seguirá subiendo hasta cubrir más de la mitad de la producción mundial de petróleo, tal como advierte la EIA.

Los analistas de la industria afirman que cuando los «productores estratégicos» —los cinco principales países productores de Oriente Medio— lleguen a controlar más de un tercio de la producción mundial, volverán a estar en posición de dictar el precio del petróleo en los mercados mundiales, tal como hicieron durante un breve período de tiempo en los años setenta. La cuestión depende en buena medida de cuándo comiencen a disminuir las exportaciones rusas de crudo.

Campbell y sus colegas prevén un proceso dividido en dos fases. En la primera de ellas, los productores estratégicos de Oriente Medio controlarán una cuota importante de la producción mundial —aproximadamente un tercio—, lo cual les permitirá aumentar el precio del petróleo. Diez años más tarde, aproximadamente en 2015, la producción de petróleo de los países del golfo Pérsico tocará techo, lo cual significa que los precios del petróleo se dispararán definitivamente. Nos guste o no, dice Walter Youngquist, «los países musulmanes del golfo Pérsico están destinados geológicamente a tener la última palabra en la cuestión del petróleo».

El nivel que alcancen los precios del petróleo vendrá determinado por una serie de variables. En primer lugar, los cinco productores estratégicos de Oriente Medio, de acuerdo con las previsiones de la IEA, deberían aumentar la producción de petróleo de 27 a 48 millones de barriles al día antes de 2010 para cubrir la creciente demanda global. Sin embargo, el incremento previsto para la producción petrolera en Oriente Medio se quedará probablemente por debajo de las necesidades de la demanda en unos 10 millones de barriles al día, una peligrosa situación de escasez de petróleo que podría aumentar significativamente el precio del barril en los mercados mundiales. En segundo lugar, incluso los actuales planes de expansión —demasiado limitados como para satisfacer la demanda prevista— serán tan costosos que llevarán probablemente a un incremento todavía mayor del precio por barril, mucho antes de que la producción de petróleo llegue a tocar techo en Oriente Medio. Joseph Riva, antiguo miembro del Servicio de Investigación del Congreso de Estados Unidos, advierte de que

el aumento previsto de la producción de petróleo [...] es menos de la mitad del que sería necesario para satisfacer la demanda mundial de petróleo en 2010, de acuerdo con las previsiones de la [IEA], pero costará más de 100.000 millones de dólares, más otros 20.000 millones de dólares adicionales destinados a modernizar y ampliar las refinerías del golfo Pérsico para que puedan cubrir la creciente demanda mundial. Aumentar la producción de petróleo más allá de lo previsto sería incluso más caro, desde el punto de vista del coste por barril, dado que el petróleo restante es cada vez más difícil de recuperar [...].

De una forma u otra, afirma un número cada vez mayor de geólogos y analistas de la industria, el precio del petróleo en los mercados mundiales está destinado a subir y es probable que lo haga mucho antes de lo que espera la mayoría de la gente. Las señales de alarma están por todas partes. Sin embargo, mientras el petróleo siga siendo relativamente barato y abundante, a corto plazo habrá pocas personas dispuestas a enfrentarse a las nubes de tormenta que se levantan en el horizonte. El mundo está entrando en aguas turbulentas, dicen las Casandras, y no está preparado para las consecuencias que nos esperan. Y esta vez la crisis del petróleo no será temporal, sino permanente, y nos obligará a realizar un cambio fundamental en nuestro estilo de vida con efectos que se extenderán hasta bien entrado el futuro.

Para muchas personas, la posibilidad de que nos estemos quedando sin las reservas necesarias de petróleo «barato» para mantener en pie el estilo de vida industrial resulta tan inimaginable que probablemente contemplarán con incredulidad la simple idea de que tal cosa pueda suceder, con independencia de cuántos estudios se realicen sobre la materia. Nuestra despreocupación es comprensible. Es raro que las sociedades respondan a un cambio «anticipado» en sus circunstancias. Pero cuando este cambio potencial puede afectar radicalmente al conjunto de nuestro estilo de vida y al propio equilibrio geopolítico del mundo en el que vivimos, la indiferencia colectiva conduce irremediablemente al desastre.

Así como la previsión de una crisis energética auténticamente global es un fenómeno nuevo, la historia de la humanidad está repleta de ejemplos de grandes civilizaciones que no hicieron caso a las señales de alarma, llevaron al límite sus regímenes energéticos locales y sufrieron finalmente un colapso catastrófico. Para tomar las decisiones correctas de cara al futuro es necesario comprender cómo se enfrentaron las civilizaciones pasadas a sus propias crisis energéticas. En realidad, existe una serie de reglas inherentes al juego de la energía que las civilizaciones deben respetar si quieren sobrevivir, prosperar y mantenerse en constante renovación. Cuando estas reglas son ignoradas o pasadas por alto, las sociedades envejecen o mueren. Aprender estas reglas es la guía más importante que tenemos para reflexionar acerca de nuestro propio futuro energético.

Capítulo 3

LA ENERGÍA Y EL AUGE Y LA CAÍDA DE LAS CIVILIZACIONES

El premio Nobel británico de química Frederick Soddy observó en una ocasión que la moneda indivisible en que se basa toda la ciencia es la energía. Cada día, los rayos del Sol bañan la Tierra con miles de kilocalorías de energía por metro cuadrado. Parte de esta energía es captada por los seres vivos y transformada en formas útiles para el mantenimiento de la vida, mientras que el resto termina convertido en calor y es irradiado de vuelta al espacio.

Si la energía es el alfa y el omega de la existencia, la potencia se define como «la tasa de flujo de energía útil». Toda forma de vida requiere energía y suficiente potencia como para mantener un flujo constante. La lucha por la supervivencia, tanto dentro de una como entre distintas especies, es en realidad una competición por captar energía útil y asegurarse su flujo continuado a través de los sistemas vivos.

La cultura y la energía

El antropólogo Leslie A. White ha observado que en la evolución cultural de los seres humanos la primera «planta energética» fueron sus propios cuerpos. Durante la mayor parte de la historia de la humanidad, el Homo sapiens vivió una existencia de cazador-recolector y se dedicó a captar la energía almacenada en las plantas y los animales en estado natural. A través de una acción colectiva y cooperativa sobre su entorno, los individuos podían aumentar su masa crítica y utilizar sus plantas energéticas humanas para asegurarse todo cuanto necesitaban y mantener pequeñas comunidades basadas en el parentesco. Más adelante, cuando nuestra especie realizó la transición desde el estadio de los cazadores-recolectores al de los ganaderos y los granjeros, los seres humanos estuvieron en condiciones de secuestrar más energía de su entorno. Gracias a la domesticación de animales y plantas, se aseguraron un suministro constante y fiable, así como excedentes de energía disponible, lo cual les permitió incrementar la cantidad de energía que fluía por sus cuerpos y sus comunidades. El cultivo de plantas, con la ayuda de sistemas de irrigación, incrementó notablemente la producción por unidad de energía humana o trabajo realizado. Los excedentes agrarios también liberaron a algunos individuos del trabajo en el campo. Estas personas liberadas del trabajo dieron origen a las jerarquías sociales y a la diferenciación de las tareas. Lentamente fueron apareciendo las clases de los sacerdotes y los guerreros y, más tarde, la clase de los artesanos. La diferenciación y especialización de las tareas generó nuevas estructuras institucionales más complejas, las cuales, a su vez, contribuyeron a promover un mayor flujo de energía.

La aparición del cultivo de los cereales hace aproximadamente 10.000 años en el norte de África, Oriente Medio, China e India marcó un punto de inflexión para la sociedad humana. Se ha dicho que los cereales han sido «el gran motor de la civilización». Los excedentes alimentarios constituían una reserva energética que permitía mantener a una población cada vez más grande y fundar primero reinos y más tarde imperios. Las grandes civilizaciones de Egipto y Mesopotamia se alzaron poco después de que se iniciara el cultivo de cereales. Se emprendieron grandes proyectos de ingeniería, entre ellos la construcción de complejos sistemas hidráulicos para la irrigación de los campos. Las mujeres inventaron la cerámica y proporcionaron con ello recipientes en los que colocar los excedentes de cereal para su almacenamiento y/o comercialización. Las artes metalúrgicas contribuyeron al desarrollo, de un armamento más sofisticado para la conquista y la captura de nuevas tierras y esclavos. Los miembros de la clase improductiva de los sacerdotes dedicaban su tiempo, en parte, a estudiar los movimientos de los planetas y las estrellas, lo cual mejoraba su capacidad de predecir las inundaciones primaverales, así como el mejor momento para plantar las semillas. Las matemáticas y la escritura también surgieron de la mano de las civilizaciones basadas en los cereales. Las matemáticas proporcionaban los medios necesarios para erigir grandes monumentos, entre los que destacan las pirámides de Egipto. La escritura demostró ser particularmente útil tanto para la conservación del conocimiento colectivo de unas sociedades cada vez más complejas y diversificadas, como para organizar el flujo de la comunicación dentro de civilizaciones que dominaban grandes territorios.

El siguiente cambio, que sustituyó el modelo de vida basado en la agricultura por otro basado en la industria, volvió a incrementar la cantidad de energía que podía ser capturada, almacenada y utilizada, esta vez en forma de combustibles fósiles procesados y puestos al servicio de máquinas. La nueva energía de las máquinas sirvió como sustituto mecánico de los esclavos, con el consiguiente aumento en la cantidad de energía y potencia per cápita disponible en el conjunto de la sociedad.

En Human Origins, George Grant MacCurdy describe la experiencia humana como un viaje evolutivo dirigido hacia el aprovechamiento de una cantidad cada vez mayor de la energía disponible. Según MacCurdy, «el grado de civilización de cada época, pueblo o grupo de pueblos se mide por su capacidad de utilizar la energía para promover el progreso o satisfacer las necesidades de la humanidad [...]». Muchos antropólogos están de acuerdo con esta idea. White, por ejemplo, utiliza la energía como criterio para medir el éxito de las culturas humanas. White sostiene que el nivel de desarrollo de una cultura está directamente relacionado con la cantidad de energía consumida per cápita. La función misma de la cultura, en opinión de White y otros antropólogos, es «aprovechar y controlar la energía para ponerla al servicio del hombre». El ser humano logra este objetivo a través de la creación de herramientas que le permitan capturar y transformar la energía, así como de mecanismos de comunicación e instituciones sociales que le permitan organizar el proceso de transmisión y distribución de la energía. Según MacCurdy y White, lo que llamamos el progreso humano consiste en buena medida en el ingenio y la habilidad de las personas a la hora de utilizar las formas simbólicas, las herramientas y las estructuras institucionales para capturar y utilizar una cantidad cada vez mayor de energía, para extender con ello su poder y aumentar su bienestar.

Howard Odum, uno de los pioneros en el campo de los sistemas energéticos naturales, recuerda que, dentro de la ecuación «hombre, mente y energía», lo que establece en último término los límites del progreso humano es la fuente de energía y no la inspiración humana. Según Odum:

Todo progreso se debe a suministros especiales de energía, y el progreso se evapora cada vez que éstos desaparecen. El saber y el ingenio son los medios que permiten aplicar los recursos energéticos cuando éstos están disponibles, y el desarrollo y la retención de conocimiento dependen también del suministro energético.

No se puede comprender adecuadamente la historia de las civilizaciones humanas —desde su auge hasta su caída— sin apreciar la importancia de estos «suministros de energía». Según Howard Odum, el punto de partida de cualquier sociedad histórica es la disponibilidad de excedentes energéticos. Toda la creatividad humana del mundo fracasará inevitablemente en su intento de mejorar el bienestar de la especie si no dispone de reservas energéticas suficientes para su aprovechamiento y explotación.

White propone una fórmula abreviada para medir la relación entre el consumo energético y la evolución cultural. Según White, existen tres factores cruciales a la hora de determinar el grado de «progreso» de una cultura. Primero, «la cantidad de energía consumida per cápita al año»; segundo, «la eficiencia de los medios tecnológicos para el control y la explotación de la energía»; tercero, «la cantidad de bienes y servicios producidos destinados a cubrir las necesidades humanas». Sumando todos estos factores, White concluye que «la cultura evoluciona a medida que aumenta la cantidad de energía consumida anualmente per cápita, o a medida que aumenta la eficiencia de los medios instrumentales para el aprovechamiento de la energía». Muy en la línea de la tradición materialista de la Ilustración europea, White sostiene explícitamente la creencia de que la energía gobierna tanto los sistemas biológicos como los culturales. «Así pues, la evolución de la cultura desde la época de los antropoides hasta la actualidad no es más que la consecuencia —según White— de una serie de incrementos periódicos en la cantidad de energía consumida anualmente per cápita, derivados de la explotación de nuevas fuentes de energía.»

Debe tenerse en cuenta que MacCurdy, White y Odum representan una particular escuela de pensamiento. Sin duda, a los tres se les puede criticar su equiparación del aumento del consumo energético con los avances sociales y culturales. Sería igual de convincente decir que el aumento del flujo de energía en una sociedad se corresponde también con un mayor grado de coerción y sumisión de los pueblos, así como una mayor degradación medioambiental.

Además, muchos antropólogos consideran que aunque la captación y la transformación de cantidades cada vez mayores de energía es un elemento asociado, incluso esencial en el desarrollo de una cultura, eso no significa necesariamente que la propia cultura —sus símbolos, herramientas, mitos e instituciones— deba entenderse únicamente como un instrumento destinado al incremento del flujo de energía en el cuerpo social. Sin embargo, dejando a un lado la vieja cuestión de sí las culturas tienen alguna finalidad más allá del terreno puramente material, la perspectiva de White, Odum y MacCurdy es útil para comprender de qué modo las reservas y los flujos de energía influyen sobre el progreso y la decadencia de las culturas.

Es indudable que las sociedades humanas no han cesado de incrementar la cantidad y la calidad del flujo energético en la vida individual y social, por lo menos a partir de la revolución neolítica y el inicio de la agricultura. Este flujo cada vez mayor ha exigido a su vez el uso de herramientas más sofisticadas y estructuras institucionales más complejas para controlar y dirigir el proceso. El desarrollo de nuevas herramientas y el aumento de la complejidad de las estructuras institucionales tienen un precio elevado, en forma de estructuras sociales más jerarquizadas, una mayor diferenciación y especialización de las tareas humanas y una mayor concentración de poder en la cúspide. En otras palabras, cuanto mayor es el flujo horizontal de poder entre el entorno y la sociedad, mayor es el flujo vertical de poder dentro de la sociedad para asegurar el proceso. En realidad, no deberíamos sorprendernos ante esta idea. Los pequeños grupos de cazadores-recolectores que dependían de sus miembros como «plantas energéticas» tan sólo generaban alrededor de 1/20 caballos de potencia per cápita al año. Un flujo de energía como éste no requiere complejas estructuras institucionales para controlar el proceso.

En una fase posterior de la historia, nuestros antepasados se tomaron y «explotaron» unos a otros como plantas generadoras de energía —un proceso que siguió adelante hasta finales del siglo XIX— y utilizaron la esclavitud humana como medio para incrementar el flujo de energía. El trabajo de los esclavos permitió construir las grandes pirámides de Egipto, la Gran Muralla china y las urnas ceremoniales de las civilizaciones maya y teotihuacana en las Américas. La construcción de la Gran Muralla en China requirió el trabajo de más de un millón de esclavos, la mitad de los cuales perecieron en el esfuerzo. Prácticamente el 20% de la población de Roma en los primeros siglos de la era cristiana eran esclavos. Los esclavos humanos también fueron usados a lo largo de la historia como «bestias de transporte». En la actualidad todavía existen numerosos rickshaws en Asia, aunque ya no se trata de un trabajo forzado. En el siglo XVI todavía eran frecuentes en el Mediterráneo las galeras impulsadas por mano de obra esclava.

Las sociedades esclavistas requerían el uso de herramientas más elaboradas, sobre todo en forma de armamento militar, para capturar esclavos, así como canales de comunicación y redes de transporte más avanzados para dirigir, conseguir y movilizar de forma eficiente la mano de obra esclava. El resultado fue una mayor concentración de poder en la cúspide de la jerarquía social para controlar y administrar mejor esta creciente complejidad. Pero incluso este grado de concentración de poder queda minimizado si se compara con los controles jerárquicos necesarios en la actual sociedad industrial avanzada para capturar la energía en forma de petróleo, carbón y gas natural, y dirigir los esclavos mecánicos que hacen circular la energía en el interior de la sociedad.

La energía es la fuerza primaria y el medio a través del cual se construye cualquier cultura humana. Y la historia de la humanidad refleja efectivamente un marcado aumento tanto del flujo de energía como de la complejidad de las instituciones sociales necesarias para dirigir este flujo. Sin embargo, para comprender de forma más completa el auge y la caída de los regímenes energéticos en las diferentes civilizaciones debemos comprender las reglas que gobiernan la energía. Estas reglas existen a priori como leyes de la naturaleza y dictan los términos del flujo energético tanto en la Tierra como en el conjunto del universo. Son las instrucciones que dicen cómo hay que jugar al juego de la energía para tener éxito. Tales reglas también nos dicen por qué fracasaron las civilizaciones del pasado y qué es lo que debe hacer nuestra sociedad para evitar unidestino parecido, ahora que nos acercamos al «punto de inflexión» de la era del petróleo.

Las leyes de la termodinámica

La energía está gobernada por dos leyes. La primera y la segunda ley de la termodinámica no fueron articuladas de forma definitiva hasta la segunda mitad del siglo XIX, casi ciento setenta años después de que Newton esbozara las leyes de la mecánica en sus Principia. Lamentablemente, mientras que todos los escolares aprenden las leyes que gobiernan la gravedad y el motivo por el cual las manzanas tienen la costumbre de caer de los árboles, pocos son introducidos en las leyes que gobiernan la transformación de la energía. Sin embargo, estas leyes resultan mucho más útiles a la hora de comprender los flujos y reflujos de la existencia, incluido el paso del tiempo y el funcionamiento de los sistemas químicos, biológicos y sociales de la Tierra. El gran matemático y filósofo del siglo XX Alfred North Whitehead comentó en una ocasión a sus alumnos que la mecánica newtoniana sólo nos habla acerca de relaciones espaciotemporales de la materia en movimiento. Whitehead añade:

Tan pronto como has establecido [...] una forma de definir un lugar determinado en el espacio-tiempo, puedes formular adecuadamente la relación de un cuerpo determinado con el espacio-tiempo diciendo simplemente que está allí, en aquel lugar; y por lo que respecta a la mera localización, no hay más que decir sobre el tema.

Las leyes de la termodinámica, en cambio, nos dicen cómo se comporta la energía y son por lo tanto más relevantes para la comprensión del funcionamiento cotidiano de los ecosistemas y los sistemas sociales. Albert Einstein reflexionó en una ocasión acerca de cuáles eran las leyes científicas más inclusivas y de mayor alcance. Volviendo a las leyes de la termodinámica, el gran físico del siglo XX opinó lo siguiente:

Una teoría es más impresionante cuanto más simples son sus premisas, cuanto mayor es el número de elementos diversos que relaciona y cuanto más extenso es su marco de aplicación. Ello explica la profunda impresión que causó en mí la termodinámica clásica. Es la única teoría física de contenido universal que estoy convencido de que nunca será destronada, dentro del marco de aplicación de sus conceptos básicos.

La primera y la segunda ley de la termodinámica establecen que «la energía total que contiene el universo es invariable y la entropía total aumenta constantemente». La primera ley, según la cual la energía total contenida en el universo es invariable, recibe a veces el nombre de ley de la conservación. Significa que la energía ni se crea ni se destruye. La cantidad de energía contenida en el conjunto del universo permanece fija desde el origen de los tiempos y seguirá así hasta el final de los mismos. Todos los seres humanos que han nacido y todas las cosas que los seres humanos han construido en el curso de la historia son energía que ha sido transformada de un estado a otro. La energía de la que está formado el cuerpo humano o las cosas que creamos tuvo su origen en algún otro lugar de la naturaleza y en un estado distinto antes de adoptar una forma humana o material. Cuando un ser humano muere y se descompone, así como cuando nuestros objetos materiales se desintegran, la energía que se libera encuentra su camino de vuelta a la naturaleza.

Aquí es donde entra en juego la segunda ley de la termodinámica. La energía no se crea ni se destruye, sino que se transforma constantemente, pero siempre lo hace en una dirección: pasando de disponible a no disponible. Si quemamos un trozo de carbón, por ejemplo, la energía permanece, pero se transforma en dióxido de sulfuro, dióxido de carbono y otros gases que se liberan en el espacio. No se ha perdido ninguna cantidad de energía en el proceso y, sin embargo, ya no podremos volver a quemar jamás ese pedazo de carbón y convertirlo en trabajo útil. La segunda ley dice que siempre que la energía se transforma, una parte de la energía disponible se pierde en el proceso, es decir, ya no está en condiciones de realizar trabajo útil. Esta pérdida de energía aprovechable recibe el nombre de entropía y es uno de los conceptos más importantes y al mismo tiempo menos comprendidos y apreciados de la física. El término fue acuñado por primera vez por el físico alemán Rudolf Clausius en 1868.

Clausius observó que para convertir la energía en trabajo debe existir una diferencia en la concentración de energía (por ejemplo, una diferencia de temperatura) en partes diferentes del sistema. El trabajo se produce cuando la energía pasa de un nivel mayor de concentración a otro menor (o de una temperatura más elevada a otra más baja). Por ejemplo, el motor de vapor realiza un trabajo porque una parte del sistema está muy fría y otra muy caliente. Y lo que es igual de importante, cuando la energía pasa de un nivel a otro el resultado es que hay menos energía disponible para realizar trabajo la próxima vez. Tomemos por ejemplo un atizador al rojo vivo: cuando lo sacamos del fuego comienza a enfriarse porque el calor siempre fluye del cuerpo más caliente al más frío. Pasada una cierta cantidad de tiempo, el atizador se encuentra a la misma temperatura que el aire que lo rodea. Esto es lo que los físicos describen como estado de equilibrio, en el cual ya no hay ninguna diferencia de niveles energéticos y en consecuencia ninguna capacidad de producir trabajo. La energía, antes útil, ya no está concentrada en el atizador al rojo vivo, sino dispersada en el aire hasta el punto de que ya no es aprovechable. La segunda ley, por lo tanto, nos dice que la energía siempre se transforma en una dirección, de caliente a frío, de concentrado a disperso, o de ordenado a desordenado.

Es posible invertir el proceso entrópico, pero sólo mediante el empleo de energía adicional. Y naturalmente esta energía adicional, una vez usada, no hace más que aumentar la entropía total. Reciclar los desechos, por ejemplo, requiere el gasto adicional de energía de recoger, transportar y procesar los materiales usados, lo cual incrementa la entropía total en el entorno. Digamos, por ejemplo, que tomamos un pedazo de mineral metálico de la superficie de la Tierra y lo convertimos en un utensilio. A lo largo de la vida de este utensilio, las moléculas metálicas se escapan constantemente del producto como resultado de la fricción y el desgaste. Las moléculas liberadas no se destruyen nunca, pero se dispersan de tal forma que ya no están disponibles para realizar trabajo útil. Finalmente regresan a la Tierra, pero en un estado mucho más disperso y anárquico que cuando formaban parte de un pedazo de mineral metálico. Se podría inventar un instrumento que recogiera y reciclara todas las moléculas metálicas que se han dispersado aquí y allá, pero tanto el uso de energía para hacer funcionar la máquina como la propia máquina estarían generando una pérdida de energía disponible en el proceso, en el primer caso en forma de gases residuales y en el segundo en forma de moléculas perdidas por la fricción y la liberación. Y de nuevo, si queremos reciclar estas moléculas metálicas dispersadas debemos consumir una parte de la energía disponible, lo que aumenta la factura total de la entropía.

Si bien es cierto que la energía solar no se agotará hasta dentro de miles de millones de años y, por lo tanto, el Sol continuará bañando la Tierra con nuevas aportaciones de energía durante todo el tiempo que seamos capaces de imaginar, debe comprenderse que la cantidad de energía concentrada en la Tierra en forma de materia, sean minerales metálicos o combustibles fósiles, es relativamente limitada dentro del período de la historia geológica relevante para la sociedad. Esto es así porque desde el punto de vista termodinámico la Tierra es un sistema cerrado en relación con el sistema solar y el universo. Los sistemas termodinámicos pueden ser de tres clases: abiertos, cerrados y aislados. Los sistemas abiertos intercambian tanto materia como energía. Los sistemas cerrados intercambian energía, pero no materia, al menos en cantidades apreciables. Los sistemas aislados no intercambian ni energía ni materia. La Tierra es un sistema cerrado, es decir, intercambia energía con el sistema solar, pero, a excepción de algún meteorito ocasional o de cierta cantidad de polvo cósmico, no intercambia demasiada materia con el universo exterior. La idea importante es que el influjo de energía procedente del Sol no produce materia por sí mismo; la energía solar puede fluir eternamente por una jarra vacía sin producir jamás ningún tipo de vida. La Tierra posee unas reservas limitadas de materia apta para ser transformada en otras formas útiles, incluida la vida, con la ayuda de la energía solar. Por ejemplo, la energía solar interactuó con la materia terrestre durante el período Jurásico y contribuyó a convertir la materia en vida. La descomposición de esta vida generó los depósitos de carbono que hoy quemamos en forma de carbón, petróleo y gas natural. La energía que se pierde en forma de gases ya no está disponible para realizar trabajo. Aunque es posible que en algún momento futuro de la historia geológica se acumulen depósitos parecidos de carbono, el futuro del que estamos hablando es en cualquier caso tan lejano que no afecta a las necesidades humanas. Por este motivo se dice que los combustibles fósiles son «fuentes no renovables de energía».

En resumen, la primera ley de la termodinámica establece que la energía total del universo es constante, que no puede ser creada ni destruida. Sólo su forma puede cambiar. La segunda ley afirma que la energía sólo puede cambiar en una dirección, a saber, de utilizable a no utilizable, de disponible a no disponible, de ordenada a desordenada. Todo cuanto existe en el universo, de acuerdo con la segunda ley, comenzó siendo energía concentrada disponible y se está transformando con el paso del tiempo en energía dispersada y no disponible. La entropía mide la tendencia que tiene la energía disponible de cualquier subsistema del universo a pasar a una forma no disponible.

Si la sociedad está organizada en torno al esfuerzo continuado por convertir la energía disponible del medio en energía aplicada a la conservación de la existencia humana, entonces la observación de Frederick Soddy acerca de la importancia de las leyes de la termodinámica parece adecuada. Según Soddy, las leyes de la termodinámica «controlan, en último término, el auge y la caída de los sistemas políticos, la libertad o la esclavitud de las naciones, los movimientos del comercio y de la industria, los orígenes de la riqueza y la pobreza, y el bienestar físico general de la especie».

Pero no vayamos tan rápido. Si bien es cierto que la energía pasa constantemente de un estado concentrado a otro aleatorio, o de una existencia ordenada a otra desordenada, ¿cómo se explica entonces la supervivencia de los seres vivos o incluso de los sistemas sociales, que parecen mantener un elevado nivel de orden y energía concentrada, en aparente desafío a la máxima de la termodinámica?

Al principio, los biólogos no encontraban la forma de reconciliar la vida con las férreas leyes de la termodinámica. En el libro Time's Arrow and Evolution, Harold Blum contribuyó a integrar la biología en el marco de la primera y la segunda ley de la termodinámica, al establecer que la vida no es más que un caso particular de la aplicación de estas leyes. Según Blum, los seres vivos evitan el estado de equilibrio mediante la absorción constante de energía libre de su entorno. Si el organismo es capaz de mantener una existencia ordenada de forma permanente es gracias a la absorción de energía disponible y al incremento de la entropía total del entorno. Blum explica que «el pequeño freno local a la entropía que representa el organismo viene contrarrestado por un aumento mucho mayor de la entropía del universo».

La fuente de la energía libre es el Sol. Las plantas absorben la energía solar por medio de la fotosíntesis y se convierten en una fuente de energía concentrada que luego pueden consumir los animales, sea de forma directa, mediante la ingestión de plantas, o bien indirecta, mediante la ingestión de otros animales. El premio Nobel de Física Erwin Schrödinger observó que «al absorber constantemente entropía negativa de su entorno [...] el organismo se alimenta en realidad de entropía negativa; absorbe constantemente orden de su entorno».

Si nos paramos a pensar en ello, lo que dicen los biólogos tiene mucho sentido. Nos mantenemos vivos gracias a que procesamos constantemente energía a través de nuestros cuerpos. Si el flujo de energía se detuviera o si nuestros cuerpos no pudieran procesar adecuadamente la energía a causa de una enfermedad, pronto pasaríamos a estar muertos, o en estado de equilibrio. Inmediatamente después de la muerte, el cuerpo comienza a descomponerse muy rápido y nuestra existencia física se diluye y se dispersa en el entorno. Los biólogos afirman, por lo tanto, que la vida es un ejemplo de desequilibrio termodinámico, es decir, la vida conserva su orden y se mantiene lejos del estado de equilibrio, o muerte, por medio de un procesamiento constante de la energía libre o disponible de su entorno.

El proceso mediante el cual se mantiene un estado de desequilibrio y se evita la muerte es costoso en términos energéticos. Los vegetales, las «centrales energéticas» más eficientes de la Tierra, no consigue absorber por medio de la fotosíntesis más que una minúscula fracción de la energía que recibe el planeta. El resto se disipa. De este modo, la pequeña disminución de la entropía se mantiene al coste de un aumento mucho mayor de la entropía en el conjunto del medio.

El gran filósofo y matemático del siglo XX Bertrand Russell señaló que «todo ser vivo es una especie de imperialista que intenta transferir todo lo que puede de su entorno hacia sí mismo y su progenie». Cuanto más evolucionada está una especie dentro de la escala natural, mayor es la cantidad de energía que se requiere para mantenerla en un estado de desequilibrio y más entropía se crea en el proceso de mantenerla con vida.

Pensemos en una cadena trófica sencilla, integrada por hierbas, saltamontes, ranas, truchas y seres humanos. De acuerdo con la primera ley, la energía nunca se pierde. Sin embargo, de acuerdo con la segunda ley, parte de la energía disponible se convierte en energía no disponible con cada eslabón que subimos en la cadena trófica, lo cual aumenta la entropía total en el medio. El químico G. Tyler Miller recuerda que en el proceso de devorar una presa «entre el 80 y el 90% de la energía se pierde simplemente como calor en el entorno». En otras palabras, el depredador sólo absorbe entre el 10 y el 20% de la energía de la presa. El motivo es que el acto de transformar la energía de una forma de vida en energía para otra requiere un gasto y, por lo tanto, tiene como resultado una pérdida de energía. La cantidad de energía libre necesaria para mantener con vida a las especies más evolucionadas de la parte superior de la cadena trófica es escalofriante. Miller calcula que se necesitan «trescientas truchas para mantener a un hombre durante un año. A su vez, la trucha debe consumir 90.000 ranas, las cuales deben consumir 27 millones de saltamontes, que viven de 900 toneladas de hierba». Así pues, cada nuevo ser vivo dentro de la escala de la evolución se mantiene ordenado y en un estado de desequilibrio a expensas de crear un desorden (energía disipada) mayor en el conjunto del medio.

La energía fluye de modo constante a través de todos los organismos vivos, en una forma sumamente ordenada en el momento de entrar y en un estado más degradado en el momento de salir del sistema en forma de desecho. Cuanto más evolucionado es el organismo, mayor es la cantidad de energía que requiere para evitar el estado de equilibrio. Esto significa que a medida que subimos en la escala de la evolución, cada nueva especie tiene que estar mejor equipada fisiológicamente para capturar la energía disponible. Según el biólogo Alfred Lotka, los seres vivos pueden ser vistos como «transformadores» de energía. «La estrecha asociación que mantienen los principales órganos sensoriales —los ojos, los oídos, la nariz, las papilas gustativas y las papilas táctiles de la punta de los dedos— con la extremidad anterior del cuerpo [la cabeza], donde se encuentra la boca, todo señala en la misma dirección», afirma Lotka. La selección natural favorece a aquellos organismos que son capaces de «incrementar la masa total del sistema, la tasa de circulación de la masa por el sistema, y el flujo total de energía por el sistema [...] en la medida en que haya un residuo de materia sin utilizar y energía disponible».

Otra forma de entender el progreso económico

La evolución consiste, pues, en el desarrollo de sistemas organizativos más complejos y en la progresiva diferenciación y especialización de las especies para capturar y concentrar más energía disponible. Visto desde el punto de vista de la termodinámica, la evolución no consiste tanto en un avance ininterrumpido como en una dialéctica constante entre un aprovechamiento cada vez mayor de la energía y una disipación cada vez mayor de la misma. La evolución tiene como resultado la creación de islas de orden cada vez más grandes a expensas de la creación de océanos todavía mayores de desorden en el mundo. Esto es tan cierto en el caso de los sistemas sociales humanos como lo es para las especies y los ecosistemas. Por si queda alguna duda acerca de esta cuestión, piénsese en la cantidad de energía libre necesaria para mantener las estructuras económicas y sociales y el estilo de vida de los norteamericanos, y cuánta entropía se crea en el proceso.

Estados Unidos ha creado una red masiva de «transformadores» para hacer circular la energía por todas las arterias del organismo social. Como ya se ha dicho, Estados Unidos acoge menos del 5% de la población mundial, pero consume aproximadamente el 25% de la producción total de energía del mundo. El norteamericano medio consume unos 3.600 kilos de petróleo, 2.130 kilos de gas natural, 2.336 kilos de carbón y 0,04 kilos de uranio por año.

Para hacernos una idea exacta de cuánta energía circula en la sociedad norteamericana cada día, el geólogo Walter Youngquist propone que calculemos la cantidad de «personas de potencia» adicional que tiene a su disposición cada individuo. Supongamos para comenzar que una «persona de potencia» (PP) = 0,25 caballos de potencia = 186 vatios = 160 Kilocalorías. Según Youngquist, si calculamos el consumo energético actual en Estados Unidos en términos del equivalente en personas de potencia que se requeriría para proporcionarnos la misma cantidad de trabajo, harían falta casi tres veces más personas de las que existen actualmente en el mundo. La dieta energética diaria del norteamericano medio es el equivalente a tener cincuenta y ocho esclavos energéticos trabajando sin cesar las veinticuatro horas del día. Si «comprásemos la energía de un barril de petróleo al mismo precio que pagamos el trabajo humano (5 dólares/hora), nos costaría más de 45.000 dólares», a diferencia de los 25 dólares que nos cuesta actualmente un barril.

Como era de esperar, mantener la dieta energética norteamericana lejos del estado de equilibrio tiene un coste igualmente elevado. Si por un lado los norteamericanos consumen el 25% de la energía mundial, también aportan el 30% de las emisiones mundiales de dióxido de carbono. Cada ciudadano estadounidense emite alrededor de 6 toneladas de gases de efecto invernadero (el equivalente a 6.800 kilos de carbono). El 82% de estas emisiones proceden de la quema de combustibles fósiles para generar electricidad y para mantener en funcionamiento los automóviles, los autobuses, los camiones y los aviones. La mayor parte de las emisiones restantes consisten en metano generado en vertederos, así como por prácticas agrícolas modernas, gasoductos y productos químicos industriales. También hay que tener en cuenta que, a medida que una sociedad madura y envejece, la cantidad de energía requerida para su mantenimiento es cada vez mayor, lo que significa que queda menos energía disponible para la innovación y la expansión. El informe de la Sociedad Americana de Ingenieros Civiles [American Society of Civil Engineers, ASCE] sobre el estado de la vasta infraestructura de Estados Unidos en 2001 constituye un buen ejemplo de aquello en lo que consiste un proceso entrópico. Según esta asociación, la infraestructura de Estados Unidos —carreteras, puentes, tráfico rodado, aviación, escuelas, agua potable, aguas residuales, embalses, desechos sólidos, residuos peligrosos, vías fluviales y energía— se halla en tan mal estado que su reparación requerirá una inversión de 1,3 billones de dólares durante los próximos cinco años. La ASCE informa de que el 33% de las principales carreteras del país se hallan en mal estado, lo cual significa un coste de 5.800 millones al año para los conductores norteamericanos. El mal estado de las carreteras es un factor que influye también en las 13.800 muertes anuales que se registran en ellas. Por otro lado, un tercio de las vías rápidas urbanas del país están congestionadas. Los puentes no se hallan en mucho mejor estado. El 29% de los puentes presentan deficiencias estructurales o están obsoletos y la Asociación calcula que su reparación exigirá una inversión de 10.600 millones de dólares por año durante los próximos veinte años. Los aeropuertos del país están sobresaturados y sus instalaciones se encuentran al límite de su capacidad, lo que provoca 50.000 retrasos mensuales en los vuelos. Las escuelas norteamericanas son viejas y están muy masificadas y, según las estimaciones de la asociación, el 75% de la infraestructura es inadecuada para cubrir las necesidades educativas. Las 54.000 plantas potabilizadoras de agua resultan anticuadas y constituyen una fuente de polución difusa. Buena parte de los 16.000 sistemas de aguas residuales está cerca del colapso. Algunos de los sistemas de alcantarillado tienen más de cien años y apenas pueden soportar las demandas actuales. El presupuesto anual destinado a la infraestructura de las aguas residuales debería aumentar en unos 12.000 millones de dólares para cubrir las necesidades. Más de 2.100 embalses han sido clasificados como inseguros y en los años 1999 y 2000 se informó de 61 fallos en varias presas. Las vías navegables interiores son tan viejas que el 44% de las esclusas han superado los cincuenta años, el período para el que fueron diseñadas. Sin embargo, se prevé que las necesidades de transporte en las vías fluviales de Estados Unidos se dupliquen para el año 2020. Por último, la capacidad eléctrica del país ha experimentado un descenso del 30% desde 1990. Actualmente se añaden 7.000 megavatios (MW) de electricidad por año, una cifra muy inferior a los 10.000 megavatios anuales necesarios para cubrir el crecimiento anual de la demanda, situado en el 1,8%.

Así pues, cuanto más evolucionado y complejo es el organismo social, mayor es la cantidad de energía que se requiere para su mantenimiento y mayor es la entropía producida en el proceso. Esta realidad tan sencilla escapa a la teoría económica ortodoxa. En realidad, ni el capitalismo ni el socialismo son capaces de dar respuesta a las duras «verdades de la vida» que la primera y la segunda ley de la termodinámica imponen a la sociedad y al medio ambiente.

La teoría capitalista clásica parte de la idea de que la actividad económica convierte lo inútil en valioso. El filósofo de la Ilustración británico John Locke sostenía que «la tierra que se abandona enteramente a la naturaleza [...] se llama tierra baldía, y no es otra cosa que eso». Locke dio la vuelta a la segunda ley de la termodinámica al proclamar que la naturaleza es inútil en sí misma y que sólo adquiere valor cuando los seres humanos aplican su trabajo sobre ella y la transforman en bienes productivos. Según Locke:

Aquel que, mediante su propio esfuerzo, se apropia de una parcela de tierra, no sólo no disminuye la propiedad común de la humanidad, sino que la acrecienta; pues los frutos en beneficio de la vida humana que son producidos por un acre de tierra cultivada resultan ser —sin exageración— diez veces más que los producidos por un acre de tierra igualmente fértil que no es aprovechado y continúa siendo terreno comunal. Por lo tanto, aquel que parcela una porción de tierra mejora su vida, mediante el cultivo de diez acres, mucho más de lo que la mejoraría dejando cien acres en su estado natural; puede decirse que está dando noventa acres al género humano.

La economía capitalista está empapada de los conceptos propios de la antigua física basada en la mecánica newtoniana y nunca se ha encontrado cómoda con las leyes de la termodinámica. A partir de la idea newtoniana de que para cada acción hay una reacción igual y de signo contrario, algunos economistas clásicos como Adam Smith y Jean-Baptiste Say compararon el mercado con un mecanismo en el que la oferta y la demanda se ajustan constantemente entre sí. Si aumenta la demanda de los consumidores sobre un determinado bien o servicio, los vendedores subirán el precio. Si el precio sube demasiado, la demanda aflojará, lo cual obligará al vendedor a rebajar el precio para reanimar la demanda. La misma lógica se aplicaba al aprovechamiento de los recursos naturales. Si éstos se vuelven escasos, el precio subirá, lo cual animará a los productores a usar nuevas tecnologías para encontrar otras reservas más difíciles de hallar, o a buscar alternativas a estos recursos. La base total de recursos se considera inagotable y siempre disponible, en una forma u otra, y a un precio adecuado. Si se presta alguna atención a la entropía es para calificarla como una externalidad de la actividad económica y como un elemento marginal dentro de los costes totales de la actividad comercial.

Las leyes de la termodinámica nos dicen algo bien distinto. La actividad económica se limita a tomar del entorno recursos energéticos de baja entropía para transformarlos temporalmente en productos y servicios valiosos. En el proceso de transformación, la cantidad de energía que se consume y se pierde en el entorno es superior a la cantidad de energía contenida en el bien o el servicio producido. Por otro lado, el producto o servicio «acabado» tiene una naturaleza meramente temporal y se disipa o desintegra con el uso o el consumo, con lo que termina finalmente por volver al medio en forma de energía consumida o desecho.

Así pues, ¿qué debemos pensar de la naturaleza del Producto Interior Bruto (PIB)? Vemos el PIB como una medida de la riqueza que genera anualmente un país, pero desde el punto de vista termodinámico es más bien una medida del valor energético contenido de forma temporal en los bienes y servicios producidos a expensas de la disminución de las reservas energéticas disponibles y de la acumulación de residuos derivados del proceso entrópico. A pesar de todas nuestras ideas de progreso económico, el balance siempre terminará en números rojos, pues incluso los bienes y productos que generamos terminarán formando parte de la corriente general de la entropía. Es decir, que al final todas las civilizaciones terminan inevitablemente por absorber más orden de su entorno del que son capaces de crear y dejan la Tierra más pobre de lo que era antes.

Las sociedades que más duran son aquellas que consiguen el mejor equilibrio posible entre el balance de la naturaleza y el de la sociedad humana, dentro de los límites que impone inevitablemente la segunda ley. Las sociedades consideradas en «estado estacionario» aprenden a vivir tan bien como pueden de acuerdo con el calendario de la naturaleza. El proceso de captación, transformación, distribución y consumo de la energía, en sus muchas formas económicas, se mantiene dentro de un ritmo más o menos proporcionado con la capacidad del entorno para reciclar los residuos y restaurar las reservas de energía renovable. El balance nunca puede ser de uno a uno, debido a la pérdida de energía inherente al proceso de transformación. Sin embargo, algunas sociedades, sobre todo las de cazadores-recolectores y las pequeñas sociedades agrícolas basadas en el parentesco, se han mantenido durante largos períodos de tiempo antes de agotar sus regímenes energéticos. Las grandes civilizaciones de la historia han tenido menos éxito. Tratar de entender el auge y la caída de las grandes civilizaciones desde un punto de vista termodinámico puede arrojar mucha luz sobre la crisis a la que se enfrenta la nuestra en el momento actual, cuando nos acercamos al final del régimen energético basado en la explotación de los combustibles fósiles.

Por qué se colapsan las grandes civilizaciones

Las civilizaciones son un fenómeno raro dentro de la historia del mundo. El historiador británico Arnold Toynbee afirma que sólo ha habido treinta civilizaciones. Otros historiadores son menos generosos: Oswald Spengler, A. L. Kroeber, Carroll Quigley y Rushton Coulborn sostienen que sólo ha habido entre ocho y quince grandes civilizaciones dignas de ese nombre.

Lo que sí sabemos es que hace aproximadamente 6.000 años comenzó a emerger en varias partes del mundo un nuevo tipo de organización social. Las pequeñas comunidades basadas en el parentesco comenzaron a dejar paso a nuevas entidades que poseían muchos de los rasgos propios de los Estados. Los lazos de pertenencia tribal, la forma tradicional de organizar la vida social, fueron subsumidos en grandes afiliaciones de base territorial. Gentes diversas se vieron reunidas por nuevas fronteras definidas en términos geográficos y regionales. Tales Estados dieron origen al control jerárquico ejercido por poderosas élites gobernantes. La actividad económica era dirigida y controlada por un gobierno centralizado. Se crearon burocracias para gestionar las actividades diarias de los súbditos y códigos legales para regular el comportamiento humano, se promulgaron estatutos y se establecieron mecanismos para recaudar tributos en forma de tasas sobre la producción agrícola. Se reclutaron ejércitos para saquear territorios vecinos, así como para garantizar la propia seguridad frente a los invasores y mantener el orden en el interior. El poder estaba concentrado en los núcleos urbanos y se extendía hasta las fronteras más lejanas del reino. Estos centros eran lugares sagrados y poseían, por lo tanto, una legitimidad divina. Los líderes, a su vez, eran vistos como «emisarios de los dioses», responsables de la administración de los reinos terrestres. El aumento de los excedentes agrícolas y el saqueo de territorios extranjeros, así como el trabajo de los esclavos, proporcionaban las reservas energéticas necesarias para mantener a una creciente población urbana improductiva y para sostener la vasta y compleja estructura social. Las civilizaciones se distinguen de otras sociedades más simples por su necesidad de controlar, procesar y consumir grandes cantidades de energía.

Hay dos preguntas que, desde la antigua Grecia hasta los tiempos modernos, han preocupado a los historiadores: ¿por qué existen tan pocas civilizaciones? —parecen ser una anomalía dentro del largo viaje de la humanidad— y ¿por qué el enorme poder que amasan y controlan institucionalmente, en apariencia inexpugnable, se desintegra a menudo tan rápidamente y sufre un colapso repentino?

Los historiadores han ofrecido varias teorías interesantes para explicar el auge y la caída de las grandes civilizaciones. El historiador alemán Oswald Spengler prefería un modelo orgánico, de acuerdo con el cual el nacimiento, la vida y la muerte de una civilización se parecían a los ciclos que gobiernan la propia vida humana. Según Spengler, toda civilización «pasa por las distintas fases de la evolución del hombre individual. Todas tienen su infancia, su juventud, su madurez y su vejez». Como todo ser humano individual, cada civilización vive de acuerdo con «una idea», que en la concepción de Spengler consistía en una forma de entender el carácter único de su identidad, sus pasiones y sus sentimientos, y el sentido de su misión y su destino en el mundo. Más aún, Spengler sostenía que la propia «existencia viva» de una civilización, «esa secuencia de grandes épocas que definen y despliegan los estadios del desarrollo, es una apasionada lucha interna por mantener la Idea frente a los poderes del Caos...». «La Idea», para Spengler, consiste en los principios organizativos y los modelos de comportamiento que dirigen la energía colectiva del pueblo y crean un universo social ordenado. Su tesis de que toda civilización organiza la energía humana en torno a una idea central para generar orden «frente a los poderes del Caos» lo sitúa indudablemente dentro del campo de la termodinámica. Pero no es el único. El historiador británico Arnold Toynbee tiene una forma distinta de diseccionar la historia y explicar el auge y la caída de las civilizaciones, pero llega finalmente a un análisis en el que también resuenan las leyes de la termodinámica. Toynbee ve el desarrollo de una civilización como una sucesión de obstáculos y respuestas por parte de la sociedad. Las sociedades se enfrentan al problema de cómo controlar los recursos y aprovechar la energía que obtienen a partir de ellos. Constantemente aparecen nuevos obstáculos que amenazan con impedir el flujo de energía y que requieren una respuesta creativa por parte de la sociedad. Toynbee cree que el colapso de las grandes civilizaciones se produce a causa de una «falta de vitalidad», una incapacidad de movilizar la energía humana suficiente para superar los obstáculos que amenazan el funcionamiento de la sociedad.

Antiguamente, el catalizador necesario para unificar y movilizar la energía humana hacia la reconstrucción constante de la sociedad debía buscarse en la renovación religiosa. Cuando el espíritu religioso se debilita, la energía humana se disipa. Desaparece la fe colectiva que había movido a las personas a sumar sus fuerzas y actuar como si fueran una sola en persecución de una misión común. El comportamiento humano se hace más azaroso. La voluntad colectiva se fragmenta y cae presa de las fuerzas del caos y la desesperación. El historiador Rushton Coulborn señala que «las sociedades conservan el vigor mientras mantienen un firme compromiso religioso y se debilitan cuando disminuye su fervor religioso». En tiempos más modernos la ideología ha desempeñado un papel similar a la hora de movilizar las energías colectivas de un pueblo. Detrás de todo ello se esconden también las leyes de la energía, que nos recuerdan constantemente que detrás del auge y la caída de las civilizaciones, así como detrás del nacimiento, la vida y la muerte de todos los seres vivos, se hallan las severas reglas del desequilibrio termodinámico.

En su influyente trabajo titulado The Collapse of Complex Societies, Joseph A. Tainter ofrece un marco útil para comprender la dinámica que lleva al colapso de las civilizaciones. Su teoría de los «rendimientos marginales» debería resultar familiar a cualquier ingeniero que haya tenido que enfrentarse alguna vez con flujos de energía y pérdidas derivadas de la entropía en máquinas.

Tainter coincide con Leslie White, Frederick Soddy, Bertrand Russell y otros en que la historia de la humanidad se ha caracterizado por la creación de una estructura social y tecnológica cada vez más compleja dirigida a captar la energía libre disponible del entorno. El incremento en el flujo energético permite a su vez el crecimiento de los asentamientos humanos. A medida que aumenta la población, la vida social se hace más densa y variada, lo cual promueve el avance de la cultura. Las sociedades se colapsan cuando el flujo de energía se interrumpe bruscamente. Ya no se dispone de energía en cantidades suficientes como para sostener el importante volumen de población, defender el Estado frente a los agresores y mantener la infraestructura interna. El colapso se caracteriza por la reducción de los excedentes alimentarios, el agotamiento de las reservas gubernamentales, la disminución del consumo de energía per cápita, el abandono de infraestructuras clave como los sistemas de irrigación, las carreteras y los acueductos, el aumento de la desconfianza popular hacia el Estado, la extensión de la anarquía, la descomposición de la autoridad central, la despoblación de las áreas urbanas y el aumento de la frecuencia de las invasiones y los saqueos por parte de grupos o ejércitos procedentes del exterior.

El colapso se produce, según Tainter, cuando una civilización madura llega al punto en que se ve forzada a destinar un porcentaje cada vez mayor de sus reservas de energía simplemente a mantener su compleja estructura social, al tiempo que experimenta un descenso en el rendimiento de la energía consumida per cápita.

En el período juvenil de la historia de una civilización, las reservas energéticas son destinadas, por ejemplo, a la creación de ejércitos y a la producción de armamento con el objetivo de conquistar otros pueblos y territorios. La cantidad de energía percibida a través del botín —en términos de mano de obra esclava, territorios conquistados y tesoros— es mayor de la que se ha gastado para conseguirlo. En los estadios finales de la civilización, el Estado acostumbra a destinar una parte importante de sus reservas energéticas a defender el territorio existente frente a las invasiones y saqueos procedentes del exterior, lo cual significa un aumento en el consumo energético con escasas o nulas contraprestaciones en términos energéticos.

Tainter propone un análisis parecido en el caso de la producción agrícola. En los primeros estadios se crean los sistemas de irrigación, se abren nuevos campos para el cultivo y se construyen carreteras para trasladar el grano del campo a la ciudad. El gasto energético tiene como resultado un incremento neto de energía. En la fase final de la historia de la civilización, el Estado se ve obligado a dedicar más dinero a conservar la infraestructura agrícola existente, así como a mantener las burocracias estatales que controlan la sociedad. Buena parte de la energía disponible se va en el mantenimiento del estilo de vida de las élites en el poder y de otros miembros no productivos de la sociedad. Para dar respuesta a estas mayores necesidades energéticas se cae a menudo en una sobreexplotación de los campos para obtener ingresos adicionales de energía, lo cual lleva a la degradación y erosión del suelo y a un descenso de la productividad. La floreciente población, cuyas cifras se dispararon durante los buenos tiempos, disfruta de una cantidad cada vez menor de energía per cápita, a pesar de que trabaja de forma más dura e intensiva. Paralelamente, el Estado acostumbra a imponer más tributos a sus súbditos para cubrir sus objetivos, con lo que no hace más que acelerar la espiral descendente. Para evitar el colapso, la sociedad agota sus últimas reservas de energía y su capacidad restante en un esfuerzo desesperado por mantenerse en su anterior estado de desequilibrio, lo cual no hace más que acercarla al «punto de no retorno». El malestar social, a su vez, obliga a los gobernantes a dedicar todas las energías restantes a mantener una apariencia de ley y orden, lo cual reduce todavía más la energía que llega al pueblo a través del sistema. A menudo ocurre que en los estadios finales de una civilización el Estado destina las reservas alimentarias restantes al estamento militar, lo cual enfurece todavía más a la población y crea una especie de círculo vicioso. La sociedad comienza a disgregarse y cada cual defiende sus intereses, lo que marca el comienzo del proceso de desintegración. A menos que se encuentre un nuevo suministro energético, sea fruto de una conquista o de la explotación de una nueva fuente de energía, el colapso es inevitable.

Roma vista desde la termodinámica

El Imperio Romano es un buen caso práctico para estudiar las políticas de la energía. En muchos sentidos, el estilo de vida y la organización económica, social y política de la Roma imperial están más cerca del mundo moderno que del mundo antiguo que le dio origen. Las lecciones que pueda ofrecer la historia de su auge y su caída están todavía frescas en la memoria histórica y tal vez puedan servirnos de guía para enfrentarnos a nuestro propio futuro.

Roma debía su grandeza a sus brillantes conquistas militares. Los ejércitos romanos derrotaron a Macedonia en el año 167 a. C., lo que significó la anexión de su territorio y la apropiación de las grandes riquezas de su monarca. El tesoro real romano creció hasta el punto de que el Estado pudo eximir de impuestos a sus ciudadanos. Poco tiempo después, Roma se anexionó el reino de Pérgamo y la recompensa duplicó de golpe el presupuesto del Estado romano. Las conquistas de Siria, en el año 63 a. C. y, más tarde, de la Galia trajeron oro y riquezas incomparables al imperio. Las conquistas militares eran tan lucrativas desde el punto de vista económico que cubrían los gastos e incluso dejaban beneficios para financiar nuevas aventuras militares. Mano de obra esclava, recursos minerales, bosques y cultivos, todo ello significaba un flujo cada vez más importante de energía disponible para el imperio. El período de expansión terminó con la conquista de Egipto por parte de Octavio Augusto. La anexión trajo tanta riqueza que Augusto la celebró distribuyendo monedas entre los plebeyos de Roma.

Tras sufrir una serie de derrotas militares frente a los germanos y otros pueblos, Roma se atrincheró en sus posiciones y concentró sus energías en construir la infraestructura necesaria para mantener su imperio. El paso de un régimen basado en la conquista a otro basado en la colonización resultó difícil. Sin los ingresos procedentes de la conquista de nuevos territorios, Roma se encontró sin los fondos necesarios para cubrir los servicios básicos. Augusto estableció un impuesto del 5% sobre las herencias y los legados para financiar la jubilación del personal militar. La creación del impuesto enfureció a los romanos, que habían estado exentos de tributos durante toda la época final de la República.

El coste de mantener un ejército resultaba particularmente gravoso. El ejército permanente absorbía grandes cantidades de energía del imperio y consumía excedentes que antes iban a parar a la población romana. El coste de los salarios, las raciones, el alojamiento y el equipamiento de los soldados no dejaba de subir. Lo mismo sucedió con el coste de mantener las obras públicas y unos servicios públicos sobredimensionados. La beneficencia pública también había aumentado considerablemente durante la euforia expansionista y ahora debía mantenerse con unos ingresos cada vez más reducidos. Durante el gobierno de Julio César, prácticamente la tercera parte de los ciudadanos de Roma recibían algún tipo de asistencia pública. Solamente entre los años 41 y 54 d. C., el gobierno proporcionó trigo de forma gratuita a más de 200.000 familias.

Las necesidades meramente logísticas de mantener un vasto imperio se hicieron cada vez más costosas. Mantener tropas acantonadas a lo largo de todo el Mediterráneo y Europa, conservar las carreteras y administrar los territorios anexionados consumía cada vez más energía, mientras que el rendimiento neto de los territorios en términos energéticos era cada vez más bajo. Habían comenzado los rendimientos marginales. En algunos casos, el coste que representaba para Roma mantener ciertas colonias, como por ejemplo España o Inglaterra, era superior a las rentas que generaban.

Como ya no podía mantener su imperio con nuevas conquistas, Roma se vio obligada a volver su mirada hacia el único régimen energético alternativo que conocía: la agricultura. La historia de la decadencia de Roma está íntimamente asociada con el declive progresivo de la producción agrícola.

La concepción más extendida afirma que Roma cayó por culpa de la decadencia de su clase gobernante, la corrupción de sus líderes, la explotación de sus esclavos y servidores y la superioridad de las tácticas militares de las hordas invasoras bárbaras. Aunque no le falta razón a este planteamiento, la causa profunda de la caída de Roma hay que buscarla en la progresiva pérdida de fertilidad del suelo y en el descenso de la producción agrícola, que no fue capaz de proporcionar la energía suficiente para mantener la infraestructura romana y el bienestar de sus ciudadanos. La historia del agotamiento del único régimen energético disponible para los romanos constituye una buena enseñanza para nuestra propia civilización, ahora que nos acercamos al final de las reservas disponibles de combustibles fósiles baratos que hasta ahora mantenían a flote la sociedad industrial.

Al comienzo del dominio romano, Italia estaba densamente poblada por bosques. Hacia el final del Imperio, Italia y buena parte de los territorios del Mediterráneo habían perdido su capa forestal. La madera se vendía en los mercados libres y la tierra era transformada en cultivos y pastos. La tierra recién deforestada era rica en minerales y nutrientes y daba buenas cosechas. Por desgracia, la eliminación de la capa boscosa dejaba el suelo expuesto al viento, el agua y los demás elementos. El viento soplaba sobre los paisajes desnudos y el agua se llevaba la tierra de las laderas de las montañas. El pastoreo excesivo contribuía también a una mayor degradación del suelo.

El continuo descenso de la productividad de la tierra coincidió con la época en que Roma comenzó a depender de la agricultura como fuente de energía, que debía ocupar el lugar de sus cada vez más escasas conquistas exteriores. Durante la última época del Imperio Romano, la agricultura aportó más del 90% de los ingresos del Estado. La producción alimentaria se había convertido en la piedra de toque para la supervivencia de Roma.

El aumento de la población urbana no productiva produjo una presión cada vez mayor sobre las pequeñas explotaciones agrícolas. La producción tuvo que intensificarse para cubrir las necesidades alimentarias de la población urbana y el ejército. La sobreexplotación del suelo hizo descender su fertilidad, lo cual llevó a su vez a una explotación aún mayor de la ya exhausta base del suelo. Los dueños de las pequeñas explotaciones no podían sacar el suficiente rendimiento a sus erosionadas tierras como para pagar los impuestos anuales que fijaba el gobierno sobre la tierra, sin tener en cuenta la producción. Los campesinos tenían que pedir préstamos para mantenerse en el negocio. La mayor parte de las deudas se iban en pagar los impuestos y apenas quedaba nada para introducir mejoras. Privados de cualquier otra fuente de ingresos, los campesinos no podían dejar descansar la tierra el tiempo suficiente para que recobrara su fertilidad, sino que se veían obligados a cultivar un suelo cada vez más degradado, lo cual hacía que sus cosechas fueran cada vez más escasas y sus deudas cada vez mayores. Los pequeños propietarios se vieron obligados a vender sus tierras o a entregarlas a las instituciones de crédito. A lo largo de toda Italia y las costas del Mediterráneo las pequeñas explotaciones pasaron a manos de terratenientes, lo que significó la creación de grandes haciendas conocidas como latifundios. Buena parte de la tierra ya no era apta para el cultivo y tuvo que ser reconvertida en pastos para la cría de ganado. Empobrecidos y desarraigados, los campesinos italianos emigraron a las ciudades, donde pasaron a depender de la beneficencia pública. En el siglo IV había más de 300.000 personas que recibían asistencia pública en la ciudad de Roma. Los crecientes gastos de la ciudad, destinados a financiar el alto nivel de vida de los ricos y a proporcionar asistencia a los pobres, proveer fondos para las obras públicas y las burocracias gubernamentales, pagar la construcción de monumentos, edificios públicos y anfiteatros, así como a cubrir todos los costes asociados a los juegos y exhibiciones públicas, contribuyeron a llevar el régimen energético agrícola más allá de sus límites. La despoblación del campo se hizo extensiva al conjunto del imperio. En el siglo III d. C. había provincias del norte de África y de las costas mediterráneas en las que la mitad de la tierra cultivable había sido abandonada.

La despoblación del campo tuvo también otras repercusiones. Las tierras abandonadas ya no recibían ningún cuidado, lo que tuvo como resultado una mayor erosión y la pérdida de fertilidad. Las tierras bajas fueron las que más padecieron la despoblación masiva del campo. Nadie se ocupaba ya de drenar las zonas húmedas a comienzos de la primavera, por lo que quedaron inundadas por el agua. Los terrenos pantanosos se convirtieron en un caldo de cultivo para los mosquitos y provocaron la extensión de la malaria, que vino a cebarse sobre una población hambrienta y desesperanzada y a minar todavía más las reservas de energía humana.

Las plagas se extendieron durante los siglos II y III d. C. y, en algunas regiones de Italia llegaron a matar a un tercio de la población. El descenso de la población significaba menos gente disponible para trabajar en los campos y para llenar las vacantes en el ejército y los servicios civiles. La situación se hizo tan desesperada que el gobierno se vio obligado a reintroducir las levas forzadas para mantener su ejército. En el año 313 el emperador Constantino promulgó un edicto por el que algunos hijos de soldados quedaban automáticamente reclutados, lo que significaba el establecimiento del servicio militar hereditario. A comienzos del siglo IV d. C. se aprobaron estatutos parecidos para instituir el servicio civil hereditario. Aproximadamente en la misma época Constantino suscitó todavía una mayor controversia con el establecimiento del colonato, que introducía la idea de la servidumbre al ligar a los campesinos a la tierra en la que vivían en aquel momento. Esta práctica sobreviviría al Imperio Romano y se mantendría en pie hasta el siglo XVI, con la introducción de las leyes de cercamiento en la Inglaterra de los Tudor, por las que se liberaba a los siervos de sus ataduras con la tierra.

El colonato era una reforma demasiado tímida y llegaba demasiado tarde. En el siglo IV la población rural era tan escasa que por más que estuviera uncida por ley a la tierra resultaba insuficiente para remontar la producción agrícola de unos campos ya moribundos.

Roma estaba experimentando las duras realidades que imponen las leyes de la termodinámica. Mantener una población y una infraestructura en un estado de desequilibrio requería grandes cantidades de energía. Sin embargo, su régimen energético comenzaba a agotarse. Sin ninguna fuente alternativa de energía disponible, Roma impuso una presión todavía mayor sobre su precario legado energético. En el siglo V, el tamaño del aparato burocrático gubernamental y militar se había multiplicado por dos. Para pagarlo se tuvieron que aumentar los impuestos, lo cual provocó un empobrecimiento aún mayor de la población, sobre todo de la cada vez más escasa población rural. Según Joseph Tainter, el imperio comenzó a consumir su propio capital en forma de «tierras productivas y poblaciones de campesinos».

Con un régimen energético debilitado y próximo al agotamiento, el imperio comenzó a resquebrajarse. Fallaron los servicios básicos. La vasta infraestructura romana comenzó a degradarse. El ejército ya no podía mantener a raya a los invasores. Las hordas bárbaras comenzaron a hostigar al decadente Imperio Romano. Primero lo hicieron en territorios distantes, pero ya en el siglo VI los invasores estaban a las puertas de Roma. El gran Imperio Romano se había hundido. La población de Roma, que antes superaba el millón de personas, había bajado en el siglo VI hasta menos de 30.000 habitantes. La ciudad misma había quedado reducida a un pálido y ruinoso recordatorio de las inflexibles leyes de la energía.

La factura de la entropía era enorme. La energía libre disponible del Mediterráneo, el norte de África y amplias regiones de Europa continental, desde España hasta Inglaterra, había sido absorbida por la maquinaria romana. Tierras deforestadas, suelos erosionados y poblaciones empobrecidas y enfermas formaban el paisaje del imperio. Europa tardaría seiscientos años en recuperarse.

* * *

Igual que Roma, los países industrializados han creado una vasta y compleja infraestructura tecnológica e institucional para captar y explotar energía. La economía industrial global depende casi exclusivamente de los combustibles fósiles —la energía nuclear y las fuentes renovables de energía cubren sólo una pequeña cuota del mercado energético en los países industrializados— para mantenerse en un estado especialmente ordenado y en desequilibrio termodinámico. La dependencia del actual sistema económico y social respecto del crudo barato y el gas natural llega a tal extremo que, cuando éstos sean realmente difíciles de encontrar, procesar y utilizar, correremos el riesgo de sufrir una serie de fallos potencialmente desestabilizadores en diversos sistemas y subsistemas en todo el espectro de la vida moderna. ¿Cabe temer que estos fallos se realimenten entre sí, en una especie de efecto en cascada como el que tuvo lugar en Roma, donde cada fallo precipitaba el siguiente, con el resultado de que los sistemas entren en una espiral descontrolada que termine por desarticular nuestro estilo de vida? ¿Cabe temer que los cambios se produzcan tan deprisa que no dispongamos del tiempo necesario para poner en funcionamiento un nuevo régimen energético de suficiente envergadura como para garantizar nuestro futuro?

Nuestra prioridad principal debería ser comprender la arquitectura de nuestra infraestructura actual basada en los combustibles fósiles. Es crucial que sepamos identificar las muchas debilidades estructurales que existen en los puntos más diversos del sistema que hemos construido para hacer circular la energía de los combustibles fósiles a través de nuestro tejido industrial. Dicho sistema no sólo incluye la infraestructura tecnológica, sino también las infraestructuras económicas, sociales y políticas que se han construido sobre ella.

Incluso los más ardientes defensores de la energía de los combustibles sólidos dudan de que la era de los hidrocarburos vaya a sobrevivir al siglo XXI. Que seamos capaces de reinventar nuestra civilización sobre una nueva base energética o, en cambio, que seamos víctimas de sucesivas crisis que vayan debilitando la infraestructura energética, dependerá de lo dispuestos que estemos a someter a nuestro régimen energético actual a un riguroso proceso de reevaluación y examen. Para alcanzar las metas que necesitamos, primero tenemos que saber exactamente cómo llegamos adonde ahora nos encontramos. La cuestión crucial que debemos plantearnos es saber qué ha fallado en la forma de explotar la energía propia de la era industrial y por qué. Encontrar la respuesta a esta pregunta es crucial para no repetir los mismos errores en la creación de un nuevo régimen energético para los siglos XXI y XXII.

Capítulo 4

LA ERA DE LOS COMBUSTIBLES FÓSILES

Si eliminásemos los combustibles fósiles de la ecuación humana, la civilización industrial moderna dejaría de existir. Calentamos nuestras casas y oficinas con combustibles fósiles, mantenemos en funcionamiento nuestras fábricas y nuestros sistemas de transporte con combustibles fósiles, iluminamos nuestras ciudades y nos comunicamos a distancia con electricidad generada a partir de combustibles fósiles, cultivamos nuestros alimentos con la ayuda de combustibles fósiles, construimos nuestros edificios con materiales hechos de combustibles fósiles, tratamos nuestras enfermedades con medicamentos derivados de combustibles fósiles almacenamos nuestros excedentes en contenedores de plástico y embalajes hechos de combustibles fósiles y manufacturamos nuestras ropas y aparatos domésticos con la ayuda de productos petroquímicos. Prácticamente todos los aspectos de la vida moderna extraen su energía de los combustibles fósiles, derivan materialmente de ellos o reciben su influencia de algún otro modo.

A principios del siglo XX, el petróleo adelantó al carbón en la lista de combustibles fósiles empleados como fuentes energéticas en Estados Unidos y otros países industrializados. Los vehículos de motor se llevaban la parte del león en el consumo de petróleo, alrededor de un tercio del consumo anual total. En la actualidad hay 520 millones de automóviles en el mundo. De éstos, 132 millones se hallan en Estados Unidos, cuyo parque móvil incluye también 1,9 millones de camiones, 715.000 autobuses y 21.000 locomotoras. En todo el mundo hay 11.000 aviones comerciales de gran tamaño, 28.070 barcos y 1,2 millones de barcos de pesca, todos ellos impulsados con petróleo.

La industria es el segundo principal consumidor de petróleo en Estados Unidos, con el 23 % del total. Más de una cuarta parte del petróleo industrial se utiliza como materia prima en la industria química. Los productos petroquímicos sirven para elaborar miles de artículos, desde piezas de televisor hasta fármacos. El 6% del petróleo se emplea para la calefacción doméstica y comercial y el 4% se usa para generar electricidad en las centrales norteamericanas.

El petróleo es una de las sustancias más versátiles que se pueden encontrar en la naturaleza. Un barril de petróleo puede producir

gasolina suficiente para conducir 320 kilómetros con un coche de tamaño mediano; suficiente combustible destilado para conducir más de 64 kilómetros con un camión de gran tonelaje [...] suficiente gas licuado para llenar 12 bombonas (0,4 litros) para uso doméstico, de camping o de taller; casi 70 kilovatios-hora en una central eléctrica [...] asfalto para elaborar unos 4 litros de alquitrán; cerca de 2 kilos de lápices de carboncillo, cera para 170 velas de cumpleaños, o 27 lápices de cera [y] lubricantes para elaborar más de un litro de aceite de motor.

El paso a una civilización basada en los combustibles fósiles fue más rápido que ningún otro cambio de régimen energético en la historia del mundo. Hace sólo ciento treinta años, las tres cuartas partes del combustible empleado en Estados Unidos era en forma de madera. Ésta se usaba no sólo para la calefacción, sino también como combustible para ferrocarriles y barcos de vapor. Buena parte de la industria de la época dependía todavía de los molinos de viento y de agua. En 1890 se producían menos de 9 millones de toneladas de petróleo en todo el mundo, la mayor parte en forma de queroseno para la iluminación. A principios del siglo XX el petróleo seguía generando menos del 4% de la energía mundial. Cuando se produjo la crisis del petróleo de los años setenta, se consumían anualmente 2.270 millones de toneladas de petróleo, es decir, 200 veces más de lo que se consumía setenta años antes. En la actualidad, los combustibles fósiles cubren más del 85 % de las necesidades energéticas del mundo: el 40% corresponde al petróleo, el 22% al carbón y el 23 % al gas natural. La energía nuclear y la hidroeléctrica aportan un 7 % adicional cada una, mientras que las energías geotérmica, solar y eólica, así como la madera y los residuos sólidos, apenas representan el 1%. El consumo mundial de energía es setenta veces mayor ahora que al comienzo de la era de los combustibles fósiles.

Cómo se escribe realmente la historia

En las escuelas se enseña que fue un joven genio llamado James Watt quién dio origen a la era de los combustibles fósiles y a la Revolución Industrial con la invención de un motor de vapor alimentado con carbón. El motor de vapor, a su vez, significó la entrada en una era moderna de progreso material y avances humanos sin precedentes en la historia. Pues bien, no fue exactamente así. La historia de cómo el mundo realizó la transición hacia el carbón y el motor de vapor abriendo la puerta a la era de los combustibles fósiles, resulta algo más complicada.

La Europa medieval había confiado durante mucho tiempo en la madera como principal fuente energética. La espesa capa forestal que cubría toda la zona nórdica y occidental del continente proporcionaba una fuente aparentemente inagotable de combustible. Ya en el siglo XIV, sin embargo, la madera era cada vez más escasa. Los nuevos avances en el campo de la agricultura, como las nuevas tecnologías de drenaje, el arado pesado, la introducción de la rotación de tres cultivos y el uso de equipos de caballos para las labores de arado, habían contribuido a aumentar la cantidad de tierra cultivada y habían multiplicado la producción alimentaria. Estos excedentes llevaron a un incremento de la población humana, lo cual incrementó a su vez la presión sobre los campesinos para que sobreexplotaran la tierra disponible y deforestaran zonas próximas para aumentar la superficie de cultivo. Hacia el siglo XIV Europa se enfrentaba a un problema de entropía no muy distinto del que había experimentado Roma durante los siglos II, III y IV d. C. La población consumía los recursos energéticos en menos tiempo del que necesitaba la naturaleza para reponerlos. La creciente deforestación y la erosión del suelo provocaron una crisis energética. Según el historiador William McNeill:

En muchas zonas del noroeste europeo la humanidad había llegado a una especie de saturación en el siglo XIV. La gran proliferación de las divisiones territoriales iniciada alrededor del año 900 llevó a una multiplicación de las haciendas y los cultivos, hasta el punto de dejar escasa superficie arbolada, al menos en las regiones más densamente pobladas. Dada la importancia que tenían los bosques como fuente de combustible y de material de construcción, la creciente escasez causaba severos problemas a los asentamientos humanos.

El agotamiento de la madera constituía un serio problema para la sociedad de la baja Edad Media, como hoy lo es el agotamiento del petróleo para nosotros. Igual que el petróleo, la madera era un recurso energético extraordinariamente versátil, que se podía aplicar a mil y una funciones distintas. El historiador Lewis Mumford recoge algunos de los muchos usos que tenía la madera en aquella época.

Las herramientas de los carpinteros eran de madera, a excepción del filo para cortar: el rastrillo, el yugo y el carro eran de madera; también lo eran las bañeras de los baños públicos, al igual que los cubos y las escobas; en algunas partes de Europa, también los zapatos de los pobres eran de madera. La madera servía tanto al granjero como al tejedor: la rueca y el telar, las prensas de aceite y vino; incluso un siglo más tarde de la invención de la prensa para la imprenta, ésta todavía estaba hecha de madera. Incluso las cañerías del agua de las ciudades consistían a menudo en troncos de árboles y lo mismo sucedía con los cilindros de las bombas [...] evidentemente, los barcos estaban hechos de madera y [...] las primeras máquinas industriales estaban hechas igualmente de madera.

Mumford resume la extraordinaria importancia que tenía la madera como régimen energético de la vida medieval diciendo que «como materia prima, como herramienta, como máquina, como utensilio y como servicio público, como combustible y como producto acabado, la madera era el recurso industrial dominante».

La mayor parte de la deforestación que se llevó a cabo durante el siglo XV tenía como objetivo aumentar la extensión de los cultivos agrícolas. En los siglos XVI y XVII se cortaron todavía más árboles para obtener ceniza de madera destinada a la producción casera de jabón o artículos de cristal, entre otros. En Inglaterra, la principal carga para los bosques provenía de las crecientes necesidades de la Armada Británica. La producción de hierro y la construcción de barcos requerían grandes cantidades de madera. Los repetidos intentos de regular la tala de árboles demostraron ser inútiles. En 1630 la madera era dos veces y media más cara que a finales del siglo XV.

Lentamente, el carbón fue ocupando el lugar de la madera, primero en Inglaterra y más tarde en el continente. Comenzaba así un nuevo régimen energético. Sin embargo, debe observarse que la transición hacia el carbón no fue precisamente saludada con una alegría desbordante. Más bien al contrario. El carbón se consideraba como un recurso energético inferior, difícil de extraer, transportar y almacenar, sucio en el manejo y contaminante cuando se quemaba. Edmund Howes se lamentaba de que «los ciudadanos en general se ven obligados a hacer sus fuegos con carbón de mar o de mina, incluso en las habitaciones de los personajes más honorables». A pesar de todo, en 1700 el carbón comenzaba a ocupar el lugar de la madera como fuente primaria de energía en Inglaterra. A mediados del siglo XIX, la mayor parte de Europa había iniciado también su reconversión al carbón.

La extracción del carbón no estaba exenta de dificultades. Una vez agotados los recursos más próximos a la superficie y fácilmente accesibles, los mineros se vieron obligados a descender a mayor profundidad bajo tierra. A partir de una cierta profundidad se alcanzaron las capas freáticas y el drenaje se convirtió en un serio obstáculo para sacar el carbón a la superficie. En 1698, Thomas Savery patentó la primera bomba de vapor, una herramienta que permitía a los mineros bombear el agua a la superficie y extraer carbón de capas más profundas.

El carbón también era mucho más pesado e incómodo de transportar que la madera. No era fácil moverlo con carros tirados por caballos y carreteras sin pavimentar. El peso de los carros convertía el transporte en una tarea casi imposible, sobre todo cuando el tiempo era lluvioso y las carreteras estaban embarradas y llenas de surcos. Por otro lado, los tiros de caballos de carga resultaban cada vez más caros. La creciente escasez de tierras cultivables hacía que fuera demasiado caro malgastar el precioso suelo para pasto de los caballos. La respuesta al problema del transporte llegó con la locomotora de vapor montada sobre raíles de hierro. La locomotora fue una de las primeras máquinas de la era de los combustibles fósiles, el presagio de una nueva época.

Estamos acostumbrados a pensar en el progreso material como un flujo ininterrumpido de nuevas y mejores ideas que vienen a sustituir a las formas más viejas y primitivas de hacer las cosas. En realidad, el avance humano consiste más bien en un proceso de prueba y error, a menudo motivado por la desesperación. Según parece, lo más probable es que «la madre de la invención» no sea otra cosa que la «necesidad». Y tal como se demostró en la transición de la madera al carbón, el cambio de régimen energético es visto a menudo como algo oneroso y molesto en sus primeros estadios. El motivo es que los seres humanos siempre tratan de explotar primero los recursos energéticos más fácilmente accesibles. Mientras nuestros antepasados cazadores-recolectores tuvieron a su disposición una reserva abundante de energía en forma de plantas y anímales silvestres, no tuvieron ninguna necesidad de pasar a una forma de vida más dura, como la agrícola. De modo parecido, los bosques son una fuente de energía mucho más fácil de controlar, transformar y aprovechar que el carbón. Una observación del historiador Richard Wilkinson, basada en la historia de la civilización humana contemplada desde el punto de vista de la energía, resulta particularmente reveladora:

A lo largo de su desarrollo económico, la humanidad se ha visto obligada una y otra vez a cambiar los recursos en los que basaba su subsistencia, así como los métodos empleados para explotarlos. Lentamente se ha ido comprometiendo con técnicas cada vez más sofisticadas de procesamiento y producción, a medida que iba pasando de recursos más fácilmente explotables a otros que lo eran menos [...] Visto dentro de un contexto ecológico más amplio, el desarrollo económico consiste en el desarrollo de formas más intensivas de explotación del medio natural.

El carbón, por ejemplo, es una forma de energía disponible más aprovechable y de acceso más sencillo que el petróleo y el gas natural. Por este motivo, a medida que los países van pasando de recursos energéticos más fácilmente accesibles a formas de energía más difíciles de encontrar y procesar, las infraestructuras tecnológicas, económicas y sociales se hacen necesariamente más complejas, jerarquizadas y centralizadas.

Nuestra civilización del petróleo se basa en el proceso de transformación de la energía más jerarquizado y centralizado de la historia. Hemos creado un complejo organismo social en desequilibrio que depende del petróleo en todas sus articulaciones. Aunque el estilo de vida industrial ha sido enormemente beneficioso para aquellos que han aprovechado inicialmente sus ventajas, es justo decir que la propia complejidad que lo hace posible amenaza hoy con destruirlo. Esto es así porque el organismo social, al igual que cualquier otro organismo vivo, funciona como un todo. Todos los subsistemas de esta civilización industrial tan sumamente organizada dependen por entero del continuo flujo de energía no renovable en forma de petróleo y, en menor medida, de carbón y el gas natural, del mismo modo que un organismo vivo necesita del flujo constante de células sanguíneas por su cuerpo para existir. Si el flujo de petróleo disminuye, todo el organismo se debilita.

El negocio del petróleo se ha asociado durante mucho tiempo con la imagen romántica de los audaces y aventureros buscadores de petróleo, seguidos de cerca por los empresarios sin escrúpulos dispuestos a sacar tajada de la buena suerte de aquéllos. Todo eso forma parte de la historia. Pero la saga del petróleo también tiene que ver con el levantamiento progresivo de un régimen energético enormemente complejo y también muy frágil, con una estructura de dirección y control tan centralizada que resulta incluso más vulnerable que ninguna otra infraestructura energética previa ante posibles crisis o incluso colapsos repentinos.

El nacimiento del petróleo

Lo primero que debemos tener presente en relación con el petróleo es que se halla irregularmente distribuido por el mundo. En el siglo XX, Estados Unidos se convirtió en la principal potencia industrial en gran medida gracias a sus ricos depósitos de petróleo. De modo parecido, el éxito de Inglaterra al comienzo de la Revolución Industrial era en gran medida atribuible a los importantes depósitos nacionales de carbón que tenía a su disposición.

Aunque ya nadie piensa en Estados Unidos como una potencia petrolera, debemos recordar que durante los primeros setenta años del siglo XX los pozos de petróleo de Texas eran tan representativos de la grandeza de Norteamérica como las cadenas de montaje de Ford. El acceso a los recursos energéticos vitales siempre ha sido un factor crucial en la prosperidad de las naciones. Por más que nos cueste comprenderlo en Occidente, casi la mitad de la humanidad —más de 2.500 millones de personas— utiliza todavía madera y residuos animales y agrícolas como combustible.

El 27 de agosto de 1859, Edwin Laurentine Drake —un jefe de tren retirado e inadaptado socialmente que se refería a sí mismo como «el coronel»— descubrió petróleo a una profundidad de 21,1 metros cerca de la pequeña localidad de Titusville, Pennsylvania, con la ayuda de una torre de perforación improvisada. El petróleo fluía a la superficie a un ritmo de 20 barriles por día. La era del petróleo había comenzado.

La Guerra Civil resultó ser una ayuda para la incipiente industria del petróleo. El crudo se usaba como combustible para las lámparas, lubricante para la maquinaria y sustituto del aguarrás para el ejército. (El petróleo no se usaba todavía como combustible para las locomotoras.) Cuando la Guerra Civil terminó en 1865 había pozos de petróleo en Virginia Occidental, Nueva York, Ohio e incluso en zonas del oeste como Colorado y California. Había cincuenta y ocho refinerías de petróleo en Pittsburg, treinta en Cleveland y algunas más en distintos puntos de la costa este, la mayoría dedicadas a la producción de keroseno para las lámparas.

En 1868 un antiguo administrativo y contable de Cleveland, John D. Rockefeller, fundó la Standard Oil Company de Pennsylvania. Rockefeller se dio cuenta de que la llave del éxito en el negocio del petróleo consistía en poseer no sólo los pozos, sino también las refinerías, y en controlar el transporte y la comercialización de los productos acabados. Su primer objetivo fue negociar acuerdos especiales con los ferrocarriles, tras lo cual se dedicó a la adquisición de oleoductos. En 1879 la Standard Oil Company controlaba prácticamente el 95 % de las refinerías del país. Rockefeller pronto comenzó a comprar también yacimientos petrolíferos, con el sueño de integrar bajo un mismo techo todos los aspectos del negocio del petróleo. Sus actividades se expandieron de forma tan rápida que hacia 1882 había integrado sus grandes holdings en la Standard Oil Company de New Jersey. El trust tenía participaciones en las acciones de docenas de industrias. En 1906 el poder de Rockefeller sobre el flujo energético en Estados Unidos había crecido tanto que el gobierno inició una acción judicial acogiéndose a la Ley Sherman contra los monopolios. En 1911, la Corte Suprema de Estados Unidos ordenó el fraccionamiento de la Standard Oil, así como la desinversión de todas las acciones que poseía el grupo en empresas subsidiarias. La acción legal tuvo escaso efecto a largo plazo. Tras la disolución del holding, las compañías individuales se volvieron a integrar dentro de los Estados donde operaban. Muchas de las personas que tenían acciones en el trust continuaron manteniendo acciones de las diversas compañías. Finalmente, las muchas ramificaciones de la Standard Oil quedaron en manos de millones de accionistas.

Paralelamente se comenzaron a formar otras compañías petroleras similares a la de Rockefeller, en cada caso una empresa energética plenamente integrada que poseía yacimientos petrolíferos, oleoductos y refinerías, y que controlaba el transporte y la comercialización de los productos hasta el nivel de la gasolinera local. En los años treinta ya se habían creado las principales compañías petroleras que iban a definir la principal industria del mundo, como Standard Oil de New Jersey, Gulf Oil, Humble, Atlantic Refining Company, Sinclair, Standard Oil de Indiana, Phillips 66, Sucony, Sun, Union 76 y Texaco. Entre veintiséis compañías controlaban dos tercios de la estructura del capital de la industria, el 60% de las perforaciones, el 90% de los oleoductos, el 70% de las actividades de refinado y el 80% de las actividades de marketing.

A comienzos del siglo XX se produjeron dos circunstancias que situaron el petróleo en el centro de la vida norteamericana y convirtieron a Estados Unidos, a su vez, en el país más poderoso del mundo. La primera fue la invención del motor de combustión interna; la segunda, el papel crucial que desempeñaría el petróleo para la victoria de Estados Unidos y sus aliados en las dos guerras mundiales.

La nueva movilidad

Los primeros que consiguieron poner un motor de combustión interna sobre ruedas fueron los alemanes Karl Benz y Gottlieb Daimler. Su carruaje sin caballos hizo su debut en 1885. El motor funcionaba con gasolina derivada de petróleo crudo. A pesar de ser un invento alemán, fue la cadena de montaje de Henry Ford, capaz de producir en serie millones de vehículos a un precio asequible, la que convirtió el petróleo y los coches en el eje central de una nueva era.

La primera gasolinera de Estados Unidos abrió en Detroit en 1911. El crecimiento vertiginoso de la producción de automóviles cogió por sorpresa a la industria petrolera. En un esfuerzo por mantenerse a la altura de la demanda casi insaciable de gasolina, las compañías energéticas ampliaron sus actividades de exploración y abrieron nuevos yacimientos a un ritmo casi semanal. En 1916 circulaban 3,4 millones de coches por las carreteras estadounidenses. Sólo catorce años más tarde había más de 23,1 millones de coches en Estados Unidos.

Los automóviles se convirtieron en la pieza clave del capitalismo industrial durante la mayor parte del siglo. Muchas otras industrias básicas estaban vinculadas a la suerte del coche. Los automóviles consumían «el 20% del acero, el 12% del aluminio, el 10% del cobre, el 51% del plomo, el 95% del níquel, el 35% del zinc, y el 60% del caucho que se producía en Estados Unidos». Los industriales se lanzaron sobre las inmensas posibilidades comerciales que abría el automóvil. Un analista observó en 1932:

¡Piensen en los efectos que puede tener para el mundo industrial la introducción en el mercado de un producto que duplica el consumo de acero maleable, triplica el consumo de cristal plano y cuadruplica el uso de caucho! [...] El automóvil no tiene rival en la historia del mundo como consumidor de materias primas.

El automóvil puso a millones de personas en la carretera. También acercó el campo y la ciudad y extendió la cultura de los barrios residenciales a costa de los conceptos tradicionales de barrio y comunidad. Por último, el automóvil contribuyó más que ningún otro invento del siglo XX a acelerar el ritmo de vida y a convertir la velocidad y la eficacia en las principales virtudes de nuestra época.

La producción de automóviles fue en buena medida responsable del espectacular crecimiento económico que experimentó Estados Unidos en las tres primeras décadas del siglo XX, similar al de Europa y Asia tras la Segunda Guerra Mundial. Pero fue el petróleo el que lo hizo posible. El estadista británico Ernest Bevin comentó en una ocasión: «Tal vez el reino del cielo esté fundado en la justicia, pero el reino de la Tierra se basa en el petróleo».

Si, por un lado, fue el automóvil el que convirtió el petróleo en un elemento indispensable para la vida social y comercial del siglo XX, las dos guerras mundiales fueron las que convencieron a los líderes políticos de la importancia estratégica del petróleo en los asuntos de Estado. Gran Bretaña había confiado durante mucho tiempo en su superioridad naval para mantener su posición de dominio en el mundo. En la primera década del nuevo siglo el gobierno alemán lanzó una ambiciosa campaña para superar a la marina británica en los océanos. Un joven político británico, Winston Churchill, fue nombrado Primer Lord del Almirantazgo en 1911 con el encargo de dar respuesta a la amenaza germana.

Churchill era consciente de lo que estaba en juego con la superioridad naval británica. En una ocasión exclamó: «La suerte de nuestra raza y de nuestro imperio, todo el tesoro acumulado durante tantos siglos de logros y sacrificios, se echaría a perder y se desvanecería enteramente si nuestra supremacía naval se viera cuestionada».

Churchill se convenció de que la llave de la victoria sobre Alemania en cualquier batalla futura consistía en hacer que la marina británica sustituyera el carbón por el petróleo. Los barcos impulsados con petróleo serían más rápidos, requerirían menos hombres para trabajar en la sala de máquinas, tendrían un mayor radio de acción y en caso necesario podrían repostar en alta mar. Churchill logró convencer al gobierno británico para que iniciara la introducción del petróleo en la marina. En 1912, el gobierno británico acordó construir los cinco primeros acorazados impulsados con petróleo. Esta simple decisión se revelaría como crucial unos años más tarde para garantizar la victoria de los aliados sobre Alemania durante la Primera Guerra Mundial.

Una vez comprometidos con la idea de una marina basada en el petróleo, Churchill y otros comandantes militares británicos dirigieron su atención hacia el problema de garantizar un suministro de petróleo seguro y constante. Entonces, el mercado del petróleo en Europa estaba dominado por la Royal Dutch Shell Company. Preocupado por la posibilidad de que esta compañía pudiera caer en manos de los alemanes, Churchill convenció al gobierno británico para que invirtiera y entrara como socio en una compañía energética británica: Anglo-Persian. Por otro lado, el gobierno negoció un acuerdo secreto con la compañía que garantizaba al Almirantazgo el suministro de petróleo durante veinte años.

El motor de combustión interna de gasolina desempeñó un papel importante en las victorias militares durante la guerra, tanto en tierra como en el mar. El ejército británico inventó un vehículo blindado impulsado por un motor de combustión interna y montado sobre orugas. El nuevo vehículo, más tarde conocido como «tanque», podía cambiar todo el equilibrio de fuerzas en el campo de batalla, ya que era capaz de cruzar las líneas defensivas protegidas con alambres de espino y pasar por encima de trincheras llenas de hombres.

La Primera Guerra Mundial también vio la introducción de las motocicletas, los jeeps y los camiones. Más rápidos y fáciles de mantener que los caballos y más flexibles que las locomotoras a la hora de trasladar tropas y suministros hacia el frente, los nuevos vehículos dieron a los aliados una ventaja crucial sobre las fuerzas alemanas.

En la Primera Guerra Mundial ambos bandos utilizaron aviones propulsados con gasolina para actividades bélicas y de reconocimiento. Al final de la guerra se habían construido y enviado a los cielos más de 200.000 aviones, una verdadera revolución en el ámbito bélico. A partir de este momento la guerra no sólo podía hacerse desde la tierra, sino también desde el aire.

En una cena de la Conferencia Interaliada sobre el Petróleo celebrada en Londres pocos días después de la firma del armisticio en el año 1918, los oficiales británicos reconocían la importancia que había tenido el petróleo en la guerra. El presidente, lord Curzon, proclamó que «la causa aliada se ha deslizado hasta la victoria sobre una ola de petróleo».

El acceso al petróleo fue todavía más importante en la Segunda Guerra Mundial. En realidad, toda la estrategia bélica se basaba en el intento de hacerse con el control de los vitales suministros de petróleo. Poco después de llegar al poder, Adolf Hitler dirigió su atención hacia la necesidad de garantizar un suministro constante de petróleo para impulsar la renovada maquinaria de guerra alemana. Hitler pensaba que el ejército del futuro dependería menos de la fuerza bruta humana y más de la movilidad y la velocidad vertiginosa que permitía el transporte blindado. La «guerra relámpago» se convirtió en la consigna de las acciones militares alemanas en el campo de batalla. Sin petróleo, el proyecto de Hitler era imposible. El problema era que Alemania, a pesar de sus ricas reservas de carbón, no disponía de petróleo. Hitler era consciente de que la dependencia de su país respecto de una maquinaria de guerra alimentada con carbón les había costado la Primera Guerra Mundial y no estaba dispuesto a repetir los mismos errores del pasado.

El gobierno alemán concentró en dos frentes su respuesta al problema del petróleo: primero, desarrollar una industria nacional de combustible sintético; y segundo, entrar en guerra con Rusia para asegurarse el acceso a los ricos yacimientos petrolíferos de Baku, en Asia central.

A finales de los años treinta, el petróleo era ya la principal fuente energética en Estados Unidos, por delante del carbón. En Alemania, en cambio, el carbón seguía aportando el 90% de la energía en vísperas de la Segunda Guerra Mundial. A comienzos de siglo, los químicos alemanes habían conseguido crear un combustible sintético a partir de la extracción de líquido del carbón, pero el proceso resultaba caro y no podía competir con los bajos precios del petróleo crudo en los mercados mundiales. Hitler se mantuvo firme en su empeño. En 1936 lanzó un ambicioso proyecto para crear una industria nacional de combustible sintético con la ayuda del gigante alemán de la química I. G. Farber y proclamó que la tarea debía «ser organizada y ejecutada con la misma determinación que la guerra, pues de su resultado [dependía] el futuro desarrollo de la misma». En 1940, las refinerías alemanas producían 72.000 barriles de petróleo sintético al día, prácticamente el 46% del suministro total de petróleo en Alemania. En 1944, la industria de combustible sintético cubría el 57 % del suministro energético destinado al esfuerzo militar, incluido el 92% del combustible para la aviación.

Cubrir las necesidades restantes de petróleo resultó ser una tarea todavía más difícil y costosa. La necesidad imperiosa de petróleo hizo que Alemania invadiera la Unión Soviética el 22 de junio de 1941. Hitler tenía la esperanza de que una rápida victoria sobre los rusos le daría acceso a los yacimientos petrolíferos de Baku en el Cáucaso y le proporcionaría el combustible necesario para ganar la guerra. En 1945, el ministro alemán de Armamento y Producción de Guerra, Albert Speer, admitió durante un interrogatorio de los aliados que «la necesidad de petróleo fue ciertamente uno de los principales motivos» en la decisión alemana de invadir Rusia.

La resistencia rusa, sin embargo, consiguió detener la ofensiva alemana. En agosto de 1942, el ejército alemán llegó hasta los yacimientos petrolíferos de Maikop, en el Cáucaso, sólo para comprobar que los rusos habían volado los pozos y las refinerías. Escasos de combustible y alejados de su país, los alemanes no pudieron derrotar a los rusos para hacerse con el control de Grozny, el epicentro del petróleo del Cáucaso. Lo irónico, según Daniel Yergen, es que «los alemanes se quedaron sin petróleo precisamente en su intento de conseguir petróleo». El fracaso del proyecto hizo que el gobierno alemán dependiera cada vez más de su producción nacional de combustible sintético, que era insuficiente para mantener el esfuerzo bélico.

El ataque de Japón a Pearl Harbor en 1941, igual que el ataque alemán contra Rusia de unos meses antes, estaba motivado por el intento desesperado de asegurarse el acceso al petróleo para su maquinaria militar. Estados Unidos y las Indias Orientales Holandesas —la actual Indonesia— eran los principales proveedores de petróleo que tenía Japón. Cuando el gobierno japonés invadió el sur de Indochina en julio de 1941, Gran Bretaña, las Indias Holandesas y Estados Unidos decretaron inmediatamente un embargo sobre las exportaciones de petróleo a la isla. Con las reservas de petróleo próximas a agotarse, Japón tomó la decisión de lanzar un ataque sorpresa sobre la flota norteamericana de Pearl Harbor. El objetivo era inutilizar o destruir la flota estadounidense en el Pacífico y luego hacerse con el control de los yacimientos petrolíferos de las Indias Orientales Holandesas. Su plan tuvo éxito. Sin embargo, en fases posteriores de la guerra el gobierno de Estados Unidos consiguió la superioridad aérea en el Pacífico y en 1944, los aviones y los barcos estadounidenses hundían los petroleros japoneses más deprisa de lo que ellos podían construirlos. La cantidad de petróleo que llegaba a Japón bajó hasta un 50% en 1944 y en 1945 se redujo prácticamente a cero. Privados de reservas de petróleo con las que mantener sus operaciones militares, los japoneses se vieron obligados a utilizar un combustible elaborado a partir de raíces de pino para sus aviones, e incluso carbón para sus jeeps. El resultado de la Segunda Guerra Mundial se explica simplemente por el hecho de que los aliados controlaban el 86% de las reservas mundiales de petróleo.

El imperio del petróleo

Cuando uno piensa en la industria global del petróleo, lo más probable es que la primera expresión que le venga a la cabeza es «Big Oil». La industria petrolera es el negocio más importante del mundo, con un valor estimado de entre dos y cinco billones de dólares. Está formada por un vasto complejo que incluye yacimientos petrolíferos, plataformas petrolíferas marinas, miles de kilómetros de oleoductos, gigantescos barcos petroleros, refinerías, sistemas informatizados de gestión del flujo energético hacia los consumidores finales, estaciones de servicio y miles de compañías dedicadas a la elaboración de productos petroquímicos, que van desde lubricantes y fertilizantes hasta plásticos y medicinas. El petróleo es la partida más importante en la balanza comercial de la mayoría de los países. Tres de las siete compañías más grandes del mundo que cotizan en bolsa son compañías energéticas. ExxonMobil ocupa el segundo lugar en la clasificación de Fortune 500, con unos ingresos de 213.000 millones de dólares.

Los años 1999 y 2000 vieron la creación de lo que los analistas de la industria llaman compañías energéticas «super major». BP se fusionó con Amoco y ARCO, Exxon con Mobil, Total con Elf y Chevron con Texaco. Estas fusiones, cuyo valor alcanza en algunos casos los 200.000 millones de dólares, están haciendo que las grandes compañías adquieran dimensiones colosales. La actual fase de consolidación ha hecho que las compañías energéticas de cotización pública se pongan al mismo nivel que las compañías petroleras estatales, como Saudi Aramco, Petróleos de Venezuela, la iraní NIOC y la mexicana Pemex. Las primeras controlan en buena medida la parte final del proceso. ExxonMobil, Royal Dutch/Shell, BP y Total Fina Elf controlan actualmente el 32% de las ventas en los mercados mundiales y el 19% de la capacidad de refinado. Las segundas controlan sobre todo la parte inicial del proceso. Saudi Aramco, Petróleos de Venezuela, NIOC y Pemex producen el 25% del petróleo mundial y poseen el 42% de las reservas. El mercado mundial de la energía está dominado por tan sólo diez o doce compañías, entre super majors y compañías petroleras estatales.

En Estados Unidos, cinco compañías —ExxonMobil, Chevron-Texaco, BP Amoco-ARCO, Phillips-Tosco y Marathón— controlan el 41% de las actividades de exploración y producción de petróleo que se realizan en el país, cerca del 47% de las refinerías nacionales y el 61% del mercado minorista. Los beneficios de estas cinco compañías descontando los impuestos pasaron de 16.000 millones de dólares en 1999 a 40.000 millones de dólares en el año 2000, un incremento del 146% en doce meses. Una vez pagados los impuestos, los beneficios de estas mismas compañías en el primer trimestre de 2001 volvieron a subir de 8.700 a 12.000 millones de dólares, un aumento del 38% en sólo tres meses. Los crecientes beneficios de la industria petrolera contrastan con el descenso del 43% en los ingresos de las otras 1.400 empresas más grandes de Estados Unidos en el primer trimestre de 2001. Resulta interesante saber que las empresas estadounidenses señalan el aumento de los costes laborales y «energéticos» como los principales responsables del descenso en los márgenes de beneficio. El hecho de que sean tan pocas las compañías que controlan el flujo energético en la economía hace que estén en una posición única para dictar sus términos a todas las demás empresas que forman el tejido industrial. En una economía nacional y global controlada por un número cada vez más reducido de jugadores en todos los campos e industrias, las compañías energéticas se encuentran cómodamente instaladas en la cúspide de la pirámide global, ya que son ellas las que reparten la energía que mantiene en funcionamiento todas las demás actividades económicas.

Encontrar, extraer, transportar, refinar y distribuir petróleo y productos petroquímicos es un negocio caro y complicado. Sólo las mayores compañías del mundo —las que tienen los bolsillos más grandes— disponen de los recursos necesarios para gestionar el proceso que va del pozo a la gasolinera. Pensemos tan sólo en el índice de activación, la «medida de la inversión total requerida para acceder a nuevas fuentes de petróleo, expresada en dólares por barril y día de producción estabilizada». En 1999, Irak anunció que tenía la intención de triplicar su producción de petróleo y pasar de dos a seis millones de barriles por día. El coste de financiar un aumento como éste en la producción rondaría los 30.000 millones de dólares, o un coste de desarrollo de 7.500 dólares por barril de petróleo y día de producción nueva. El índice medio de activación en el mundo en 1999 era de 2.000 dólares por barril y por día. En algunos lugares, como las aguas profundas del golfo de México, los costes de activación pueden llegar a los 9.000 dólares por barril y por día. Se espera que el coste total en capital de inversión para la exploración y la producción de petróleo a nivel mundial supere el billón de dólares durante los próximos diez años. Las grandes compañías como ExxonMobil, BP Amoco y Shell tienen un flujo de caja varias veces superior al de Arabia Saudí. Las principales compañías energéticas del mundo realizan habitualmente inversiones de 1.000 millones de dólares o más en grandes proyectos, como las plataformas de aguas profundas. En 1999, las ocho principales compañías petrolíferas aportaban más del 80% de la inversión en investigación y desarrollo destinada a la exploración y la producción.

Imaginemos lo exigente y compleja que debe ser la tecnología y la infraestructura de soporte para procesar, transformar y mover millones de barriles de petróleo cada día alrededor del mundo y hacerlos llegar a todos los rincones de la vida cotidiana sin que haya retrasos o interrupciones relevantes en el flujo. Existen incontables sistemas y subsistemas, y todos ellos deben estar coordinados entre sí para garantizar un flujo continuado de energía. El geólogo Robert O. Anderson compara este proceso con un gran «Amazonas de petróleo».

La industria energética emplea una tecnología más variada y un personal más especializado que ninguna otra industria. La exploración requiere el uso de satélites y conocimientos de geoquímica y geofísica.

Con la ayuda de sofisticados ordenadores y programas informáticos se recogen datos sísmicos tridimensionales de reflexión para crear imágenes sísmicas en 3D del interior de la Tierra. Perforar pozos hasta profundidades que pueden alcanzar los 6.100 metros requiere el uso de un equipamiento complejo y de alta tecnología. Las plataformas de perforación petrolera en alta mar son una maravilla de la ingeniería moderna, capaz de resistir huracanes y tifones. Analizar la perforación y el corte de las rocas, lo que los geólogos llaman «perfilar el pozo», requiere el uso de instrumentos de neutrones y rayos gamma para registrar las curvas eléctricas en el lugar donde se halla el pozo. Es necesario traer a ingenieros especializados para lubricar la broca con un barro especial para la perforación. Su trabajo consiste en ajustar el barro de perforación a las condiciones del pozo, para asegurar que el barro evite el desmoronamiento del agujero durante la perforación y cuando los restos salen a la superficie. Instalar centenares de kilómetros de oleoductos por algunos de los terrenos más inhóspitos del mundo y mantenerlos también exige la participación de ingenieros especializados. El propio proceso de refinado es tal vez la fase más complicada. Los químicos orgánicos rompen la cadena de hidrocarburos del petróleo crudo y la reconstruyen de nuevo para crear toda una gama de productos que van desde la gasolina al plástico.

Coordinar todo el flujo, es decir, hacer circular el petróleo a través de los distintos canales y transformaciones hasta el usuario final, pone en juego un segundo equipo de personal de gestión y especialistas en marketing. R. O. Anderson será nuestro guía en el laberinto bizantino de las actividades que integran el negocio del petróleo.

Para empezar, el 60% del petróleo que se extrae ya está comprometido, a menudo con las propias refinerías del productor. El resto se pone a la venta en el mercado abierto. Los compradores y los intermediarios, que actúan unas veces en representación de grandes compañías petroleras y otras de forma independiente, venden a su vez el crudo a refinerías más pequeñas. El comprador debe conocer bien las complicaciones del negocio del petróleo, como por ejemplo saber qué refinerías pueden procesar crudos pesados, sólidos, dulces o ligeros. También debe estar familiarizado con las necesidades de los derivados del petróleo propias de cada región geográfica, así como con el tipo de crudo más adecuado para elaborar estos productos.

El refinador, a su vez, debe poner cuidado en distinguir las propiedades únicas de cada tipo de crudo. El crudo procedente de Arabia Saudí tiene propiedades distintas del procedente de Venezuela. Las compañías petroleras identifican el crudo en función de su gravedad, viscosidad, contenido de cera y contenido de sulfuro. Las refinerías actuales están construidas para procesar un tipo determinado de crudo. Por ejemplo, si una refinería procesara un crudo con un alto contenido de sulfuro a través de un sistema diseñado para materiales bajos en esta sustancia, la maquinaria se corroería, provocando unas pérdidas de cientos de millones de dólares.

El sistema de marketing puede resultar igualmente problemático. Las ventas de muchos productos derivados del petróleo varían según la temporada. Las ventas de petróleo para la calefacción suben en invierno, mientras que las de gasolina lo hacen habitualmente en verano. El equipo directivo prevé las necesidades futuras con una anticipación de entre seis meses y un año y se asegura de que se introduzcan en el sistema los tipos adecuados de crudos y se dirijan hacia las refinerías apropiadas, las cuales, a su vez, generan los productos requeridos para cada temporada. Si una refinería se encuentra con un exceso de crudo provocado por una bajada de la demanda, o si se cierran temporalmente unas instalaciones para hacer reparaciones, el petróleo será transferido a una o varias refinerías con necesidades y demandas adecuadas entre los centenares que operan en Estados Unidos. Anderson compara la industria petrolera con un oleoducto y señala que está pensada para que el producto fluya. Si el petróleo se mantiene almacenado durante demasiado tiempo en tanques, barcos u oleoductos, se pierde dinero. Es más, precisamente porque se trata de un sistema de flujo, la capacidad de almacenamiento en cada punto del sistema es limitada. La mayoría de los operadores mantienen unas reservas de trabajo de menos de catorce días, lo cual significa que el sistema de coordinación para garantizar que el flujo de petróleo vaya a los lugares exactos donde se necesita debe estar bien ajustado.

Según Anderson, los departamentos de marketing de las compañías energéticas se dividen en el sector industrial, el mayorista y el minorista y el de ventas de productos especiales. Los principales usuarios industriales, como las líneas aéreas, los servicios públicos, las plantas químicas y las refinerías, acostumbran a negociar directamente con las compañías energéticas. Las ventas industriales abarcan toda una gama que va desde el asfalto hasta los abonos y los combustibles de aviación, así como el coque para las industrias productoras de metal y de caucho y los líquidos procedentes del gas natural para elaborar productos químicos y agrícolas. La producción de gasolina y otros derivados para la automoción aún representa el 50% de las ventas de petróleo en Estados Unidos.

La reestructuración del comercio

La infraestructura del petróleo es con diferencia la red energética más compleja jamás creada. La naturaleza de la energía ha dictado los términos de su explotación. Irregularmente distribuido, difícil de extraer, caro de transportar, complicado de refinar y diversificado en sus aplicaciones, el petróleo ha requerido desde el principio una estructura de dirección y control altamente centralizada para financiar la exploración y la producción y para coordinar el flujo del petróleo hasta los usuarios finales. El carácter altamente centralizado de la infraestructura del petróleo ha generado inevitablemente empresas comerciales organizadas del mismo modo. El debate sobre la emergencia del capitalismo industrial ha prestado escasa atención al hecho de que la naturaleza de las fórmulas comerciales que aparecieron vinieron determinadas en buena medida por el nuevo régimen energético.

En una sociedad construida a partir de la energía de la madera, las empresas comerciales tienden a ser pequeñas y locales, y el comercio de productos acostumbra a mantenerse dentro de mercados limitados desde el punto de vista geográfico. Las empresas son mayoritariamente familiares y requieren una escasa financiación externa, ya que el equipamiento básico es sencillo y resulta relativamente fácil de reunir a partir del conocimiento, las herramientas y otros recursos disponibles en la comunidad. La madera es una fuente de energía limitada. El ritmo, el flujo y el volumen de producción que permite la madera no son lo bastante importantes como para introducir un cambio cualitativo en la velocidad y la diversidad de la actividad comercial, que haría necesario un mayor grado de coordinación y unos mecanismos de dirección y control más jerárquicos y centralizados.

Los combustibles fósiles son distintos. El carbón, el petróleo y el gas natural son formas de energía más concentradas y cuando se aprovechan de forma adecuada incrementan notablemente el rendimiento y la densidad de la actividad económica. El nuevo ritmo e interactividad se transmiten a su vez al mundo político y cultural, e imponen la creación de mecanismos de control y dirección centralizados y jerarquizados en todos los sectores para gestionar el volumen creciente de interacciones humanas.

El ferrocarril y el telégrafo, dos de las primeras industrias modernas en adoptar los combustibles fósiles como fuente de energía, establecieron el marco operativo para la introducción de las nuevas empresas más centralizadas y jerarquizadas características del capitalismo del siglo XX.

En Estados Unidos, la era del ferrocarril comenzó a tomar cuerpo en la década de 1850. Sólo en esa década se instalaron más de 30.000 kilómetros de raíles. El cambio introducido en la velocidad del transporte no tenía precedentes en la historia. Por primera vez, los seres humanos podían rebasar ampliamente los límites máximos de velocidad impuestos por sus propias piernas, el transporte animal y el aprovechamiento de los vientos y las olas de la Tierra. Si en 1850 un viaje de Nueva York a Chicago podía durar tres semanas o más, en 1857 había trenes que llevaban a los pasajeros de una ciudad a otra en sólo setenta y dos horas. El ferrocarril hizo que las empresas comerciales pudieran enviar sus mercancías de forma rápida, barata y segura. También podían trasladar tres veces más carga que el transporte fluvial, a un coste equivalente.

Realizar el seguimiento de unos cargamentos distribuidos en miles de vagones de ferrocarril a lo largo de miles de kilómetros, conservar las vías en buen estado, reparar periódicamente los motores y los vagones, supervisar a miles de empleados y garantizar la seguridad del transporte, todo ello requería un nuevo modelo organizativo. El historiador económico Alfred Chandler señala que los ferrocarriles fueron el primer negocio moderno que separó la propiedad de la gestión. Ninguna familia podía asumir en solitario los inmensos costes de capital que exigía la construcción de una línea de ferrocarril. Así pues, en la década de 1850 las casas de inversión europeas comenzaron a financiar los ferrocarriles estadounidenses. La gestión de los ferrocarriles fue encargada a una nueva clase de directivos profesionales. Estos crearon el precedente de la moderna estructura organizativa: un mecanismo de control y dirección centralizado y jerarquizado, integrado por una serie de centros de toma de decisiones en la parte superior de la pirámide y con los peldaños intermedios ocupados por gestores responsables de funciones específicas relacionadas con la actividad diaria de la organización, bajo la guía y la supervisión general de sus superiores.

Según Chandler, la estructura organizativa de los ferrocarriles en la década de 1890 era notablemente parecida a la que dominaría el mundo de los negocios en el siglo XX. En la cima del organigrama se hallaba el consejo de dirección, e inmediatamente por debajo el presidente, el director general y el tesorero. Por debajo de la dirección de la compañía estaban los gerentes generales, que supervisaban a dos o tres inspectores generales. En el siguiente nivel se hallaban los directivos intermedios, responsables de departamentos y funciones específicos, y entre los que se hallaban el jefe de máquinas, el jefe de mantenimiento de las vías, las mercancías y el tráfico de pasajeros, el controlador, el director del departamento legal, el ingeniero en jefe y el gerente de construcción. «Los ferrocarriles», dice Chandler, fueron «las primeras empresas comerciales modernas.»

Fueron las primeras [empresas] que necesitaron un gran número de directivos a sueldo; las primeras en tener una oficina central gestionada por directivos intermedios y dirigida por altos directivos que rendían cuentas ante el consejo de dirección. Fueron las primeras empresas comerciales norteamericanas que levantaron una estructura organizativa interna de gran envergadura, con líneas de responsabilidad, autoridad y comunicación cuidadosamente definidas entre la oficina central, los centros departamentales y las unidades de campo; y fueron las primeras en desarrollar flujos financieros y estadísticos para controlar y evaluar la labor de sus muchos directivos.

En 1891, la Pennsylvania Railroad daba trabajo a 110.000 trabajadores, lo que la convertía en la empresa comercial más grande del mundo en la época. En realidad, la compañía comenzaba a rivalizar en tamaño con el gobierno federal. En 1893, los ingresos del gobierno de Estados Unidos eran de 387,5 millones de dólares. Ese mismo año la Pennsylvania Railroad anunció unas rentas de casi 100 millones de dólares, convirtiéndose en la segunda institución de Estados Unidos en cuanto a generación de ingresos.

La revolución del transporte ferroviario vino de la mano de la revolución en las comunicaciones. El telégrafo hizo posible por primera vez que los seres humanos se comunicaran de forma instantánea a través de grandes distancias geográficas. Los ferrocarriles fueron los primeros en beneficiarse del telégrafo, que les servía para dirigir el tráfico, controlar los envíos y coordinar los derechos de preferencia de trenes que venían en direcciones distintas por la misma vía. En 1866, el negocio del telégrafo estaba controlado por una única compañía: Western Union. A partir de entonces su nombre sería sinónimo de telégrafo. El primer gigante norteamericano de la comunicación adoptó un tipo de estructura organizativa muy parecida a la del ferrocarril.

La velocidad y las economías de escala dictaban los términos del desarrollo del ferrocarril y el telégrafo en Norteamérica. Se requerían grandes cantidades de capital para iniciar y mantener las operaciones, y unas funciones de dirección y control sumamente centralizadas para coordinar el ritmo, el flujo, la velocidad y la densidad crecientes de la actividad comercial. Los economistas, que durante mucho tiempo habían concebido los mercados como espacios integrados por pequeños compradores y vendedores independientes que se reunían para realizar intercambios relativamente sencillos de bienes y servicios, pronto comenzaron a hablar de los méritos del «monopolio natural».

Además de proporcionar el nuevo modelo organizativo para el mundo de los negocios, el ferrocarril y el telégrafo aportaron también la infraestructura básica para el advenimiento del moderno sistema industrial. Un sistema de transporte rápido, fiable y activo durante todo el año y unas comunicaciones instantáneas permitían a las empresas un acceso ininterrumpido tanto a sus proveedores como a los mercados minoristas. La industria, que durante mucho tiempo había trabajado de forma estacional, podía funcionar ahora los 365 días del año. Primero el carbón y más tarde el petróleo proporcionaron la energía necesaria para dar luz y calor a las fábricas y para hacer funcionar la maquinaria.

Los costes de capital para el mantenimiento de una red energética de combustibles fósiles eran más favorables para las grandes fábricas que para los pequeños talleres. Las grandes fábricas, a su vez, requerían unos mecanismos centralizados de dirección y control para coordinar su actividad.

La moderna burocracia empresarial nació como resultado de la era de los combustibles fósiles. Llegó a su madurez en los años veinte, con el paso del carbón al petróleo y del vapor a la electricidad en las fábricas. Aunque habían existido diversas clases de burocracias en las civilizaciones del pasado, la nueva burocracia vinculada a los negocios era en muchos sentidos única. El gran sociólogo del siglo XX, Max Weber, intentó definir sus rasgos. Algunas de sus características esenciales son: las reglas predeterminadas para gobernar la toma de decisiones, el ejercicio vertical de la autoridad, la definición precisa de las tareas en todos los niveles de la organización, la aplicación de criterios objetivos para evaluar el rendimiento y el progreso, y la división del trabajo en tareas y funciones especializadas. Según Weber, este tipo de proceso racionalizado de dirección hacía posible controlar organizaciones grandes y complejas, integrar múltiples actividades bajo un mismo techo e incrementar constantemente el ritmo y la velocidad de producción.

Durante los años de transición hacia la plena madurez del capitalismo industrial aparecieron otros muchos sistemas racionalizados. Los ferrocarriles introdujeron por primera vez las zonas horarias estandarizadas, por ejemplo para regular mejor el tráfico. En 1870, un pasajero de tren que viajara de Washington a San Francisco habría tenido que cambiar la hora de su reloj más de doscientas veces para ajustarse a todos los sistemas horarios locales del país. La gran disparidad de zonas horarias locales creaba muchos problemas a los ferrocarriles cuando querían programar los horarios de los trenes de pasajeros y de los transportes de mercancías. En 1884 se establecieron zonas horarias estándar para todo el mundo y se escogió la localidad inglesa de Greenwich para situar allí la longitud cero.

Pronto se pusieron en marcha otros procesos racionalizados para apoyar las nuevas estructuras burocráticas y contribuir a hacer más rápida la ya acelerada actividad económica, como por ejemplo la clasificación estándar de los productos, el empaquetado mecanizado estándar y el sistema estandarizado de etiquetado en el mercado minorista. Las primeras factorías automatizadas producían cigarrillos, cerillas, sopa y harina en unidades precisas y de composición invariable. Nuevas herramientas de marketing, como por ejemplo la venta por catálogo, la creación de marcas para los productos, y fórmulas innovadoras como las franquicias —introducidas por primera vez por Internacional Harvester y Singer Sewing Machine Company y, más tarde, por la industria automovilística— convirtieron la vida comercial en un flujo acelerado de bienes racionalizados de calidad constante y predecible.

Frederick Winslow Taylor, el primer experto en gestión del nuevo siglo, introdujo sus principios de gestión científica en las fábricas y oficinas norteamericanas. Más tarde, sus teorías se introdujeron en el hogar, la escuela y prácticamente en cualquier otro ámbito de la vida contemporánea. Taylor se propuso racionalizar el propio comportamiento humano, amoldarlo a las nuevas formas racionalizadas de organización burocrática que estaban reelaborando la vida comercial americana.

El objetivo de Taylor era dotar de una mayor eficiencia a cada trabajador mediante la aplicación de principios empleados por los ingenieros para mejorar el rendimiento de la maquinaria. Taylor dividía las tareas de cada trabajador en las unidades operativas más pequeñas identificables y, con la ayuda de un cronómetro, determinaba cuál era el mejor tiempo que se podía conseguir bajo condiciones óptimas. Sus estudios sobre el rendimiento del trabajador llegaban a calcular los tiempos en fracciones de segundo. Mediante el estudio de los promedios y los mejores tiempos conseguidos en cada unidad de trabajo, Taylor podía hacer recomendaciones acerca de las modificaciones que se debían introducir en los detalles más pequeños de la actividad del trabajador, con objeto de ahorrar unos preciosos segundos.

Taylor creía que la mejor forma de optimizar la eficiencia de los trabajadores era ejercer un control absoluto sobre seis dimensiones temporales: la secuencia, la duración, el horario, el ritmo, la sincronización y la perspectiva temporal. Ningún aspecto del tiempo de los trabajadores podía abandonarse al azar o al arbitrio de éstos.

La dirección debía ejercer, en cambio, un control total sobre el trabajador. Éste debía ser privado del conocimiento y las habilidades necesarias para supervisar su propio trabajo y convertirse en un autómata, igual que las máquinas que atendía. Según Taylor, «si es el trabajador el que dirige su actividad de acuerdo con sus propias concepciones, es imposible [...] que el capital pueda imponerle ni la eficiencia metodológica ni el ritmo de trabajo deseados». El instrumento del que disponía la dirección para ejercer este control total sobre el trabajador era el programa de trabajo. Según escribió Taylor:

El trabajo de cada hombre está completamente planificado por la dirección cotilo mínimo con un día de antelación y cada hombre recibe, en la mayoría de los casos, unas instrucciones completas por escrito, en las que se describe con todo detalle la tarea que debe realizar, así como los medios que debe utilizar para hacer el trabajo [...] la tarea determina no sólo lo que tiene que hacer, sino cómo debe hacerlo y el tiempo exacto que tiene para hacerlo.

Taylor popularizó la idea de la eficiencia humana y convirtió a todas las personas de un país en eficientes máquinas humanas. Consiguió acelerar el ritmo de la actividad humana con el fin de adecuarlo al ritmo marcado por las tecnologías eléctricas y la maquinaria alimentada con carbón y petróleo de las fábricas, las oficinas y las empresas comerciales. A partir de este momento, la consigna tanto de la vida comercial como de la vida privada sería maximizar la producción en el menor tiempo posible y con la mínima inversión de trabajo y capital. Tal vez ningún otro individuo haya tenido jamás un impacto tan grande a la hora de racionalizar el comportamiento humano para adaptarlo a los dictados de la nueva cultura basada en la producción acelerada y el elevado gasto energético como el que tuvo Taylor. El sociólogo Daniel Bell habla de Taylor en los siguientes términos: «Si es posible atribuir alguna revolución social a un solo hombre, la lógica de la eficiencia como forma de vida es obra de Taylor».

La forma organizativa que desarrollaron inicialmente los ferrocarriles y que Frederick Taylor y una nueva clase de expertos en gestión institucionalizaron en el conjunto de la industria norteamericana apenas experimentó cambios durante los siguientes setenta años. La actividad comercial en Estados Unidos se convirtió en sinónimo de una gestión burocrática centralizada dentro de una actividad económica integrada, dado que esta estructura institucional era la más adecuada para gestionar una producción cada vez más acelerada gracias a la adopción del carbón y más adelante del petróleo y el gas natural.

Tanto en Estados Unidos como en el resto del mundo la actividad comercial continuó haciéndose cada vez más jerárquica y centralizada en proporción directa al espectacular aumento del flujo energético y la productividad económica que traía consigo cada nueva década del siglo XX. En los años sesenta, las doscientas empresas más importantes poseían el 56,3 % de los activos industriales del país.

El nacimiento de la sociedad de la información en los años ochenta cambió la forma de trabajar de las empresas. El modelo organizativo jerárquico tradicional resultó ser demasiado lento para adecuarse al ritmo acelerado y a la densidad de la actividad comunicativa y comercial que habían generado las revoluciones de la informática y de las telecomunicaciones. Los ordenadores personales, los ordenadores portátiles, los teléfonos móviles y la World Wide Web generaron una red móvil de conexiones horizontales instantáneas entre las personas. Mover la información dentro de la empresa a la manera tradicional, primero hacia arriba a través de los sucesivos niveles hasta alcanzar la cima, y luego esperar a que las decisiones bajaran lentamente de nuevo de nivel en nivel hasta alcanzar la base, resultaba cada vez más caro en un mundo en el que la información se movía en todas las direcciones a la velocidad de la luz.

Las empresas comenzaron a nivelar sus jerarquías organizativas y a crear modelos organizativos horizontales que se acomodaban mejor a la movilidad, la flexibilidad y la velocidad de la nueva era comercial. Se recortaron o eliminaron capas enteras de directivos intermedios. El nuevo modelo organizativo descentralizaba la toma de decisiones y delegaba más autoridad sobre el personal que tenía un trato directo con los proveedores y los clientes.

Si, por un lado, las jerarquías dejaron paso a las redes y buena parte de las decisiones operativas quedaron descentralizadas para reducir los costes de transacción y mejorar los márgenes de beneficio, por otro lado las compañías se hicieron todavía más grandes e inclusivas desde el punto de vista comercial para controlar una red de relaciones y actividades económicas cada vez más interconectadas. Las fusiones y las adquisiciones se convirtieron en la norma en todas las ramas de la industria. Cada vez había menos participantes en el juego y éstos concentraban y centralizaban más poder en sus campos respectivos, entre ellos la banca, las empresas de seguros, las telecomunicaciones, los servicios, el entretenimiento, los productos farmacéuticos, la agricultura, la automoción, el acero y otras muchas industrias, incluida la energética.

Las fusiones y las adquisiciones han alcanzado nuevas cotas en todo el mundo durante los últimos años. En 1999, el valor global de todas las fusiones y adquisiciones era de 3,4 billones de dólares, una cifra récord que representaba un aumento del 40% respecto de los 2,5 billones del año anterior. A lo largo de los últimos veinte años las fusiones anuales se han multiplicado por cien, hasta alcanzar la escandalosa cifra acumulada de 15 billones de dólares. En 1999 se anunciaron más de 32.000 fusiones y adquisiciones independientes, el triple de las que se habían producido diez años antes. En la actualidad, las fusiones internacionales representan el 33% del total, un 13% más que a comienzos de los años ochenta, un claro signo del acelerado proceso de globalización que experimenta el comercio.

Las fusiones que se practican actualmente alcanzan cifras que cortan la respiración. La fusión de America Online y Time Warner en 2000 fue valorada en 165.000 millones de dólares. Un tercio del valor total de las fusiones realizadas en 1999 a nivel mundial se concentró en tres industrias: las telecomunicaciones, la banca comercial y la radiotelevisión.

La concentración corporativa de poder sobre el comercio mundial aumenta constantemente. Cada año que pasa disminuye el número de jugadores que dominan la economía internacional. De las 100 principales economías que existen en el mundo actualmente, 51 son corporaciones y sólo 49 corresponden a países (datos basados en la comparación del volumen de ventas de las distintas corporaciones y el PIB de los países). Las ventas combinadas de las doscientas principales compañías superan la suma de las economías de todas las naciones del mundo, excluyendo las diez más ricas. Las ventas de las cinco principales corporaciones superaron en 1999 el PIB de 182 países. Frente al mito popular de que las posiciones dominantes en el mercado cambian constantemente de manos, a medida que las viejas compañías van siendo arrinconadas y sustituidas por otras de nueva creación, el hecho es que la mitad de las organizaciones que figuraban entre las doscientas compañías más importantes en 1983 seguían estando en la lista en 1999, aunque a veces con el nombre cambiado como consecuencia de las fusiones.

A la entrada del siglo XXI un buen número de industrias básicas estaban controladas por menos de diez compañías globales. La consolidación y la centralización del poder económico ha ido siempre de la mano del aumento del flujo energético y es probable que siga adelante hasta que la producción global de petróleo toque techo en algún momento de la próxima década.

La era del petróleo se ha caracterizado desde el primer momento por las economías de escala. Las grandes compañías petroleras y las grandes corporaciones han ido siempre asociadas. En la actualidad, el control de la energía global y del conjunto de las actividades comerciales derivadas de ella se halla concentrado en aproximadamente quinientas compañías transnacionales, muchas de las cuales se hallan integradas dentro de cada sector de actividad en una densa red de relaciones de interdependencia. La globalización ya no es un objetivo del futuro, sino una realidad emergente. En ningún otro momento de la historia ha habido tantos seres humanos en el mundo cuya supervivencia y bienestar dependiera de un número tan reducido de instituciones. Estas instituciones, a su vez, existen gracias al flujo continuado de combustibles fósiles —en especial el petróleo—, que mueve todos los aspectos de la vida comercial moderna.

Si la medida de una civilización reside, al menos en parte, tal como sugieren Leslie White y otros antropólogos e historiadores, en la cantidad de energía que fluye a través de ella, entonces la civilización de los combustibles fósiles obtiene una alta calificación, en la medida en que ha consumido más energía y ha producido el nivel de vida más elevado de la historia para aquellos que se han beneficiado de ella. Pero si tenemos en cuenta que el incremento en la producción energética ha sido sinónimo de una mayor concentración de poder institucional sobre el flujo energético y sobre la actividad comercial que lo acompaña, resulta incuestionable también que la era de los combustibles fósiles ha creado las instituciones de dirección y de control más centralizadas y jerarquizadas de la historia para administrar su régimen energético. El siglo XX se ha caracterizado verdaderamente por la creación de un nuevo tipo de imperio, en este caso basado en el petróleo y gestionado por empresas gigantescas que trabajan en colaboración —y a veces en abierta competencia— con los gobiernos nacionales.

El problema es que una mayor concentración y centralización de poder en un menor número de instituciones ha significado siempre menos flexibilidad a la hora de enfrentarse a nuevos retos y una mayor vulnerabilidad ante las interferencias procedentes del interior o del exterior. Una mayor producción de energía también ha provocado siempre un incremento de la entropía en el conjunto del entorno. La era del petróleo parece anunciar cuando menos tantos costes en el futuro como beneficios ha reportado en el pasado. A medida que ascendíamos en la curva de la producción global de petróleo, es comprensible que nuestra atención se centrara en la maximización de las ganancias. Ahora, a medida que nos acercamos a la cima y al largo descenso por el otro lado de la cuesta, debemos dedicar la misma atención a minimizar las pérdidas y a prepararnos para un nuevo régimen energético. Lo primero que debemos hacer es comprender la enormidad del reto que se presenta ante nosotros.

El descenso esperado en la producción global de petróleo crudo convencional se recorta sobre el fondo de otras dos fuerzas potencialmente desestabilizadoras: el auge del fundamentalísimo islámico en Oriente Medio y el resto del mundo, y el creciente calentamiento del clima de la Tierra provocado por la quema de combustibles fósiles. Los efectos sinérgicos de cada uno de estos tres fenómenos sobre los demás serán cruciales para determinar las perspectivas de la civilización humana en el próximo siglo.

Capítulo 5

EL CABO SUELTO ISLAMISTA

«Un privilegio de Alá.» Así es como ven muchas personas del mundo islámico las inmensas riquezas petrolíferas enterradas justo bajo sus pies. A los islamistas radicales les gusta hacer notar que allí donde se congrega un número importante de musulmanes, la tierra sobre la que caminan rebosa oro negro. Es como si Alá hubiera ungido el suelo que pisan sus mensajeros en la Tierra y lo hubiera convertido en un lugar sagrado, lleno de significado religioso e histórico. Sea fruto del azar o del designio divino, la coincidencia es bastante notable. Diez de las trece naciones que integran la OPEP son Estados musulmanes: Arabia Saudí, Emiratos Árabes Unidos, Qatar, Irán, Irak, Kuwait, Argelia, Libia, Indonesia y Nigeria (la mitad de cuya población es musulmana). Otros destacados productores de petróleo, como es el caso de Omán, Bahrein, Siria, Egipto, Brunei, Túnez y Malasia, también son musulmanes.

La referencia central tanto para el petróleo como para el Islam es Arabia Saudí, el principal productor de petróleo del mundo y la tierra sagrada del Islam, lugar de nacimiento del profeta Mahoma y guardián de los lugares santos de la Meca y la Medina. Por más que sonrían los escépticos analistas geopolíticos occidentales ante la idea de que Alá haya podido conceder un regalo como éste a sus creyentes, nadie se atreve a reír cuando Osama bin Laden exhorta a sus seguidores de todo el mundo para que reclamen la tierra sagrada saudí, establezcan un estado islámico universal y eleven el precio del petróleo hasta los 144 dólares por barril.

La historia del petróleo da credibilidad a la idea de que en la historia «todo lo que sube baja». El petróleo, la energía que contribuyó a hacer de Occidente una fuerza política, económica y cultural sin rival en el mundo del siglo XX, podría convertirse ahora en su talón de Aquiles en manos de un mundo islámico determinado a dar la vuelta a la situación y recuperar su estatus anterior como árbitro cultural y espiritual del mundo. De algo al menos podemos estar seguros: el petróleo y el islam están inseparablemente unidos. El destino de uno determinará en buena medida el destino del otro en el próximo siglo.

En la actualidad hay 1.200 millones de musulmanes en todo el mundo, lo que representa el 20% de la humanidad. Son mayoría en 52 países y una importante minoría en muchos otros. Los musulmanes son el grupo humano que crece más rápidamente en el mundo. Los expertos en demografía prevén que en 2025 una de cada cuatro personas del planeta será musulmana. Si la demografía es el poder, entonces la próxima centuria parece tener signo musulmán.

A pesar del aumento en sus cifras de población, parece que su fortuna económica está en declive. Según el Banco Mundial, la renta media de lo que se conoce como el cinturón islámico, que se extiende desde Marruecos hasta Bangladesh, está por debajo de los 3.700 dólares anuales, la mitad de la media mundial, situada en 7.350 dólares. Pero no es sólo que los países del cinturón islámico sean pobres, lo más grave es que pierden terreno constantemente frente a otros países en vías de desarrollo. Por ejemplo, en 1950, Egipto y Corea del Sur disfrutaban aproximadamente del mismo nivel de vida. En la actualidad, el nivel de vida de Corea es cinco veces superior al de Egipto.

La generación más joven de musulmanes comienza a ver el petróleo como el «gran igualador», un arma a la vez espiritual y geopolítica que si se emplea al servicio de Alá puede conducir a un resurgimiento del islam. El rey Fahd de Arabia Saudí pensaba de este modo en la época posterior a la crisis del petróleo de los años setenta y principios de los ochenta, cuando dijo a los musulmanes que «después de Alá, el principal recurso en el que podemos confiar es el petróleo».

El gran resurgimiento del islam entre los jóvenes y la correlativa islamización y politización del petróleo constituyen el último capítulo de una larga historia de 1.500 años de enfrentamiento con el cristianismo y Occidente, en la que el mundo islámico ha sido —según las épocas— vencedor y vencido, gobernante y gobernado. Para muchos musulmanes, que han experimentado un sentimiento de derrota y humillación bajo el dominio de los poderes occidentales durante la mayor parte del siglo XX, la perspectiva de controlar el acceso a las últimas reservas de petróleo crudo del mundo representa una oportunidad para ajustar cuentas. Para los poderes occidentales, las compañías energéticas, la comunidad económica global y los consumidores, la sola idea de que Arabia Saudí y los demás países productores de petróleo del golfo Pérsico pudieran caer en manos de una joven generación de fundamentalistas decididos a imponer al mundo un Estado islámico universal —la versión musulmana de la globalización— resulta aterradora.

El proyecto de Mahoma

El actual resurgimiento fundamentalista forma parte de un complicado proceso histórico que muchos de nosotros desconocemos en Occidente. Lo que debemos entender es por qué hay tantos musulmanes jóvenes en Oriente Medio y otras partes del mundo, tanto ricos como pobres, educados o analfabetos, que encuentran un punto de acuerdo en lo que consideran un renacimiento espiritual y en lo que tendemos a contemplar como una amenazadora polarización del mundo. Comprender la historia del islam desde una perspectiva musulmana nos dará algunas claves acerca de lo que nos depara el futuro en la próxima década, cuando tanto la producción global de petróleo como la revolución islámica juvenil alcancen su punto álgido.

Lo primero que hay que saber acerca del islam es que si bien coincide con el cristianismo en el hecho de hundir sus raíces en el judaismo y pertenecer a la gran tradición monoteísta que tuvo su origen en el profeta Abraham, difiere de éste en un aspecto fundamental: su interpretación del papel de la religión en la historia terrenal. Para los cristianos, el viaje terrenal tiene mucha menos importancia que la salvación eterna en el mundo por venir. Desde el principio, el cristianismo ha sido una religión centrada en el otro mundo. San Agustín y otros líderes religiosos dejaron claro muy pronto que este mundo tiene mucho menos valor que el otro y que el tiempo que permanecemos aquí debemos dedicarlo a difundir la buena nueva de la venida de Cristo y a prepararnos para el mundo que está por venir. Si la misión de los cristianos era prestar testimonio de la venida del reino de Dios y ser sus servidores aquí en la Tierra, los asuntos cotidianos de la historia debían dejarse en manos de los poderes y los príncipes. «Dadle al César lo que es del César...» se convirtió en el lema de la fe. Si los seres humanos y el resto de la creación estaban realmente caídos —si se hallaban inmersos en el pecado original por la pérdida de la Gracia en el Jardín del Edén—, el perdón sólo podía esperarse en el otro mundo.

El islam, en cambio, nació con otra vocación. Su fundador, el profeta Mahoma, reconocía la presencia de Dios en la historia humana y creía que ésta era el escenario principal donde los seres humanos interpretaban su relación con Dios. Una idea sin duda muy distinta de la que empujó a los primeros creyentes cristianos a apartarse de los asuntos mundanos y crear órdenes monásticas en la época medieval.

El profeta Mahoma nació en el año 570 d. C. y creció en el seno de la entonces próspera tribu de los Quraysh, instalada en la ciudad de la Meca. Hacia la mitad de su vida comenzaron a angustiarle los efectos que la recién obtenida opulencia material estaba teniendo sobre su tribu, en especial sobre sus líderes. Muchos miembros de la tribu estaban obsesionados por enriquecerse, a menudo a expensas de sus vecinos. También eran menos caritativos y estaban menos dispuestos a compartir su buena fortuna con los miembros más débiles de la tribu, lo que creaba nuevas divisiones que amenazaban con minar y destruir el tejido social que durante tanto tiempo había mantenido unida a la tribu. Dios respondió a su angustia y descendió sobre él la noche del decimoséptimo Ramadán, en el año 610 d. C. Mahoma despertó de su sueño con la sensación de estar envuelto por una gran presencia. De su boca fluyeron las primeras palabras de las nuevas escrituras árabes. En los años siguientes Mahoma experimentaría repetidas veces la presencia de Dios y cada vez venía acompañada de nuevos versos coránicos. Pronto comenzó a compartir estas palabras con otras personas, primero con sus familiares más cercanos, luego con sus amigos y, finalmente, con extraños venidos de todas partes.

El Corán (recitación) era ante todo una guía práctica para la vida terrenal. Exhortaba a los creyentes a crear una sociedad justa y humana, donde los débiles recibieran cuidados y los pobres asistencia, y en la que cada persona viviera de acuerdo con un principio de amor y respeto hacia los demás seres humanos. Aunque el judaismo, el cristianismo y las otras grandes religiones monoteístas también predicaban la justicia económica y social y ensalzaban las virtudes de la práctica de la Regla de Oro, lo que daba al islam un carácter único era su énfasis en la necesidad de trasladar las propias creencias al día a día del escenario político y social. La misión de los defensores de la fe era redimir la historia mediante la creación de una sociedad que fuera el reflejo de su fe. De acuerdo con este nuevo orden de cosas, los reinos espiritual y temporal debían experimentarse como una misma realidad. Vivir una vida espiritual significaba vivir una vida justa en una sociedad caritativa. A la inversa, vivir de forma justa y caritativa en la sociedad era la forma de alcanzar la espiritualidad.

El mensaje de Mahoma encajó perfectamente con el talante de su época. En el mundo de la Meca y la Medina, donde la nueva opulencia material convertía a unos en ricos y a otros en pobres y donde las obligaciones y responsabilidades tradicionales hacia los familiares y los vecinos se veían amenazadas por la codicia y los bienes materiales, los poéticos versos del Corán, con su elocuente llamada a vivir una vida más justa, llegaron al oído tanto de los miembros ricos y alienados de la sociedad como de los débiles y los pobres.

El proyecto de Mahoma era crear una Ummah (comunidad) universal de los creyentes, unida por el compromiso colectivo de crear una sociedad justa y equitativa. Con este fin, el Corán ofrecía una serie de instrucciones muy específicas acerca de cómo debían comportarse los creyentes. Por ejemplo, todos los musulmanes debían dar parte de sus ingresos a los pobres. Durante los días sagrados del Ramadán, los creyentes debían ayunar para sentir el dolor y el sufrimiento de aquellos que no tienen bastante comida y bebida para llenar sus estómagos.

Si la ummah triunfaba, si la comunidad musulmana crecía y prosperaba, ello significaría que los creyentes estaban viviendo sus vidas de acuerdo con la voluntad de Dios. Si, en cambio, la ummah estaba plagada de divisiones y animosidades y atormentada por el sufrimiento, eso sólo podía significar que la gente no obedecía la voluntad divina.

Menos de un siglo después de la muerte de Mahoma, el islam se había extendido desde los Pirineos hasta el Himalaya, había unido pueblos diversos en una nueva hermandad universal y había creado un nuevo e inmenso imperio cuya influencia se dejaría sentir durante los siguientes 1.300 años. Las conquistas reforzaban la idea de que se estaba cumpliendo la voluntad de Dios. El éxito militar y económico facilitó que los creyentes se sintieran parte de una experiencia trascendente.

El islam es, pues, esencialmente distinto del cristianismo. La diferencia es que no separa la ciudad del hombre de la ciudad de Dios. El verdadero musulmán vive en un mundo sin fisuras. Su vida diaria está dedicada a construir la ummah —una hermandad universal— que reflejará la voluntad divina, lo que supone en sí mismo un signo de trascendencia.

Al separar la existencia terrenal y la eterna en dos reinos independientes, el mundo cristiano creó las condiciones necesarias tanto para la aparición del Estado independiente secular como para la privatización de la fe. El islam no hace tal separación. La política y la teología están íntimamente conectadas. Llevar la vida correcta desde el punto de vista político significa llevar la vida correcta desde el punto de vista espiritual. En este sentido, el proyecto del islam es verdaderamente universal. Pero como la vida es al mismo tiempo política y espiritual y, por tanto, indivisible, no puede haber realmente una separación de la Iglesia y el Estado, o de la razón y la fe, el tipo de división que contribuyó a crear la mentalidad occidental moderna.

Durante los primeros años de su historia, el islam triunfó gracias a este proyecto unitario. En el momento de su apogeo, el imperio islámico rivalizaba con la antigua Roma. Mientras la Europa Occidental se internaba en las tinieblas de la Edad Media y llegaba al siglo VIII en un estado casi agonizante, el islam se convertía en el centro indiscutible del saber y la cultura. Donde se hacía más evidente la preeminencia del islam era en los campos de la filosofía y la ciencia. Los musulmanes redescubrieron la filosofía, la matemática y la astronomía de los antiguos griegos y crearon los primeros prototipos de lo que más tarde se convertiría en la universidad moderna. Durante los siguientes quinientos años los sabios musulmanes se dedicaron a la investigación científica. Las bibliotecas que tenían en Córdoba y Toledo, en España, eran la envidia del mundo.

Así pues, la pregunta que debemos hacernos es la siguiente: ¿por qué fue Occidente y no el islam el que consiguió finalmente aplicar la ciencia al dominio de la naturaleza y colonizar buena parte del mundo en la era moderna?

El motivo reside en la naturaleza indivisible del proyecto islámico. Para los musulmanes, la buena ciencia, al igual que la buena política, debe reflejar la gloria de la presencia divina y contribuir a que los creyentes lleven una vida más justa y espiritual. La filosofía griega tenía sentido en la medida en que preguntaba constantemente el porqué de las cosas para comprender el mundo tal como ha sido creado. Farouk el-Baz, geólogo de la Universidad de Boston y anteriormente asesor científico del presidente egipcio Anwar al-Sadat, explica la concepción islámica de la ciencia del siguiente modo: «Cuanto más sabes, mayor es la evidencia de la presencia de Dios». Los musulmanes ven la ciencia como una vía hacia una mejor comprensión de la unidad de la existencia, que es obra de Dios. Eso no significa que la ciencia no deba usarse también para fines prácticos. El gran interés que desplegaron por la astronomía, por ejemplo, se debía en parte a una necesidad de carácter práctico: asegurar que todos los creyentes pudieran saber en qué dirección debían girarse cada día para estar orientados hacia la Meca al hacer sus plegarias. Los sabios musulmanes crearon complejos diagramas y tablas con ayuda de las cuales los musulmanes de todo el mundo podían encontrar las direcciones sagradas. Aunque a veces pudiera ser práctica, la ciencia era y sigue siendo vista como una ventana hacia lo divino. Para los sabios musulmanes, la ciencia nunca debía estar al servicio del hombre, sino al servicio de Dios.

La concepción europea de la ciencia

Occidente, en cambio, experimentó un lento y prolongado conflicto que llevó finalmente a la liberación de la ciencia de cualquier consideración ultramundana y desató con ello el espíritu prometeico que se lanzaría a reconstruir el mundo sobre la base del materialismo occidental. Max Weber se refiere a este conflicto épico como el «desencantamiento del mundo». Las raíces de este desencantamiento deben buscarse de nuevo en las primeras etapas del cristianismo y las encontramos en su clara distinción entre dos reinos de la existencia: el terrenal y el eterno. La experiencia del mundo que tenían los cristianos era muy distinta de la que tenían los musulmanes. Si éstos vivían su fe en el mundo temporal, los cristianos se encontraban en un mundo al que no pertenecían. La existencia terrenal debía ser tolerada, pero no se debía encontrar satisfacción en ella. No tenía ningún sentido trascendente. Hay que recordar también que los primeros cristianos esperaban el retorno de Cristo de forma inminente y no estaban, por lo tanto, muy preocupados por su viaje terrenal. Al no producirse la segunda venida de Cristo, los padres de la Iglesia comenzaron a institucionalizar su dominio sobre los asuntos terrenales mediante la formación de alianzas con «los ricos y los poderosos» de cada lugar para asegurarse un flujo constante de ingresos en las arcas de la Iglesia. Pero no se daba demasiada importancia a las artes prácticas, que eran cultivadas sobre todo por las órdenes monásticas y, por tanto, se las dejó bastante en paz.

En el siglo XIII, sin embargo, la Europa cristiana comenzaba a despertar lentamente de su largo sueño. Los nuevos métodos agrícolas habían incrementado la producción alimentaria y generaban los primeros excedentes desde la caída del Imperio Romano. El comercio comenzaba a tomar impulso, se abrían nuevas rutas mercantiles, los villorrios se convertían en pueblos y después en pequeñas ciudades, y la producción artesanal crecía hasta formar industrias incipientes. Todo este desarrollo provocó una especie de crisis interna en la Iglesia, que tradicionalmente había tolerado las artes prácticas en la medida en que quedaran relegadas a un papel secundario. Los nuevos intereses terrenales, sin embargo, ponían en peligro la fe. El gran pensador de la Iglesia santo Tomás de Aquino trató de prevenir en su Summa Theologica del cisma que estaba a punto de producirse en el siglo XIII entre el mundo temporal y el espiritual. Santo Tomás pensaba que Dios había entregado al hombre dos facultades superiores: la razón y la fe. La razón permitía al hombre explorar el mundo material para comprender mejor la creación divina, motivo por el cual debía ser fomentada. Pero debía quedar claro que era de naturaleza inferior a la fe. Lo que la razón no puede responder debe dejarse en el terreno de la fe. El compromiso de santo Tomás, lo que los estudiosos llaman «la delicada síntesis», estaba orientado a permitir el desarrollo de la razón en el terreno práctico mientras no tratara de usurpar el reino espiritual a la fe. Si la razón prevalecía sobre la fe, el hombre podía afirmar entonces su soberanía sobre Dios. Sin embargo, la defensa de santo Tomás no tuvo éxito.

Aunque su intención era purificar la fe de las corrupciones terrenales, la Reforma protestante del siglo XVI tuvo la consecuencia indeseada de agravar el cisma entre el reino material y el espiritual. Martín Lutero y Juan Calvino predicaban un nuevo catecismo de las vocaciones terrenales. Ambos sostenían que por más que la doctrina de la Iglesia insistiera en las «buenas obras», éstas no podían asegurar un lugar en el cielo a ningún creyente. Según los reformadores, todo individuo ha sido elegido para la salvación eterna o condenado al infierno desde el nacimiento. A pesar de ello, según Calvino, uno tiene la responsabilidad de actuar como si hubiera sido elegido a través del cultivo de una «vocación» y de un esfuerzo incansable por mejorar la propia fortuna, como forma de superar las constantes dudas y convencerse a uno mismo de que está entre los bienaventurados. A partir de este momento los creyentes organizaron sus vidas de la forma más metódica posible con objeto de alcanzar el éxito, no por amor a la ganancia personal, sino como «signo» de pertenencia al grupo de los elegidos. El nuevo énfasis en los triunfos conseguidos en este mundo dio lugar a una ética más materialista. Max Weber se refería a este nuevo estilo de vida como la «ética protestante» y sostenía que de forma indirecta había generado una mentalidad a la medida de la actividad industrial y el comportamiento capitalista moderno.

Durante el siglo XVII, los dos reinos —el terrenal y el eterno— libraron una guerra abierta en varios frentes. La nueva generación de filósofos se amparaba en la razón para cuestionar algunas de las más sagradas doctrinas de la Iglesia. Francis Bacon escribió en 1620 su Novum Organum, en el que exaltaba las virtudes de lo que llamaba el «método científico», una forma de perseguir el conocimiento que desvelaría los secretos de la naturaleza y convertiría al hombre en el dueño del universo. Bacon se mostraba particularmente crítico con la ciencia griega, que tanto reverenciaban los sabios musulmanes. Según Bacon, el énfasis de la tradición socrática en el porqué de las cosas era completamente estéril a la hora de mejorar el bienestar material de la humanidad. Bacon despotricaba contra los griegos diciendo que no habían «aportado ningún experimento que tendiera a aliviar y beneficiar la condición humana». Los griegos, escribió ácidamente Bacon, «poseían el rasgo característico de los niños; siempre estaban dispuestos a parlotear, pero no eran capaces de producir nada; la prueba es que su sabiduría abunda en palabras, pero anda escasa en hechos». Bacon estaba mucho más interesado en los beneficios prácticos que podía generar la ciencia. No veía en la naturaleza una ventana a lo sagrado, sino más bien una «ramera» colectiva y llamaba a las generaciones futuras a «sacudirla hasta los cimientos», «exprimirla», «moldearla» y «darle forma» con el objetivo de «expandir los límites del imperio humano hasta hacer realidad todas las posibilidades». Las ideas de Bacon sobre la naturaleza y el papel de la ciencia eran una herejía para los pensadores musulmanes. La suya era una filosofía puramente utilitaria pensada para dominar la creación y optimizar las ganancias materiales de la sociedad.

La Europa del siglo XVII, sin embargo, estaba más que dispuesta a escuchar una herejía como ésta. Las economías mercantiles emergentes, con sus lejanas aventuras coloniales, ya se habían entregado a la tarea de domesticar el mundo natural y expandir los límites del imperio humano. Las ideas de Bacon sobre la ciencia y la naturaleza estaban en sintonía con el espíritu de los tiempos.

La Iglesia se encontraba entre dos aguas. A pesar de que tenía interés en cristianizar a los salvajes del Nuevo Mundo, los líderes religiosos no se sentían cómodos con las fanfarronadas humanistas que acompañaban la aventura colonial. El Vaticano trató de contener el nuevo materialismo racional que comenzaba a infiltrarse en el pensamiento europeo y a influir en las ideas acerca de la ciencia y la naturaleza. Pero sus esfuerzos eran poco decididos. La Iglesia intentó silenciar al astrónomo italiano Galileo cuando éste afirmó que la Tierra se movía alrededor del Sol y lo sentenció a cadena perpetua por herejía, aunque rápidamente conmutó esa sentencia por la de un arresto domiciliario permanente. Por aquel entonces el progreso material en Occidente estaba demasiado adelantado como para que la doctrina de la Iglesia pudiera frenarlo. La fe dejó paso a la razón y sólo un siglo más tarde los filósofos ya proclamaban la llegada de un nuevo reino material en la Tierra. El marqués de Condorcet, un destacado representante de la filosofía francesa, advirtió al comienzo de la Revolución francesa:

No se ha fijado ningún límite al desarrollo de las facultades humanas [...] la perfectibilidad del hombre es infinita [...] el progreso de esta perfectibilidad se halla a partir de ahora por encima del control de cualquier poder que pudiera obstruirlo, y no tiene otro límite que la duración del globo sobre el que la naturaleza nos ha emplazado.

Occidente había conseguido separar la razón de la fe y había entregado a la primera el dominio de los asuntos de la sociedad, mientras que la segunda quedaba relegada a la vida privada.

La Iglesia podía seguir predicando la fe, pero ya no volvería a ser la estructura suprema alrededor de la cual se organizaba la sociedad. Su lugar sería ocupado por un nuevo modelo institucional, el Estado-nación, un instrumento secular diseñado para difundir la «razón» en forma de ciencia moderna, tecnología y comercio por los cuatro confines de la Tierra. La nueva trinidad gobernaría el mundo a lo largo de los dos siglos siguientes y allí donde alcanzaban sus tentáculos el islam se veía obligado a retirarse, someterse y en último término humillarse ante Occidente.

Hacia finales del siglo XIX, muchos musulmanes comenzaban a preguntarse cuál era el secreto de los modelos educativos, las tecnologías y las instituciones políticas y seculares de Occidente que les permitía ejercer una influencia tan poderosa sobre el mundo. La ruptura final del Imperio Otomano tras la Primera Guerra Mundial convenció a muchos musulmanes de que la única forma de frenar la retirada y recuperar la posición de dominio del islam era aprender el modo de reproducir las cosas que había hecho poderoso a Occidente y adaptarlo a sus propias circunstancias.

Las influencias occidentales

A comienzos del siglo XX emergieron dos grupos: los occidentalizadores y los modernistas. Los primeros pretendían tomar prestado el modelo occidental, en bloque, e implantarlo en el mundo islámico, un injerto cultural que pondría en peligro la ortodoxia del islam en su misma esencia. Algunos intelectuales y teólogos musulmanes, sobre todo en Egipto e India, crearon una especie de versión islámica de la Ilustración europea. Pretendían revisar el islam a la luz del modelo occidental por medio de profundas reformas sociales y estructurales, como la separación entre Estado e Iglesia, el establecimiento de una sociedad civil, la emancipación de la ciencia de la supervisión religiosa, la creación de un poder judicial ajeno a la influencia de los líderes religiosos y el establecimiento de una política democrática. Los modernistas eran más prudentes. Esperaban poder adoptar selectivamente algunos elementos de los modelos políticos, educativos y legales de Occidente y adaptarlos al pensamiento islámico sin perder la esencia del islam. Los occidentalizadores ganaron la partida, en buena medida porque la influencia colonial de Occidente era ya dominante en el mundo musulmán. Entre el final de la Primera Guerra Mundial y los años sesenta, las élites musulmanas de Oriente Medio y otros lugares del mundo recibían su educación en Europa y Norteamérica y adoptaban modos de vestir y estilos de vida occidentales. Oriente Medio se europeizó y luego se americanizó. Se introdujo el modelo del Estado-nación en toda la región y la nueva generación de líderes políticos inició el proceso de crear una cultura secular.

La historia de la occidentalización de Oriente Medio ha sido la historia de un constante fracaso. Simplemente, el modelo era demasiado extraño al concepto islámico de la ummah para tener éxito. Conseguir que los musulmanes aceptaran el Estado-nación —un modelo de gobierno secular, basado en la creación arbitraria de barreras políticas— era una tarea imposible, si se tiene en cuenta que la lealtad más profunda de los musulmanes se había dirigido durante siglos hacia una hermandad universal sin fronteras y basada en la fe religiosa. Eso no significa que los musulmanes no fueran tribales y territoriales. Sin embargo, su sentimiento de pertenencia en un sentido más amplio siempre se ha dirigido hacia la ummah. El Estado-nación ha sido y sigue siendo visto por muchas personas en Oriente Medio y otras partes del mundo como una institución colonial diseñada para dividir y conquistar a los musulmanes.

Dicha percepción tiene su origen en la historia de la región. Las potencias europeas, especialmente los británicos, mantuvieron su presencia colonial en Oriente Medio durante más de cien años. A lo largo de la era colonial se establecieron una serie de Estados-nación, a menudo con la ayuda de caudillos locales, que reflejaban los intereses de las potencias europeas y más tarde de Estados Unidos. Estos pretendidos Estados-nación estaban diseñados para consolidar rutas comerciales, usurpar recursos vitales y proteger intereses militares estratégicos antes que para reunir a pueblos de mentalidades parecidas en un objetivo político común. A menudo, las fronteras políticas eran impuestas por la fuerza a diversas tribus locales, lo cual generaba disensiones y animosidades que debían ser atajadas periódicamente con intervenciones de la policía y el ejército.

Después de la Segunda Guerra Mundial se impuso durante un tiempo un nacionalismo autóctono: el panarabismo. Una nueva generación de líderes árabes, guiados por el egipcio Gamal Abdel Nasser e inspirados por los procesos de independencia que tuvieron lugar en India, África y Asia a finales de los años cuarenta y principios de los cincuenta, hicieron un llamamiento en favor de una concepción alternativa de la nacionalidad. Distanciándose del modelo colonial de nacionalismo que se había impuesto en Egipto en la primera mitad del siglo XX, Nasser consolidó su poder en los años cincuenta e introdujo un régimen secular de estructura moderna y tendencia socialista. Nasser soñaba con unir el mundo árabe en una única nación que hablara la misma lengua, de forma muy parecida a como ochenta años antes los nacionalistas habían convertido las regiones germanohablantes e italianohablantes en naciones unificadas.

El proyecto panarabista de Nasser tuvo un éxito arrasador en Oriente Medio. Después de un siglo de usurpación colonial, los líderes de la nueva generación buscaban desesperadamente un proyecto propio y alternativo. Sin embargo, el entusiasmo inicial no duró mucho tiempo. El nacionalismo panarabista pronto quedó teñido por un nuevo tipo de dependencia colonial, esta vez derivado de la agenda geopolítica de la antigua Unión Soviética. Sustituir el colonialismo occidental por el colonialismo soviético parecía un negocio más bien pobre. Por otro lado, el socialismo egipcio demostró ser incapaz de impulsar el desarrollo del país o de la región. A mediados de los años sesenta, buena parte de los planes comerciales de Nasser habían fracasado. Egipto se estaba deslizando inexorablemente cuesta abajo, junto a otros países de la región. El gran proyecto de llevar a cabo una versión panarabista del proceso de modernización y construcción nacional se desvanecía. El tiro de gracia para el panarabismo llegó en forma de una derrota humillante ante Israel en la guerra de 1967. Si todo el mundo árabe unido era incapaz de derrotar a una pequeña nación de sólo 3 millones de judíos, hablar de un resurgimiento panarabista parecía una necedad.

La versión de Nasser del panarabismo, vinculada al socialismo, tuvo un profundo efecto «en la calle». Nasser sabía que su sueño de un Estado panárabe dependía de su capacidad para reducir la influencia del islam en la vida cotidiana de la gente. El proyecto de secularizar la sociedad egipcia supuso inevitablemente un enfrentamiento directo de Nasser y su gobierno con los defensores de la fe islámica. Nasser practicó una represión particularmente dura contra la Hermandad Musulmana, el grupo islámico más visible del país. Más de un millar de miembros de la organización fueron recluidos en campos de concentración, a menudo durante muchos años. Otros regímenes de Oriente Medio fueron igualmente severos en su trato con los grupos islámicos, bajo la bandera de la modernización y la secularización. En Irán, el sha Muhammad Reza Pahlevi cerró madrasas (escuelas islámicas), torturó, encarceló y exilió a los ulemas (líderes religiosos), fusiló a los disidentes islámicos y llevó a cabo una campaña pública sistemática orientada a purgar el país de influencias islámicas, desde los códigos de vestir hasta los estilos de vida. Ésta y otras campañas parecidas en los países de Oriente Medio sirvieron para aumentar el resentimiento de los musulmanes hacia la idea de una sociedad secular.

El proceso modernizador dirigido por los nuevos Estados-nación tuvo otro resultado que traería consecuencias profundas y duraderas sobre la población, en especial sobre los jóvenes. En países como Egipto, Irán, Jordania y muchos otros la urgencia por modernizar las economías se tradujo en una despoblación de las áreas rurales y en una emigración masiva hacia las ciudades. Desplazados de su tierra natal y de sus comunidades locales y obligados a instalarse en áreas urbanas densas y superpobladas como El Cairo y Teherán, muchos musulmanes sentían que habían perdido sus raíces. La sociedad secular de la ciudad proporcionaba escasas o nulas compensaciones. Empobrecidos, aislados y despojados de su sentido de identidad personal, muchos jóvenes musulmanes terminaron por verse a sí mismos como extraños en un mundo hostil. El panarabismo, el socialismo, el secularismo, el modernismo: todos les habían fallado. Cada vez eran más los musulmanes que se giraban hacia su fe en busca de una respuesta y una guía frente a las desgracias que se acumulaban sobre sus espaldas. Muchos encontraron aquello que estaban buscando en un nuevo movimiento: la islamización.

La islamización

El padre espiritual del moderno fundamentalismo islámico es un egipcio: Sayyid Qutb. En su juventud, cuando era un miembro activo de la Hermandad Musulmana, Qutb tenía la esperanza de encontrar una forma de adaptar la democracia occidental al contexto islámico que no corrompiera la esencia espiritual de la fe con influencias seculares. La actitud de Qutb cambió después de que Nasser le encarcelara por sus actividades en la Hermandad. Tras ser testigo de las torturas y los asesinatos brutales de los que eran víctimas sus «hermanos» a manos de la policía egipcia, Qutb llegó a la conclusión de que el islam era incompatible con los principios de una sociedad secular.

Estando en prisión, Qutb escribió un libro titulado Signposts, en el que condenaba a Nasser y a los líderes de prácticamente todos los demás países árabes de la época por ser jahiliyya. Originalmente, el término se usaba para referirse a la Arabia preislámica, un período que los musulmanes veían como «la Edad de la Ignorancia». Jahiliyya significaba «los enemigos de la fe», las fuerzas bárbaras que se negaban a someterse a la voluntad de Dios. Acusar a los líderes árabes de ser jahiliyya era una bomba política.

Qutb estaba diciendo, efectivamente, que los líderes del mundo árabe estaban contaminados por las venenosas influencias de Occidente y que aplicaban políticas activas diseñadas para alejar a los creyentes del islam. En consecuencia, Qutb sostenía que los verdaderos musulmanes tenían la obligación de derrocar estos regímenes. Sus palabras equivalían a una declaración de rebeldía frente a aquellos que detentaban el poder en el mundo musulmán. Su mensaje comenzó a reverberar lentamente en Oriente Medio. Había nacido el fundamentalismo islámico.

Aunque existen diversas tendencias dentro del fundamentalismo islámico musulmán contemporáneo, hay una serie de ideas genéricas a las que se adhieren todas las sectas fundamentalistas. En primer lugar, el motivo de la decadencia del mundo musulmán es que el pueblo y sus gobernantes han abandonado las enseñanzas de Mahoma y el Corán. En segundo término, el debilitamiento de la fe ha permitido que las perniciosas influencias de Occidente —especialmente el materialismo, el secularismo, el nacionalismo y las costumbres inmorales— penetren en la vida de los musulmanes y se arraiguen en todo el mundo musulmán. En tercer lugar, la solución del problema es la reislamización del mundo musulmán. Esto significa, entre otras cosas, el reestablecimiento de la shariah (la ley islámica), una estricta adherencia a los códigos islámicos tradicionales de comportamiento y la eliminación de las influencias occidentales de la sociedad, en especial su estilo de vida y sus valores culturales decadentes. Las tecnologías y algunas formas comerciales occidentales podían ser conservadas, sin embargo, si eran debidamente islamizadas. En cuarto lugar, la reislamización de la sociedad sólo es posible a través de una repolitización del islam. Los fundamentalistas islámicos acostumbran a ser muy críticos con los propios ulemas —los guardianes de las tradiciones legales y religiosas del islam—, a los que acusan de haber perdido su sentido del compromiso político, que constituye una parte esencial de la misión del islam. Según ellos, muchos líderes religiosos se han convertido en apolíticos, o incluso peor, en apologetas de los regímenes corruptos, lo cual los incapacita para liderar la lucha por crear una sociedad justa. En consecuencia, muchos de los movimientos fundamentalistas están dirigidos por líderes laicos y no religiosos.

Los jóvenes fundamentalistas islámicos de hoy cantan «el islam es la solución». El primer indicio de que la islamización podía dar pie a un resurgimiento de los creyentes llegó con la guerra árabe-israelí de 1973. Siete años antes, el grito de guerra de los musulmanes había sido «tierra, mar, aire», reflejo de las esperanzas que depositaban en el uso de un sofisticado equipamiento militar. Fueron aplastados. En 1973, el grito de guerra había sustituido la tecnología por la fe: «Dios es grande», cantaban los guerreros. Su renovada fe pareció dar buenos resultados en el campo de batalla. Al menos, así es como interpretaron el evento muchos jóvenes fundamentalistas.

Muchos ven el origen del actual renacimiento islamista en la revolución iraní de 1979, un acontecimiento que dio alas a los movimientos fundamentalistas en el conjunto de Oriente Medio y el resto del mundo. Para muchos musulmanes, el sha de Irán representaba los peores excesos de la influencia occidental en Oriente Medio. El derrocamiento de su régimen y su sustitución por otro de corte fundamentalista, bajo la tutela del ayatollah Jomeini, dio a los fundamentalistas el primer ejemplo concreto de un triunfo político. El sueño y la realidad se habían unido y por primera vez la reislamización de Oriente Medio parecía una posibilidad real.

En los años setenta, sin embargo, se estaba gestando otra fuerza probablemente mucho más poderosa y que convencería a millones de musulmanes de Oriente Medio y otros lugares del mundo de que la reislamización estaba destinada a cambiar el curso de la historia. El gran boom del petróleo de los años setenta cambió profundamente la percepción que los musulmanes tenían de sí mismos, así como la imagen que tenía Occidente del mundo musulmán. Estos cambios de percepción han desempeñado, un papel crucial en la redefinición del islam, tanto desde el punto de vista interno como en su relación con el resto de la comunidad global.

Para comprender la enormidad del cambio es necesario recordar que en 1970 el crudo se vendía en los mercados mundiales a un precio relativamente estable de 3 dólares por barril. Diez años más tarde, el barril de petróleo se vendía a 34 dólares. El aumento del precio del petróleo se vio acompañado por incrementos igualmente espectaculares en la producción. En 1970, Arabia Saudí producía 3 millones y medio de barriles de petróleo al día. En 1980 estaba vendiendo 10 millones de barriles diarios. Las arcas de los países productores de petróleo veían entrar una fortuna sin precedentes. En 1970, Arabia Saudí tuvo unos ingresos de 1.200 millones de dólares. Una década más tarde recibía más de 101.000 millones de dólares al año.

El paso de un mercado dominado por los compradores a otro dominado por los vendedores cogió al resto del mundo desprevenido. De golpe, la comunidad internacional descubría que el petróleo, el cordón umbilical de toda la economía global, estaba en manos de pueblos y gobiernos considerados hasta el momento poco más que como meros proveedores de materias primas. Los líderes mundiales agasajaban a los jeques árabes para ganarse su favor. Las compañías globales enviaban a sus directivos más importantes a Oriente Medio en reactores privados para que trataran de influir sobre los gobiernos y obtener lucrativos contratos comerciales. Tal como observa Daniel Pipes en su libro El islam: de ayer a hoy: «Tan sólo unos años antes, la idea de que los países árabes pudieran contarse entre los más ricos del mundo hubiera resultado ridícula; su repentina opulencia pareció aún más impresionante e incluso milagrosa por el hecho de ser tan inesperada».

Pipes afirma que el boom del petróleo tuvo otros efectos igualmente interesantes que contribuyeron a reforzar la renovada confianza en el islam. Llevados por la opulencia de su nueva fortuna, los gobiernos, los ricos jeques petroleros y otros intereses comerciales relacionados con el petróleo comenzaron a invertir fondos en proyectos misioneros, como la fundación de madrasas por todo el mundo. También se difundieron libros y publicaciones relacionadas con el islam. Las inversiones iban a parar a instituciones y organizaciones religiosas nacionales y extranjeras, lo que aceleraba el ritmo de la reislamización.

La idea de que el petróleo podría contribuir algún día a escribir un nuevo capítulo en la historia del islam estaba presente desde hacía tiempo en el mundo musulmán. Hubo que esperar hasta finales de 1973, sin embargo, para que el sueño de la reislamización se hiciera realidad. En el lapso de unas pocas semanas, los regímenes musulmanes de Oriente Medio dieron la vuelta al orden económico mundial cuidadosamente forjado por Estados Unidos y sus aliados tras la Segunda Guerra Mundial y en el proceso se convirtieron, al menos durante un tiempo, en un actor importante dentro de la escena mundial.

La cuarta guerra árabe-israelí comenzó con el ataque de Egipto y Siria contra Israel durante el Yom Kippur, la más sagrada de las fiestas judías. Diez días más tarde, los delegados de la OPEP, reunidos en la ciudad de Kuwait, tomaron la decisión de subir un 70% el precio del petróleo, hasta los 5,11 dólares por barril. Tan importante como el incremento del precio fue el hecho de que era la primera vez que los delegados de la OPEP tomaban una decisión de forma unilateral y sin el consentimiento de las compañías petroleras. Durante medio siglo, los productores de Oriente Medio habían luchado por emanciparse del yugo de los gobiernos y las compañías petroleras occidentales. En su libro La historia del petróleo, Daniel Yergin nos recuerda que la lucha por la independencia había sido larga y tortuosa, desde las primeras épocas en que las compañías petroleras marcaban el precio hasta los días en que los países exportadores podían ejercer un veto, una era de precios mutuamente negociados. Ahora la OPEP se había atrevido a actuar por su cuenta y riesgo. El ministro saudí del Petróleo, Ahmed Zaki Yamani, levantó acta de la importancia de lo que había ocurrido: «Ha llegado el momento —proclamó Yamani—. Somos los dueños de nuestro propio producto».

Tres días más tarde, el presidente Richard Nixon anunció un paquete de ayuda militar a Israel de 2.200 millones de dólares, lo cual levantó las furias de los líderes de Oriente Medio. El mismo día, Libia anunció el embargo de todos sus envíos de petróleo a Estados Unidos. Arabia Saudí y otros productores de petróleo pronto se sumaron al embargo.

Además de cortar el suministro de petróleo a Estados Unidos, el embargo petrolero significó la imposición de restricciones en la producción. En diciembre se producían 4,4 millones menos de barriles de petróleo al día. Si el suministro de petróleo ya iba escaso en los mercados mundiales, la caída de casi el 9% en la producción hizo que compañías y consumidores se pelearan literalmente por el petróleo. En el mundo entero cundió el pánico cuando se vio hasta qué punto dependía Occidente del crudo de Oriente Medio. El precio del petróleo se disparó en los mercados mundiales. En Estados Unidos, los precios de la gasolina subieron un 40% y comenzaron a formarse largas colas de automovilistas en las estaciones de servicio, ya que no se sabía si una semana más tarde quedaría gasolina. En diciembre se volvieron a reunir los ministros árabes del petróleo y aumentaron oficialmente el precio hasta 11,65 dólares por barril, lo que significaba que el precio del petróleo se había multiplicado por cuatro desde el inicio de la guerra, dos meses antes.

El embargo petrolero puso seriamente en peligro la alianza occidental. Las potencias europeas, en particular, no querían seguir provocando a los jeques del petróleo y recordaron a Estados Unidos que su dependencia respecto del petróleo de Oriente Medio era mayor incluso que la norteamericana. En noviembre, la Comunidad Económica Europea aprobó una resolución en apoyo de los países árabes en el conflicto árabe-israelí. Este gesto les valió la suspensión de las restricciones petroleras que los países árabes habían impuesto para diciembre. Japón, todavía más dependiente que Europa del petróleo de Oriente Medio, aprobó una declaración parecida en apoyo de los países árabes en su guerra contra Israel. Japón también fue premiado con la suspensión del recorte de diciembre.

El espectacular aumento de los precios del petróleo hundió a los países industrializados en una profunda depresión. La tasa de desempleo se multiplicó por dos y el PNB bajó por primera vez desde el final de la Segunda Guerra Mundial.

El impacto psicológico del embargo petrolero terminaría siendo tan importante como sus efectos económicos y geopolíticos. Tras varios siglos de decadencia, el mundo islámico había ascendido a una posición de preeminencia mundial virtualmente de un día para otro. Udo Steinbach, director del Instituto Alemán de Estudios de Oriente Medio, en Hamburgo, resumió del siguiente modo las profundas implicaciones del cambio que se había producido en la mentalidad de los árabes musulmanes:

La importancia del petróleo reside en este punto: la dependencia de la mayor parte del mundo industrializado respecto del petróleo árabe (y especialmente saudí) puso en manos del árabe musulmán un poder y un respeto internacional que le eran necesarios para apreciar la validez de su religión (política) [...] De este modo recuperó su plena identidad y se encontró de nuevo en un mundo bien ordenado.

Irónicamente, el boom del petróleo también había tenido el efecto de llevar la modernidad occidental a Oriente Medio a un ritmo acelerado. Los países árabes se convirtieron en clientes privilegiados de los traficantes de armas occidentales y adquirieron el equipamiento más caro y sofisticado disponible. Oriente Medio se vio inundado por tecnologías y artículos de consumo, así como por el estilo de vida y las formas de vestir occidentales. La juventud de Oriente Medio —financiada con subvenciones y becas gubernamentales— emigró en masa a las escuelas europeas y norteamericanas para mejorar su educación. Así pues, mientras que el flujo del petróleo circulaba desde Oriente Medio hacia Occidente, el flujo de los artículos y las ideas occidentales se movía en dirección opuesta. Todo aquello sucedió de forma tan rápida que muchos musulmanes experimentaron una especie de «shock de futuro», seguido por una crisis de identidad. La gente comenzó a buscar respuestas a qué significaba ser musulmán en un mundo repleto de influencias y tentaciones occidentales. En su búsqueda, muchos jóvenes musulmanes se concentraron en las enseñanzas de Mahoma.

Recordemos que el profeta Mahoma había llamado a los creyentes a crear una sociedad justa y caritativa capaz de dar respuesta a las desigualdades. Los jóvenes musulmanes se veían rodeados por una riqueza inimaginable de la que disfrutaban sólo unos pocos, en un mundo donde la mayor parte de los árabes vivía todavía en medio de la miseria y la desesperación. La nueva opulencia había dado prestigio a Oriente Medio, pero sus regímenes seguían marcados por la corrupción generalizada. El control del petróleo permitía a los líderes políticos musulmanes ejercer una gran influencia en el extranjero, pero también aplicar una mayor represión en el interior. El recién conseguido dominio sobre el petróleo había liberado a Oriente Medio de la hegemonía comercial de Occidente, pero había hecho poco por liberar a millones de árabes de los regímenes dictatoriales. Muchos jóvenes encontraron lo que estaban buscando en el renacimiento islámico y el nuevo fundamentalismo. A partir de este momento su objetivo sería la reislamización de Oriente Medio y del mundo.

Tiempos difíciles

El boom del petróleo en los años setenta dio a los musulmanes árabes una renovada conciencia de su destino colectivo y contribuyó a impulsar el renacimiento islámico, pero fueron las duras realidades que emergieron tras el colapso de los precios del petróleo en todo el mundo a mediados de los años ochenta las que dieron forma, consistencia y profundidad al actual impulso fundamentalista que domina el mundo árabe. Si el boom del petróleo contribuyó a restaurar la confianza en el islam, la caída de los precios del crudo ayudó a convencer a la joven generación de musulmanes de la necesidad de enfrentarse no sólo a Occidente, sino también a la corrupción, la autocracia y la irresponsabilidad de sus propios gobiernos locales. En resumen, los gobiernos árabes pudieron apaciguar durante un tiempo a sus propios ciudadanos y, cuando menos, mantener controlados a los reformadores fundamentalistas a base de garantizar el empleo y los ingresos, proporcionar servicios básicos y distribuir regalos, favores, becas y subsidios de diversos tipos para aplacar a sectores como los trabajadores, los grupos religiosos y otros. Cuando los ingresos derivados del petróleo descendieron y las arcas gubernamentales comenzaron a vaciarse, se hizo necesario recortar las diversas clases de ayudas públicas, y lo único que había crecido en los últimos años era la rabia y el resentimiento popular hacia unos regímenes que cada vez parecían estar más alejados y ser más insensibles a las necesidades de su pueblo.

Las cifras del petróleo son desconcertantes. Los ingresos de la OPEP alcanzaron los 340.000 millones de dólares anuales tras el embargo petrolero de 1974. Con la caída del sha de Irán en 1979 y el comienzo de la guerra Irán-Irak de 1980, dichos ingresos subieron hasta los 438.800 millones de dólares. Sólo seis años más tarde, sin embargo, se habían desplomado hasta los 83.000 millones de dólares. Los ingresos de la OPEP se han mantenido bajos desde entonces.

En los años del boom, muchos gobiernos de Oriente Medio —sobre todo en el golfo Pérsico, donde se halla el 90% de las reservas petroleras de Oriente Medio— se convirtieron esencialmente en sociedades rentistas. Esto quiere decir que los ingresos gubernamentales no procedían de los impuestos, sino de las rentas del petróleo. El motivo no es difícil de comprender. En el punto máximo del boom del petróleo, a comienzos de los años ochenta, las exportaciones de petróleo constituían más del 50% del PIB de los países del golfo Pérsico. Por más que a los occidentales nos cueste imaginar un sistema en el que no pagamos impuestos al gobierno, pero en el que tenemos todas nuestras necesidades básicas cubiertas por una fuente independiente de ingresos, eso es lo que sucede en países como Arabia Saudí, Kuwait, Qatar, Abu Dhabi y Dubai. En los años dorados —con las subidas del petróleo de los años setenta— los gobiernos del golfo Pérsico se convirtieron en actores centrales y omnipresentes en las vidas de sus ciudadanos. En ningún lugar se hizo más evidente el nuevo papel ampliado del gobierno como en el empleo. Incluso después del crash del petróleo de mediados de los años ochenta, el gobierno de Kuwait continuó dando empleo en el sector público a más de la mitad de la población activa. En Omán, más del 60% de la población activa cobraba del gobierno en los años noventa por trabajar en lo que se designaba eufemísticamente como «servicios comunitarios».

El empleo público es sólo el principio de lo que se ha convertido en una dependencia «desde la cuna hasta la tumba». Muchos países del golfo Pérsico proporcionan educación pública gratuita hasta el nivel universitario, servicios médicos gratuitos, viviendas de protección oficial, becas y préstamos a bajo interés para iniciar negocios, así como seguridad social para los discapacitados y la tercera edad. Arabia Saudí y Kuwait suministran incluso alimentos a través de cooperativas financiadas por el gobierno. La gasolina está rebajada y servicios públicos como el agua, la electricidad y el teléfono son gratuitos, o están subvencionados.

Naturalmente, nadie regala nunca nada. A cambio de cuidar de sus ciudadanos, los gobiernos de los países del golfo Pérsico esperan de ellos una lealtad total e inquebrantable. No se tolera ninguna disensión política, ni siquiera de tipo moderado. Los sindicatos independientes, que antes constituían una fuerza política en muchos países de Oriente Medio, han sido ilegalizados, al menos en la zona del golfo Pérsico. Los partidos políticos también son ilegales en algunos países como Kuwait, Omán, Qatar, Arabia Saudí y los Emiratos Árabes Unidos, y el único partido político legalmente permitido en Irak es el de Saddam Hussein. Los medios de comunicación en Irak se hallan bajo control estatal y los periódicos están sujetos a censura. Por último, los gobiernos están dirigidos por élites hereditarias, lo cual deja muy poco espacio para la aparición o la manifestación pública de puntos de vista políticos alternativos.

Mientras los ingresos derivados del petróleo superaban los gastos gubernamentales en servicios, las monarquías del golfo Pérsico pudieron comprar la lealtad y la obediencia de la gran mayoría de sus ciudadanos. A lo largo de la última década, sin embargo, las reducidas rentas del petróleo no han podido cubrir unos gastos gubernamentales cada vez más importantes. La creciente deuda pública y los constantes recortes en los servicios han hecho que los países de la región sean más inestables políticamente que en ninguna otra época de su historia y mucho más vulnerables a los movimientos insurrectos adscritos al fundamentalismo islámico.

Si la mala situación económica se debe, en buena medida, a la caída de los ingresos derivados del petróleo, el espectacular aumento de la natalidad en Oriente Medio no hace más que agravarla. Esta parte del mundo ha experimentado una «explosión juvenil». El crecimiento medio de la población en los años noventa era del 2,7%. Más inquietante aún es el hecho de que el 40% de la población de la región se halle por debajo de los diecisiete años. En la actualidad, el desempleo entre los jóvenes de entre 18 y 25 años se sitúa cerca del 20%, lo que convierte a estos países en una bomba de relojería política.

Incapaces de diversificar la economía y proporcionar nuevas fuentes de empleo, los países del golfo Pérsico se enfrentan a dos alternativas igualmente inaceptables. O bien dan trabajo a todos estos jóvenes en el sector público y se hunden todavía más en la deuda, o bien limitan la nómina gubernamental y dejan a una gran cantidad de jóvenes en la calle, lo cual crearía un buen caldo de cultivo para engrosar las filas cada vez más numerosas de las sectas islámicas militantes.

De un modo u otro, a corto plazo —es decir, en los próximos años— la renta per cápita real continuará probablemente su caída, a medida que Rusia y otros productores ajenos a la OPEP inunden el mercado global con petróleo crudo barato. En los países del sur del golfo Pérsico, la renta per cápita real es sólo el 40% de lo que era hace veinte años, en la cima del boom del petróleo, y se espera que siga descendiendo, con el correlativo aumento del riesgo de que se extiendan el malestar social y la agitación política.

Arabia Saudí

A los saudíes les gusta decir: «Mi padre iba en camello, yo voy en coche, mi hijo va en jet... y su hijo irá en camello». A pesar de que un cuarto de las reservas de petróleo que quedan en el mundo se hallan en Arabia Saudí, muchos saudíes tienen la sensación casi fatalista de que su país vive en un «tiempo prestado». De cómo utilicen los saudíes este «tiempo prestado» depende probablemente la forma en que el mundo realice la transición más allá de la era del petróleo.

Arabia Saudí, el hogar de los lugares sagrados del islam, la Meca y la Medina, y la tierra natal de Mahoma, es todo un experimento sobre el choque entre la tradición y la modernidad, y entre la vida espiritual y la secular. Nos hallamos ante una sociedad que hace sólo cincuenta años estaba poblada en buena parte por tribus nómadas que vivían de forma muy parecida a como lo hacían sus antepasados en la época de Mahoma. En 1950 no había una sola carretera asfaltada en todo el país. La localidad portuaria de Jiddah era una ciudad amurallada de unos 50.000 habitantes. No había ningún enlace ferroviario con la capital, Riyadh.

Sólo existía una emisora de radio en funcionamiento cuando el presidente John F. Kennedy fue asesinado en 1963. Sin embargo, en la actualidad el país está atravesado por miles de kilómetros de autopistas que conectan todos los pueblos y ciudades. Jiddah es una ciudad cosmopolita en la que vive un millón y medio de personas. Sus avenidas están flanqueadas por altos bloques de apartamentos y torres de oficinas. El aeropuerto saudí de Jiddah tiene una superficie de 103 km² y es un 50% más grande que los aeropuertos Kennedy, La Guardia, O'Hare y Los Ángeles juntos. El nuevo aeropuerto de Riyadh es dos veces mayor que el complejo de Jiddah y cubre la portentosa superficie de 243 km². En 1978 había pocos equipos de televisión y sólo 125.000 teléfonos en todo el país. En la actualidad, Arabia Saudí cuenta con 250 televisores por cada 1.000 personas y un sistema de comunicaciones vía satélite integrado por 11 estaciones terrestres móviles y tres fijas capaces de dar acceso telefónico a todos los lugares del mundo. La primera central eléctrica no fue instalada hasta después de la Segunda Guerra Mundial. A comienzos del siglo XXI, se ha hecho llegar la electricidad a todo el país y la mayoría de sus edificios importantes poseen aire acondicionado.

Sin embargo, en Arabia Saudí no se permite que las mujeres conduzcan un coche, están prohibidos los cines, no hay instancias gubernamentales electas y los tribunales religiosos deciden sobre materias relacionadas con los matrimonios, los divorcios y las herencias.

El gobierno saudí debe su existencia a una especie de asociación que formaron hace doscientos cincuenta años un líder religioso y otro tribal. En 1745, Mohammed ibn Saud, el jefe de una pequeña localidad próxima a un oasis, Dariyah, estableció una alianza con un religioso islámico, Abd al-Wahhab, que predicaba el retorno a una práctica estricta del islam. El acuerdo entre los dos exigía del líder saudí que aceptara la interpretación wahhabista del islam como base de su gobierno, a cambio de la aceptación por parte del líder religioso de la legitimidad del cuerpo gobernante. A lo largo de los años, la alianza se mantuvo viva a través de los matrimonios cruzados entre las dos familias. El fundador de la moderna Arabia Saudí, el rey Abd al-Aziz Al Sa'ud ar ibn Sa'ud, reunió los diversos territorios de la península arábiga bajo su dominio en las primeras décadas del siglo XX, de nuevo con la ayuda crucial de la secta Wahhabi como elemento legitimador del nuevo régimen.

En Arabia Saudí, el liderazgo religioso de los Wahhabi apenas se halla por debajo de la familia real en influencia. Los Wahhabi ejercen un control directo y parcial sobre la educación y las cuestiones sociales, y asesoran a la familia real en los asuntos de Estado. El gobierno, por su parte, financia la construcción de mezquitas, apoya las actividades religiosas y paga un sueldo a los líderes religiosos. El ulema tiene garantizado el monopolio sobre las prácticas religiosas y un control parcial sobre la vida social del pueblo saudí, a cambio de lo cual el gobierno se ha asegurado un liderazgo religioso complaciente y bien dispuesto a cumplir sus ordenanzas.

Arabia Saudí constituye, por lo tanto, un extraño híbrido, parte autocracia y parte teocracia. En el pasado, la familia real ha visto la presencia de los Wahhabi como una especie de póliza de seguro. El gobierno ha prohibido el desarrollo de organizaciones civiles en el país y ha dejado el control y el arbitraje de la vida social y cultural del pueblo saudí en manos de los líderes religiosos Wahhabi. Lo interesante es que, una vez prohibidas las demás formas de expresión cívica, las mezquitas se han convertido en el único lugar donde la gente tiene libertad para reunirse, expresar opiniones y debatirlas. No es de extrañar, pues, que buena parte de la rabia y la angustia reprimida, sobre todo entre los jóvenes, se desahogue a través de la participación en foros y reuniones de carácter religioso. El resultado es que las mezquitas locales han experimentado una profunda politización en los últimos años. Según algunos observadores, es probable que en el futuro las instituciones religiosas locales se conviertan en el caldo de cultivo de una nueva generación de fundamentalistas dispuestos a derrocar la autoridad estatal, del mismo modo que antes habían servido como válvulas de seguridad para preservar los intereses de la familia gobernante. Ello dependerá en buena medida de si el gobierno saudí es capaz de dar respuesta a una situación económica cada vez más penosa, derivada de los bajos precios del petróleo en los mercados mundiales durante los últimos años.

Arabia Saudí ha cambiado radicalmente en poco más de una generación. Adaptarse a la vida moderna y a la globalización en un país comprometido con un estricto código islámico de comportamiento social ha demostrado ser una tarea difícil, incluso imposible, según dicen algunos. En buena medida, los problemas del país tienen su origen en la excesiva rapidez del crecimiento económico, que se ha visto seguido por una desaceleración igualmente brusca. Los precios mundiales del petróleo han sido los responsables del auge y de la caída de la economía, con lo que el petróleo ha sido al mismo tiempo un regalo y una maldición.

En 1970, Arabia Saudí contaba con una población de sólo 6,2 millones de personas, un PIB de 4.000 millones de dólares y una renta per cápita de sólo 2.800 dólares. En la actualidad, la población ha alcanzado los 22,7 millones de personas, de las cuales el 43% se halla por debajo de los 14 años de edad. El PIB es de 173.000 millones de dólares y la renta per cápita se sitúa alrededor de los 8.500 dólares.

En menos de treinta años, la población activa ha pasado de dedicarse a las labores agrícolas en un 64% a una situación actual en la que sólo el 6% trabaja en el campo, mientras que el 47% lo hace en la industria y el 47% restante en diversas clases de servicios. La despoblación de las áreas rurales ha sido una de las más rápidas del mundo en vías de desarrollo. En 1995, casi el 80% de la población vivía en zonas urbanas y una de cada cinco personas residía en ciudades con una población superior al millón de personas.

El índice de alfabetización ha subido del 15% al 63% entre la población saudí. El 10% de los jóvenes saudíes abandona la escuela en el cuarto curso y sólo el 5% de los hombres y el 2% de las mujeres accede a la escuela secundaria. Además, buena parte de lo que aprenden los jóvenes saudíes viene a ser instrucción religiosa, una preparación escasamente adecuada para encontrar un empleo real en una economía diversificada.

Una mano de obra mal preparada dentro de una economía basada en un único recurso es un mal presagio de cara al futuro. La única forma que ha encontrado el gobierno de absorber los nuevos contingentes que ingresan en la población activa ha sido ponerlos a trabajar en el sector público. En 1997, más de la mitad de la mano de obra adulta del país trabajaba para el Estado. Financiar todo esto, además del resto de servicios públicos creados durante los años del boom del petróleo, cuesta una fortuna: más de 50.000 millones de dólares al año. Como los ingresos derivados del petróleo no consiguen cubrir las necesidades económicas y sociales de una población en constante crecimiento, el gobierno saudí se ha visto obligado a gastar más de lo que ingresa y a mantener un presupuesto deficitario durante los últimos quince años.

Añádase a todas estas desgracias el coste de más de un tercio de billón de dólares que supondrá la modernización de las refinerías, los oleoductos, los servicios públicos, los medios de transporte y las infraestructuras del país durante las dos próximas décadas, simplemente para mantener en funcionamiento su industria petrolera y su economía, y se hará evidente la dimensión del problema.

Arabia Saudí se encuentra en una situación delicada. Su problema es parecido al que tienen ante sí todos los demás países productores de petróleo del golfo Pérsico: cómo sobrevivir políticamente a los peligrosos años que nos esperan inmediatamente antes de que la producción global de petróleo toque techo y Oriente Medio se convierta de nuevo en el proveedor de los últimos recursos de la economía global.

Lo que preocupa a los líderes saudíes son los millones de jóvenes mal preparados y sin perspectivas de encontrar un empleo real, de los que tiene que hacerse cargo un gobierno ya hundido en la deuda. Sólo en 1998, el PIB de Arabia Saudí bajó un 7% y su déficit aumentó hasta los 13.000 millones de dólares. Mientras tanto, su renta per cápita sigue bajando en una tasa cercana al 2,9% anual.

El futuro de la democracia

En el golfo Pérsico cada vez hay más miedo a que el «polvorín esté a punto de estallar». Sin duda, una parte de la presión que viene desde abajo tiene un origen económico. Sin embargo, también posee un objetivo político. Estamos hablando de algunos de los gobiernos más represivos y tiránicos que existen en el mundo. Según el índice de la Auditoría Mundial de la libertad económica, la mayoría de los países musulmanes están clasificados entre los más restrictivos del mundo. Cinco de estos gobiernos, los de Arabia Saudí, Irak, Libia, Sudán y Somalia, se cuentan entre los ocho regímenes más represivos del mundo.

Los jóvenes musulmanes, exasperados por el constante deterioro de la situación económica, no están dispuestos a tolerar las políticas represivas de sus gobiernos. Éstos, por su parte, están aparentemente paralizados por los intratables problemas económicos que tienen ante sí, al tiempo que no parecen dispuestos a crear espacios en los que el pueblo pueda expresar su frustración y ofrecer propuestas alternativas para el futuro.

Si los gobiernos de Oriente Medio no tienen respuestas, los fundamentalistas, por el contrario, dicen que sí las tienen. Su mensaje es ávidamente escuchado por los oídos más jóvenes y cada vez es mayor el número de conversos al último de los «ismos»: el islamismo.

Para los jóvenes, una figura como Osama bin Laden resulta particularmente atractiva. Le ven hablar contra sus gobernantes corruptos y contra la injusticia de la sociedad. Sus triunfos militares les dan la esperanza de una victoria final sobre las fuerzas que han creado todos los problemas del mundo. Bin Laden, un producto de la educación wahhabista de Arabia Saudí, es visto por muchos jóvenes como un gran reformador, alguien que no tiene miedo de enfrentarse, en nombre de la «verdadera» fe, tanto a los «hipócritas» internos como a los «infieles» externos. El hecho de que naciera en una rica familia saudí y eligiera abandonar su privilegiada posición para luchar en la jihad (guerra santa) no hace más que aumentar el brillo de su imagen ante los jóvenes musulmanes de Oriente Medio y del resto del mundo.

Al vincular, en una brillante maniobra política, el destino de los gobernantes saudíes a las odiosas maquinaciones de Norteamérica y los poderes occidentales, Bin Laden ha creado, según algunos observadores, una nueva y poderosa dinámica capaz de atraer a la generación más joven hacia el fundamentalismo islámico. La oportunidad llegó cuando el gobierno saudí permitió la entrada en el reino a las tropas de Estados Unidos y otros países occidentales durante la Guerra del Golfo. Bin Laden denunció esta decisión, en lo que venía a ser como un desafío público. «Al abrir la península arábiga a los cruzados —proclamó Bin Laden— el régimen ha desobedecido y actuado en contra de lo que había ordenado el mensajero de Dios.» Al hacerlo, según Bin Laden, los líderes saudíes habían dejado de ser musulmanes.

Para una generación joven y empobrecida de musulmanes, zarandeada por un mundo aparentemente indiferente a su destino, el sencillo mensaje de los fundamentalistas toca una fibra sensible. El mundo, según les dicen, está dividido en dos grupos, el de los verdaderos creyentes del islam y el de los bárbaros. Escoger el primero significa seguir los pasos de Mahoma y desempeñar un papel vital en la redención del mundo. La sola idea de poder participar en la jihad para cambiar el destino de la humanidad resulta excitante para unos jóvenes a quienes se han negado incluso las recompensas más básicas de la vida. Nos encontramos ante otro tipo de participación, no la que en Occidente asociamos normalmente al voto y las políticas democráticas. Nuestra democracia civil tiene escaso o nulo atractivo para la mayoría de los jóvenes de Oriente Medio. No es algo con lo que estén familiarizados. En su mundo, los soberanos gobiernan. Pero para el islam, les dicen, todos son iguales, todos cuentan. Al entregarse a la causa de los fundamentalistas, consiguen un acceso inmediato a un mundo en el que se les restaura la dignidad, se otorga sentido a su vida y se les ofrece una causa justa: sin duda, un tónico poderoso.

En su libro The Arab Predicament, Fouad Ajami escribe acerca del poder de seducción que posee la concepción fundamentalista de la participación:

La llamada de los fundamentalistas encuentra eco porque invita a los hombres a participar [...] [en] contraste con una cultura política que reduce a los ciudadanos al papel de espectadores y les pide que dejen las cosas en manos de sus gobernantes. En un momento en que el futuro se presenta incierto, les pone en contacto con una tradición que reduce su sensación de desconcierto.

Estados Unidos y los demás países occidentales han sido muy ambiguos a la hora de promover cualquier tipo de democracia real en el golfo Pérsico. Ciertamente, la clase de participación que tienen en mente los fundamentalistas resulta inaceptable para los líderes políticos norteamericanos, pero en los círculos de poder incluso se ha rechazado la forma más benigna de democracia electoral. Los fundamentalistas acostumbran a acusar al gobierno norteamericano de tener dos varas de medir, es decir, de promover la democracia en el interior y en cambio apoyar las dictaduras en Oriente Medio. Hay parte de verdad en estas acusaciones. A Estados Unidos y a sus aliados occidentales les preocupa la posibilidad de que la extensión de la democracia en el golfo Pérsico pueda tener como resultado que los fundamentalistas alcancen el poder en las urnas. La perspectiva de que haya «muchos Iranes» pone nerviosos a los líderes políticos norteamericanos. Por este motivo, Estados Unidos ha tomado partido por regímenes autocráticos como el de Arabia Saudí, pues prefiere la calma relativa que trae consigo la represión a la inestabilidad política que podría acompañar al áspero toma y daca de la democracia representativa. Tal como lo expresó un profesional saudí: «Los norteamericanos quieren un imperio inodoro».

Tras los ataques del 11 de septiembre contra las torres gemelas del World Trade Center, en Nueva York, y contra el Pentágono, en Washington, D.C., los políticos norteamericanos y un número cada vez mayor de gobernantes del golfo Pérsico comienzan a ver claro que cierto tipo de democracia llegará a Oriente Medio, guste o no, y que será mejor que nos vayamos acostumbrando a la idea. Sólo dos meses después de los ataques, el príncipe Walid bin Talal, inversor multimillonario y uno de los hombres de negocios más ricos del mundo, además de miembro de la familia real de Arabia Saudí, dijo por primera vez lo indecible. Llamó a una transformación del régimen saudí que trajera al país al menos una forma limitada de democracia. Preocupado por el creciente descontento político que se extiende entre los jóvenes saudíes —la mayoría de los secuestradores de los aviones que participaron en los ataques del 11 de septiembre eran jóvenes saudíes—, el príncipe dijo que ya era hora de abordar la cuestión de la democracia representativa, hasta ahora un tabú político. «Si el pueblo hablara con mayor libertad y tuviera una mayor participación en el proceso político —sostenía el príncipe— sería posible contenerlo y hacer que formara parte del proceso.»

El príncipe Walid dejó claro que no estaba abogando por la eliminación de la monarquía como cuerpo gobernante en Arabia Saudí, sino que simplemente sugería que los miembros del actual consejo del rey debían ser elegidos democráticamente y no por designación directa. Sin embargo, sus comentarios despertaron un terremoto en todo el reino e hicieron que muchos se preguntaran si se había abierto finalmente la puerta a la libre expresión de opiniones políticas en el país.

Lo que resulta más difícil de aceptar para los norteamericanos y otros occidentales es que la implantación de la democracia en Oriente Medio podría llevar a los fundamentalistas al poder en toda la región. En realidad, casi seguro que esto ocurrirá. Por más que a nosotros nos gustaría contemplar el surgimiento de partidos políticos seculares, tolerantes y libres de discriminaciones de género, la realidad es que prácticamente todos los movimientos políticos que cuentan con algún apoyo popular en aquella parte del mundo tienen una prioridad distinta: la islamización de su sociedad y del mundo. Para comprender por qué sucede esto, dice el historiador Karen Armstrong, debemos distinguir entre la concepción occidental de la política y la islámica. Según Armstrong, en Occidente asociamos la política con «el gobierno del pueblo, por el pueblo y para el pueblo». En el mundo islámico, en cambio, no es el pueblo el que da legitimidad al gobierno, sino Dios. La sola idea de que el pueblo pueda legitimar al gobierno antes que Dios sería considerada una blasfemia por parte de la mayoría de los musulmanes, algo así como una «usurpación directa de la soberanía de Dios».

Las reformas democráticas permitirían el acceso de los islamistas al gobierno a través de las urnas. Una vez en el poder, sin embargo, los extremistas islámicos acostumbran a introducir cambios legislativos que restringen o recortan las propias reformas democráticas que les habían permitido inicialmente acceder al poder. Argelia es un caso paradigmático. En 1989, el Frente de Liberación Nacional (FLN), que había gobernado en Argelia como partido único desde que el país obtuvo su independencia de Francia en 1962, estableció una nueva constitución que legalizaba la creación de otros partidos políticos. En las elecciones nacionales de 1991, el Frente Islámico de Salvación (FIS), un partido fundamentalista islámico, obtuvo numerosos escaños en la Asamblea Nacional en la primera vuelta de las elecciones. Se esperaba que el FIS ganara en la segunda vuelta y se hiciera con el control del gobierno. En este punto, el ejército intervino y anuló las elecciones para bloquear el acceso del FIS al poder. Si el partido hubiera logrado su objetivo, habría reestructurado el país sobre la base de las tradiciones islámicas. Los tribunales religiosos habrían reemplazado a los tribunales civiles, los códigos de vestuario islámicos se habrían convertido en obligatorios y las expresiones culturales se habrían visto limitadas a prácticas aceptables desde el punto de vista islámico. El FIS respondió al ataque y estalló la guerra civil. Los líderes del FIS fueron perseguidos y arrestados, y los militares tomaron el poder efectivo en Argelia.

Reveses como el producido en Argelia no han conseguido frenar el proceso de «democratización» en Oriente Medio. Los movimientos fundamentalistas de toda la región se han organizado en partidos políticos y exigen reformas democráticas. En Kuwait, donde ya existe una Asamblea Parlamentaria, muchos observadores muestran su preocupación ante la posibilidad de que una mayoría islamizada pueda socavar las frágiles reformas democráticas realizadas hasta el momento. «La democracia ha llevado a los islamistas al Parlamento», dice Ahmed E. Bishara, un liberal que dirige la Unión Nacional Democrática de Kuwait, y como resultado «el Parlamento termina legislando en contra de la libertad pública». En Kuwait, los islamistas que hay en el Parlamento han votado a favor de la separación de hombres y mujeres en las universidades, se han opuesto a la extensión del voto a las mujeres y están presionando para sustituir la legislación civil por códigos y tribunales islámicos. Islamistas como Khalid al-Essa, líder del conservador Movimiento Solafi de Kuwait, sostiene que «puedes tener una organización democrática, pero ésta debe seguir la guía del islam». Al-Essa y los fundamentalistas consideran que el islam es un «marco infranqueable» para la democracia.

En muchos sentidos, el destino de los países del golfo Pérsico y del conjunto de Oriente Medio depende de si los millones de jóvenes empobrecidos y privados de derechos políticos de la región ven la renovación islámica como su única esperanza real de construirse un futuro mejor. Si terminan por verse a sí mismos como los guerreros de Alá que siguen el camino justo marcado por Mahoma, como los encargados de corregir los males de una sociedad injusta, pueden convertirse en una fuerza imparable. Si los gobernantes actuales son incapaces de encontrar soluciones al problema del constante deterioro de la situación económica y siguen gobernando mediante una combinación de poder autocrático y de represión brutal de cualquier voz opositora, es probable que los movimientos fundamentalistas logren derrocar muchos regímenes de Oriente Medio e implantar Estados islamistas radicales parecidos al de Irán en el curso de los próximos diez años. La perspectiva de que el golfo Pérsico esté plagado de jóvenes y fieros regímenes fundamentalistas cuando la producción global de petróleo toque techo es una posibilidad muy real y algo para lo que debe prepararse el resto del mundo. Reconozcamos que es probable que sea tarde ya para remediar lo que muchos ven como inevitable.

La politización del petróleo

La islamización del petróleo en Oriente Medio es un tema que preocupa a los líderes políticos y a los estrategas geopolíticos, aunque lo cierto es que jugar la carta del petróleo no es nada fácil, tal como ha aprendido la OPEP a lo largo de los años. En el pasado, siempre que los grandes productores de petróleo han recortado la producción para mantener los precios altos en los mercados mundiales, el resultado ha sido en general un aumento a corto plazo del precio del barril seguido por un descenso a largo plazo tanto del precio como de la cuota de mercado de los países responsables, como consecuencia de la búsqueda de proveedores más baratos por parte de las compañías petroleras. Eso fue lo que sucedió en los años setenta y ochenta. Los productores del golfo Pérsico cogieron al mundo desprevenido en los años setenta y pudieron sacar provecho de su embargo petrolero y de sus recortes en la producción y obtener enormes beneficios durante casi una década. En los años ochenta, sin embargo, las compañías petroleras habían conseguido encontrar fuentes alternativas y obligaron a los productores de Oriente Medio a bajar los precios y reducir su cuota en el mercado mundial del petróleo.

Sin embargo, es probable que en el momento en que la producción global de petróleo toque techo las nuevas circunstancias generen una dinámica muy distinta. En esta ocasión no habrá fuentes alternativas disponibles de crudo barato en cantidades suficientes como para cubrir los déficit generados por los recortes de producción y las subidas de precios que puedan imponer los países del golfo Pérsico. Tal como hemos visto, por más que los geólogos discrepen en cuanto al momento exacto en que la producción global de petróleo tocará techo, todos están de acuerdo en que dos tercios de las reservas globales de petróleo que quedarán a partir de ese momento se hallarán en Oriente Medio. Así pues, tanto si se mantienen los regímenes autocráticos existentes en Oriente Medio como si son derrocados por los fundamentalistas, el dominio sobre el petróleo va a pasar en los próximos años a la región del golfo Pérsico. Y cuando esto ocurra, aquel que tenga el poder se hallará en posición de dictar sus propios términos a los mercados mundiales del petróleo, simplemente porque no habrá ningún otro lugar donde ir a buscar petróleo crudo y abundante.

Así pues, la única diferencia real entre la posibilidad de que la vieja guardia se mantenga en el poder y que se vea sustituida por los nuevos movimientos es que los gobiernos de Oriente Medio utilicen su nueva posición de dominio para fines estrictamente comerciales, o bien que la circulación del petróleo se vea afectada por consideraciones de tipo político. Desde una perspectiva de mera conveniencia, mantener el flujo del petróleo —aunque sea a un precio exorbitante— tiene todo el sentido del mundo, dado que significa mayores ingresos para los países productores. Por otro lado, no es imposible imaginar situaciones en las que algunos regímenes fundamentalistas decidan cerrar el grifo, por lo menos durante un breve período de tiempo, y poner al mundo contra la pared con la intención de conseguir diversos tipos de concesiones políticas.

El 8 de abril de 2002, el presidente Saddam Hussein trató de hacer precisamente eso al anunciar que su país suspendía todas sus exportaciones de petróleo durante treinta días en protesta por la incursión israelí en los «territorios palestinos». Irak es el sexto proveedor principal de petróleo de Estados Unidos, con el 9% de las importaciones norteamericanas de crudo. Las exportaciones iraquíes representan un total de dos millones de barriles al día, lo que supone el 4% del petróleo que circula en los mercados mundiales. El ministro de Asuntos Exteriores iraní Kamal Kharrazi había anunciado anteriormente que su país daría su apoyo a un embargo petrolero —Irán es el segundo productor principal de petróleo de la OPEP— si otro de los países productores árabes se sumaba a los iraquíes. Libia dijo también que daría su apoyo a un embargo petrolero si otros países árabes acordaban hacerlo a su vez.

Arabia Saudí —el principal productor de la OPEP— se opone a utilizar el petróleo como arma y señala que hacerlo perjudicaría a los países productores árabes, que no pueden hacer frente a un descenso en sus ingresos derivados del petróleo. Para muchos países del golfo Pérsico el petróleo representa más de dos tercios de las rentas gubernamentales. Por el momento, el rumor dominante en la calle es que hay «una probabilidad muy baja» de que los países productores árabes recorten su producción de petróleo para presionar a Israel y Estados Unidos. Sin embargo, las cosas podrían cambiar en poco tiempo y las consecuencias serían profundas y duraderas.

A pesar de que a corto plazo países como Rusia, Noruega, Canadá, México y otros productores de petróleo no pertenecientes a la OPEP podrían incrementar su producción para mantener el flujo de petróleo en los mercados, una retirada parcial del petróleo de la OPEP de los mercados mundiales podría disparar los precios hasta los 50 dólares o más por barril. Si esto sucediera, la economía global recibiría un golpe devastador. Aunque no se hiciera efectiva la amenaza de un embargo petrolero, lo cierto es que los precios del petróleo están destinados a subir cuando la producción global toque techo y los países productores de petróleo de Oriente Medio serán los beneficiarios económicos y políticos de esta subida. Se producirá una enorme transferencia de riqueza desde las potencias industriales como Estados Unidos hacia los productores del golfo Pérsico. La fuerte dependencia del petróleo extranjero podría provocar una salida de 100.000 miñones de dólares al año de Estados Unidos, lo que comprometería seriamente la ya deficitaria balanza comercial norteamericana. Para los países del golfo Pérsico, la nueva realidad podría significar un incremento de 160.000 millones de dólares en los ingresos anuales de 2010, o un aumento acumulado de riqueza de 1,5 billones de dólares en este mismo período.

Una fortuna como ésta, generada en un período tan corto de tiempo, tendrá un profundo impacto tanto sobre la cultura árabe como sobre la política regional de Oriente Medio. Es probable que el flujo unidireccional de riqueza desde el resto del mundo hacia el golfo Pérsico también venga a agravar las tensiones geopolíticas entre los países musulmanes y Occidente, y tenga como resultado un conflicto abierto y prolongado entre ambos poderes.

Capítulo 6

LA CRISIS GLOBAL

El acceso de los fundamentalistas al poder en los países de Oriente Medio coincidiendo con el pico en la producción global de petróleo tendrá consecuencias que no sólo se dejarán sentir en las gasolineras. Los gobiernos y las compañías energéticas ya se han lanzado a una carrera por diversificar su catálogo de fuentes de energía. En el futuro más inmediato, el interés principal se ha dirigido hacia la exploración y el desarrollo del gas natural. Lamentablemente, al tratar de contrarrestar los efectos del descenso de la producción global de petróleo con una excesiva dependencia del gas natural, las compañías energéticas y eléctricas podrían dar pie a una segunda crisis energética, que vendría poco tiempo después de la crisis del petróleo y que podría resultar devastadora para la economía global.

Otros promueven el uso del carbón, así como el aprovechamiento de los crudos pesados, las arenas asfálticas y el esquisto, todos combustibles «sucios» que producen importantes emisiones de CO2 y que podrían agravar el ya peligroso aumento de las temperaturas de la Tierra.

Si, por un lado, la vía del gas natural podría llevar a un callejón sin salida, el empleo de combustibles más sucios podría significar un choque frontal entre la geopolítica y la protección de la biosfera, y crear un riesgo sin precedentes tanto para la civilización humana como para la Tierra misma.

Apañarse con el gas natural

La buena noticia en relación con el gas natural es que es un combustible fósil menos contaminante que el petróleo o el carbón. El petróleo produce un tercio más de CO2 que el gas por unidad equivalente de energía producida, mientras que el carbón produce dos tercios más.

Durante años, los ecologistas y los líderes políticos han venido haciendo llamamientos a la industria para que utilice gas natural —el combustible más limpio—, con la esperanza de disminuir las peligrosas emisiones de gases de efecto invernadero.

La mala noticia es que los estudios más recientes sugieren que probablemente la producción global de gas natural tocará techo poco tiempo después de que lo haga la producción global de petróleo. Algunos analistas prevén que eso podría suceder en una fecha tan cercana como 2020. Según Dutch Shell, «la escasez [de gas natural] podría producirse ya en 2025». Para complicar aún más las cosas, buena parte de las reservas restantes de gas natural —más del 40% del total— se hallan en Oriente Medio. Con las reservas norteamericanas aproximándose ya al pico, hacia la tercera década del siglo nos encontraremos —al igual que muchas otras regiones del mundo— cada vez más a merced de los productores estratégicos de Oriente Medio y Rusia, una situación que no hará sino reducir aún más nuestras opciones energéticas y poner en peligro la economía global.

A menudo se encuentra gas en regiones donde hay grandes depósitos de petróleo o de carbón, pero también aparece en lugares donde no se ha encontrado ninguna de dichas sustancias. Hasta ahora el gas natural se ha empleado para la calefacción o como combustible de cocina. En la actualidad se está convirtiendo rápidamente en el combustible elegido para la generación de electricidad. También se usa cada vez más como combustible para el transporte. Los nuevos avances tecnológicos que permiten pasar el gas a estado líquido han permitido reducir los costes y han conseguido que resulte competitivo respecto a la gasolina para algunas actividades relacionadas con el transporte.

Aproximadamente el 14% del consumo de gas natural en Estados Unidos se dirige actualmente a la generación de electricidad. Esta cifra experimentará un incremento espectacular en el curso de la próxima década. En Estados Unidos hay actualmente 272 centrales eléctricas de gas en fase de proyecto o de construcción. Se espera que entren en funcionamiento a lo largo de los próximos diez años, con lo que la red eléctrica norteamericana pasará a ser virtualmente dependiente del gas natural. Las compañías de servicios públicos prefieren los generadores de gas antes que los de carbón, petróleo o energía nuclear porque exigen menos inversión de capital, sus plazos de construcción son más cortos, resultan más eficientes y producen menos emisiones.

El Departamento de Energía de Estados Unidos prevé que el consumo de gas natural aumente desde los 0,606 billones de metros cúbicos de 1999 hasta una cifra de entre 0,912 y 1,022 billones de metros cúbicos para el año 2020. Cerca del 57 % del incremento irá a parar a la nueva generación de centrales eléctricas de gas y el resto se empleará para cubrir la creciente demanda que se espera en los sectores doméstico, comercial, industrial y del transporte.

Sin embargo, algunos estudios basados en nuevos modelos informáticos sobre las reservas de gas natural que quedan en Norteamérica sugieren que podría ser un error poner demasiadas ilusiones en el gas natural como alternativa al petróleo. Según el estudio dirigido por el ingeniero eléctrico Richard Duncan, la producción estadounidense tocó techo en 1971, con 0,623 billones de metros cúbicos (bmc), un año después de que lo hiciera la producción de petróleo. Entre 1971 y 1999, la producción de gas de Estados Unidos descendió a un ritmo del 0,5% anual. Duncan prevé que se produzca un segundo pico de producción en el año 2007, con 0,569 bmc, seguido por un descenso constante del 1,5% anual hasta 2040. Si tenemos en cuenta que se espera un incremento del 62% en la demanda interna de gas natural entre la actualidad y 2020 —solamente la demanda de gas natural para la electricidad se triplicará durante este período—, no hay duda de que las reservas restantes se agotarán rápidamente. De modo parecido, se espera que la producción de gas natural de Canadá toque techo en 2005 y comience su descenso a una media del 4,3% anual durante los próximos treinta y cinco años. La predicción en el caso de México es que su producción toque techo en 2011, con 0,04 bmc, y caiga a un ritmo del 2,7% anual durante los próximos veintinueve años.

La producción de gas natural del conjunto del continente norteamericano tocará techo en una fecha tan cercana como 2007, con 0,807 bmc, y experimentará una reducción del 51% entre ese momento y 2040, con una tasa medía de descenso del 2,1% anual. Estas cifras preocupan a algunos expertos, que se preguntan por las posibles consecuencias de un agotamiento de las reservas de gas natural en Estados Unidos justo en el momento en que buena parte de su red eléctrica dependa de él. En su discurso central en la Cumbre de la Sociedad Geológica de América en 2000, Richard Duncan predijo que aproximadamente en 2012 comenzarían a multiplicarse los apagones en la red eléctrica como consecuencia de la escasez de gas natural.

Podríamos importar gas natural del extranjero —como probablemente haremos—, pero no será barato. La mayor parte del gas natural se transporta por tierra a través de gasoductos. También puede ser enfriado hasta alcanzar el estado líquido y enviado por mar en tanques criogénicos, pero el coste es mucho más elevado. Mantener frío el gas natural resulta caro. Con todo, teniendo en cuenta que también se espera una subida en los precios del petróleo, es probable que a finales de esta década la importación de gas resulte competitiva desde el punto de vista económico. En la actualidad ya se han realizado envíos de gas natural a Estados Unidos desde Argelia, y Japón recibe cargamentos por vía marítima procedentes de Alaska y Oriente Medio.

Cada vez son más los geólogos que afirman que la importación de gas natural sólo nos permitirá ganar algunos años de tiempo adicional. C. J. Campbell cree que la producción mundial de gas podría tocar techo en una fecha tan cercana como 2020. A partir de este momento, la economía global dependerá cada vez más de Oriente Medio y de la antigua Unión Soviética para cubrir sus necesidades de gas, y a unos precios cada vez más altos. Irán, un país recientemente islamizado, posee el 16% de las reservas mundiales de gas, y entre Qatar y los Emiratos Árabes Unidos poseen otro 10%, lo cual convierte a estos tres países en jugadores clave en el futuro mercado de la energía. Como en el caso del petróleo, de nuevo nos encontramos con que Oriente Medio se convierte en el productor estratégico en un mundo que se está quedando sin reservas de gas natural y de petróleo barato.

El conflicto global para asegurarse el acceso al petróleo crudo y al gas natural no hará más que intensificarse a medida que nos acerquemos a la segunda década del siglo XXI. En esta ocasión, el mundo en vías de desarrollo —sobre todo China e India— será tan dependiente de estos combustibles fósiles como las principales potencias industriales, con lo que la competición por conseguir ambos combustibles será un fenómeno auténticamente global. La constante subida de los precios causará estragos en la economía global y aumentará las probabilidades de que se produzcan situaciones de hiperinflación, recesión e incluso depresión. A medida que el petróleo y el gas se vayan haciendo más escasos, todas las demás actividades económicas se verán afectadas. Dejamos atrás los días de la energía barata y dentro de este proceso no resulta realista esperar la clase de crecimiento económico que hemos experimentado durante el siglo XX. Si las carencias energéticas fueran lo bastante graves como para comprometer la propia red eléctrica y provocar bajadas de tensión y apagones por todo el planeta, podría ser que nos enfrentáramos a un colapso potencial de toda la infraestructura que soporta la compleja economía global y la sociedad humana. Si esto sucediera, el futuro podría ser muy distinto de todo lo que conocemos o siquiera de lo que somos capaces de proyectar o imaginar.

Los crudos pesados y el aumento de las temperaturas

Los geólogos y los economistas no dejan de recordarnos que el hecho de que reservas de crudo barato y gas natural se agoten no significa que el mundo se esté quedando sin combustibles fósiles. Todavía queda mucho carbón, arenas asfálticas, crudos pesados y petróleo de esquisto por descubrir, extraer y procesar, tan pronto como las condiciones del mercado energético hagan que su coste sea competitivo. Dichas fuentes no convencionales de petróleo, según se las conoce, son vistas como un remedio de última hora y los expertos en desarrollo industrial ya están tanteando el terreno en los círculos comerciales y políticos. En su Informe Mundial sobre la Energía, la Agencia Internacional de la Energía de la OCDE, con sede en París, predice que cuando el precio del petróleo suba, los países consumidores de petróleo comenzarán a volver su mirada hacia estas fuentes no convencionales. Dicho proceso podría verse acelerado si los productores de Oriente Medio provocaran una rápida subida de los precios y/o recortaran drásticamente la producción, sea por razones políticas o comerciales.

Pasar a utilizar otras fuentes no convencionales de petróleo tiene un precio muy alto, tanto para la sociedad como para el planeta. Dichos combustibles fósiles son más sucios que el petróleo o el gas natural. En un mundo desesperado por mantener las luces encendidas y los coches en marcha, a muchos les puede parecer que no queda otra opción que emplear una cantidad cada vez mayor de combustibles no convencionales y sacrificar con ello los intereses de la biosfera a largo plazo en beneficio de las necesidades a corto plazo de la economía. Los combustibles derivados del carbón, los crudos pesados y las arenas asfálticas aumentarían la emisión de dióxido de carbono a la atmósfera y provocarían un incremento de las temperaturas aún mayor del que prevé actualmente la comunidad científica internacional. Debe recordarse que los modelos actuales sobre el calentamiento global asumen un uso continuado de petróleo convencional y el gas natural hasta mediados del siglo XXI. Si recortamos algunas décadas esta previsión e incrementamos el uso de fuentes no convencionales de petróleo alteraremos también el ritmo y los plazos temporales del calentamiento global.

¿Cuánto petróleo no convencional deberá quemar el mundo para mantener a flote la economía global? Las previsiones conservadoras predicen en la mayoría de los casos que para cubrir las crecientes necesidades de una población en constante aumento el consumo de energía se duplicará entre los años 2000 y 2040, se triplicará antes de 2070 y se cuadruplicará antes de 2100. Esto significa triplicar las emisiones anuales de dióxido de carbono, desde 6.000 millones de toneladas de carbono en el 2000 hasta 20.000 millones de toneladas en el año 2100. Si la transición hacia los petróleos no convencionales se produjera en 2015 y no en 2050, tal como se había previsto hasta ahora, y continuara aumentando a medida que el petróleo y el gas se hicieran más escasos y se incrementara la demanda energética, las emisiones de CO2 aumentarían de forma equivalente, con efectos potencialmente devastadores sobre el clima.

Estados Unidos posee las mayores reservas de carbón del mundo. También existen importantes reservas de carbón en la antigua Unión Soviética, más o menos equivalentes a las de Estados Unidos. Países como China, India, Alemania, Australia y Sudáfrica poseen según los casos reservas que van del 6 al 12% del total mundial. Estados Unidos utiliza el carbón para cubrir el 23% de sus necesidades energéticas primarias. En la actualidad, el 55% de la electricidad que se genera en Estados Unidos procede de centrales eléctricas de carbón.22 Los analistas de la industria del carbón tienen la esperanza de que a medida que aumente el coste del petróleo y el gas natural, la liquefacción del carbón para elaborar combustibles sintéticos se convierta en una opción cada vez más atractiva desde el punto de vista económico.

En principio se espera que China e India cubran más del 29% del aumento total del consumo de carbón desde ahora hasta el año 2020, aunque un descenso temprano en la producción global de petróleo podría hacer que el carbón volviera a ocupar un lugar destacado en la lista de combustibles primarios de muchos países industrializados. El carbón está experimentando actualmente una especie de renacimiento en Estados Unidos. A los representantes de la industria del carbón les gusta recordarnos que debajo de Estados Unidos se encuentran los mayores depósitos de carbón del mundo y que debidamente utilizados podrían liberarnos de la dependencia del petróleo extranjero, procedente sobre todo de Oriente Medio, y darnos la autonomía energética.

Tras los ataques del 11 de septiembre, la industria del carbón encontró un gran apoyo entre los analistas políticos de Washington, interesados en reducir la dependencia de Estados Unidos del petróleo del golfo Pérsico.

La Administración Bush promovió una legislación que destinaba varios miles de millones de dólares para becas de investigación, exenciones fiscales y subvenciones al desarrollo de lo que la industria llama eufemísticamente la «tecnología limpia del carbón». (Los críticos aseguran que las nuevas tecnologías hacen poco por reducir las emisiones de CO2.) El nuevo interés que despierta el carbón hace que los analistas de la industria prevean un crecimiento en el consumo de carbón que puede estar entre el 1-2% y el 3-4% anual.

Según el geólogo Craig Hatfield, incluso si aceptamos el supuesto relativamente «optimista» de que Estados Unidos dispone de suficientes reservas recuperables de carbón como para cubrir sus necesidades durante trescientos años de acuerdo con las tasas actuales de consumo —las últimas revisiones de las estimaciones indican que las reservas son inferiores—, lo cierto es que dichas reservas durarían únicamente sesenta y cuatro años si se produjera un incremento del consumo del 4%, tal como prevé actualmente la industria. Hatfield señala, además, que esa estimación supone que la producción de carbón mantendrá el mismo ritmo hasta el agotamiento total del recurso. Igual de importante, según Hatfield, es la cuestión de la escasa cantidad de combustible líquido que se puede extraer de una tonelada de carbón: sólo 5,5 barriles de líquido por tonelada. Eso significa que para reemplazar el 10% del consumo global de petróleo en 1996, por ejemplo, sería necesario licuar una cantidad de carbón equivalente a la mitad de toda la que se ha extraído en el país durante los últimos diez años, lo que representa un aumento del 50% en la tasa de extracción de carbón.

Las arenas asfálticas y los crudos pesados también se presentan como posibles sustitutos del petróleo crudo. Los geólogos estiman que puede haber hasta 300.000 millones de barriles de petróleo recuperable en arenas asfálticas en el norte de Alberta, Canadá. Se cree que Venezuela contiene el equivalente a 300.000 millones de barriles recuperables de crudo extrapesado. Las reservas potencialmente recuperables de Canadá y Venezuela sumadas duplican las reservas de petróleo convencional de Arabia Saudí y equivalen al total de las reservas recuperables de petróleo convencional de todo Oriente Medio.

También se han descubierto grandes depósitos de arenas asfálticas en Estonia, Australia, Brasil, Estados Unidos y China. El Centro de Crudo Pesado y Arenas Asfálticas del Instituto de las Naciones Unidas para la Formación y la Investigación [United Nations Institute for Training and Research, UNITAR] estima que la cantidad de crudo pesado recuperable que hay en el mundo equivale a un tercio de las reservas totales de petróleo y gas natural, lo cual puede convertirlo en una pieza importante dentro del rompecabezas energético global. Un informe reciente del centro concluía que «el descenso de la producción de petróleo convencional, combinado con los recortes físicos y la escalada del precio del crudo [que se producirán en algún momento de la segunda década del próximo siglo], marcará el comienzo de la explotación comercial de los grandes depósitos mundiales de arenas asfálticas y crudo extrapesado»

Hasta ahora, el desarrollo del crudo pesado ha sido lento a causa de su alta viscosidad y sus elevados niveles de sulfuro, metales y nitrógeno, que encarecen el proceso de extracción, transporte y refinado. En la actualidad, el crudo pesado representa sólo el 3,5% de la producción mundial de petróleo. Sin embargo, países como Venezuela y Canadá

están realizando importantes inversiones para la exploración y la extracción de crudo pesado y arenas asfálticas, con la esperanza de poder ocupar rápidamente el vacío que se cree cuando la producción global de petróleo toque techo. Venezuela prevé que hacia 2010 el crudo pesado aportará más del 40% de su producción total de petróleo. En Canadá, los observadores de la industria estiman que hacia finales de esta década las arenas asfálticas constituirán el 75% de la producción de petróleo del país. Juntos, Canadá y Venezuela esperan producir, en 2010, un millón de barriles al día.

Syncrude Canadá, que gestiona el mayor centro de procesamiento de arenas asfálticas del mundo, cubre actualmente más del 13 % de las necesidades totales de petróleo de Canadá. Shell Canadá, Chevron y Western Oil Sands apuestan por las arenas asfálticas como el combustible del futuro y han iniciado recientemente la construcción de un ultramoderno centro de procesamiento de arenas asfálticas en el noroeste de Canadá, con una inversión inicial de 2.350 millones de dólares.

A pesar del entusiasmo suscitado, todavía quedan importantes obstáculos en el camino antes de conseguir que las arenas asfálticas sean comercialmente competitivas en relación con el petróleo crudo y el gas natural. Para comenzar, hacen falta dos toneladas de arenas asfálticas para producir un solo barril de petróleo. Hay que extraer toneladas de roca de minas abiertas y triturarlas en trozos más pequeños. Luego hay que extraer el petróleo de las arenas asfálticas a través de procesos térmicos con agua caliente o bien por medio de disolventes. Finalmente, el petróleo extraído debe ser refinado para obtener fuel-oil. El proceso en conjunto resulta caro. Syncrude Canadá gasta alrededor de 12 dólares para producir un barril de petróleo. Los saudíes, en cambio, gastan sólo 1 dólar por cada barril de petróleo que producen. Syncrude sostiene que puede obtener unos beneficios aceptables si el precio del barril de petróleo se sitúa alrededor de los 21 dólares por barril. Los analistas de la industria, en cambio, consideran que el precio mínimo para que el crudo pesado sea comercialmente viable se acerca probablemente a los 25 dólares por barril. La mitad de las reservas técnicamente recuperables podrían ser transformadas en combustible líquido, según los economistas, si el precio del petróleo subiera hasta los 45 dólares por barril. Aunque un precio de 45 dólares por barril parece exagerado hoy en día, estos precios podrían convertirse en la norma una vez que la producción global de petróleo toque techo y el crudo sea cada vez más escaso. Al menos ésa es la apuesta de los que invierten en arenas asfálticas.

Por otro lado, el proceso de extracción y refinado del crudo pesado es muy sucio. La firma canadiense Syncrude emite unas 218 toneladas de dióxido de sulfuro al día, veinticinco veces más del que emite una refinería convencional de Texas para producir la misma cantidad de petróleo.

La extracción y el procesamiento de las arenas asfálticas también requiere grandes cantidades de agua, lo que significa menos agua disponible para otros fines, incluidos los usos agrícolas, domésticos y comerciales. Y también es necesario calentar el agua. Un estudio reciente prevé que en 2010 cerca del 25% de la gasolina producida en Alberta se empleará simplemente para calentar el agua necesaria para disolver el bitumen. La escasez de agua se ha convertido en una preocupación central para los ecologistas en los últimos años y es probable que lo sea cada vez más a medida que el procesamiento de las arenas asfálticas vaya absorbiendo cada vez más agua.

Otro problema central son los lodos residuales que genera el proceso de extracción. Los lodos contienen hidrocarburos, sales inorgánicas y metales pesados. Estos residuos, de consistencia gelatinosa, no pueden ser reciclados. El volumen de lodos que se acumulan en los estanques de residuos de los yacimientos de arenas asfálticas de Canadá resulta inquietante. Suncor y Syncrude, dos de los principales productores, habrán generado más de 1.000 millones de metros cúbicos de residuos en 2020. Como estos lodos pueden tardar más de un siglo en consolidarse en un estado apto para el reciclado, los estanques de residuos no pueden protegerse con garantías frente a la erosión y las filtraciones. La liberación de residuos contaminantes en el suelo y las aguas subterráneas podría plantear graves problemas medioambientales a largo plazo.

Si los riesgos medioambientales de la extracción y el refinado de crudos pesados se limitaran a la contaminación del suelo y de las aguas subterráneas, el daño estaría al menos localizado, por más grave que pudiera ser. El principal problema que plantea la sustitución del petróleo crudo barato, cada vez más escaso, por el crudo pesado y las arenas asfálticas es el incremento de las emisiones de CO2. Cuanto más baja es la eficiencia del proceso de transformación del combustible, mayores son las emisiones de CO2. Producir petróleo sintético a partir de petróleo de esquisto genera un 39% más de emisiones de CO2 que producir petróleo crudo. Producir petróleo sintético a partir de carbón genera un 72% más de emisiones de CO2 que producir crudo. Del mismo modo, convertir el crudo pesado y las arenas asfálticas en combustibles sintéticos produce unas emisiones significativamente superiores.

Es imposible exagerar el peligro potencial que plantea este simple hecho termodinámico, sobre todo si tenemos en cuenta la dependencia que puede llegar a desarrollar el mundo respecto del crudo pesado y las arenas asfálticas en los próximos años. Un estudio realizado por Chevron hace más de una década preveía que a mediados del siglo XXI los crudos pesados y los bitúmenes podrían cubrir más de la mitad del suministro energético mundial. Si se incrementara el empleo de crudos pesados, arenas asfálticas y carbón para producir combustibles líquidos sintéticos con los que suplir las carencias del suministro de crudo barato, el impacto a largo plazo sobre el clima mundial podría ser incluso peor del que se prevé actualmente.

La factura entrópica de la era industrial

El calentamiento global constituye el pasivo del balance de la era industrial. A lo largo de los últimos centenares de años, y sobre todo en el siglo XX, los seres humanos han quemado enormes cantidades de «sol almacenado» en forma de carbón, petróleo y gas natural para producir la energía que ha hecho posible el modelo de vida industrial. Este gasto energético se ha acumulado en la atmósfera de la Tierra y ha comenzado a tener efectos adversos sobre el clima del planeta y sobre el funcionamiento de sus diversos ecosistemas. Al igual que otras civilizaciones anteriores, la sociedad industrial se está acercando al estadio final de su régimen energético, en el cual los costes de haber absorbido los residuos acumulados de toda la energía consumida se han convertido en un factor económico comparable al valor neto de la energía disponible que se produce y consume actualmente.

Lamentablemente, si midiéramos los éxitos de la humanidad en función del impacto que han tenido nuestras actividades sobre la vida del planeta que habitamos, la conclusión debería ser que el recalentamiento global es el principal resultado que ha conseguido la humanidad hasta la fecha, por más que se trate de un resultado negativo. Hemos comenzado a influir sobre la bioquímica de la Tierra y lo hemos hecho en menos de un siglo. Cuando las generaciones futuras vuelvan sus ojos hacia este período, dentro de decenas de miles de años, el único legado que encontrarán por nuestra parte será un cataclismo climático inscrito en los anales geológicos de la Tierra.

El calentamiento de la Tierra es el resultado de la progresiva acumulación en la atmósfera de gases que impiden que el calor se escape del planeta. Veamos cómo funciona el efecto invernadero.

La radiación solar entra en la atmósfera de la Tierra. Cuando choca con la superficie del planeta, se transforma en energía infrarroja y en calor. El calor asciende y provoca que las moléculas de dióxido de carbono y otros gases de la atmósfera terrestre entren en vibración. Las moléculas de gas actúan como reflectores y devuelven parte del calor a la superficie, lo que produce un efecto de calentamiento. El dióxido de carbono, el metano y otros gases que producen efecto invernadero constituyen una especie de manta atmosférica que hace posible que la Tierra conserve una cantidad suficiente del calor generado por la radiación solar y que se den las condiciones necesarias para la vida. Durante los últimos diez mil años previos a la era industrial, el equilibrio de los gases de efecto invernadero ha sido relativamente estable, de modo que la temperatura del planeta se ha podido mantener dentro de un estrecho margen. La quema de cantidades masivas de carbón y más tarde de petróleo y de gas natural, a lo largo de los siglos XIX y XX, ha roto este equilibrio.

En la actualidad, la atmósfera contiene aproximadamente un 31% más de CO2 del que contenía en 1750, al comienzo de la era de los combustibles fósiles. Según el Panel Intergubernamental sobre el Cambio Climático [Intergovernmental Panel on Climate Change, IPCC] de Naciones Unidas, dichos niveles de concentración no se han superado en los últimos 420.000 años, ni probablemente en los últimos veinte millones de años. Según el Panel, la tasa de aumento actual de las concentraciones de CO2 no tiene precedentes en los últimos 20.000 años. Prácticamente el 75% del incremento en las concentraciones de CO2 de los últimos veinte años es atribuible a la quema de combustibles fósiles. El resto es el resultado de la deforestación y del cambio en los usos del suelo, que contribuyen a aumentar las emisiones de CO2. La tierra y los océanos absorben la mitad del incremento de las emisiones de CO2 y el resto se libera en la atmósfera. El metano, otro gas de efecto invernadero, también ha contribuido a hacer más espesa la capa reflectora y a aumentar la cantidad de calor que queda atrapado en la superficie de la Tierra. Más de la mitad de las emisiones de metano son provocadas por el hombre e incluyen las emisiones de los arrozales, los vertederos y las flatulencias animales. La concentración de metano en la atmósfera ha experimentado un aumento del 151% desde 1750. Al igual que las concentraciones de CO2, la cantidad de metano que contiene la atmósfera no tiene precedentes en la historia geológica de los últimos 420.000 años.

La concentración de óxido nitroso, el tercer gas de efecto invernadero, ha aumentado un 17% desde 1750. Casi un tercio de este aumento ha sido provocado por el hombre y procede de los alimentos utilizados para el ganado, la industria química y el uso extensivo de fertilizantes químicos en el suelo agrícola.

El aumento del dióxido de carbono es responsable de más del 70% del calentamiento global, mientras que el metano lo es del 24% y el óxido nitroso sólo del 6%.

La preocupación por el calentamiento global tuvo su origen en un artículo científico publicado en 1957 por Roger Revelle y Hans Suess, del Instituto Scripps de Oceanografía, de California. Los dos científicos advertían de que las actividades agrícolas e industriales estaban teniendo como resultado un peligroso incremento de los niveles de CO2 en la atmósfera, de consecuencias imprevisibles para la temperatura de la Tierra.

Durante los años setenta y ochenta creció el interés por el tema, en parte a causa de la crisis del petróleo y también por la creciente preocupación que despertaba el medio ambiente. Científicos de los más diversos campos comenzaron a realizar estudios, previsiones y a discutir entre ellos acerca de una serie de hechos, cifras y su forma de interpretar los datos. En un intento de sacar algo en claro entre la multitud de posturas distintas que había suscitado el cambio climático, Naciones Unidas reunió a finales de los años ochenta a un grupo integrado por centenares de científicos de todo el mundo pertenecientes a más de doce disciplinas distintas y les encargó que realizaran un estudio riguroso sobre la cuestión. Tras más de una década de trabajo y varios informes provisionales, el Panel Intergubernamental sobre el Cambio Climático de las Naciones Unidas emitió en enero de 2001 un voluminoso informe de consenso. Sus conclusiones dan que pensar.

Los científicos del IPCC afirman que la temperatura media global de la superficie del planeta aumentó 0,6 ± 0,2 °C a lo largo del siglo XX. Además, es probable que este incremento sea el mayor ocurrido en un siglo durante los últimos mil años. Y sus modelos informáticos prevén que la temperatura media global de la superficie aumentará entre 1,4 y 5,8ºC antes de 2100.

Para poner todos estos datos en perspectiva, los científicos afirman que los incrementos de temperatura previstos «no tienen precedentes al menos durante los últimos 10.000 años, de acuerdo con los datos paleoclimáticos». En otras palabras, si la proyección se mantiene, el cambio de temperatura previsto para los próximos cien años será probablemente mayor que cualquier otro cambio climático que haya experimentado la Tierra durante diez milenios. Los efectos sobre la biosfera del planeta van a ser cualitativos. Recordemos que un incremento de la temperatura de 5 °C entre la última edad glacial y la actualidad ha tenido como resultado que una gran parte del hemisferio norte del planeta haya pasado de estar enterrada bajo cientos de metros de hielo a quedar al descubierto.

Por todas partes aparecen muestras puntuales de los efectos adversos que tiene el aumento de las temperaturas sobre el ecosistema de la Tierra. En ocasiones un pequeño cambio crea grandes problemas. Recientemente, los investigadores han informado de que la mariposa Edith's Checkerspot, que habita en la parte occidental de Norteamérica, ha emigrado más de 100 kilómetros al norte de su hábitat tradicional. Las poblaciones de salmón del Pacífico cayeron en picado en 1997-1998, cuando las temperaturas del océano subieron más de 3,3 °C. El monte Kilimanjaro, la montaña más alta de África, ha perdido el 75 % de su casquete de hielo en el curso del último siglo y terminará de perderlo probablemente en menos de quince años. Los osos polares de la bahía de Hudson tienen cada vez menos cachorros y los científicos especulan con la posibilidad de que se deba al adelantamiento del deshielo de primavera. La fiebre dengue y otras enfermedades tropicales están comenzando a invadir por primera vez el sur de Estados Unidos. Los científicos estiman que un aumento de la temperatura de entre 1,4 y 5,8 °C a lo largo del próximo siglo podría tener efectos devastadores y duraderos sobre los ecosistemas de la Tierra, entre ellos el deshielo de los glaciares y del casquete polar Ártico, la subida del nivel del mar, el incremento de las precipitaciones y las tormentas y un ciclo climático más violento, la desestabilización y pérdida de hábitats enteros, la migración de los ecosistemas hacia el norte, la contaminación del agua potable con agua salada, la deforestación masiva, la aceleración del proceso de extinción de las especies y un incremento de las sequías. El informe del IPCC también advierte sobre los efectos negativos que tendrá sobre los asentamientos humanos, como la inmersión de islas y países situados en tierras bajas, la disminución de la productividad del campo, sobre todo en el hemisferio sur, y la extensión de las enfermedades tropicales hacia el norte, hacia regiones anteriormente temperadas.

El alcance y la dimensión de los cambios que van a producirse resultan abrumadores. Pensemos por ejemplo en el glaciar Mendenhall, situado en el sudeste de Alaska. Este privilegiado enclave natural, situado cerca de Juneau, atrae a más de 300.000 turistas al año y es uno de los glaciares más visitados del mundo. Según los climatólogos, el glaciar está comenzando a retroceder. En verano de 2000 se había retirado más de 100 metros, y había dejado al descubierto tierras que llevaban siglos cubiertas por el hielo. Durante los últimos setenta años el glaciar ha retrocedido un kilómetro. Keith Echelmeyer, un geólogo que ha estado estudiando el cambiante paisaje de los glaciares de Alaska, afirma que si se mantiene el ritmo de deshielo actual, el Mendenhall podría desaparecer antes del final del siglo XXI. Otros glaciares de la región están experimentando retrocesos parecidos y, según Echelmeyer, muchos no sobrevivirán a la próxima mitad del siglo.

Los datos por satélite muestran un descenso del 10% de la capa de nieve en las latitudes medias y altas del hemisferio norte desde los años sesenta. Otros datos muestran una reducción de dos semanas en la duración del hielo sobre los ríos y los lagos en las mismas regiones. El hielo del océano Ártico también ha perdido un 40% o más de su espesor entre finales de verano y comienzos del otoño durante las últimas décadas. Se prevé que todos estos procesos se aceleren a lo largo del próximo siglo.

El informe del IPCC también revela una inquietante subida del nivel del mar en todo el mundo. Los últimos datos mostraban un aumento en el nivel medio de los mares de entre 0,1 y 0,2 metros durante el siglo XX. Y lo que es más alarmante, los modelos informáticos indicaban que los niveles subirían entre 0,09 y 0,88 metros más antes de terminar ese siglo. Aunque parte de la subida se debe al deshielo de los glaciares, los autores prevén que buena parte del incremento será el resultado de la expansión térmica de los océanos causada por un aumento de su temperatura. A lo largo de los próximos quinientos años, el nivel del mar podría llegar a subir entre 7 y 13 metros. Como el calentamiento de la atmósfera tarda casi mil años en llegar al fondo del océano, los niveles del mar seguirán notando durante siglos los efectos del incremento de las temperaturas de la superficie de la Tierra que se producirá en esta centuria.

Una cuestión que preocupa especialmente a los climatólogos es el progresivo deshielo de Groenlandia. Si las temperaturas globales en Groenlandia rebasan los 20,47 °C, la capa de hielo desaparecerá, lo que provocará una subida de hasta 7 metros en los niveles del mar. Si los niveles marinos llegaran a subir 10 metros o más durante los próximos mil años, a causa de los cambios de temperatura que ya han comenzado, la inundación resultante podría representar una pérdida de tierras emergidas comparable a la superficie de Estados Unidos. Florida y muchas ciudades de la costa este de Estados Unidos quedarían bajo el agua. Más de mil millones de personas se verían obligadas a desplazarse y buena parte de las tierras más fértiles del mundo quedarían sumergidas.

Los que saldrían peor parados serían los países insulares, como las Maldivas, en el océano Indico y las islas Marshall, en el Pacífico. Una subida de sólo medio metro en el nivel del mar dañaría sus costas y reduciría sus reservas de agua potable. Una subida de tres metros las sumergiría completamente bajo el agua.

La mitad de la humanidad vive en zonas costeras, por lo que buena parte de la población es vulnerable a la subida de los niveles del mar. En los países situados en tierras bajas, como Bangladesh, donde ya se está experimentando un incremento de las inundaciones, una subida de 1 metro en el nivel del mar provocaría una pérdida del 7% del territorio, con efectos directos sobre 6 millones de personas. La población del delta del Nilo, en Egipto, afrontaría problemas parecidos.

El incremento de las temperaturas de la Tierra influirá igualmente sobre los patrones de precipitación. El IPCC observa que las precipitaciones aumentaron entre un 0,5% y un 1% por década, a lo largo del siglo XX, en las latitudes medias y altas de los continentes del hemisferio norte. Las lluvias torrenciales han aumentado entre un 2 % y un 4 % en estas mismas regiones y algunos científicos predicen un aumento de la intensidad y la violencia de las tormentas, sobre la base de los modelos informáticos más recientes. El IPCC prevé que un incremento del oleaje en la costa debido a las tormentas podría multiplicar el número de personas afectadas por las inundaciones.

Imaginemos lo que sucedería si las zonas interiores de Canadá tuvieran el clima que se da actualmente en el interior de Illinois, o si Nueva York tuviera el clima de Miami Beach, Florida. Las poblaciones humanas podrían desplazarse hacia el norte, pero es posible que las plantas y los animales no pudieran hacerlo con la rapidez necesaria para mantenerse dentro de su margen de temperaturas. La imposibilidad de adaptación hará que muchos ecosistemas desaparezcan o se vean reemplazados por nuevos regímenes.

Sir John Houghton, copresidente del IPCC y presidente de la Comisión Real sobre Contaminación Ambiental del Reino Unido, señala que los microorganismos, las plantas y los animales de todo el mundo han evolucionado y se han adaptado a unas zonas climáticas específicas. «Los cambios climáticos —dice Houghton— alteran la sostenibilidad de una región para diferentes especies y modifican su competitividad dentro de un ecosistema, de modo que incluso unos cambios relativamente pequeños en el clima tendrán como resultado, con el tiempo, cambios importantes en la composición de un ecosistema.» Naturalmente, los cambios de temperatura en los que está pensando Houghton tendrían lugar a lo largo de miles de años. El calentamiento global, en cambio, amenaza con alterar las condiciones climáticas en menos de un siglo. Según Houghton, «la mayoría de los ecosistemas no pueden responder o emigrar a esa velocidad». Cuando aumenta el desajuste entre el ecosistema y el clima, el ecosistema se vuelve más vulnerable a las enfermedades, las plagas y otras agresiones.

El hecho de que los árboles sean particularmente longevos y necesiten mucho tiempo para reproducirse hace que los bosques resulten especialmente vulnerables a las alteraciones, las desestabilizaciones y las plagas cuando se producen cambios rápidos de temperatura. Los árboles cubren una cuarta parte de la superficie de la Tierra y dan cobijo a una parte importante de sus criaturas, por lo que cualquier cambio en la temperatura global tendría un efecto significativo sobre los ecosistemas forestales del planeta.

Un grupo de investigadores que ha estado trabajando en Costa Rica durante los últimos dieciséis años ha registrado un descenso constante en la tasa de crecimiento de los árboles en la selva correlativo al aumento de las temperaturas locales. Deborah A. Clark, investigadora de la Universidad de Missouri, informa de que «los árboles tropicales se hallan cada vez más presionados por el incremento de las temperaturas nocturnas». Este aumento de las temperaturas nocturnas obliga a los árboles a respirar más y a generar más CO2. Estos datos preocupan a los científicos. Los investigadores que se dedican a estudiar los efectos del aumento de la temperatura sobre el descenso de la tasa de crecimiento de los árboles observan que las selvas tropicales absorben una tercera parte del CO2 liberado en la atmósfera por la fotosíntesis. Si continúan aumentando los niveles de CO2 que generan los árboles como subproducto de su respiración y se agrava el desequilibrio entre la cantidad que absorben y la que producen, podría ocurrir que el exceso de CO2 liberado en la atmósfera provocara un incremento en la temperatura global significativamente superior al que se prevé actualmente. Peter Cox, miembro de la Oficina Británica de Meteorología de Bracknell, afirma que en las próximas décadas seremos testigos de una defoliación masiva en el Amazonas como resultado de una mayor presión térmica y de la liberación de millones de toneladas de CO2 en la atmósfera. El virtual colapso y extinción de uno de los principales desagües de CO2 de la Tierra podría hacer que las temperaturas subieran un grado más a lo largo del siglo, en opinión de John Mitchell, miembro también de la Oficina Británica de Meteorología.

El aumento global de las temperaturas no sólo trae desgracias, señalan los climatólogos. Dicho aumento ya está teniendo efectos positivos sobre el crecimiento forestal en latitudes más frías. Los datos recogidos por los satélites de la NASA muestran una tendencia hacia el verde en algunas partes de Canadá, el norte de Estados Unidos, el norte de Europa, Rusia y Asia Central. La llegada anticipada de la primavera está teniendo como efecto una ampliación de la estación fértil de hasta doce días en Norteamérica y dieciocho en Europa y Asia, con el resultado de que el entorno es entre un 8 y un 18% más verde en estas regiones. Se desconoce aún si el aumento de la vegetación en las latitudes nórdicas puede compensar el descenso de la tasa de crecimiento en las regiones tropicales, en términos de equilibrio entre absorción y generación de CO2.

El cambio climático también va a tener un efecto significativo sobre la «distribución, el tamaño y la densidad de las poblaciones, y el comportamiento de la fauna animal», según los autores del IPCC. El calentamiento de los océanos, por ejemplo, significará una reducción de los hábitats de los peces de agua fría y una expansión de las zonas aptas para los peces de agua caliente. Los grandes arrecifes coralinos de los océanos, uno de los sistemas vivos más frágiles que existen, podrían desaparecer por completo en menos de cincuenta años, según un informe publicado por el Centro de Biología Marina de las Universidades de Glasgow y Londres, como consecuencia de la decoloración masiva provocada por el aumento de las temperaturas oceánicas. Los autores del informe aseguran que a medida que aumenten las temperaturas marinas se extinguirán las plantas microscópicas que constituyen el alimento de los animales coralinos y la desestabilización de este delicado equilibrio provocará a su vez la muerte del coral. Dicho proceso ya ha comenzado y afecta a los arrecifes coralinos de todo el mundo. Rupert Orman, miembro del Centro de Biología Marina, afirma que «dentro de cincuenta años quedará bien poco de los corales».

La intensificación de las sequías es otro factor destacado en los modelos informáticos más recientes. Un estudio realizado por el Centro de Investigaciones Tyndall sobre el Cambio Climático de la Universidad de East Anglia, en Norwich, Reino Unido, predice que la temperatura podría aumentar hasta 5 °C en algunos países durante las próximas décadas. Algunos países asiáticos corren un riesgo especial, entre ellos Kazajstán, Uzbekistán, Tayikistán, Afganistán e Irán. Algunos países africanos, entre ellos Etiopía, Sierra Leona y Tanzania, corren también el riesgo de ver cómo empeoran las sequías, que ya han asolado sus territorios en los últimos tiempos.

Donde más se dejarán sentir los efectos del calentamiento global será en la producción agrícola. Bill Easterling, profesor de geografía y agronomía de la Universidad Estatal de Pennsylvania y uno de los principales autores del informe del IPCC, afirma que sí el incremento de las temperaturas en las latitudes nórdicas fuera moderado —apenas unos pocos grados— se traduciría en una mejora de las cosechas. Sin embargo, si superara los 1,6 °C tendría graves efectos en lugares como Estados Unidos, donde la «producción agrícola comenzaría a disminuir rápidamente». En los trópicos y en el hemisferio sur, donde los cultivos se hallan ya al límite de su tolerancia al calor, los efectos del calentamiento global se dejarían sentir antes y serían más pronunciados y duraderos. Los países en vías de desarrollo del hemisferio sur, presionados por el crecimiento demográfico, van a tener cada vez más problemas para alimentar a su propia población y, más aún, para generar excedentes que enviar a los mercados mundiales.

Tal vez el mayor de los imponderables sea el efecto que va a tener el aumento de las temperaturas sobre la migración de las enfermedades hacia regiones del mundo en las que anteriormente no se encontraban presentes. Sobre la base de los resultados presentados por sus modelos predictivos, los autores del informe del IPCC dicen que «hay motivos para esperar» un «aumento neto del marco geográfico potencial de transmisión de la malaria y el dengue». La malaria encuentra su medio idóneo en climas cálidos y húmedos y se transmite a través de los mosquitos. Más de 350 millones de personas se contagian y dos millones mueren cada año como consecuencia de esta enfermedad, sobre todo en el hemisferio sur.

También es probable que otras enfermedades como la encefalitis vírica se extiendan más al norte a medida que encuentren temperaturas adecuadas para su desarrollo.

El peor de los escenarios posibles

Debe señalarse que las proyecciones actuales sobre el calentamiento global y sus consecuencias ecológicas, económicas y sociales se basan en el supuesto de una subida constante de las temperaturas, distribuida de forma más o menos regular a lo largo del siglo XXI. Pero este supuesto puede ser erróneo. La Academia Nacional de Ciencias de Estados Unidos [U.S. National Academy of Sciences, NAS] emitió en 2002 un informe auténticamente escalofriante en el que planteaba la posibilidad de que las temperaturas de la Tierra aumentaran de forma rápida y drástica en tan sólo unos pocos años y dieran lugar a un nuevo régimen climático prácticamente de la noche a la mañana.

Los autores del informe señalan que durante los últimos 100.000 años se han producido varios cambios climáticos bruscos, cuyos efectos se han dejado sentir durante mucho tiempo. Por ejemplo, a finales del período Younger Dryas, hace aproximadamente 11.500 años, «el clima global cambió drásticamente, en muchas regiones entre el 30% y el 50% de la diferencia existente entre las condiciones de la edad glacial y las actuales, y lo hizo en la mayoría de los casos en pocos años».

Según este estudio, «los cambios abruptos de clima se producen cuando el sistema climático se ve forzado a atravesar cierto umbral que dispara una transición hacia un nuevo estado a un ritmo que viene determinado por el propio sistema climático, y no por la causa». Por otro lado, los datos paleoclimáticos muestran que «los cambios más drásticos en el clima se han producido en momentos en los que se estaban produciendo cambios en los factores que controlan el clima». Si tenemos en cuenta que a lo largo del presente siglo se espera que se duplique la cantidad de CO2 emitida a la atmósfera como consecuencia de la actividad humana —en especial por la quema de combustibles fósiles—, podrían darse las condiciones para un cambio abrupto en el clima en todo el mundo, tal vez en unos pocos años. Según escriben los autores del informe de la NAS:

Tanto las tendencias actuales como las previsiones para el próximo siglo indican que las medias y las variables climáticas alcanzarán niveles desconocidos en los registros instrumentales o en la historia geológica reciente. Dichas tendencias podrían hacer que el sistema del clima atravesara el umbral hacía un nuevo estado climático.

Lo verdaderamente inquietante es que según el comité de la NAS sólo hace falta una ligera desviación en las condiciones límite o una pequeña fluctuación aleatoria en algún lugar del sistema para «provocar grandes cambios [...] cuando el sistema se halla cerca del umbral»

Un cambio abrupto en el clima como el que se produjo durante el período Younger Dryas podría tener efectos catastróficos para los ecosistemas y las especies de todo el mundo. Durante este período en concreto, por ejemplo, las piceas, los abetos y los abedules del sur de Nueva Inglaterra se extinguieron en menos de cincuenta años. Las extinciones masivas de caballos, mastodontes, mamuts y tigres dientes de sable que se produjeron entonces en Norteamérica superaron a cualquier otro proceso de extinción conocido en millones de años.

El comité dibuja un posible escenario de pesadilla en el que una serie de estímulos casuales provocarían un cambio de régimen climático y desatarían el caos y la destrucción en todo el mundo. Los ecosistemas entrarían en crisis, los bosques se verían diezmados por incendios devastadores y las praderas se secarían y quedarían reducidas a desiertos de polvo. La vida animal podría llegar a desaparecer, mientras que las enfermedades que se transmiten por el agua, como el cólera, y las enfermedades de transmisión vectorial del tipo de la malaria, el dengue y la fiebre amarilla podrían extenderse incontroladamente más allá del radio de influencia del agente transmisor y poner en peligro la salud de la población humana en todo el mundo.

La NAS concluye su informe con una terrible advertencia:

Tomando los registros paleoclimáticos como base de inferencia, es posible que los cambios previstos no se produzcan siguiendo una evolución gradual proporcional a las concentraciones de gas de efecto invernadero, sino por medio de una sucesión de cambios abruptos de régimen que afectarían a regiones subcontinentales enteras o incluso mayores [...] quitar relevancia o negar la posibilidad de que se repitan los cambios abruptos que se dieron en el pasado podría resultar caro.

Durante los últimos años se ha hecho cada vez más evidente que la era de los combustibles fósiles tiene rostro de Jano. Resulta imposible resumir en una lista las extraordinarias ventajas que comporta el uso del carbón, el petróleo y el gas natural. Baste decir que entre ocho y diez generaciones de seres humanos residentes en Europa, Norteamérica, Japón y otros lugares del mundo —aquellos que han vivido desde la extensión del uso del carbón y de la tecnología del vapor hasta la actualidad— han disfrutado de enormes beneficios derivados de la explotación de estos incomparables recursos no renovables. Hemos desenterrado los restos orgánicos de una era geológica previa y nos hemos bañado en la opulencia material que hacía posible su energía. En la actualidad se está produciendo una convergencia de factores que podría disparar una cadena de acontecimientos de grandes proporciones históricas. Hemos alcanzado el punto crítico al que han tenido que enfrentarse muchas civilizaciones del pasado, unas con éxito y otras no. Se trata del momento en que la energía necesaria para mantener la civilización en funcionamiento se hace cada vez más escasa y difícil de obtener y en que también aumentan los costes de absorber los residuos acumulados y las externalidades generadas por las actividades del pasado. Cuando se supera este punto, se produce un descenso en la circulación de energía, el funcionamiento de los muchos subsistemas que conforman la sociedad se hace más lento y se debilita el tejido institucional, económico y social, todo lo cual hace que la estructura operativa en conjunto resulte más vulnerable ante las amenazas externas y la posibilidad de un colapso interno.

Las decisiones que toman las civilizaciones en este «punto de inflexión» de su régimen energético determinan si tendrán éxito o no a la hora de reorganizar sus sistemas y experimentar un renacimiento, o si deberán enfrentarse a un proceso de deterioro progresivo y desintegración de su infraestructura que llevará finalmente a la muerte y la descomposición de la sociedad. La civilización del petróleo, el régimen energético más poderoso de la historia de la humanidad, se halla apenas a unos pocos años de distancia del «punto de inflexión». La convergencia de tres tendencias básicas hará que la sociedad se vea pronto obligada a tomar una serie de decisiones acerca de su futuro. La interacción entre el inminente descenso de la producción global de petróleo, la concentración de las reservas restantes de crudo en Oriente Medio, la región más inestable del planeta desde el punto de vista político y social, y el proceso de calentamiento de la atmósfera del planeta provocado por el gasto energético —o entropía— acumulado a lo largo de la era industrial, ha dado pie a una dinámica mundial inestable y peligrosa, cuyo resultado resulta en buena medida imprevisible.

La civilización de los hidrocarburos se encuentra bajo asedio. Nuestra capacidad de dar respuesta a las tres amenazas que se presentan ante nuestras puertas depende, en parte, de lo vulnerable que sea la actual infraestructura ante las agresiones, los trastornos y el deterioro progresivo. En este sentido, las previsiones son malas. La compleja y centralizada infraestructura que hemos creado para gestionar la economía de los combustibles fósiles —antes nuestro principal activo— se está convirtiendo rápidamente en la mayor de nuestras cargas. Somos cada vez más vulnerables ante las amenazas y los trastornos, tanto internos como externos, lo que convierte este momento de la era postindustrial en uno de los más precarios de la historia.

Capítulo 7

VIVIR BAJO UN TEJADO DE VIDRIO

El 11 de septiembre de 2001, diecinueve terroristas árabes se hicieron con el control de cuatro aviones comerciales estadounidenses en pleno vuelo, armados tan sólo con cúters. Dos de los aviones colisionaron contra las torres gemelas del World Trade Center y derribaron ambos edificios, clasificados como los terceros más altos del mundo. El tercer avión alcanzó el Pentágono, en Washington, y el cuarto cayó en Pennsylvania cuando los pasajeros iniciaron una violenta pelea por recuperar el control del aparato.

Fue el peor ataque terrorista ocurrido en suelo estadounidense en sus 225 años de historia. Murieron más de 3.000 personas en los aviones y los edificios. Más tarde supimos que quince de los secuestradores eran saudíes vinculados a una secta islámica paramilitar radical llamada Al Qaeda, una red dirigida por el terrorista saudí Osama bin Laden, responsable de los anteriores ataques contra las embajadas estadounidenses en África y contra el destructor Cole en Yemen, así como del atentado con bomba de 1993 contra el World Trade Center.

Bin Laden había luchado en los años ochenta junto a los otros «guerreros de la libertad» árabes en Afganistán en la guerra santa para expulsar a las tropas soviéticas de aquel país musulmán. Sus iras no se dirigieron hacia Estados Unidos hasta los años noventa, durante la Guerra del golfo Pérsico. Irak invadió Kuwait en agosto de 1990, en un intento de anexionarse sus ricos yacimientos de petróleo. Estados Unidos lideró una coalición mundial para repeler el ataque. 500.000 soldados norteamericanos fueron llamados al servicio y en enero de 1991 el ejército norteamericano, con la ayuda de 160.000 efectivos adicionales procedentes de otros países aliados —treinta y cuatro países se unieron a la coalición—, lanzaron un ataque masivo por tierra y aire que logró expulsar al ejército iraquí de Kuwait. 100.000 soldados iraquíes murieron en el conflicto, así como 148 soldados norteamericanos.

De gran importancia para la guerra fue el acuerdo alcanzado entre el gobierno de Estados Unidos y la familia real saudí para permitir que los aviones de combate norteamericanos pudieran usar los aeropuertos y el espacio aéreo saudí. La presencia militar estadounidense se mantuvo una vez terminada la guerra. Como ya se ha mencionado, Bin Laden y otros islamistas radicales estaban indignados por el hecho de que el gobierno saudí hubiera permitido que los «infieles» profanaran con su presencia la tierra santa del islam. Bin Laden declaró la jihad para expulsar a los norteamericanos de su país. Sus posteriores ataques contra instalaciones estadounidenses estaban orientados a conseguir este objetivo.

El 11 de septiembre Bin Laden hizo llegar por segunda vez su jihad a las costas estadounidenses, en una misión suicida dirigida contra la población civil y el personal militar. Por desgracia, el éxito de su plan superó sus más descabelladas expectativas. El país, conmocionado, respondió al ataque. El presidente George W. Bush realizó un llamamiento a la comunidad internacional para que sumara sus fuerzas en una guerra continuada contra el terrorismo. El gobierno estadounidense lanzó una serie de ataques aéreos contra el régimen islámico radical de los talibanes, que tenía el poder en Afganistán y que había dado refugio a Bin Laden y a sus terroristas. Los ataques aéreos fueron seguidos por una ofensiva terrestre liderada por los guerreros de la libertad afganos —la Alianza del Norte—, que luchaban desde hacía años por liberar su país del dominio talibán. En diciembre de 2001 cayó el régimen islamista y miles de soldados talibanes y guerreros de Al Qaeda fueron capturados. Muchos otros miembros de la red de Al Qaeda huyeron a Pakistán. Al mismo tiempo se puso en marcha un programa de vigilancia de las células terroristas de Al Qaeda, consideradas activas en cincuenta países. El gobierno de Estados Unidos y otros gobiernos del mundo llevaron a cabo una serie de arrestos y bloquearon los activos de organizaciones sospechosas de blanquear dinero para los terroristas. Sin embargo, la red continúa existiendo, al igual que otros grupos extremistas musulmanes, como Hamas, la Jihad Islámica y Hezbollah.

El ataque del 11 de septiembre contra las torres del World Trade Center forma parte de un continuo de acontecimientos que se inició hace una década con la Guerra del golfo Pérsico y cuyas raíces hay que buscar todavía más atrás, en los primeros años de la época colonial, cuando los gobiernos occidentales y las compañías petroleras, asociados con algunos jefes árabes, impusieron su presencia en Oriente Medio con el objetivo de asegurarse el suministro de petróleo crudo barato para alimentar la máquina industrial.

El acceso a las reservas petroleras de Oriente Medio ha sido un elemento central de la política exterior estadounidense a partir de la Segunda Guerra Mundial. Los británicos, los franceses, los alemanes, los holandeses y otras potencias europeas ya estaban presentes en la región desde antes. Durante buena parte de la Guerra Fría, Estados Unidos y la antigua Unión Soviética compitieron por conseguir una posición de dominio en el golfo Pérsico, rico en petróleo.

El coste militar de proteger los intereses petroleros de Estados Unidos en Oriente Medio ha sido enorme. Durante más de medio siglo, el ejército de Estados Unidos ha tenido que mantener flotas navales, campos de aviación y otras instalaciones y efectivos militares en el golfo Pérsico y sus alrededores para garantizar la seguridad de la circulación del petróleo desde esta región hasta Estados Unidos y otros países consumidores. La Guerra del golfo Pérsico marcó una especie de punto de inflexión. Por primera vez, Estados Unidos comenzó a gastar más dinero en proteger sus intereses vitales en aquella parte del mundo del que percibía en términos del valor del petróleo importado. Al igual que el Imperio Romano en sus estadios finales, los costes militares de mantener el flujo de la energía comenzaban a superar el valor neto de la energía conseguida. Habían comenzado los rendimientos marginales. Las cifras son escalofriantes.

Se estima que el gobierno de Estados Unidos gastó entre 61.000 y 71.000 millones de dólares en el despliegue y las operaciones de la Guerra del golfo Pérsico, unos fondos que deben añadirse a las cantidades de dinero ya invertidas en el equipamiento militar empleado en el conflicto. 54.000 millones de dólares provenían de las contribuciones de los aliados. La presencia militar estadounidense en el golfo Pérsico se ha mantenido desde entonces con 9.000 soldados en tierra, sobre todo en Arabia Saudí, y 15.000 en el mar. Cientos de aviones de guerra estadounidenses y docenas de barcos patrullan en la región y cada año 50.000 soldados toman parte en los ejercicios militares combinados. Se estima que el gobierno estadounidense gasta actualmente entre 30.000 y 60.000 millones de dólares anuales para defender sus intereses petroleros en Oriente Medio. Otros sitúan la cifra entre los 20.000 y los 30.000 millones de dólares.

Si añadimos los costes económicos de los ataques del 11 de septiembre a los costes actuales de mantener la presencia militar norteamericana en Oriente Medio y los nuevos costes generados por la guerra en Afganistán, las cifras comienzan a resultar aún más prohibitivas. Los economistas estiman que los costes de los daños físicos y de las tareas de limpieza derivados de los ataques del 11 de septiembre ascienden a 30.000 millones de dólares. Pero eso es sólo el principio. Se prevé que el comercio minorista de la ciudad de Nueva York pierda entre 4.000 y 5.000 millones de dólares adicionales en los doce meses siguientes al ataque y entre 7.000 y 13.000 millones de dólares en ingresos derivados del turismo a finales de 2003. El equipo de gobierno de Nueva York también está preocupado por las perspectivas a largo plazo de la ciudad. Nueva York ya es actualmente el área metropolitana más cara del país para las empresas y preocupa la posibilidad de que un aumento de los costes y de los inconvenientes derivados de mantener unos sistemas de seguridad aún mayores —sobre todo en el distrito financiero, considerado la zona cero en la guerra de los terroristas contra la presencia económica norteamericana en Oriente Medio y en el resto del mundo— podría disuadir a las empresas de instalarse en Nueva York y hacer que las firmas ya instaladas allí abandonaran la ciudad. Muchas compañías financieras que tenían oficinas en el World Trade Center y sus alrededores han experimentado graves pérdidas en personal e ingresos como consecuencia del ataque y de sus efectos posteriores, y están comenzando a preguntarse si vale la pena mantener sus operaciones en lo que actualmente se considera una zona de alto riesgo.

El ataque terrorista también tuvo un profundo impacto sobre la vida de los ciudadanos de Nueva York. Al estrés psicológico que representa vivir en una isla pequeña y densamente poblada con tan sólo unas pocas vías de escape a través de puentes y túneles hay que añadir el deterioro progresivo de la situación económica, que muchos piensan que durará años. Más de 109.000 ciudadanos perdieron su puesto de trabajo en sectores como las compañías aéreas, las agencias de viajes, los hoteles, los restaurantes y las agencias de alquiler de coches durante los dos meses siguientes al ataque, una cifra superior a la que habría provocado una simple recesión.

Las consecuencias económicas se han extendido más allá de Nueva York y han afectado al conjunto de la economía global. La empresa de previsión Macroeconomic Advisers estima que el ataque ha provocado más de 13.000 millones de dólares en pérdidas de capital tanto privado como público. Boeing, por ejemplo, despidió a más de 30.000 trabajadores en previsión de una fuerte caída de la demanda de aviones comerciales. Macroeconomic Advisers calcula que el ataque ha provocado una reducción de la actividad económica en el tercer trimestre de 2001 de hasta 24.000 millones de dólares.

El Instituto Milken, un centro de investigación económica de Santa Mónica, California, anuncia que a finales de 2002 se habrán perdido más de 1,8 millones de puestos de trabajo en Estados Unidos como consecuencia del ataque terrorista. Aparte de Nueva York, otras ciudades que conocerán pérdidas importantes según el estudio del Instituto son Los Angeles, Chicago y Las Vegas.

En ningún lugar se han dejado sentir tanto las consecuencias negativas de los ataques del 11 de septiembre como en la cuenta de pérdidas de la industria del turismo. Dicha industria es una de las más importantes del mundo. En el año 2000 generó unos ingresos de 4,5 billones de dólares, lo que representa el 11% del PIB mundial y el 8,2% de los puestos de trabajo en todo el mundo (207.062.000 puestos de trabajo). El Consejo Mundial de Viajes y Turismo espera un descenso de entre el 10 y el 20% en la actividad turística en Estados Unidos en los doce meses siguientes al ataque, cifras que serán más bajas para otros países. Un descenso del 10% en Estados Unidos representaría unas pérdidas de 48.000 millones de dólares, mientras que si el turismo descendiera hasta un 20% alcanzarían los 96.000 millones de dólares. El consejo afirma que el descenso del 10% que probablemente experimentará la actividad turística en todo el mundo significará una caída del 1,7% en el PIB mundial y una pérdida de 8,8 millones de puestos de trabajo.

El ataque también tuvo un coste elevado para los contribuyentes norteamericanos. La guerra de Afganistán costó alrededor de 1.000 millones de dólares mensuales y se espera que el próximo año el gobierno de Estados Unidos destine 28.000 millones de dólares más para inversiones relacionadas con las actividades militares en la región. Por otro lado, Estados Unidos ha prometido una ayuda de 300 millones de dólares al nuevo gobierno de Afganistán para contribuir a la reconstrucción de este país devastado por la guerra. Además del incremento de los gastos militares que exige el mantenimiento de la guerra contra el terrorismo en Afganistán y en el resto del mundo, los contribuyentes han financiado una serie de ayudas para las familias de las víctimas de los ataques de Nueva York y Washington que alcanzan los 1,65 millones de dólares de media y cuyo valor total se estima en 6.000 millones de dólares. El gobierno federal también se ha comprometido a aportar un total de 11.200 millones de dólares para contribuir a la recuperación de la ciudad de Nueva York, así como 15.000 millones de dólares para contrarrestar la crisis de la industria aérea.

Por último, la nueva oficina federal de Seguridad Nacional, cuya misión «consistirá en desarrollar y coordinar la implementación de una estrategia integral a escala nacional para garantizar la seguridad de Estados Unidos frente a las amenazas o los ataques terroristas», ha invertido más de 19.000 millones de dólares de fondos federales en sus actividades durante 2002 y estaba previsto que recibiera 37.000 millones de dólares más para financiar sus programas en 2003. A todo ello debe añadirse una partida de emergencia de 3.500 millones de dólares destinada al ejército —además de los fondos ya incluidos en su presupuesto— para contribuir a la campaña militar contra Osama bin Laden y la red de Al Qaeda. La Asociación Nacional de Gobernadores estimó que harían falta otros 4.000 millones de dólares a nivel estatal para financiar el incremento de los costes de seguridad tras el ataque.

No hay que olvidar que todas estas pérdidas materiales y de vidas humanas que han sufrido Estados Unidos, la economía mundial y los contribuyentes son el resultado de una misión suicida de tan sólo diecinueve radicales islámicos armados con cúters y movidos por su profundo odio en relación con el papel que desempeña Estados Unidos en el mundo y, sobré todo, por su presencia militar en Arabia Saudí y Oriente Medio. La lección que debemos aprender es que en una sociedad urbana, densa y altamente compleja —como el área metropolitana de Nueva York— la capacidad que tiene un pequeño grupo de terroristas para sembrar el caos, la muerte y la destrucción masiva es alta.

El bioterrorismo

La vulnerabilidad de las poblaciones humanas agrupadas en entornos de alta densidad se hizo evidente dos semanas después de los ataques a las torres gemelas del World Trade Center y al Pentágono, cuando comenzaron a aparecer una serie de cartas que contenían una cepa mortal de ántrax en las redacciones de los medios periodísticos nacionales y poco después en las oficinas del Congreso, en la capital del país. El ántrax llegaba hasta sus víctimas —en forma de un polvo blanco— a través de la red de correos estadounidense. Dieciocho ciudadanos estadounidenses se vieron expuestos a las esporas de ántrax y cinco murieron como consecuencia de la inhalación del agente patógeno. Se encontraron cartas con mensajes terroristas que contenían alabanzas a Alá y condenas a la política del gobierno estadounidense.

Pocos días antes, el FBI había informado de que algunos de los secuestradores responsables de los ataques del 11 de septiembre habían realizado una serie de visitas en las semanas previas a unos hangares de Florida que albergaban aviones fumigadores. Según los propietarios, los secuestradores hicieron preguntas acerca de la capacidad de carga, el alcance y el funcionamiento de estas avionetas especializadas. A raíz de ello, el FBI ordenó que los 3.500 aviones fumigadores privados del país se quedaran en tierra hasta disponer de más información. Mientras tanto, universidades como las de Michigan, Penn State, Clemson y Alabama prohibieron el vuelo de aviones sobre sus estadios durante los partidos de fútbol, por miedo a posibles ataques con armas biológicas. En Washington, los políticos se apresuraron a calmar la creciente ansiedad pública con una serie de inversiones destinadas a aumentar las reservas de antibióticos y vacunas y a poner al día los programas de emergencia de los hospitales y clínicas del país.

Hasta la fecha, las autoridades federales aún no están seguras de si los envíos de ántrax por correo están relacionados con la red terrorista Al Qaeda. Lo que sí saben es que la red terrorista de Osama bin Laden ha tratado activamente de obtener información sobre los agentes biológicos, incluida la forma de obtenerlos, de producirlos a gran escala, de incrementar su capacidad letal con la ayuda de avanzadas bases de datos biológicas y de dispersarlos sobre grandes áreas geográficas.

El ataque terrorista con ántrax pone de relieve lo mal preparados que están Norteamérica y el resto del mundo contra esta clase de ataques y, en especial, lo vulnerables que son las grandes poblaciones urbanas ante esta forma especialmente letal de hacer la guerra. Incluso cuando el ántrax volvía a disparar las alarmas de la nación, tanto los políticos y los expertos militares como los medios de comunicación evitaron hablar de una realidad todavía más inquietante que se encuentra detrás de los nuevos temores que despierta el bioterrorismo. La nueva información genómica que está siendo descubierta y aplicada a la ingeniería genética con fines comerciales en los campos de la agricultura, la cría de ganado y la medicina puede contribuir potencialmente al desarrollo de una amplia gama de nuevos agentes patógenos susceptibles de atacar a poblaciones vegetales, animales y humanas.

Además, los materiales y los instrumentos necesarios para producir agentes para la guerra biológica son baratos y fácilmente accesibles, a diferencia de las bombas nucleares, lo cual explica por qué se habla a menudo de esta arma como «la bomba nuclear de los pobres». Se puede construir y hacer operativo un laboratorio biológico de última generación con unos equipos que se pueden comprar en cualquier tienda especializada por sólo 10.000 dólares e instalarse en una habitación de apenas 1,5 metros cuadrados. Todo lo que realmente se necesita es un fermentador de cerveza, un cultivo de base proteínica, prendas plásticas y una máscara de gas. Igualmente aterrador es el hecho de que miles de biólogos moleculares y alumnos de posgrado que trabajan en laboratorios de todo el mundo tengan conocimientos suficientes sobre el uso rudimentario de las tecnologías de recombinación del ADN y de la clonación como para diseñar y producir masivamente este tipo de armas.

La guerra biológica (GB) supone el uso de organismos vivos para fines militares. Las armas biológicas pueden consistir en virus, bacterias, hongos, rickettsias y protozoos. Los agentes biológicos pueden mutar, reproducirse, multiplicarse y propagarse a lo largo de un extenso territorio geográfico a través del viento, el agua, los insectos, los animales y los seres humanos. Una vez liberados, muchos agentes patógenos biológicos son capaces de desarrollar nichos viables y mantenerse indefinidamente en el medio.

Las armas biológicas no se han usado nunca de forma extensiva a causa del peligro y los gastos que conlleva el procesamiento y el almacenaje de grandes cantidades de materiales tóxicos, así como de la dificultad de controlar la diseminación de los agentes biológicos. Sin embargo, los avances en las tecnologías de ingeniería genética que se han producido durante la última década han conseguido que la guerra biológica sea viable por primera vez.

Hay muchas formas de crear «armas de diseño» de ADN recombinante. Las nuevas tecnologías se pueden usar para programar genes en microorganismos infecciosos y aumentar así su resistencia, su virulencia y su estabilidad medioambiental. Se pueden insertar genes en microorganismos que afecten a las funciones reguladoras que controlan el estado de ánimo, el comportamiento y la temperatura corporal. Los científicos afirman que podrían clonar toxinas selectivas para eliminar a determinados grupos raciales o étnicos, cuyo genotipo los hace vulnerables a determinados patrones de enfermedad. Si el objetivo consiste en dañar la economía de un país, la ingeniería genética también puede servir para destruir determinadas cepas o especies de cultivos agrícolas o de animales domésticos.

Las nuevas tecnologías de ingeniería genética proporcionan un tipo de armamento versátil aplicable a una amplia gama de fines militares, desde actividades terroristas y operaciones antiinsurrectivas hasta una guerra a gran escala contra poblaciones enteras.

Los observadores militares profesionales no muestran demasiado optimismo sobre la posibilidad de mantener la revolución genética fuera del alcance de los terroristas. Como herramienta de destrucción masiva, el armamento genético rivaliza con el armamento nuclear y puede ser desarrollado con una mínima parte de su coste. Por sí solos, estos dos factores convierten la tecnología genética en el arma ideal del futuro.

Cuando se supo que Irak almacenaba grandes cantidades de agentes patógenos para la guerra y que se preparaba para utilizarlos durante la Guerra del Golfo, se renovó el interés del Pentágono por el desarrollo de mecanismos defensivos, con el objetivo de contrarrestar una posible escalada en la carrera del armamento biológico. El gobierno de Saddam Hussein había desarrollado lo que llamaba el «gran igualador», un arsenal de veinticinco cabezas de misiles cargadas con más de 5.000 kilos de agentes biológicos, algunos de ellos mortales, como la toxina del botulismo y los gérmenes de ántrax. Se habían colocado otros 15.000 kilos de agentes patógenos en bombas que podían ser lanzadas desde aviones militares. Si se hubiera hecho uso de las armas biológicas, las consecuencias habrían sido tan terribles como las que se vieron en Hiroshima y Nagasaki tras el lanzamiento de las bombas atómicas en 1945. Para hacerse una idea del daño que podría haberse causado sólo hace falta comparar el arsenal iraquí con los cálculos de un estudio realizado por el gobierno de Estados Unidos en 1993: sólo con lanzar desde un avión 100 kilos de esporas de ántrax sobre Washington D.C. se podría provocar la muerte de hasta tres millones de personas.

Irak no es el único país interesado en desarrollar una nueva generación de armas biológicas. En un estudio realizado en 1995, la Central Intelligence Agency (CIA) informaba de que había diecisiete países sospechosos de desarrollar y almacenar agentes para la guerra patógena. Estos países eran Irak, Irán, Libia, Siria, Corea del Norte, Taiwan, Israel, Egipto, Vietnam, Laos, Cuba, Bulgaria, India, Corea del Sur, Sudáfrica, China y Rusia. Es probable que los terroristas, en colaboración con algunos países delincuentes, acudan a las nuevas armas genéticas para difundir el miedo y el caos con el objetivo de imponer sus demandas a la sociedad.

El nuevo director de Seguridad Nacional en Estados Unidos, el gobernador Tom Ridge, ha prometido que el bioterrorismo será la prioridad principal en sus esfuerzos por proteger al pueblo estadounidense. Su tarea será difícil, si no imposible. Por más mecanismos de defensa de que se disponga, poco se puede hacer por evitar que los bioterroristas rompan la red de seguridad y liberen patógenos letales capaces de matar a miles e incluso cientos de miles o millones de personas. Igual que los secuestradores árabes consiguieron poner de rodillas a la ciudad más poderosa del mundo, al menos durante algunos días, armados sólo con unos cúters, un grupo de bioterroristas armados con ántrax letal u otros patógenos, y con un cierto conocimiento de cómo usarlos, podrían causar un daño parecido y debilitar aún más la seguridad norteamericana y la economía mundial. Si un grupo de militantes islamistas provocara de forma coordinada varias epidemias simultáneas de viruela en diferentes partes del mundo, por ejemplo, con cepas genéticamente diseñadas para las que no hubiera antídoto, los efectos serían casi inimaginables. Millones de personas podrían morir en un ataque de este tipo, cuyas repercusiones económicas y sociales se dejarían sentir durante mucho tiempo.

Incluso si Osama bin Laden y los líderes de Al Qaeda mueren o son arrestados y su red queda desarticulada, pocos creen que ése pueda ser el fin de la historia. Cada vez son más los jóvenes musulmanes reclutados para las actividades terroristas en todo el mundo y la mayoría de los observadores no esperan otra cosa que una extensión e intensificación de los niveles de violencia. Ello plantea la necesidad de que Estados Unidos y otros gobiernos establezcan nuevas medidas de seguridad para defender sus intereses vitales en Oriente Medio y asegurar su seguridad interna. Es probable que el proceso se acelere a medida que el mundo, en algún momento de los próximos diez años, se vaya acercando a la cima de la curva de la producción global de petróleo. Mientras tanto, la escalada de la violencia y los crecientes costes militares para garantizar la seguridad de la circulación del petróleo de Oriente Medio seguirán teniendo como resultado un descenso en los ingresos de Estados Unidos y de las principales naciones industriales.

Los puntos débiles

La escalada del terrorismo en territorio estadounidense está haciendo que los norteamericanos tomen conciencia, por primera vez, de lo vulnerable que es el país ante las amenazas dirigidas contra su infraestructura y su población. La Casa Blanca, el Congreso y el Pentágono dirigen ahora su atención hacia los muchos puntos débiles que existen actualmente y que plantean una seria amenaza para la seguridad norteamericana. Lo que están descubriendo es que nuestra infraestructura basada en los combustibles fósiles es vulnerable a posibles fallos e interferencias en una serie de puntos críticos. Si la circulación energética se viera bloqueada en alguno de estos puntos durante un largo período de tiempo podría llegar a paralizar el conjunto del sistema.

Hay tres circunstancias que hacen que la época actual sea mucho más vulnerable que las épocas históricas pasadas. En primer lugar, el salto cualitativo en la producción agrícola conseguido gracias a la aplicación de pesticidas y fertilizantes derivados de los combustibles fósiles a la producción agrícola, y la correlativa sustitución del trabajo humano por el trabajo mecanizado en las granjas, ha permitido que millones de personas pudieran emigrar desde las zonas rurales hacia las urbanas. En segundo término, el siglo XX ha sido testigo del mayor salto demográfico de la historia, gracias en parte a los inmensos excedentes de la producción agrícola moderna. En tercer lugar, la energía de los combustibles fósiles ha permitido a la humanidad construir un sistema de transporte muy potente y una red eléctrica capaz de conectar a millones de personas en una matriz estrechamente organizada e interdependiente, cuyo funcionamiento es esencial para la supervivencia de todas las personas. Así pues, los puntos críticos de la actual arquitectura social son los siguientes: el sistema agrícola basado en la industria petroquímica; la forma de vida urbana y masificada; el modelo de transporte impulsado con combustibles fósiles que permite la rápida circulación de pasajeros y cargas entre las comunidades rurales, urbanas y suburbanas, así como a través de continentes y océanos; y, por último, la red eléctrica, el «sistema nervioso central» que proporciona potencia, luz y calor y hace posible una red de telecomunicaciones que coordina el funcionamiento de todos los subsistemas que integran el conjunto del organismo. Todos estos sistemas vitales que constituyen el superorganismo que conocemos como la economía industrial de la era de la información existen únicamente gracias al flujo continuado del petróleo y, en menor medida, del carbón y el gas natural.

Cuando aparece el riesgo de que se produzca un desabastecimiento de petróleo crudo barato, casi todo el mundo se preocupa inmediatamente por no tener suficiente gasolina para sus automóviles. La posibilidad de que la flota de coches, camiones, autobuses y aviones del país pudiera quedar frenada o paralizada debido a una escasez de petróleo resulta bastante alarmante y tendría sin duda un efecto devastador sobre la economía y la sociedad. Sin embargo, a menudo se pasa por alto el hecho de que tanto el petróleo como el gas natural son igualmente cruciales a la hora de mantener tanto la producción agrícola como el sistema eléctrico que nos proporciona potencia, luz y calor. Si llegaran a fallar estos sistemas, el organismo social en su totalidad entraría en colapso.

Cultivar a base de petróleo

La agricultura proporciona la energía básica para mantener al conjunto de la sociedad industrial en un estado de desequilibrio. La moderna agricultura depende casi exclusivamente del petróleo. Si éste fuera más escaso, caro y difícil de obtener, todos los demás aspectos de la vida contemporánea entrarían en crisis.

La producción alimentaria moderna hace posible todas las demás economías que se sitúan por encima de ella. Los enormes excedentes alimentarios y la liberación de la mano de obra campesina hicieron posible la revolución industrial del siglo XX y, más tarde, el paso a la economía de los servicios y de la información. Pero por más que la producción agrícola se haya multiplicado durante la última mitad de siglo y los precios de los alimentos se hayan mantenido relativamente bajos y estables durante este período, todo eso podría cambiar de la noche a la mañana cuando la producción global de petróleo toque techo y los precios del crudo se disparen.

En la actualidad, el 4 % de toda la energía consumida en Estados Unidos se destina a la producción de alimentos. Aproximadamente entre un 10 y un 13% del consumo total de energía en Norteamérica se dedica al transporte, el procesamiento y la distribución de los mismos en los supermercados. Eso significa que más del 17% de la energía se va en llevar la comida a nuestra mesa.

Muchos antropólogos consideran que el gran aumento de la producción agrícola es el principal éxito de la era moderna. Dicho éxito ha sido posible gracias a la sustitución del trabajo humano por el trabajo mecánico —basado en el petróleo— y por el uso creciente de fertilizantes petroquímicos y pesticidas para maximizar la producción.

El primer tractor de gasolina fue construido por John Froehlich en 1892. En 1910 había 25.000 tractores en las granjas norteamericanas. Cuando Henry Ford introdujo el Fordson, un modelo barato y fiable, las ventas de tractores se multiplicaron. En vísperas de la Segunda Guerra Mundial había 1,6 millones de tractores en funcionamiento y en los años sesenta su número había subido hasta 4,7 millones. También aumentó el uso de camiones. En 1915 sólo había 25.000 camiones en las granjas. En 1980 había más de 3,5 millones de camiones en funcionamiento. El motor de gasolina aplicado a los tractores, los camiones y las máquinas cosechadoras, en menos de un siglo, ha reemplazado a la fuerza humana y a la de los caballos, muías y bueyes como fuente primaria de energía en la granja.

La revolución mecánica que experimentó la agricultura durante la primera mitad del siglo XX fue seguida en la segunda mitad por una revolución química. Entre 1950 y 1989, el uso de fertilizantes inorgánicos nitrogenados derivados de productos petroquímicos aumentó desde los 13 millones hasta los 130 millones de toneladas. El uso de pesticidas derivados del petróleo pasó de los 90.000 kilos en 1950 a más de 2.900 millones de kilos en 1986.

La mecanización de la agricultura y el uso de pesticidas y fertilizantes nitrogenados derivados del petróleo, junto con los avances en las técnicas de cultivo y la introducción de monocultivos altamente productivos, han disparado la producción alimentaria en el curso del siglo XX, al tiempo que han reducido la cantidad de trabajo humano requerido en la granja. En 1850, el 60% de la mano de obra estadounidense trabajaba en el campo. En la actualidad, menos del 2,7% de los trabajadores norteamericanos se dedica directamente a la agricultura. Al mismo tiempo, la producción agrícola se ha multiplicado extraordinariamente. En 1850, un solo agricultor generaba alimentos suficientes como para mantener a cuatro personas. En la actualidad, un solo agricultor genera alimentos suficientes como para mantener a setenta y ocho personas. La productividad agrícola aumentó un 25 % en los años cuarenta, un 20% en los cincuenta, un 17% en los años sesenta y más del 28% en la década de los ochenta.

Sin embargo, la mejora de la productividad y de las cosechas se ha realizado a costa de un constante aumento de la cantidad de petróleo consumido en el proceso. Desde una perspectiva termodinámica, la agricultura moderna es la menos productiva de la historia, es decir, consume mucha más energía por unidad de energía producida que en ningún otro período anterior. Un campesino tradicional produce alrededor de 10 calorías por cada caloría gastada. Un granjero de Iowa, empleando la tecnología más avanzada disponible, puede producir 6.000 calorías por cada caloría de trabajo humano, pero su hazaña resulta menos impresionante si calculamos la cantidad de energía consumida para generar los beneficios energéticos netos. Para producir una lata de cereales de 270 calorías, el granjero consume la ingente cantidad de 2.790 calorías para mantener la maquinaria en funcionamiento y obtener los fertilizantes y los pesticidas. Así pues, por cada caloría de energía producida, el tecnificado granjero norteamericano termina consumiendo diez calorías de energía en el proceso.

El aumento del flujo energético también ha tenido como consecuencia una mayor entropía en el medio ambiente. A medida que la base del suelo se ha ido agotando y erosionando como consecuencia de las prácticas agrícolas intensivas, se ha tenido que aumentar la cantidad de fertilizantes sintéticos empleados para mantener la producción. La contaminación con nitratos derivada de los fertilizantes es responsable de la mitad de la contaminación actual del agua y de dos tercios de nuestros residuos sólidos. La práctica del monocultivo —aplicar un solo tipo de cultivo sobre una gran superficie— ha generado economías de escala y ha aumentado la productividad y los beneficios, pero ha exigido el empleo de mayores cantidades de pesticidas. Las formas agrícolas más tradicionales se basaban en la diversificación de los cultivos que atraían a una amplia gama de insectos, algunos de los cuales eran enemigos naturales de las plagas. Abandonar los cultivos diversificados en favor del monocultivo eliminó los insectos beneficiosos de los campos e hizo que los cultivos fueran más vulnerables a las plagas de insectos, lo que ha llevado a un incremento constante en el uso de pesticidas. Buena parte de los pesticidas lanzados se filtran hasta las capas freáticas y se convierten en una fuente importante de contaminación del agua en todas las regiones agrícolas del mundo.

Los pesticidas también contribuyen a la degradación del suelo. Este contiene millones de bacterias microscópicas, hongos, algas y protozoos, así como gusanos y artrópodos. Dichos organismos mantienen la fertilidad y la estructura del suelo. Los pesticidas destruyen a estos organismos y sus complejos hábitats, con lo que aceleran el proceso de agotamiento y erosión del suelo. Los granjeros norteamericanos pierden anualmente más de 3.600 millones de toneladas del mantillo superficial, en buena medida por culpa de las prácticas agrícolas altamente tecnificadas introducidas durante la segunda mitad del siglo XX. En los años setenta, Estados Unidos había perdido más de un tercio de la tierra apta para la agricultura. El agotamiento y la erosión del suelo, a su vez, han exigido el uso de cantidades cada vez mayores de petróleo en forma de fertilizantes petroquímicos para mantener la producción agrícola. Los rendimientos marginales han comenzado. Cada vez debemos consumir más energía para obtener unas ganancias menores en producción energética neta. Entre 1945 y 1970, por ejemplo, los productores de cereales estadounidenses elevaron un 400% su consumo energético, pero sólo elevaron su producción un 138%. En las dos primeras décadas de intensa explotación agrícola basada en productos petroquímicos que vinieron después de la Segunda Guerra Mundial, el consumo total de energía en el sector agrícola aumentó un 70%, pero la producción alimentaria sólo creció un 30%. Además, la agricultura intensiva es uno de los principales factores que contribuyen al calentamiento global. La creciente mecanización ha supuesto un incremento del consumo de gasolina y de las emisiones de CO2, mientras que la mayor dependencia de los fertilizantes petroquímicos ha multiplicado las emisiones de óxido nitroso, un poderoso gas de efecto invernadero.

Los costes cada vez más altos que supone mantener una agricultura intensiva y tecnológicamente sofisticada han sacado del negocio a la mayoría de las pequeñas explotaciones familiares. La nueva agricultura requiere economías a escala y grandes inversiones de capital. El 38% de la producción agrícola estadounidense procede de menos de 32.000 grandes explotaciones.

La revolución química que se había producido en Estados Unidos y Europa se extendió en los años sesenta al mundo en vías de desarrollo, en lo que se conoce como la «revolución verde». Los países asiáticos experimentaron un crecimiento demográfico particularmente acelerado, en una región del mundo donde hacía tiempo que toda la tierra disponible estaba siendo ya explotada. Para responder a las crecientes demandas de la población, los científicos desarrollaron nuevas cepas de trigo y de arroz que multiplicaron la producción por hectárea. Estas variedades altamente productivas permitieron duplicar las cosechas en menos de una década en lugares como India y Pakistán, pero exigían grandes cantidades de fertilizantes petroquímicos y de pesticidas químicos. La extensión de los monocultivos, buena parte de los cuales estaban destinados a la exportación en los mercados mundiales más que al consumo interno, también proliferó gracias a unas economías de escala que aumentaban los costes de capital y tuvieron el efecto de borrar del campo a un gran número de explotaciones de subsistencia, para dejar espacio a las grandes explotaciones agrícolas. Los empobrecidos campesinos del Tercer Mundo se han visto obligados a emigrar a las grandes ciudades, donde muchos viven de la beneficencia pública o caen en la indigencia.

Hemos construido una infraestructura agrícola mundial basada en los combustibles fósiles, cuyo efecto a corto plazo de aumentar la producción ha permitido multiplicar tanto la población humana total como el número de personas que viven en las zonas urbanas. Ahora que nos acercamos al punto de inflexión en la curva de la producción global de petróleo, que se alcanzará probablemente en algún momento de la próxima década, y a una drástica subida de su precio en los mercados, ¿cómo vamos a mantener la producción agrícola necesaria para alimentar a una población humana en constante aumento durante los siglos XXI y XXII? Mientras todavía disfrutamos de petróleo relativamente barato y abundantes cosechas, la desnutrición afecta a casi mil millones de personas. Imaginemos el futuro que nos espera cuando la producción global de petróleo toque techo y los precios se disparen de forma irreversible.

Las empresas de biotecnología buscan la solución a este problema en nuevos cultivos genéticamente diseñados. Sin embargo, estos cultivos también exigirán grandes aportaciones de energía, sobre todo en forma de fertilizantes petroquímicos. Hasta el momento, los científicos no han conseguido desarrollar cultivos que sean capaces de absorber el nitrógeno del aire, y no del suelo. Por otro lado, los diversos estudios ofrecen resultados contradictorios en cuanto a la productividad. Las perspectivas de futuro para la agricultura resultan todavía más siniestras si tenemos en cuenta que el 11% de la superficie del planeta ya ha sido destinado a la producción agrícola, lo que deja escasas tierras aptas para el cultivo. En su desesperación, los seres humanos han comenzado a talar grandes extensiones de selvas tropicales en el Amazonas y en otros lugares del mundo para crear más espacio destinado a la producción agrícola. La destrucción de la selva elimina un hábitat crucial para muchas de las especies de plantas y animales que quedan en la Tierra. La propia base del suelo es demasiado delgada como para mantener la producción alimentaria más allá de unos pocos años. El resultado es una mayor erosión y una extensión de los terrenos estériles, inhabitables tanto para los seres humanos como para las plantas y los animales.

Para empeorar aún más las cosas, un tercio de las tierras cultivadas del mundo ha pasado de producir cereales para el consumo humano a producir cereales-pienso para la cabaña bovina y otras clases de ganado. La producción para el ganado es la actividad agrícola que más energía consume del mundo. En Estados Unidos se requiere el equivalente a 4 litros de gasolina para producir algo menos de medio kilo de carne alimentada con piensos. Para responder a la demanda anual de carne de una familia media de cuatro personas es necesario consumir más de 984 litros de combustibles fósiles. La quema de estos combustibles libera en la atmósfera 2,25 toneladas adicionales de CO2, la misma cantidad que emite el coche medio en 6 meses de funcionamiento normal.

Si los ricos consumidores de Occidente y otras partes del mundo estuvieran dispuestos a renunciar a las dietas ricas en carne y pasaran a la parte baja de la cadena alimentaria con una dieta en buena medida vegetariana, se podría liberar una buena cantidad de terrenos de cultivo para la producción de alimentos de los que podrían beneficiarse millones de personas. Pero el cambio de hábitos alimentarios en Occidente debería ir acompañado por una reforma agraria y otros cambios estructurales en el hemisferio sur que restituyeran el acceso de los pobres a la tierra y les permitieran cultivar el alimento necesario para sus familias y comunidades.

Los líderes políticos, los economistas y los consumidores están angustiados por la posibilidad de quedarse sin gasolina para los coches. Sin embargo, es mucho más grave la perspectiva de que el coste de la propia producción alimentaria aumente tanto cuando entremos en la fase descendente de la curva del petróleo que cientos de millones de seres humanos, tal vez incluso miles de millones, no estén en condiciones de adquirir los alimentos necesarios para garantizar la propia subsistencia y la de sus familias. Aunque actualmente ya existen fuentes alternativas de energía para alimentar los coches y los camiones, tal como explicaremos con detalle en el capítulo 8, no existen tales sustitutos para los fertilizantes petroquímicos. Eso significa que por cada litro de gasolina que consumimos ahora para hacer funcionar nuestros vehículos, nos quedamos con un litro menos para producir alimentos más adelante. El intercambio es inquietante. Consideremos el caso de un coche que consume 4 litros de gasolina cada 50 kilómetros: en 10 kilómetros quema la cantidad de gasolina necesaria para producir una barra de pan. En el futuro tal vez nos veamos obligados a adoptar compromisos difíciles entre la movilidad y la comida. Resulta sorprendente ver que en todo el debate acerca del petróleo y el futuro de la era de los hidrocarburos no ha habido un solo líder mundial dispuesto a plantear públicamente esta cuestión.

El impulso energético propiciado por los combustibles fósiles ha llevado a una explosión demográfica durante los últimos 150 años. Hubo que esperar desde el origen de la historia hasta el año 1825 para que la población humana alcanzara los 1.000 millones. Con el inicio de la revolución del carbón y el correspondiente salto en la circulación de energía, la población humana se duplicó en menos de un siglo, hasta alcanzar los 2.000 millones. El nacimiento de la era del petróleo hizo que se sumaran otros 1.000 millones de personas entre 1925 y 1960. La población mundial aumentó hasta alcanzar los 4.000 millones de personas entre 1960 y 1975, y hasta los 5.000 millones doce años más tarde, en 1987.

El rápido incremento de la población humana, resultado de una alimentación mejor y más abundante —así como de la mejora de la sanidad—, generó una espiral ascendente. Una población más numerosa requiere una organización social más compleja, lo que significa consumir una cantidad todavía mayor de energía disponible para mantener la estructura institucional. El Estado-nación es un ejemplo paradigmático de los efectos de la circulación de la energía. Los ferrocarriles y el telégrafo permitieron que el comercio extendiera su radio geográfico de acción. La expansión del comercio, a su vez, dio pie a un nuevo tipo de unidad política, el Estado-nación, una estructura de poder lo bastante amplia como para permitir el acceso a recursos lejanos, movilizar diversos centros de mano de obra y coordinar mercados de masas. El historiador social Charles Tilly estima que a comienzos del siglo XVI Europa estaba integrada por más de quinientos pequeños gobiernos autónomos, entre ciudades-Estado, principados y reinos. En 1975, el número de unidades políticas se había reducido a treinta y cinco. En el siglo XXI, el tiempo y la distancia se han visto recortados otra vez por unos transportes todavía más rápidos, tanto por tierra como por aire, y por un sistema de comunicaciones que se mueve a la velocidad de la luz, con el resultado de que Europa se está convirtiendo rápidamente en una única entidad política. La Unión Europea adoptó una moneda única en 2002 y es probable que en los próximos años se amplíe desde sus quince miembros actuales hasta integrar a veintisiete países en una estructura de gobierno que abarcaría todo un continente, desde la costa atlántica hasta la antigua Unión Soviética.

Más impresionante aún es el espectacular crecimiento que han experimentado algunas megaciudades durante la era de los combustibles fósiles. Las ciudades anteriores a nuestra época eran mucho más modestas de tamaño. La antigua Babilonia, en su momento de mayor esplendor, tenía tan sólo 100.000 habitantes. La población de Atenas no llegaba a las 50.000 personas. Londres fue la primera ciudad de la era de los combustibles fósiles en alcanzar el millón de habitantes, aproximadamente en 1820. Cien años más tarde, sólo había once ciudades en el mundo que superaran el millón de habitantes. Sin embargo, en 1950 lo habían superado setenta y cinco ciudades y en 1976 había 191 zonas urbanas con más de un millón de residentes. En la actualidad, hay un gran número de ciudades de todo el mundo que cuentan con poblaciones superiores al millón de personas. Diecinueve ciudades, entre ellas Tokio, Ciudad de México, Bombay, Sao Paulo, Shangai y Nueva York, constituyen verdaderas megaciudades, con poblaciones que van de los 10 a los 25 millones de personas.

Hace tan sólo dos siglos, la mayor parte de la humanidad vivía en zonas rurales y en pequeños pueblos. Hoy en día, más de la mitad de la población del planeta vive en zonas urbanas densamente pobladas. Estas ciudades y sus suburbios existen sólo gracias al hecho de que absorben grandes cantidades de energía disponible de su entorno. Las ciudades requieren un flujo constante de alimentos, energía, agua y recursos minerales para su sustento y mantenimiento. Una ciudad estándar de aproximadamente un millón de personas necesita más de 1.800 toneladas de alimentos, 567.000 toneladas de agua dulce y 8.600 toneladas de combustible cada 24 horas, la mayoría de los cuales ha de transportarse a través de grandes distancias, sólo para mantenerse en funcionamiento. Las infraestructuras de la ciudad también consumen grandes cantidades de energía. La Torre Sears de Chicago, por ejemplo, consume más energía en 24 horas que una ciudad de 150.000 habitantes.

Las ciudades también existen gracias a los vehículos de motor de combustión interna, que permiten conectar el campo y la ciudad, los suburbios y los centros urbanos en una red de transporte integral. Transportar los productos agrícolas a lo largo de miles de kilómetros desde las regiones productoras hasta las áreas urbanas no habría sido posible antes de la llegada del ferrocarril y el camión, así como de los modernos sistemas de refrigeración, todos ellos basados en el consumo de grandes cantidades de petróleo.

La idea que debe quedar clara es que sería inconcebible mantener a más de la mitad de la especie humana en entornos urbanos si no fuera por el incremento en la producción agrícola derivado del uso del petróleo aplicado a la maquinaria agrícola, la fertilización de la tierra y la eliminación de las plagas, así como al transporte de los productos hasta las lejanas áreas metropolitanas. Así pues, las ciudades constituyen estructuras precarias con una base agrícola frágil y vulnerable. Seguirán existiendo sólo mientras la producción agrícola siga siendo capaz de mantenerlas.

Cuando se va la electricidad

Es comprensible que no nos demos cuenta de la importancia que tiene el petróleo para la subsistencia de nuestras familias, dado que la actividad agrícola está muy alejada de nuestras vidas urbanas, tanto en el espacio como en el tiempo. Lo mismo puede decirse de la electricidad, que se ha convertido en una condición indispensable para nuestras actividades cotidianas. La red eléctrica es el sistema nervioso central que coordina la vida en zonas urbanas densamente pobladas. Sin energía eléctrica, la vida urbana dejaría de ser posible, la era de la información se convertiría en un pálido recuerdo y la producción industrial se detendría. La forma más rápida de asegurar el final de la era moderna sería darle al interruptor y apagar el flujo de la electricidad. Dejaría de haber luz, calor y potencia. La civilización, tal como la conocemos, llegaría a su fin.

Es difícil imaginar cómo sería la vida sin electricidad, aunque sólo llevamos menos de un siglo usándola como fuente de energía. La mayoría de nuestros bisabuelos nacieron en un mundo sin electricidad. Raras veces nos preguntamos de dónde viene o cómo llega hasta nosotros. Es una especie de fuerza silenciosa, transportada por cables que pasan por encima de nuestras cabezas, enterrada bajo tierra o escondida detrás de nuestras paredes. Incolora, inodora, es una presencia invisible pero indispensable en nuestras vidas.

¿Cuánta gente ha estado alguna vez en una central eléctrica y ha visto cómo se genera la electricidad? La mayoría de nosotros pensamos en la electricidad como una fuente primaria de energía, sin darnos cuenta de que la mayor parte de la electricidad que usamos ha sido generada a partir de la quema de carbón, petróleo o gas natural.

¿Qué probabilidades hay de que se vaya la electricidad no por un rato, sino durante largos períodos de tiempo? Por desgracia, la red eléctrica nacional es cada vez más vulnerable ante los fallos provocados tanto por los terroristas como por los cortes en el suministro de energía. Incluso antes del ataque del 11 de septiembre, los expertos gubernamentales estaban preocupados por la posibilidad de que las centrales eléctricas, las líneas de transmisión y la infraestructura de telecomunicaciones del país pudieran convertirse en objetivo de los terroristas. En 1997, la Comisión Presidencial para la Protección de Infraestructuras Críticas advirtió de que el próximo objetivo de los ciberterroristas podrían ser los programas informáticos que controlan la circulación de la electricidad por el país desde los centros de conmutación. Bloquear la red eléctrica podría provocar el caos en las infraestructuras económicas y sociales del país. Richard A. Clark, encargado del control del ciberterrorismo de la Casa Blanca en la Administración Bush, pone en guardia ante un posible «Pearl Harbor electrónico». Un informe emitido por el Instituto de Estudios de Tecnología de Seguridad poco después del ataque del 11 de septiembre prevenía sobre la posibilidad de que la respuesta militar contra la red de Al Qaeda y el régimen talibán se viera contestada por un contraataque terrorista dirigido contra la infraestructura electrónica norteamericana. Jeffrey A. Hunker, decano de la Escuela Heinz de Política y Gestión Pública de la Universidad Carnegie Mellon y anterior director del área de protección de infraestructuras críticas para el Consejo de Seguridad Nacional, cree que el país está «sentado sobre una "ciberbomba" de relojería».

El gobierno estadounidense ha creado recientemente un Centro de Protección de Infraestructuras a nivel nacional y ha mejorado la colaboración entre las agencias de orden público a nivel local, estatal y federal para proteger la infraestructura electrónica norteamericana frente a posibles delitos informáticos, a pesar de lo cual muchos expertos en el campo de la seguridad electrónica admiten en privado que no están seguros de que se pueda mantener la seguridad de la red eléctrica nacional. El propio informe de la Comisión Presidencial señalaba que en el año 2001 habría más de 19 millones de personas en el mundo con los conocimientos necesarios para provocar pequeñas interferencias en la estructura eléctrica del país y que 1,3 millones de personas tendrían unos conocimientos suficientemente avanzados sobre el funcionamiento de la red eléctrica y de telecomunicaciones como para causar daños significativos.

La red eléctrica norteamericana ha experimentado algunos fallos importantes a lo largo de los últimos treinta y siete años, y en todos los casos ha provocado el pánico y nos ha permitido conocer un anticipo de lo que sucedería si los cortes de electricidad se hicieran más largos y frecuentes en el tiempo. El primer gran apagón se produjo el 9 de noviembre de 1965. El mal funcionamiento de un simple relé en Canadá provocó una serie de fallos en cadena que pronto dejaron a oscuras a la mayor parte del noreste de Estados Unidos. Más de 30 millones de norteamericanos se quedaron sin electricidad durante más de doce horas. Se vieron afectados los Estados de Nueva York, Connecticut, Massachusetts, Vermont y Maine. En Nueva York hubo gente que se quedó atrapada en los ascensores y el tráfico quedó prácticamente paralizado tan pronto como los semáforos dejaron de funcionar en toda el área metropolitana. En todas partes se había ido la luz, el teléfono y la potencia eléctrica. Un extraño silencio se alzó sobre Nueva York y el resto del noreste norteamericano. En un momento, millones de personas acostumbradas a vivir en un mundo mediado por la electricidad se encontraron sin recursos y en una situación vulnerable. Habían desaparecido muchas comodidades que habían llegado a tener una importancia vital para ellos. Los ciudadanos fueron bajando a las calles en busca de información y calor humano que encontraron en unos extraños que acababan de convertirse en vecinos y, durante algunas horas, volvió a imponerse una especie de vida de pueblo sobre las duras calles de Nueva York. Por primera vez, la policía informó de un descenso de la criminalidad en la ciudad de Nueva York. Nueve meses más tarde, las autoridades sanitarias de la ciudad anunciaron, en tono ligeramente jocoso, un aumento en la cifra de nacimientos. Aparentemente, la falta de señal en el televisor hizo que un buen número de ciudadanos volvieran a formas más tradicionales de entretenimiento.

Sin embargo, la población estaba indignada por el apagón. Se pusieron en marcha investigaciones, se hicieron recomendaciones y se introdujeron cambios para garantizar que no volviera a producirse ningún corte en el suministro eléctrico. Pero volvió a suceder, a pesar de todas las precauciones públicas. En la noche del 13 de julio de 1977, un rayo alcanzó una torre de alta tensión del condado de Westchester, al norte de la ciudad de Nueva York, y provocó un cortocircuito entre dos líneas cuyos efectos posteriores en cadena terminaron por causar el fallo de la red eléctrica en la ciudad de Nueva York y sus alrededores. El apagón afectó a nueve millones de personas. El corte de electricidad duró más de quince horas. A diferencia del de 1965, que tuvo lugar en una fría noche de noviembre, esta segunda emergencia se produjo a las 21.34 h de una noche cálida y húmeda. Los aparatos de aire acondicionado dejaron de funcionar y las emociones subieron hasta alcanzar el punto de ebullición. Bandas de alborotadores —sobre todo en los barrios pobres de la ciudad— salieron a causar destrozos, quemar edificios y saquear tiendas. Más de 4.000 personas fueron arrestadas durante el apagón y setenta y ocho policías recibieron heridas durante los disturbios. El presidente del consejo de dirección de Consolidated Edison, la compañía eléctrica encargada de gestionar la red eléctrica de la ciudad de Nueva York y sus alrededores, calificó el apagón como un «acto de Dios».

Viendo los daños que habían causado los saqueadores en el barrio de Bushwick, en Brooklyn, un sacerdote católico dio al evento una interpretación opuesta al decir que «nos hemos quedado sin Dios».

La costa oeste de Estados Unidos ha experimentado fallos parecidos en la red eléctrica y apagones masivos. El primer corte de electricidad importante se produjo a las 15.45 h del 10 de agosto de 1996 y sus efectos se extendieron desde Oregón hasta la frontera mexicana. Nueve Estados en total se vieron afectados por el apagón, que se produjo durante uno de los días más calurosos del año, con temperaturas que llegaron a los 45 °C. Cinco millones de residentes californianos se quedaron sin electricidad. El apagón fue posteriormente atribuido a unas líneas de alta tensión de Oregón que se habían destensado como consecuencia del excesivo calor del verano. Los cables eléctricos tocaron las copas de algunos árboles, lo que provocó el apagón. El suceso inició una reacción en cadena que se extendió por toda la costa oeste y fue dejando sin electricidad una región tras otra.

En marzo de 2001 se repitieron los apagones en cadena en la región, con 800.000 afectados desde Oregón hasta el condado de Orange, en el sur de California. En esta ocasión la causa fue que las centrales eléctricas no generaron electricidad suficiente para responder a un pico de demanda energética motivado por una subida inusual de las temperaturas en primavera. A diferencia de los cortes de electricidad que habían afectado al cuadrante noreste del país, los que se produjeron en la costa oeste anularon los sistemas informáticos de las empresas de toda la región y tuvieron graves efectos sobre el comercio. En el último cuarto del siglo pasado, el país había desarrollado una dependencia cada vez mayor con respecto a los ordenadores, Internet y las intranets para el intercambio de información, el almacenamiento de datos, la realización de transacciones comerciales, operaciones bancarias y movimientos de crédito, así como para toda clase de servicios básicos y vitales. Un corte de electricidad de apenas unos minutos tiene unos efectos mucho más graves hoy en día que en el pasado, cuando los ordenadores y las conexiones de software desempeñaban un papel menor en la vida cotidiana.

Si, por un lado, los cortes de electricidad producen graves trastornos en las autopistas de la información del país, vale la pena señalar que la extensión de la informática ha contribuido a ejercer una presión todavía mayor sobre la red eléctrica tanto en Estados Unidos como en otros países, lo que no hace más que aumentar la probabilidad de que se produzcan apagones en el futuro. Aunque es cierto que los microprocesadores son cada vez más eficientes y capaces de procesar grandes cantidades de información en un intervalo inferior de tiempo y con un gasto menor de electricidad, la demanda total de capacidad de procesamiento informático está aumentando a un ritmo superior al de estas mejoras en materia de eficiencia. Se necesita medio kilo de carbón para «crear, ordenar, almacenar y mover dos megabytes de datos», señala el analista del ciberespacio Peter W. Huber. El resultado es que la demanda de energía eléctrica para los ordenadores personales se está duplicando cada pocos años. Huber afirma que no se está prestando suficiente atención al hecho de que «los chips están cada vez más calientes, los ventiladores giran cada vez más rápido y el consumo de electricidad de nuestras unidades de disco y de nuestras pantallas está aumentando».

Los primeros entusiastas de la revolución del ciberespacio hablaban de la gran cantidad de energía que se ahorraría en costes de transporte y mantenimiento de cemento y ladrillos cuando la mayor parte de los negocios y la vida social se realizaran en la realidad virtual —todo lo cual es cierto—, pero sólo recientemente nos hemos dado cuenta de la escalada en la demanda de electricidad que genera esta emigración al ciberespacio. Huber nos recuerda que los 50.000 millones de circuitos integrados y los 200.000 millones de microprocesadores que se producen al año funcionan con electricidad. Los chips funcionan con energía eléctrica y requieren una potencia elevada: el ordenador personal medio necesita, según Huber, 1.000 vatios de potencia. Si tenemos en cuenta que el usuario típico de Internet está conectado alrededor de doce horas semanales, según un estudio realizado por IntelliQuest, su consumo eléctrico es probablemente superior a los 1.000 kilovatios por hora. Si multiplicamos esta cifra por los 50 millones de ordenadores personales que hay en los hogares, los 150 millones de ordenadores de las oficinas y los 20 millones de personas que acceden anualmente a Internet tan sólo en Estados Unidos, se hace evidente la magnitud del reto que se presenta ante la red eléctrica nacional. Según Hubert, «para el viejo complejo termoeléctrico, que en opinión de muchas personas se halla ya en pleno declive, las implicaciones son terribles». Y sólo nos encontramos en la fase inicial de la revolución del ciberespacio. La red sin cables, según Huber, requiere aún más potencia eléctrica porque las señales ya no viajan por un cable de fibra, sino que son emitidas en todas direcciones. La industria de la transmisión sin cables ya está proyectando la construcción de 70.000 estaciones de radio para los próximos años y el doble de esta cifra en el curso de la presente década. Cada estación consume dos kilovatios de electricidad.

Una nación en peligro

Tras el embargo petrolero de los productores de la OPEP en los años sesenta y la subsiguiente crisis energética que se dejó sentir en todo el mundo, la Agencia de Preparación para la Defensa Civil del Pentágono encargó un estudio para determinar los puntos vulnerables y las debilidades del sistema energético norteamericano. A pesar del escaso interés que despertó en su momento el informe final, emitido el 13 de noviembre de 1981 por la Agencia de Gestión de Emergencias, encontramos en él un análisis penetrante de los muchos puntos débiles del sistema energético norteamericano. La imagen que da el informe es la de un sistema energético altamente complejo, interdependiente y frágil, vulnerable ante toda clase de agresiones terroristas, fallos técnicos, averías, desastres naturales y recortes energéticos.

El informe del gobierno abordaba virtualmente todos los aspectos del sistema energético nacional. Sin embargo, prestaba una atención especial al análisis de las deficiencias en la red eléctrica. Los problemas que destaca el estudio han quedado en la mayoría de los casos sin resolver hasta el día de hoy, mientras que su peligrosidad potencial para la economía y la seguridad del país no ha hecho más que aumentar a causa de la triple amenaza que representan el descenso de la producción mundial de petróleo y gas natural, la escalada de las amenazas terroristas procedentes de extremistas de Oriente Medio contra la infraestructura del país, y el aumento de las temperaturas de la Tierra provocado por la quema de combustibles fósiles. Dos analistas que tomaron parte en la elaboración del informe, Amory y Hunter Lovins, del Instituto Rocky Mountain, en Snowmass, Colorado, han resumido algunos de los puntos cruciales del informe.

Para empezar, la energía necesaria para mantener en funcionamiento la red eléctrica de Estados Unidos acostumbra a estar ubicada en lugares alejados del consumidor final. Por ejemplo, un barril de petróleo de procedencia nacional acostumbra a viajar una media de 1.000 o 1.200 kilómetros antes de llegar a su destino final. Muchas centrales eléctricas están situadas a gran distancia de los consumidores. La electricidad se desplaza a una distancia media de 350 kilómetros o más desde la central eléctrica hasta llegar al consumidor. Eso significa que tanto la energía como la electricidad son más vulnerables tanto al sabotaje como a unas condiciones climáticas adversas.

Las elevadas inversiones de capital necesarias para construir nuevas centrales eléctricas, el largo intervalo de espera antes de su entrada en funcionamiento y la larga vida operativa esperada de esta clase de instalaciones —algunas se mantienen en funcionamiento durante varias décadas— hacen que el sistema sea relativamente poco flexible y no resulte fácil introducir modificaciones rápidas para responder a situaciones inesperadas, como una interrupción a corto plazo del suministro energético, el aumento o la disminución de la demanda, o bien un agotamiento del conjunto de las reservas disponibles de energía. Tal como se ha señalado anteriormente, la mayor parte de las compañías eléctricas norteamericanas han realizado grandes inversiones en una nueva generación de centrales eléctricas que funcionan con gas. Pero si algunos informes geológicos recientes resultan ser correctos y la producción global de gas natural toca techo poco después de que lo haga la producción global de petróleo, alrededor del año 2020, las empresas de servicios públicos del país podrían enfrentarse a recortes en el suministro de gas natural y verse forzadas a reducir la cantidad de electricidad generada, lo cual podría provocar graves trastornos en la economía y la sociedad en su conjunto.

A diferencia de las formas primarias de energía, la electricidad no se puede almacenar. Consiste más bien en un flujo continuo. En el mismo momento en que es generada en la central debe comenzar a viajar por las líneas de transmisión hacia el usuario final. El hecho de que no pueda ser almacenada hace que cuando se produce una interrupción en algún punto del flujo no acostumbre a haber un excedente de energía disponible para compensar la pérdida.

Cuando falla una red eléctrica, todo lo que depende de la electricidad dentro de su marco geográfico se ve afectado. Un corte de electricidad significa que todo lo que está conectado a la red deja de funcionar. Los fallos eléctricos son catastróficos, porque afectan a todos los niveles por igual. No se salva ninguna actividad relacionada con la electricidad. Así, aunque sea más importante mantener la corriente eléctrica en la sala de emergencias de un hospital o en la torre de control de un aeropuerto comercial que asegurar que siguen funcionando las neveras domésticas, la red no puede hacer estas distinciones. Cuando se corta la corriente, lo hace en todo el sistema. Ése es el motivo de que la mayor parte de los servicios y actividades básicos dispongan de generadores de emergencia propios.

Los cortes de electricidad afectan también al flujo de otras fuentes primarias de energía. Las calderas domésticas funcionan con petróleo o gas, pero necesitan la electricidad para iniciar la combustión, bombear el combustible y distribuir el calor. Del mismo modo, las estaciones de servicio locales funcionan con la electricidad de la red general. Los sistemas de agua municipales y las plantas depuradoras de aguas residuales también funcionan con electricidad. Las refinerías de petróleo también dependen de la electricidad para su funcionamiento, al igual que la maquinaria que extrae el petróleo en los pozos.

Los oleoductos y gasoductos que transportan el combustible hasta las refinerías son tan complejos que actualmente funcionan por medio de sofisticados programas informáticos gestionados por un personal altamente especializado. Si los ciberterroristas fueran capaces de penetrar en estos programas, podrían provocar un fallo temporal en el sistema y obligar a la compañía a hacer funcionar manualmente los conductos, con todos los costes y riesgos añadidos que ello conlleva.

Las centrales eléctricas actuales son tan grandes y complejas, y los componentes de la maquinaria son tan especializados y caros, que sólo hay recambios para las piezas pequeñas y menos importantes. Cuando falla una pieza importante del sistema, lo habitual es que se tenga que encargar especialmente y el proceso de elaboración de la pieza puede llegar a tardar un año o más. Después del apagón de 1977 en la ciudad de Nueva York, por ejemplo, Consolidated Edison encargó un recambio para un transformador-regulador de fase que habría reducido el impacto del apagón en caso de haberse tenido en su momento. La fabricación de la pieza tardó más de un año. Además, buena parte del proceso de generación de electricidad se halla actualmente automatizado y es extremadamente sofisticado, motivo por el cual se requieren contratistas especiales para reparar las averías. Si se produjeran múltiples fallos en centrales eléctricas situadas en distintos puntos del país, sea a causa de ataques terroristas o de desastres naturales, es probable que no hubiera suficiente personal especializado para realizar las reparaciones a tiempo.

Los oleoductos y gasoductos que transportan las fuentes primarias de energía a las refinerías y centrales eléctricas se encuentran entre los puntos más vulnerables de la matriz energética. El informe del Pentágono de 1981 llamó la atención sobre las posibles ramificaciones de los daños provocados por un ataque terrorista contra las principales conducciones del país. Ahora, veinte años más tarde, el Congreso, el Pentágono y la industria energética están volviendo a interesarse por este antiguo estudio, preocupados por la posibilidad de que las amenazas teóricas que preveía la generación anterior se hayan convertido hoy en realidades.

Consideremos, por ejemplo, el Sistema de Oleoductos Colonial, que se extiende a lo largo de 3 .200 kilómetros, desde Texas hasta Nueva Jersey. Sus tres grandes conducciones transportan el petróleo de un extremo a otro en doce días. La operación depende de ochenta y cuatro estaciones de bombeo y consume grandes cantidades de electricidad para mover el petróleo a lo largo del sistema. Un estudio sobre los problemas de seguridad relativos a los oleoductos llegó a la siguiente conclusión:

Fueron construidos y son gestionados prácticamente sin tener en consideración su vulnerabilidad ante personas que podrían [...] desear interferir en esta vital circulación de combustible. Se hallan expuestos y prácticamente indefensos en innumerables puntos, y resultan fácilmente accesibles virtualmente a lo largo de todo su recorrido, incluso cuando no se hallan expuestos [...] Esta vulnerabilidad del sistema de transporte de energía más importante del país supone una amenaza para la seguridad nacional [...] Aunque todas las formas de transporte de energía son vulnerables en alguna medida, los oleoductos lo son tal vez más. Ningún otro medio de transporte de energía desplaza tanta cantidad de ésta a lo largo de distancias tan grandes y en una corriente continua, cuya continuidad es un aspecto crítico de su importancia.

Las centrales eléctricas y las líneas de transmisión son particularmente vulnerables a los trastornos que puedan ocasionar los terroristas. Hace treinta años, el Comité Conjunto del Congreso sobre Producción de Defensa advirtió sobre el hecho de que las grandes estaciones generadoras que alimentan las principales áreas metropolitanas «constituyen un conjunto de objetivos relativamente compacto y atractivo para un saboteador, un terrorista o un agresor en general». Y no se trata de una amenaza meramente hipotética. Los ataques a las centrales eléctricas se han convertido en un elemento habitual dentro de las actividades terroristas y guerrilleras en países como Afganistán, El Salvador, Chipre e incluso en países del G-7 como Italia y Gran Bretaña.

Los funcionarios del gobierno encargados de la seguridad nacional han estado preocupados desde el 11 de septiembre por la seguridad en las centrales nucleares. Las torres de enfriamiento, los edificios de contención y otras instalaciones fueron diseñados hace años para resistir un impacto directo de aviones más pequeños. Nadie sabe lo que sucedería si los terroristas estrellaran un Jumbo 747 en una central nuclear. También preocupa la posibilidad de que los terroristas puedan dirigir su ataque contra los residuos radiactivos que se almacenan en las propias centrales nucleares en contenedores relativamente accesibles. Un escape importante de radiactividad en una central nuclear podría llegar a causar 60.000 muertes por cáncer, 60.000 nacimientos con defectos genéticos, 450.000 nódulos de tiroides, la extensión de la contaminación sobre una superficie de 13.700 km² y la generación de daños materiales valorados en cientos de miles de millones de dólares, todo ello a lo largo de un dilatado período de tiempo.

Las líneas de transmisión constituyen un blanco igualmente atractivo para los terroristas. Hace cuarenta años, la Administración de la Energía Eléctrica de Defensa realizó la siguiente advertencia:

Es difícil proteger del sabotaje a las principales líneas de transmisión, dado que se extienden por todo el territorio de los Estados y atraviesan miles de kilómetros de áreas remotas y accidentadas. Por más que se realicen patrullas periódicas en estas instalaciones, el saboteador sigue disponiendo de mucho tiempo para trabajar sin ser visto.

Los funcionarios gubernamentales, sobre todo en el Pentágono, el Departamento de Energía y el Departamento de Interior, son muy conscientes de estas vulnerabilidades, pero no están seguros de que se pueda hacer algo para evitarlas, excepto quizá crear un régimen energético alternativo acompañado de una red eléctrica radicalmente distinta. A falta de un nuevo proyecto energético para el país, en los próximos años seremos cada vez más vulnerables a los ataques terroristas. Un funcionario del Departamento de Interior resumió la dimensión del problema cuando reconoció que «un grupo relativamente pequeño de individuos competentes y totalmente entregados [...] podría provocar un corte [en el suministro eléctrico] en casi cualquier sector del país».

* * *

La civilización actual basada en los combustibles fósiles es vulnerable en tantos frentes que muchos observadores reconocen con inquietud que el colapso del sistema ya no es una posibilidad inconcebible. Se espera que la producción global de petróleo toque techo dentro de las dos próximas décadas —algunos geólogos dicen que mucho antes— y que el gas natural lo haga poco después. El creciente enfrentamiento de los jóvenes militantes islamistas con los autócratas y los gobiernos dictatoriales de Oriente Medio, donde se halla la mayor parte de las reservas restantes de petróleo y gas natural, hace que la región sea cada día más inestable desde el punto de vista político. Los terroristas musulmanes siguen representando una seria amenaza para la seguridad interna de Estados Unidos y otros países del G-7, lo que no hace más que aumentar los costes militares para garantizar el acceso a unas reservas cada vez más escasas de petróleo y gas natural en el golfo Pérsico. Mientras tanto, y para adelantarse a las previsiones de un descenso en el suministro de petróleo crudo barato y un incremento espectacular de su precio en los mercados mundiales, las compañías energéticas están comenzando a volver al carbón y a aprovechar reservas todavía no explotadas, como el crudo pesado y las arenas asfálticas, con la esperanza de generar la suficiente cantidad de combustible sintético como para compensar el creciente déficit de petróleo crudo y gas natural. Sin embargo, es probable que el paso hacia la utilización de combustibles más sucios signifique un aumento en las emisiones de dióxido de carbono en la atmósfera, con lo que se multiplicaría el problema del incremento de las temperaturas en la Tierra durante el siglo XXI.

Estamos viviendo los estadios finales de un régimen energético maduro, con todos los problemas que ello representa. Las reservas de energía son cada vez más escasas y aumenta la factura de la entropía. La vasta infraestructura energética que hemos creado para explotar el petróleo está envejeciendo y es cada vez más vulnerable a las alteraciones y a los fallos. Cada vez tenemos que pagar más para mantener la infraestructura, y sin embargo recibimos cada vez menos beneficios energéticos netos. Al mismo tiempo, todas las instituciones económicas y sociales asociadas al régimen de los combustibles fósiles se hallan también amenazadas. Particularmente vulnerable es la agricultura basada en la industria petroquímica, responsable del mantenimiento de una población cada vez más numerosa y que hace posible que más de la mitad de los habitantes de la Tierra vivan en ciudades. La agricultura será probablemente una de las primeras víctimas del descenso de la producción global de petróleo. La progresiva escasez de las reservas y el incremento constante de los precios del petróleo podrían provocar un colapso de la producción agrícola en todo el mundo y el subsiguiente hundimiento de las economías construidas a partir de ella: la industria, los servicios y la economía de la información y la economías de escala.

El agotamiento de las reservas energéticas podría tener como resultado un aumento de la frecuencia de los cortes de electricidad dentro de pocos años, especialmente en las principales zonas urbanas del mundo, con efectos potencialmente devastadores sobre nuestro modelo de vida. El colapso de la red eléctrica llevaría inevitablemente a una despoblación de unas áreas metropolitanas ya inhabitables y el retorno a las zonas rurales.

Sin duda, muchos escépticos dirán que todo esto ya se ha oído antes. Los catastrofistas ya habían anunciado en los años setenta y principios de los ochenta que el mundo se iba a quedar pronto sin petróleo y sus profecías resultaron estar equivocadas. ¿Por qué habrían de ser más creíbles estas predicciones? No les falta razón en sus dudas. Tal vez no se esté acabando el petróleo crudo barato, por lo menos en un futuro próximo. Y las predicciones sobre un aumento de las temperaturas en la Tierra también podrían estar equivocadas. El auge del fundamentalismo islámico y la amenaza de una intensificación en todo el mundo del terrorismo dirigido contra Estados Unidos y otras naciones ricas, así como contra los ricos países petroleros de Oriente Medio, podrían resultar exagerados, o bien podrían ser reales, pero disolverse rápidamente. La compleja infraestructura energética y el sistema económico cada vez más centralizado construido a partir de ella podrían no ser tan vulnerables ante las amenazas externas o los trastornos y los fallos internos como pretenden algunos.

Ambas partes del debate pueden aportar argumentos razonables. Sin embargo, la evidencia factual y estadística acumulada se inclina en buena medida en favor de quienes sostienen que el régimen energético del petróleo se acerca a su ocaso. Es difícil ignorar las señales que marcan el comienzo de los rendimientos marginales, el aumento inexorable de la factura de la entropía, la creciente tensión a la que está sometida la propia infraestructura a causa de las demandas que se imponen sobre ella y la mayor vulnerabilidad del conjunto del sistema ante alteraciones de todo tipo.

Tal vez la evidencia más clara de que nos acercamos a las últimas décadas de la era del petróleo proviene del análisis estadístico del aumento y la posterior caída de la producción energética per cápita en el mundo. Una serie de científicos, entre ellos Robert Romer, John Gibbons y Richard Duncan, así como la propia BP Amoco, han analizado la curva mundial de producción energética per cápita. Las conclusiones de todos ellos son notablemente parecidas y proporcionan una evidencia irrefutable en favor de la hipótesis de que la era del petróleo está perdiendo impulso tan rápidamente como antes lo había ganado.

Según BP Amoco, la producción mundial de petróleo per cápita tocó techo en 1979 y ha estado descendiendo desde entonces. El motivo es que si, por un lado, la producción de petróleo ha ido en aumento, la población mundial ha ido creciendo a un ritmo mayor.

Ni siquiera los más optimistas pueden explicar el continuado descenso de la producción de petróleo per cápita. Con una población humana en constante crecimiento, no hay ningún escenario previsible en el futuro de acuerdo con el cual el petróleo —ni tampoco el gas natural, el carbón, los crudos pesados, las arenas asfálticas o la energía nuclear— pueda invertir la tendencia y proporcionar la suficiente energía per cápita como para repetir el valor máximo alcanzado en 1979. En lo que se refiere a la energía per cápita, la humanidad resbala inexorablemente cuesta abajo por la curva de campana de la era del petróleo. Si tal es la dura realidad, la cuestión más importante que tiene ante sí la civilización es si es posible descubrir y explotar un nuevo régimen energético a tiempo para reemplazar los hidrocarburos y cubrir las necesidades de una población humana cada vez más numerosa en el próximo siglo.

Capítulo 8

EL NACIMIENTO DE LA ECONOMÍA DEL HIDRÓGENO

En 1874, el popular escritor de ciencia ficción Julio Verne publicó un curioso libro titulado La isla misteriosa. El libro narraba las aventuras de cinco soldados del norte que se veían desviados de su camino cuando trataban de huir en globo de un campamento confederado durante la guerra civil estadounidense. Finalmente consiguieron tomar tierra en una pequeña isla situada a 11.000 kilómetros de distancia de su punto de partida. Un día estaban especulando sobre el futuro de la Unión y uno de los miembros del grupo, un marinero llamado Pencroft, preguntó al ingeniero Cyrus Harding qué pasaría con el comercio y la industria si Norteamérica se quedara sin carbón. «¿Qué es lo que van a quemar en lugar de carbón?», preguntó Pencroft. «Agua», exclamó Harding, ante la sorpresa de todos. Harding procedió entonces a explicar su idea:

La electricidad ha permitido descomponer el agua en sus elementos primitivos, lo cual hará que se convierta en una fuerza poderosa y manejable [...] Sí, amigos míos, creo que algún día se empleará el agua como combustible, que el hidrógeno y el oxígeno de los que está formada, usados por separado o de forma conjunta, proporcionarán una fuente inagotable de luz y calor, de una intensidad de la que el carbón no es capaz [...] El agua será el carbón del futuro.

Ciento veintisiete años después de que Verne diera las primeras pistas sobre un futuro en el que todas las necesidades energéticas de la civilización serían cubiertas por el hidrógeno extraído del agua, Phil Watts, el presidente de Royal Dutch Shell, ofreció un discurso sobre el futuro de la energía en un foro patrocinado por el Programa de Desarrollo de las Naciones Unidas. El escenario era la ciudad de Nueva York, sólo dos semanas después del ataque terrorista contra las torres gemelas del World Trade Center. Con el aire de Manhattan todavía cargado con el olor de los gases tóxicos procedentes de la zona cero, los pensamientos de Watts se volvieron hacia el futuro de la energía. Según informó a su audiencia, Shell se estaba preparando para «el final de la era de los hidrocarburos». Watts sostuvo que el carbón, el petróleo y el gas natural, los grandes combustibles fósiles que han impulsado el mundo hacia la era industrial, dejarían paso en el siglo XXI a un régimen energético nuevo y revolucionario basado en el hidrógeno, y dijo que Shell ya llevaba más de 1.000 millones de dólares invertidos en la transición hacia una economía basada en dicho recurso renovable.

La intuición de Verne sobre un futuro basado en el hidrógeno es motivo de encendidos debates en los consejos de dirección de las principales compañías energéticas, de transportes y de servicios públicos de todo el mundo, así como entre los líderes políticos y un número cada vez mayor de organizaciones no gubernamentales, tanto en los países industrializados como en el Tercer Mundo.

La descarbonización

El hidrógeno es el elemento más abundante en el universo. Constituye el 75% de la masa del universo y el 90% de sus moléculas. Encontrar la forma de explotarlo efectivamente proporcionaría a la humanidad una fuente de energía virtualmente ilimitada, la clase de elixir de la energía que durante tanto tiempo ha escapado a los esfuerzos de alquimistas y químicos. En cierto sentido, la premonición que tuvo Julio Verne de un futuro basado en el hidrógeno ya estaba comenzando a anunciarse en el último cuarto del siglo XIX. En menos de un siglo, el uso de la madera como combustible había dejado paso al carbón y éste estaba comenzando a verse amenazado por un recién llegado, el petróleo. Había comenzado ya el proceso de «descarbonización» de la energía que llevaría inevitablemente a un futuro basado en el hidrógeno.

«Descarbonización» es un término que emplean los científicos para referirse a la progresiva sustitución de los átomos de carbono por otros de hidrógeno con cada nueva fuente de energía. La madera, la principal fuente de energía durante la mayor parte de la historia de la humanidad, tiene la proporción más alta de carbono en relación con el hidrógeno, con diez átomos de carbono por cada átomo de hidrógeno. Entré los combustibles fósiles, el carbón posee la mayor proporción de carbono, alrededor de uno o dos átomos de carbono por uno de hidrógeno. El petróleo contiene un átomo de carbono por cada dos átomos de hidrógeno, mientras que el gas natural tiene sólo un átomo de carbono por cada cuatro átomos de hidrógeno. Esto significa que cada nueva fuente de energía emite menos CO2 que su predecesora. Nebojsa Nakicenovic, miembro del Instituto Internacional de Análisis de Sistemas Aplicados, con sede en Viena, estima que la emisión de carbono por unidad de energía primaria consumida globalmente ha descendido en un 0,3% anual a lo largo de los últimos 140 años.

Naturalmente, el elevado consumo de carbón y petróleo ha hecho que las emisiones de CO2 hayan seguido aumentando, sin embargo, a lo largo de este período, provocando correlativamente el aumento de las temperaturas en la superficie de la Tierra. Aunque el proceso actual de sustitución del carbón y el petróleo por el gas natural promete reducir aún más las emisiones de CO2 por unidad de energía producida, la gran cantidad de gas natural consumido continuará significando una mayor cantidad de emisiones de CO2 y un aumento de las temperaturas en la Tierra, aunque no tanto como si siguiéramos dependiendo primariamente del carbón o el petróleo. Jesse Ausubel, investigador asociado de la Universidad Rockefeller de Nueva York, resume la importancia histórica de la evolución de la energía en el mundo:

El hecho más importante, sorprendente y afortunado que revelan los estudios sobre la energía es que durante los últimos doscientos años el mundo ha favorecido progresivamente los átomos de hidrógeno sobre los de carbono [...] la tendencia a la «descarbonización» es un elemento central para la comprensión de la evolución del sistema energético.

El hidrógeno completa el camino hacia la descarbonización. No contiene ningún átomo de carbono. Su emergencia como fuente primaria de energía del futuro señala el final del largo reinado de la energía de los hidrocarburos en la historia de la humanidad. El hidrógeno, la fuente de la energía solar —constituye el 30% de la masa del Sol—, se está convirtiendo en la gran esperanza para la continuidad del progreso de la humanidad sobre la Tierra. Es la más ligera e inmaterial de todas las formas de energía y la más eficiente cuando es quemada.

La constante progresión de lo pesado a lo ligero y de lo material a lo inmaterial en nuestras formas energéticas se ha visto acompañada en cada uno de los estadios por una mayor ligereza de la actividad industrial, desde las tecnologías de la era del vapor propias del primer capitalismo industrial hasta la ligereza y la virtualidad de las tecnologías de la era de la información, en el siglo XXI. En realidad, la desmaterialización de la energía y la desmaterialización de la actividad económica van inevitablemente asociadas. La descarbonización no sólo ha significado la eliminación sistemática de los átomos de carbono, sino también la desmaterialización de la energía, que ha pasado de fuentes sólidas (como el carbón) a otras líquidas (como el petróleo) y ahora a gases (como el gas natural y el hidrógeno). El paso de sólidos a líquidos y posteriormente a gases en el terreno energético implica una producción energética más eficiente —el petróleo viaja más rápido por los oleoductos que el carbón, que debe ser trasladado por ferrocarril, y el gas natural viaja de forma todavía más fácil y rápida por los gasoductos que el petróleo líquido— y da lugar a tecnologías, bienes y servicios también más rápidos, eficientes, ligeros y virtuales.

En un discurso realizado ante el Comité Científico de la Casa de los Representantes de Estados Unidos, en abril de 2001, el directivo de Texaco Frank Ingriselli estableció un paralelismo entre los grandes cambios que están teniendo lugar en la economía global y en la sociedad con el nacimiento de la era del hidrógeno. Observó que «el ecologismo, la innovación y las fuerzas del mercado están marcando el futuro de nuestra industria y nos impulsan inexorablemente hacia la energía del hidrógeno» y advirtió que «aquellos que no den este paso, se arrepentirán».

El hidrógeno está por todas partes en la Tierra, tanto en el agua como en los combustibles fósiles y en todos los seres vivos. El hidrógeno que hay en el agua y los organismos constituye el 70% de la superficie de la Tierra. Sin embargo, tal como pronto veremos, raramente se halla en estado libre y aislado, como sucede con el carbón, el petróleo y el gas natural. El hidrógeno es un vehículo de la energía, una forma secundaria de energía que antes tiene que ser producida, como la electricidad.

La panacea energética

El hidrógeno fue descubierto por el científico británico Henry Cavendish. En un artículo presentado ante la Real Sociedad de Londres, en 1776, informó de un experimento en el que había obtenido agua a partir de la combinación de oxígeno e hidrógeno, con la ayuda de una chispa eléctrica. Como estos elementos todavía no tenían nombre, los llamó «aire sustentador de la vida» y «aire inflamable» respectivamente. El químico francés Antoine Laurent Lavoisier consiguió repetir con éxito el experimento de Cavendish en 1785 y dio el nombre de oxígeno al «aire sustentador de la vida» y el de hidrógeno al «aire inflamable».

Como no podía ser de otro modo, el primer uso práctico del hidrógeno fue militar. El químico Guyton de Norveau, que también era miembro del Comité de Salvación Pública, una de las facciones enfrentadas en el conflicto que había estallado en Francia tras la toma de la Bastilla en 1793, sugirió que el hidrógeno podía producirse en grandes cantidades para hacer globos de reconocimiento. El primer generador de hidrógeno fue construido en un campamento militar situado en las afueras de París en 1794.

En la década de 1920 comenzó a producirse hidrógeno con fines comerciales en Europa y Norteamérica. La primera en hacerlo fue la compañía canadiense Electrolyser Corporation Limited. Dicha compañía, que al principio se llamaba Stuart Oxygen Company, fue también la que fabricó y vendió en 1920 los primeros electrolizadores comerciales para una compañía estadounidense situada en San Francisco. Los electrolizadores son unas máquinas que descomponen el agua en hidrógeno y oxígeno. En la actualidad, la Electrolyser Corporation es uno de los principales productores de plantas generadoras de hidrógeno por electrólisis del mundo.

El primer científico importante que supo ver todo el potencial del hidrógeno fue John Burden Sanderson Haldane, que más tarde se convertiría en uno de los principales genetistas del siglo XX. En 1923, cuando todavía era un joven veinteañero, Haldane dio una conferencia en la Universidad de Cambridge en la que predijo que la energía del hidrógeno sería el combustible del futuro. Más tarde desarrolló sus argumentos en favor del hidrógeno en un artículo científico en el que también detallaba la forma en que sería producido, almacenado y aprovechado. Sus ideas eran tan revolucionarias en su tiempo que sólo encontró incredulidad entre sus compañeros de la Academia. Sin embargo, sus tesis constituyen un modelo funcional y extraordinariamente detallado sobre cómo se llevaría a cabo más adelante la explotación y el aprovechamiento del hidrógeno.

Haldane comenzaba con una encendida defensa de la superioridad del hidrógeno sobre otras formas de energía. Escribió que «el hidrógeno líquido es comparativamente el método más eficiente de almacenar energía, puesto que produce tres veces más calor por litro que el petróleo». Haldane pasaba luego a la cuestión de la cantidad de hidrógeno que se produciría. Predijo que cuatro siglos más adelante las necesidades energéticas de Gran Bretaña se verían cubiertas por

hileras de molinos de viento conectados a motores eléctricos que a su vez suministrarán corriente de alto voltaje a una red eléctrica gigantesca. A una distancia adecuada se situarán grandes centrales que durante los períodos de más viento aprovecharán el excedente energético para efectuar la descomposición electrolítica del agua en oxígeno e hidrógeno. Estos gases serán después licuados y almacenados en grandes depósitos cubiertos y al vacío, probablemente situados bajo tierra [...] En períodos de calma los gases serán recombinados en motores de explosión conectados a dinamos para producir de nuevo energía eléctrica o, más probablemente, pilas de oxidación. Estos grandes depósitos de gases licuados permitirán almacenar la energía eólica para que pueda ser aprovechada en la industria, el transporte, la calefacción y la iluminación en el momento deseado.

Haldane llegaba incluso a prever los inmensos obstáculos que se opondrían a la transición hacia el régimen energético del hidrógeno, así como las importantes consecuencias sociales y medioambientales que tendría como resultado. En relación con lo primero, reconocía que «los costes iniciales serán muy elevados, pero los gastos de mantenimiento serán menores que los del sistema actual». La gran ventaja social de adoptar el régimen energético del hidrógeno es que «la energía será tan barata en una parte como en otra del país, de modo que la industria quedará en gran medida descentralizada». Los beneficios medioambientales, según Haldane, serían igualmente atractivos, ya que «no se producirán humos ni cenizas».

El hidrógeno comenzó a ser utilizado como combustible para la aviación en los años veinte y treinta. Los ingenieros alemanes lo empleaban como combustible secundario de los dirigibles —el combustible primario de la aeronave era una mezcla de benceno y gasolina— que realizaban el transporte comercial de pasajeros a través del Atlántico. Los motores fueron modificados de forma que las emisiones normales de hidrógeno que se utilizaban para mantener la fuerza ascensional de la aeronave pudieran ser aprovechadas como combustible extra. En los años treinta y cuarenta el hidrógeno era utilizado en Alemania e Inglaterra como combustible experimental para automóviles, camiones, locomotoras e incluso submarinos y torpedos silenciosos.

Hoy en día se producen globalmente 400.000 millones de metros cúbicos de hidrógeno, aproximadamente el equivalente al 10% de la producción mundial de petróleo en 1999. Buena parte del hidrógeno se emplea como materia prima química para la obtención de productos como los fertilizantes basados en el amoníaco y para la hidrogenación de aceites orgánicos comestibles derivados de la soja, el pescado, los cacahuetes o los cereales. El hidrógeno también se utiliza para el proceso de elaboración del polipropileno, así como para la refrigeración de motores y generadores.

El hidrógeno se ha utilizado de manera generalizada en los procesos de refinado y como materia prima para la producción de una amplia gama de productos, pero su valor como combustible prácticamente se ignoró hasta después de la Segunda Guerra Mundial, a pesar de los primeros éxitos experimentales obtenidos en los años veinte y treinta en el campo de la aviación y la automoción. Hasta la crisis del petróleo de 1973, los científicos, los ingenieros y los líderes políticos no decidieron dar una segunda oportunidad al hidrógeno como forma genérica de energía. Ese mismo año se celebró la primera Conferencia Internacional sobre el Hidrógeno en Miami Beach y se fundó la Asociación Internacional para la Energía del Hidrógeno junto con una revista mensual: la International Journal of Hydrogen Energy. Un pequeño grupo de entusiastas, que se daban a sí mismos el nombre de «románticos del hidrógeno», comenzó a hacer proselitismo en la industria energética, con la esperanza de ganar adeptos para el proyecto del hidrógeno. T. Nejat Veziroglu, presidente de la asociación y miembro del grupo, resumió el entusiasmo de aquella época diciendo que el hidrógeno «era una solución permanente para el agotamiento de los combustibles convencionales, [y una] solución permanente para el problema ecológico global».

En los años siguientes, el gobierno de Estados Unidos y otros países comenzaron a invertir pequeñas cantidades de dinero público en investigaciones relacionadas con el hidrógeno. El programa estadounidense nunca superó los 24 millones de dólares. La Comunidad Económica Europea destinó entre 72 y 84 millones de dólares a la investigación sobre el hidrógeno en los años setenta. Cuando en los años ochenta la crisis comenzó a diluirse y el precio del petróleo volvió a caer en los mercados mundiales, las inversiones gubernamentales en investigaciones relacionadas con el hidrógeno descendieron de forma significativa.

El interés por el hidrógeno volvió a ganar fuerza en los años noventa, con la publicación de una serie de estudios e informes alarmantes según los cuales el aumento de las emisiones de CO2 derivadas de la quema de combustibles fósiles estaba teniendo como resultado un calentamiento del planeta, lo que representaba una amenaza potencialmente grave para la biosfera de la Tierra. Cada vez eran más los científicos que planteaban la posibilidad de realizar una transición de los combustibles basados en los hidrocarburos al hidrógeno, como forma de responder al problema del calentamiento global. La descarbonización se convirtió en un eslogan compartido por geólogos, climatólogos y ecologistas. Al mismo tiempo, los programas de investigación experimental que se estaban desarrollando tanto en el sector académico como en el comercial sentaron las bases técnicas para el futuro del hidrógeno.

La Unión Soviética adaptó un avión de pasajeros para que funcionara parcialmente con hidrógeno líquido en 1988, el mismo año que un norteamericano, William Conrad, se convertía en la primera persona que conseguía hacer despegar un avión impulsado únicamente con hidrógeno líquido. En 1992, el Instituto Fraunhofer de Sistemas Energéticos Solares creó en Alemania la primera casa solar que utilizaba hidrógeno para el almacenamiento de energía a largo plazo. Al año siguiente, Japón destinó 2.000 millones de dólares a un plan de treinta años con el objetivo de promover la energía del hidrógeno en todo el mundo. En 1994, los primeros autobuses de hidrógeno salieron a las calles de Geel, Bélgica. Un año más tarde la Autoridad de Tránsito de Chicago comenzó a probar sus propios autobuses de hidrógeno. El grupo Royal Dutch/Shell dio sus primeros pasos en la era del hidrógeno en 1998, con la creación de un «equipo del hidrógeno» para que explorara proyectos comerciales y, un año más tarde, creó una división del hidrógeno.

Todos estos hitos y otros innumerables pequeños esfuerzos que han tenido lugar durante el siglo pasado han contribuido a elevar el perfil del hidrógeno. Hasta 1999, sin embargo, no comenzó a verse claramente todo el impacto potencial que podía tener la energía del hidrógeno. En febrero de ese año, Islandia anunció un plan ambicioso y arriesgado para convertirse en la primera economía del mundo basada en el hidrógeno.

El responsable de implementar el plan será un consorcio en el que participan tres compañías transnacionales —Royal Dutch/Shell Group, Daimler-Chrysler y Norsk Hydro— y seis socios islandeses —la Planta Geotérmica de Reykjanes, la Compañía Eléctrica Municipal de Reykiavik, una fábrica de fertilizantes, la Universidad de Islandia, el Instituto de Investigación de Islandia y la New Business Venture Fund—. Los socios islandeses controlan el 51,01 % de la sociedad.

Thorsteinn Sigfusson, profesor de física en la Universidad de Islandia de Reykiavik y presidente de Iceland New Energy, afirma que el objetivo del consorcio es convertir al hidrógeno toda la economía de Islandia en el plazo de veinte años y eliminar virtualmente los combustibles fósiles del país. El plan propone convertir primero la flota nacional de coches, autobuses, camiones y barcos pesqueros, y poco después utilizar el hidrógeno para generar electricidad y proporcionar calor, luz y potencia a sus fábricas, oficinas y hogares. Islandia comienza a ser conocida como el «Bahrein del norte» y se habla incluso de la posibilidad de exportar finalmente hidrógeno a Europa, lo que convertiría al país en el primer productor mundial de hidrógeno.

En Hawai se ha puesto en marcha un proyecto similar. El Estado, que importa buena parte de su petróleo en barcos procedentes sobre todo de Asia y Alaska, tiene la esperanza de conseguir la autonomía energética mediante el aprovechamiento de la abundante energía geotérmica y solar que tiene a su disposición y su conversión en combustible de hidrógeno. La legislación del Estado aprobó en abril de 2001 una pequeña subvención para ayudar a crear un consorcio público/privado para explotar la energía del hidrógeno. La Universidad de Hawai recibió una subvención adicional de 2 millones de dólares del Departamento de Defensa de Estados Unidos para llevar adelante el proyecto del hidrógeno. Hermina Morita, representante estatal que preside el comité legislativo para reducir la dependencia de Hawai respecto del petróleo, dice que el objetivo a largo plazo es «que seamos capaces de producir más hidrógeno del que necesitamos, de modo que podamos enviar los excedentes a California».

Vale la pena señalar que el primer organismo que utilizó la expresión «economía del hidrógeno» fue General Motors (GM), el principal fabricante de automóviles del mundo. Lo hizo en 1970, cuando los ingenieros de GM comenzaron a ver en el hidrógeno el posible combustible del futuro. Treinta años más tarde, después de que un buen número de esfuerzos pioneros hayan demostrado la viabilidad de un futuro basado en el hidrógeno, el director ejecutivo de GM para los Vehículos de Tecnologías Avanzadas, Robert Purcell, decía ante los miembros congregados en la reunión anual de la Asociación Nacional de Industrias Petroquímicas y Refinerías que «nuestro proyecto a largo plazo es una economía basada en el hidrógeno».

La producción de energía a partir del hidrógeno

Tal como señalábamos antes, el hidrógeno se halla prácticamente en todas partes, pero raramente aparece en la naturaleza en estado libre. Se halla siempre combinado en el agua, los combustibles sólidos y los seres vivos, y debe ser extraído antes de que pueda ser usado como forma de energía.

Existen varias formas de producir hidrógeno. En la actualidad, casi la mitad del hidrógeno que se produce en el mundo es extraído del gas natural por medio de un proceso de reformación con vapor. El método consiste en provocar una reacción entre el gas natural y el vapor con la ayuda de un convertidor catalítico. El proceso libera átomos de hidrógeno y deja como residuo dióxido de carbono. También se puede reformar el carbón para producir hidrógeno por medio de un proceso de gasificación, pero resulta más caro que hacerlo con gas natural. También se puede obtener hidrógeno a partir del petróleo o de la biomasa gasificada.

Aunque el uso de vapor para reformar el gas natural ha demostrado ser la forma más barata de producir hidrógeno con fines comerciales, se sigue basando en un hidrocarburo y emite CO2 en el proceso de conversión. Sus defensores argumentan que en el futuro el CO2 generado en el proceso podría ser aislado y guardado en depósitos subterráneos, como por ejemplo yacimientos agotados de petróleo o gas natural y lechos profundos de carbón, aunque reconocen que eso aumentaría los costes de la producción de hidrógeno. La viabilidad de la tecnología de aislamiento está todavía en duda, e incluso sus defensores reconocen que faltan como mínimo diez años para que resulte aplicable comercialmente.

La mayoría de los analistas de la industria están convencidos de que en el futuro más inmediato las fuentes primarias de hidrógeno serán el gas natural y, en menor medida, otros combustibles fósiles. Sin embargo, sus análisis se basan en el dudoso supuesto de que seguirá habiendo suficiente gas natural barato como para cubrir no sólo la demanda de hidrógeno, sino también las crecientes demandas de la industria eléctrica, que está preparando una nueva generación de centrales de gas natural con las que espera producir buena parte de nuestra electricidad en las próximas décadas, por lo menos en Estados Unidos. Sin embargo, si la producción global de gas natural toca techo alrededor de 2020, tal como prevén actualmente algunos geólogos, entonces será necesario encontrar otras formas de producir hidrógeno. Incluso el Instituto de Investigación sobre la Energía Eléctrica [Electric Power Research Institute, EPRI], el centro de estudios estratégicos de la Industria Americana de Servicios Públicos, reconoce en su estudio interno que tal vez no sea posible generar la suficiente cantidad de gas natural a precios baratos como para sostener los incrementos actualmente previstos en la producción de electricidad, y eso antes de considerar siquiera la posibilidad de un aumento significativo del uso de gas natural para producir hidrógeno. El estudio del EPRI prevé que la producción de gas natural aumentará entre un 15 % y un 60% en los próximos veinte años, simultáneamente a la introducción de centenares de nuevas plantas generadoras basadas en el gas natural. A pesar de que las compañías eléctricas ya han apostado por las centrales de gas natural, uno de los miembros del EPRI, el doctor Gordon Hester, afirma a partir de su estudio que «no se puede mantener durante tanto tiempo una dependencia tan elevada del gas natural para la generación de electricidad». Según el Instituto, el aumento de la demanda de electricidad probablemente causará un aumento en el precio del gas natural, lo cual animará a sustituirlo por otros combustibles más baratos y que generen menos emisiones. El resultado, según el estudio del EPRI, sería que el uso del gas natural para generar electricidad descendería de forma significativa a partir de 2025.

Si dentro de veinte años no habrá la suficiente cantidad de gas natural disponible para cubrir la demanda de electricidad, parece un error confiar en él como fuente para la producción de hidrógeno libre. Existe, sin embargo, otra forma de producir hidrógeno sin utilizar combustibles fósiles en el proceso. Tal como se ha señalado antes, la electrólisis utiliza la electricidad para descomponer el agua en átomos de hidrógeno y oxígeno. Se trata de un proceso conocido desde hace cien años. Funciona del siguiente modo: se sumergen dos electrodos, uno positivo y el otro negativo, en agua pura cuya capacidad conductiva ha sido aumentada por medio de la incorporación de un electrolito; cuando se aplica electricidad —corriente directa—, el hidrógeno se desplaza hacia el electrodo de carga negativa (el cátodo) y el oxígeno hacia el de carga positiva (el ánodo).

En varios países existen plantas de electrólisis industrial. El equipamiento incluye el tanque básico, así como un convertidor para cambiar la corriente alterna a corriente directa, tuberías para conducir el hidrógeno y el oxígeno desde las pilas, y el equipo necesario para secar los gases después de separarlos del electrolito.

La electrólisis no está demasiado extendida —sólo el 4% del hidrógeno que se produce anualmente procede de la electrólisis del agua— porque los costes de la electricidad empleada en el proceso hacen que no resulte competitiva en relación con el proceso de reformación del gas natural. El coste de la electricidad puede llegar a ser tres o cuatro veces superior al de los materiales empleados para obtener hidrógeno a partir del gas natural. Es importante destacar esta última idea, porque muchos observadores han terminado por creer que el propio proceso de electrólisis es caro e ineficiente, cuando en realidad lo que encarece el proceso es el coste de generar la electricidad en grandes plantas eléctricas centralizadas. Según el Instituto de Tecnología del Gas, «la mayoría de los electrolizadores comerciales disponibles hoy en día pueden alcanzar una eficiencia superior al 75% en términos de electricidad obtenida por unidad de hidrógeno, mientras que sus costes de inversión son potencialmente muy inferiores a los de las centrales eléctricas que harían falta para mantenerlos en funcionamiento».

La verdadera cuestión, por lo tanto, es si es posible emplear formas renovables de energía sin carbono como la fotovoltaica, la eólica, la hidráulica y la geotérmica para generar la electricidad que se consume en el proceso de la electrólisis para descomponer el agua en hidrógeno y oxígeno. Cada vez hay más expertos que dicen que sí, aunque matizan que los costes de dichas formas renovables de energía deberán bajar considerablemente antes de que el proceso resulte competitivo frente a la reformación del gas natural con vapor. Seth Dunn, miembro del World Watch Institute, observa que «los costes de los electrolizadores basados en la energía solar o eólica son todavía altos», aunque «se prevé que se reduzcan a la mitad durante la próxima década». Si la producción de gas natural toca techo y su precio se dispara, se podría alcanzar un punto en el que fuera más barato usar fuentes renovables de energía para producir la electricidad que requiere el proceso electrolítico.

Aprovechar directamente la energía del Sol y convertirla en energía útil ha sido el sueño de ingenieros y científicos durante mucho tiempo. La cantidad de energía potencialmente disponible en los rayos solares es realmente increíble. John Houghton afirma que «la Tierra recibe del Sol en cuarenta minutos la misma energía que consumimos en todo un año». Captar esta energía solar era tan sólo un sueño remoto hace cien años. Ya no lo es. Los dispositivos fotovoltaicos (FV), que utilizan materiales semiconductores para convertir la luz solar en electricidad, están cada vez más extendidos en todo el mundo y, aunque todavía son caros, su precio está bajando lentamente. El coste de las células solares ha bajado un 95% desde los años setenta.

En la actualidad ya se ha extendido el uso comercial de dispositivos fotovoltaicos como fuente de alimentación de relojes y calculadoras. Las naves espaciales utilizan «paneles solares» u otros dispositivos cubiertos de células FV que les permiten obtener la electricidad necesaria para que los astronautas puedan trabajar en el espacio exterior. La eficiencia de los sistemas fotovoltaicos se encuentra entre el 10 y el 20% y un panel de células solares de un metro cuadrado produce entre 100 y 200 vatios de energía eléctrica.

El primer proyecto a gran escala de aprovechamiento de la energía solar para la generación de electricidad tuvo lugar en los años ochenta. En el desierto de Mojave, entre Las Vegas y Los Ángeles, se construyeron nueve centrales eléctricas térmicas solares que usaban espejos parabélicos estriados para captar los rayos solares. Entre todos proporcionan 354 megavatios de electricidad a los hogares y las industrias de la región.

Los dispositivos fotovoltaicos se han hecho también cada vez más populares a lo largo de los últimos años en los países en vías de desarrollo del hemisferio sur. BP Solar, una compañía que produce el 10% de las células fotovoltaicas del mundo, ha reunido 48 millones de dólares para poner en marcha la iniciativa más importante del mundo en el campo de la energía solar en la isla de Mindanao, Filipinas. Cuando el proyecto esté terminado, proporcionará electricidad a los 400.000 habitantes de los 150 pueblos de una de las regiones más pobres del Pacífico. La electricidad también mantendrá en funcionamiento 69 sistemas de irrigación, 97 plantas potabilizadoras de agua, y proporcionará potencia y luz a docenas de escuelas y centros médicos.

La electricidad generada con los sistemas fotovoltaicos es todavía entre dos y cinco veces más cara que la electricidad convencional generada a partir de combustibles fósiles, pero los costes no paran de bajar gracias a las innovaciones tecnológicas y la aparición de economías de escala. En 1998, el precio de la electricidad generada con los sistemas fotovoltaicos bajó por primera vez de los 4 dólares por vado.

El 26 de septiembre de 1995 comenzó a funcionar la primera planta solar de hidrógeno de Estados Unidos, situada en El Segundo, California. El proyecto, que costó 2,5 millones de dólares y en el que participaban Clean Air Now (CAN), una organización ecologista local y Xerox Corporation, captaba la radiación solar por medio de un avanzado sistema fotovoltaico diseñado por Solar Engineering Applications Corporation, una empresa californiana de alta tecnología, y la transformaba en electricidad que luego era enviada a un electrolizador fabricado por la empresa canadiense Electroliser Corporation. El electrolizador producía entre 42 y 56 metros cúbicos estándar de hidrógeno al día. Una vez eliminado el vapor de agua sobrante, el hidrógeno en estado gaseoso era comprimido hasta una presión de 351 kg/cm², secado y almacenado. El gas hidrógeno producido en la planta sirve para alimentar unos camiones Ford Ranger especialmente modificados para funcionar con combustible descarbonizado. En la actualidad, compañías como Royal Dutch/Shell y British Petroleum están invirtiendo miles de millones de dólares en tecnología solar y otras energías renovables para anticiparse al final de la era de los combustibles fósiles. Shell cree que en 2050 dichas energías cubrirán más de la tercera parte del mercado de la electricidad, con unas cifras totales de ventas de 150.000 millones de dólares. Recientemente, Shell se asoció con Siemens para crear la cuarta compañía de energía solar más grande de Estados Unidos. John Browne, director ejecutivo de British Petroleum, va incluso más lejos y prevé que dentro de cuarenta y ocho años la energía solar y otros recursos renovables cubrirán el 50% de las demandas totales de energía.

El 2% de la energía solar se convierte en energía eólica por medio de la circulación atmosférica. Hoy por hoy, el viento es la fuente renovable de energía con una mejor relación coste-eficacia. Un generador eólico está formado por dos hélices de aproximadamente 50 metros de diámetro. En una zona donde la velocidad media del viento sea de 7,5 metros/segundo, las hélices generarán alrededor de 250 kilovatios de electricidad. Según la Asociación Americana de la Energía Eólica, el coste del kilovatio/hora de energía generada a partir del viento ha caído desde los 40 centavos a comienzos de los años ochenta hasta menos de 5 centavos y, en algunos lugares, hasta los tres centavos. El Departamento de Energía de Estados Unidos afirma que en algunas regiones del país la energía eólica es tan barata y eficiente en la actualidad que resulta competitiva con las centrales eléctricas de gas. Cuando en los próximos años la energía eólica baje hasta situarse por debajo de centavos por kilovatio/hora, el hidrógeno generado eléctricamente a partir de ella podrá competir con la gasolina.

En los últimos años, la capacidad eólica mundial ha estado creciendo a una tasa del 27,75% anual. La Asociación Europea de la Energía Eólica predice que en 2020 la energía eólica producirá el 10% de la electricidad a nivel mundial. La industria eólica es uno de los mercados de la economía mundial que crece más rápidamente. El analista Michael Kujawa, de Allied Business Intelligence Inc., estima que en 2010 las ventas de turbinas eólicas superarán los 200.000 millones de dólares. Europa lidera la transición hacia la energía eólica. De los 15 GW de capacidad eólica que existen actualmente en el mundo, 10 GW se han instalado en Europa. En Dinamarca, el viento aporta actualmente el 14% de la producción eléctrica nacional. En algunas regiones del norte de Alemania la energía eólica aporta el 15% de la electricidad que se genera. Los fuertes vientos de la costa oeste británica harán que la energía eólica se convierta en un importante competidor en el futuro mercado energético de este país. Algunas previsiones estiman que la energía eólica podría cubrir en un futuro cercano el 10% o más de las necesidades eléctricas de Gran Bretaña. Un estudio preparado por Germanischer Lloyd y Garrad Hassan estima que el potencial de la energía eólica de las regiones costeras del Báltico y el Mar del Norte podría ser suficiente como para cubrir las necesidades eléctricas de todo el continente europeo.

Los países en vías de desarrollo también están incrementando su capacidad eólica: la India es uno de los cinco primeros productores mundiales, con 1 GW de potencia ya instalado. En 2030, India podría producir 10 GW de potencia eléctrica, el 25% de sus necesidades actuales, gracias a la tecnología eólica.

En Estados Unidos se considera que los Estados llanos que se extienden desde Texas hasta las dos Dakotas podrían ser regiones privilegiadas para la generación de energía eólica. En Texas ya se está construyendo una granja eólica que proporcionará electricidad a 139.000 hogares.

La energía hidroeléctrica es otra fuente potencial de energía renovable para la producción de hidrógeno. Prácticamente el 20% de la energía solar que llega a la superficie de la Tierra se consume en la evaporación de agua. Cuando el vapor de agua se condensa y cae en forma de precipitaciones hace posible la generación de energía hidroeléctrica. La energía hidroeléctrica aporta actualmente el 10% de la energía eléctrica generada en Estados Unidos y el 19% de la electricidad generada en el mundo. Seth Dunn sugiere que otros países con energía hidroeléctrica barata, como Brasil, Canadá, Noruega, Suecia e Islandia, podrían ser de los primeros en utilizar a gran escala la electrólisis a partir de energías renovables.

Aunque no está siendo demasiado explotada —apenas representa el 0,1% de la energía mundial— la energía geotérmica también tiene un enorme potencial como recurso energético renovable. El agua caliente y el vapor de las profundidades de la Tierra que emergen en géiseres, fuentes termales y rocas volcánicas pueden ser transformados en electricidad. El Departamento de Energía de Estados Unidos estima que los recursos geotérmicos del país exceden los 70.000.000 quads. Una cantidad como ésta de energía potencial es suficiente como para garantizar la energía necesaria para el consumo humano durante cientos de miles de años. Islandia y Hawai, en su intento de convertirse en economías basadas en el hidrógeno, van a depender cada vez más de su energía geotérmica para poder generar la electricidad con la que producir hidrógeno a partir del agua. Existen regiones privilegiadas desde el punto de vista geotérmico en el océano Pacífico, la India, el sureste asiático, las costas de China y Japón, las costas occidentales de Canadá, de Estados Unidos y Sudamérica, así como partes del Mediterráneo, Rusia y la zona este de África.

La biomasa, en forma de residuos agrícolas e industriales, también puede generar la electricidad necesaria para la producción de hidrógeno por electrólisis del agua. El Reino Unido, por ejemplo, produce 27 millones de toneladas de residuos tóxicos al año. Si estos residuos fueran incinerados para generar electricidad podrían llegar a cubrir el 5 % de la demanda total del Reino Unido. Según el Departamento de Energía de Estados Unidos, la gasificación de la biomasa por medio de turbinas de última generación podría reducir el coste del kilovatio/hora hasta 4,5 centavos en los próximos años. Shell prevé que hacia el final de la primera década del siglo XXI la energía producida a partir de la gasificación de la biomasa podría cubrir el 5% de las necesidades energéticas mundiales. Aunque la biomasa produce CO2 durante la gasificación, la emisión queda compensada por un nuevo crecimiento vegetal, por lo que el proceso es neutral desde el punto de vista del carbono.

El aspecto más importante de la producción de hidrógeno a partir de recursos renovables es que permitirá finalmente «almacenar» la energía solar, eólica, hidráulica y geotérmica para usarla en formas concentradas cuando y donde sea necesario, sin producir ninguna emisión de CO2. Conviene hacer hincapié en esta cuestión. Sin el uso del hidrógeno como un medio de almacenar energía, una futura energía renovable, si no imposible, entraña por lo menos gran dificultad. Ello obedece a que cuando cualquier forma de energía se aprovecha para producir electricidad, ésta fluye de inmediato. Por tanto, si el Sol no brilla, el viento no sopla, el agua no corre o no se pueden quemar combustibles fósiles, no se puede generar electricidad y la actividad económica sufrirá un parón. El hidrógeno es un modo muy ventajoso de almacenar energía, lo cual asegura que la misma pueda ser suministrada de forma ininterrumpida para la sociedad.

Sin embargo, crear una infraestructura que permita almacenar el hidrógeno plantea problemas adicionales de coste. Los defensores de la combinación «energías renovables-hidrógeno» tienen la esperanza de que se realicen avances importantes en el desarrollo de pequeñas pilas de combustible, tanto fijas como portátiles, así como en su rápida comercialización como miniplantas energéticas de uso corriente en fábricas, oficinas, tiendas, hogares y automóviles.

Las pilas de combustible: las miniplantas energéticas

Las pilas de combustible no son un invento nuevo. En realidad, son anteriores al motor de combustión interna. Sin embargo, no despertaron demasiado interés comercial hasta los años sesenta, cuando la NASA tomó la decisión de utilizarlas en su programa espacial para proporcionar energía eléctrica a los vehículos espaciales. Las misiones lunares Apolo utilizaron pilas de combustible desarrolladas por la división Pratt & Whitney de la United Aircraft Corporation. La compañía, que más tarde pasaría a llamarse United Technologies, también sería la encargada de desarrollar las pilas de combustible para el programa del transbordador espacial.

Las pilas de combustible son como las baterías, pero con una gran diferencia: las baterías almacenan energía química y la convierten en electricidad. Cuando se termina la energía química, la batería se tira. Las pilas de combustible, en cambio, no almacenan energía química, sino que convierten en electricidad la energía química de un combustible que reciben del exterior. No necesitan ser recargadas y continúan generando electricidad mientras les sea suministrado combustible y oxidante.

Las pilas de combustible requieren hidrógeno como combustible. Los hidrocarburos son demasiado «sucios» para ser usados como combustible primario de la pila. Una pila de combustible está formada por un ánodo de carga negativa en un lado, un cátodo de carga positiva en el otro y un electrolito en el medio consistente en una solución alcalina o de ácidos acuosos, o bien en una membrana de plástico que permita el paso de los átomos de hidrógeno eléctricamente cargados desde el ánodo hasta el cátodo. Las pilas de combustible comerciales son una suma de pilas individuales. Cuando se introduce el hidrógeno en el lado del ánodo de la pila se produce una reacción química que rompe el átomo de hidrógeno en un protón y un electrón. Los electrones liberados salen a través del circuito eléctrico externo en forma de corriente eléctrica directa. Los iones de hidrógeno (los protones) viajan a través de la capa del electrolito hacia el cátodo, cargado positivamente. El flujo de electrones regresa al cátodo, donde reaccionan con los iones de hidrógeno y con el oxígeno del aire para formar agua. Las pilas de combustible invierten el proceso de la electrólisis, no tienen partes móviles, son silenciosas y dos veces y media más eficientes que los motores de combustión interna. Además, sólo producen electricidad, calor y agua pura destilada.

Las pilas de combustible, en combinación con el hidrógeno, podrían producir potencialmente suficiente electricidad como para cubrir las necesidades de la especie humana a largo plazo. Superar la era de los combustibles fósiles, sin embargo, no será fácil. Producir hidrógeno es caro. Además, la mayoría de las pilas de combustible funcionan actualmente con gas natural y otros combustibles fósiles. Tal como se ha señalado antes, ya existe cierta producción de hidrógeno a pequeña escala gracias al aprovechamiento de formas renovables de energía como la solar, la eólica, la hidráulica y la geotérmica, que producen la electricidad necesaria para separar el hidrógeno del agua. En años recientes, se han instalado sistemas electrolíticos basados en la energía solar y eólica en Alemania, Italia, España, Suiza, Finlandia, Estados Unidos e incluso Arabia Saudí. Sin embargo, el proceso sigue siendo más caro que el reformado de hidrocarburos con vapor.

Por otro lado, las pilas de combustible son caras. Como toda tecnología nueva, la producción de pilas de combustible no ha alcanzado aún el umbral crítico en el que las economías de escala permiten reducir de forma significativa el coste por unidad fabricada. Sin embargo, un gran número de compañías nuevas y algunas de las principales multinacionales del mundo comienzan a entrar en este campo con la esperanza de liderar la transición hacia la economía del hidrógeno. Las empresas Ballard Power Systems, de Burnaby, en la Columbia Británica, y Plug Power, de Latham, en Nueva York, han lanzado ambiciosos proyectos de marketing para equipar los hogares y las empresas comerciales con pilas de combustible fijas. Las unidades domésticas consisten en plantas energéticas de 1 a 15 kilovatios de potencia, mientras que las comerciales van de 60 a 250 kilovatios. Plug Power cuenta con la participación de General Electric y espera poder salir al mercado a finales de 2002 con miles de unidades domésticas.

La generación distribuida

Prácticamente todos los participantes en el juego de la energía del futuro están atentos a una nueva forma de distribuir la electricidad, llamada generación distribuida (GD), que podría responder a los problemas de coste y abrir el camino hacia la nueva era energética. La GD invierte completamente la lógica convencional de la distribución de la electricidad. Durante la mayor parte del siglo XX la electricidad se ha generado en grandes centrales eléctricas y luego ha tenido que ser transportada a través de largas distancias mediante líneas de transmisión hasta los consumidores finales. La centralización de la electricidad generaba economías de escala, lo que hacía que la producción y la distribución de la electricidad fueran relativamente baratas. Los elevados costes de capital necesarios para construir centrales eléctricas gigantescas y vastas redes de transmisión sólo podían compensarse dejando que las compañías controlaran todo el mercado regional. Como consecuencia, tanto en Estados Unidos como en el resto del mundo el mercado eléctrico se ha constituido como un servicio público de titularidad estatal o bien como un servicio privado regulado por el gobierno como un monopolio natural.

Sin embargo, en los años setenta y ochenta la infraestructura centralizada de generación eléctrica recibía cada vez más críticas de personas que consideraban que su tamaño mismo la convertía en disfuncional a la hora de enfrentarse a todo tipo de problemas nuevos, como el drástico aumento del coste de la energía derivado del embargo petrolero árabe y las subidas de precio decretadas por la OPEP, así como el problema cada vez más grave de las emisiones de CO2 y otros contaminantes que comenzaban a amenazar el medio ambiente y la salud pública. En respuesta a la creciente presión pública por encontrar nuevas formas de ahorrar energía, el Congreso de Estados Unidos aprobó en 1978 la Ley Reguladora de los Servicios Públicos [Public Utilities Regulatory Policy Act, PURPA], una legislación diseñada, en parte, para fomentar la extensión de la cogeneración (reciclar el calor producido por la generación de electricidad para la calefacción y el suministro eléctrico en fábricas y oficinas). La Ley fomentaba la entrada de nuevas compañías en el mercado de la electricidad y la creación de una incipiente competencia.

Mientras tanto, la desregulación del mercado del gas natural llevó a un descenso de los precios de este combustible y estimuló nuevos avances en el campo de la generación de electricidad con gas natural. Las nuevas turbinas de gas tenían una buena relación coste-eficiencia para potencias iguales o inferiores a los 100 megavatios y requerían una inversión de capital muy inferior a las plantas nucleares o de carbón de 1.000 megavatios convencionales. Las nuevas centrales eléctricas de gas también requerían menos tiempo de instalación y eran más fáciles de mantener. Apareció una nueva generación de productores energéticos independientes que ponían en cuestión el viejo fundamento del estatus de «monopolio natural» del que disfrutaban las compañías energéticas —a saber, los elevados costes de capital empleados en dar acceso comercial a la electricidad— a la luz de las nuevas innovaciones técnicas que hacían que la generación de electricidad fuera cada vez más barata y versátil.

La industria de los servicios públicos se enfrentaba a otros problemas que hacían aumentar la presión de la industria y la opinión pública para que abriera las puertas a nuevas formas de competencia y nuevos modelos de distribución de la electricidad. En los años sesenta y setenta las compañías eléctricas habían realizado fuertes inversiones en centrales nucleares. En los años ochenta todos los endeudamientos y cierres provocados por dichas inversiones comenzaban a repercutir sobre los consumidores en forma de drásticos aumentos del precio de la electricidad en algunas regiones. La descapitalización de muchas de las principales compañías energéticas del país hizo que no realizaran las inversiones necesarias en ampliación de la capacidad de generación eléctrica para dar respuesta a la demanda comercial y doméstica. Los apagones, las bajadas y los cortes de tensión se hicieron más frecuentes, para indignación de los clientes comerciales, que tenían que cubrir las pérdidas derivadas del tiempo de inactividad, y también de los consumidores domésticos, poco acostumbrados a sufrir incomodidades. El resultado fue que todos estos monopolios naturales tenían cada vez menos amigos dispuestos a defender su estatus especial, justo en el momento en que comenzaban a aparecer nuevos productores energéticos independientes que les disputaban los clientes.

El fervor desregulador, que alcanzó su punto máximo durante los mandatos presidenciales de Ronald Reagan y George Bush en Estados Unidos y los de Margaret Thatcher y Helmut Kohl como primeros ministros del Reino Unido y Alemania respectivamente, sacudió todas las industrias, pero ninguna se vio más afectada que la industria energética. En 1992 se aprobó en Estados Unidos la Ley de Política Energética, que abría el mercado eléctrico a la competencia. Los productores independientes comenzaron a crear problemas a los grandes gigantes con la introducción de tecnologías de pequeño alcance para servir a mercados especiales. Había nacido la era de la generación distribuida.

La expresión «generación distribuida» se refiere habitualmente a un conjunto de pequeñas plantas generadoras de electricidad situadas cerca del usuario final, o en su mismo emplazamiento, y que pueden bien estar integradas en una red o bien funcionar de forma autónoma. Sus usuarios pueden ser fábricas, empresas comerciales, edificios públicos, barrios o residencias privadas.

En la actualidad, la tecnología más popular en el campo de la generación energética a pequeña escala son los motores alternativos alimentados con combustible diesel o gas natural. El uso de turbinas de gas y microturbinas alimentadas con diversos combustibles fósiles también está cada vez más extendido en el mercado de la energía distribuida. Sin embargo, los expertos coinciden cada vez más en que a largo plazo el dominio y el liderazgo en el mercado de la generación distribuida corresponderá a la pila de combustible de hidrógeno. Además de generar electricidad de forma más eficiente y ser menos contaminante que el motor de combustión, la pila de combustible también es más flexible. Las pilas de combustible se comercializan en módulos, de modo que el usuario final puede personalizar su unidad para adaptarla a sus necesidades energéticas actuales y, si éstas aumentan, en el futuro se pueden incorporar nuevos módulos con un escaso gasto añadido.

La electricidad generada por una pila de combustible cuesta actualmente entre 3.000 y 4.000 dólares por kilovatio, mientras que la electricidad generada por una central eléctrica de gas clásica oscila entre los 500 y los 1.000 dólares por kilovatio. Naturalmente, dicho coste bajará a medida que aumenten las ventas y se generen economías de escala y estímulos a la innovación. Los analistas de la industria se muestran optimistas con respecto al potencial del mercado de las pilas de combustible y la generación distribuida, y por diversas razones consideran probable que la red eléctrica evolucione desde un modelo de generación centralizada hacia un modelo de generación descentralizada próxima al usuario final.

En primer lugar, cabe señalar la creciente inquietud que suscitan las bajadas y los cortes de tensión en los sectores comerciales e industriales, sobre todo en los ámbitos de la alta tecnología, la informática y el software. En el mundo de los negocios se habla actualmente de la necesidad de una electricidad de «alta calidad». En sectores como el industrial, el bancario y el de las telecomunicaciones, que dependen virtualmente no sólo del flujo ininterrumpido de información electrónica por sus internets e intranets, sino también del mantenimiento de bases de datos electrónicas de importancia crítica y de equipos digitales de todo tipo, un corte de electricidad puede significar una pérdida significativa en términos de producción y distribución, así como de activos cruciales de información.

Una interrupción del flujo eléctrico de tan sólo 8 milésimas de segundo puede causar verdaderos desastres. En 1997, una breve interrupción del suministro eléctrico en el Banco Nacional de Omaha, Nebraska, provocó el colapso de los sistemas informáticos que gestionaban las principales transacciones con tarjetas de crédito. El banco estima que un corte de corriente de una hora tendría un coste de más de seis millones de dólares en ingresos perdidos para la institución. Como consecuencia de ello, el banco instaló en su centro tecnológico un sistema de pilas de combustible de 200 kilovatios para garantizar su seguridad frente a los fallos de la red eléctrica general.

En algunas industrias, una simple bajada de tensión puede provocar la desconexión de los equipos y causar unas pérdidas de millones de dólares. Hewlett Packard estima que un apagón de 15 minutos en una de sus fábricas de chips costaría a la compañía 30 millones de dólares, el equivalente a todo el consumo energético de la planta en un año. Los cortes en el suministro eléctrico cuestan a la industria estadounidense entre 12.000 y 26.000 millones de dólares anuales, unas cifras que probablemente no harán más que aumentar a medida que las relaciones comerciales de las compañías dependan cada vez más de las tecnologías digitales, el software informático y las redes electrónicas de todo tipo.

Incluso los grandes edificios comerciales están comenzando a instalar pilas de combustible para no ser tan vulnerables ante los posibles apagones como los que en el pasado paralizaron ciudades como Nueva York, Un nuevo edificio que se acaba de construir en Times Square, Nueva York, dispone de 200 kilovatios de potencia generados con pilas de combustible propias que se utilizan para el agua caliente, la iluminación de la fachada y como reserva de emergencia para la red principal. La mejora de la calidad del suministro eléctrico constituye un mercado que mueve cada año entre 7.000 y 10.000 millones de dólares en Norteamérica, sobre todo en forma de generadores eléctricos de emergencia. Dicho mercado podría llegar a ampliarse extraordinariamente. Si aumenta la frecuencia de los cortes de electricidad en las redes generales al tiempo que suben los precios del servicio, las compañías podrían optar por convertir sus generadores de emergencia locales en su fuente primaria de electricidad.

Los servicios públicos básicos son también cada vez más vulnerables a los cortes de electricidad de la red centralizada. Los hospitales, las comisarías de policía y las estaciones de bombeo de agua ya poseen actualmente generadores de emergencia locales. El Departamento de Policía de Nueva York ha optado por instalar una pila de combustible en una comisaría de Central Park porque sus costes son sumamente inferiores a los de excavar en el parque para instalar líneas eléctricas subterráneas. Los consumidores domésticos también están preocupados por los problemas que crean los cortes de electricidad. La primera pila de combustible doméstica fue instalada en una casa estilo rancho cerca de Albany, Nueva York, en junio de 1998. Una pila de combustible del tamaño de una nevera puede proporcionar hasta 50 kilovatios de electricidad para una casa. En el futuro, la generación distribuida podría convertirse en el negocio estrella del mercado doméstico. En la actualidad hay millones de personas que trabajan desde sus hogares y dependen del flujo ininterrumpido de electricidad para mantenerse conectados a la red. Para ellos, la posesión de un generador de emergencia podría convertirse en un coste necesario del trabajo. Muchos otros consumidores simplemente se han cansado de los apagones y de la correspondiente pérdida de calefacción, aire acondicionado, refrigeración y otros servicios, y podrían optar por jugar a lo seguro e instalar su propia miniplanta eléctrica. El envejecimiento de la población hará que su existencia sea cada vez más doméstica y dependiente de equipos médicos de soporte vital y toda clase de servicios, lo cual podría hacer que se instalaran más generadores de emergencia cuando disminuya la confianza en el servicio que proporcionan las compañías eléctricas. Por último, una sociedad cada vez más sensibilizada en relación con los problemas de seguridad, preocupada por el crimen y ahora por el terrorismo, podría aumentar sus exigencias de seguridad e incluir pronto los generadores de emergencia en su lista de prioridades. En definitiva, más de un millón de consumidores domésticos compran cada año un tipo u otro de generador eléctrico de reserva. A medida que baje el precio de las pilas de combustible y sean más fáciles de instalar y usar, es probable que se conviertan en la opción más popular.

La generación distribuida también está ganando adeptos como consecuencia de la creciente preocupación que suscita el calentamiento global y el deseo de usar la energía de forma más eficiente para reducir las emisiones de CO2. Mediante la instalación de una miniplanta en casa o en la oficina, los usuarios finales pueden aprovechar el calor generado por la electricidad como calefacción o bien para generar electricidad adicional. La cogeneración aumenta en gran medida la eficiencia al reducir hasta en un 50% la cantidad de combustible empleado. La cogeneración también reduce hasta en un 50% las emisiones de CO2, ya que no es necesario producir y transportar separadamente la electricidad y la energía térmica hasta los usuarios finales.

Una transición general desde la generación eléctrica centralizada, basada en la energía de los combustibles fósiles hacia un modelo de pilas de combustible de hidrógeno integradas en una red de generación distribuida —sobre todo si el hidrógeno se produce a partir de la energía solar, eólica, hidráulica y geotérmica— podría recortar las emisiones de CO2 de forma más drástica que ningún otro cambio que se haya planteado hasta el momento. El Informe Mundial sobre la Energía, publicado conjuntamente por el Programa de Desarrollo de las Naciones Unidas, el Departamento de Asuntos Económicos y Sociales de las Naciones Unidas y el Consejo Mundial de la Energía, concluye que un régimen energético basado en el hidrógeno y prácticamente exento de emisiones «proporcionaría a la sociedad la capacidad de alcanzar, a largo plazo, grandes reducciones en las emisiones de CO2 [...] y ayudar con ello a limitar los niveles atmosféricos de CO2 hasta el doble de los niveles de la era preindustrial o incluso menos, en respuesta a la preocupación por el cambio climático».

Cada vez son más los clientes que utilizan la generación distribuida para sumarse a lo que los industriales llaman «regulación de costes» [peak shatang]. El precio de la electricidad puede variar de una hora a otra en respuesta a la demanda y a la capacidad disponible. Dichas fluctuaciones se traducen según el consumo diario y estacional en categorías como las horas punta, normal y reducida. En la hora punta, cuando la demanda es elevada, las compañías de servicios acostumbran a recurrir a sus plantas más viejas y menos eficientes. El coste adicional repercute sobre los consumidores en forma de una subida de las tarifas eléctricas. En la hora punta los propietarios de plantas de generación distribuida pueden optar por salir de la red principal y usar su propia planta eléctrica para ahorrar dinero.

Según escribe Peter Fairley en Technology Review, en el futuro las pilas de combustible podrán controlar las tarifas a través de Internet o de señales digitales transmitidas por la propia electricidad. Las pilas de combustible analizarán la información recibida, por ejemplo, sobre el precio del gas natural y de la electricidad en aquel momento, y si resulta ventajoso pasar a la generación distribuida, la unidad se pondrá automáticamente en funcionamiento. Como la mayoría de los consumidores domésticos y comerciales no son expertos en las complejidades del negocio de la energía, surgirá un nuevo tipo de intermediarios que actuarán como servicios de suministro de combustible. Un ejemplo es Williams International, una compañía energética de Tulsa cuyos gasoductos transportan actualmente el 20% del suministro de gas natural en Estados Unidos y que «ofrece un completo servicio de suministro energético: financia la miniplanta eléctrica, suministra electricidad desde la red y ayuda a los consumidores a regular los costes».

Resulta interesante saber que las compañías de servicios públicos también se beneficiarán de la generación distribuida, aunque hasta hace poco muchas han tratado de impedir su desarrollo. Como la generación distribuida se adapta a las necesidades energéticas específicas del usuario final, constituye una forma más barata y eficiente de proporcionar un suministro adicional de electricidad que confiar en una fuente centralizada. La instalación de una línea eléctrica de 10 kilómetros para un cliente de 3 MW le cuesta a una compañía de servicios entre 365 y 1.100 dólares por kilovatio/hora. Un sistema de generación distribuida puede cubrir estas mismas necesidades a un coste que oscila entre los 400 y los 500 dólares por kilovatio/hora. Generar la electricidad cerca de los usuarios finales también reduce la cantidad de energía consumida, dado que entre el 5 y el 8% de la energía transportada mediante líneas de larga distancia se pierde durante la transmisión.

Las compañías eléctricas estadounidenses están interesadas en evitar grandes aportaciones de capital porque las nuevas leyes que reestructuran los servicios públicos no les permiten repercutir sobre sus consumidores el coste de las nuevas inversiones realizadas para mejorar su capacidad. El alto grado de competencia del mercado actual hace que las compañías eléctricas se muestren reacias a retirar fondos de sus reservas para financiar nuevas instalaciones. El resultado es que las compañías explotan las plantas existentes hasta más allá de su capacidad, lo cual hace que los fallos y los apagones sean cada vez más frecuentes. Por este motivo algunas compañías eléctricas están volviendo su mirada hacia la generación distribuida como una forma de dar respuesta a la creciente demanda comercial y doméstica, con una exposición financiera limitada. El factor crítico, desde el punto de vista de las compañías de servicios públicos, es controlar la generación distribuida y hacerla trabajar «a su favor, y no en su contra».

En su libro Distributed Generation, Ann Chambers señala dos estrategias que han comenzado a aplicar las compañías eléctricas para «controlar el nuevo activo». La compañía puede construir e instalar nuevas plantas de generación distribuida a lo largo de sus propios sistemas de transmisión. También puede arrendar las pilas de combustible a los usuarios finales o instalar sus propias miniplantas a domicilio y llegar a un acuerdo con el usuario para que durante los períodos de máximo consumo salgan de la red eléctrica principal y utilicen su propio generador para disminuir la carga y evitar cortes de electricidad masivos en el conjunto del sistema. Los usuarios finales se verían compensados con un descuento en su factura energética.

La generación distribuida promete grandes beneficios, tanto para las compañías eléctricas como para los usuarios finales. En su extenso informe de 1999 sobre las ventajas de la generación distribuida, la empresa de consultoría e investigación Arthur D. Little concluía que «la GD tiene el potencial necesario para desempeñar una función crucial como complemento o alternativa a la red eléctrica [...] la amplia gama de tecnologías de GD y la variabilidad de su tamaño, rendimiento y aplicaciones posibles sugiere que la GD podría aportar soluciones al problema del suministro eléctrico en muchos centros industriales, comerciales y residenciales de Estados Unidos». Incluso los analistas de la industria más conservadores predicen que la generación distribuida cubrirá en el futuro el 30% de la nueva capacidad de generación en Estados Unidos.

La red energética del hidrógeno [Hydrogen Energy Web, HEW]

Las revoluciones económicas verdaderamente importantes de la historia se producen cuando una nueva tecnología de comunicación se funde con un régimen energético emergente para crear un paradigma económico completamente nuevo. La introducción de la imprenta en el siglo XV, por ejemplo, estableció una nueva forma de comunicación que cuando más adelante se combinó con la tecnología del carbón y el vapor dio origen a la revolución industrial. La imprenta hacía posible una forma de comunicación lo bastante rápida y ágil como para coordinar un mundo impulsado por la energía del vapor. No habría sido posible coordinar el incremento de la velocidad, el ritmo, el flujo, la densidad y la interactividad de la vida social y comercial que hacía posible la energía del vapor sólo con las tecnologías de la comunicación oral y escrita. Del mismo modo, el telégrafo y más tarde el teléfono proporcionaron formas de comunicación lo bastante rápidas como para adaptarse al nuevo salto en el ritmo, el flujo, la densidad y la interactividad que tuvo lugar cuando la producción de carbón tocó techo y fue sustituido por otro hidrocarburo todavía más ágil, el petróleo crudo.

En la actualidad, el hidrógeno y las nuevas tecnologías de generación distribuida por medio de pilas de combustible se están comenzando a fusionar con la revolución informática y de las telecomunicaciones para crear una era económica completamente nueva. El sofisticado software informático, las tecnologías digitales inteligentes y el acceso a Internet permiten conectar las pilas de combustible individuales en las que se basa la incipiente revolución de la generación distribuida para formar los rudimentos de una red de energía distribuida. Pronto los usuarios finales no sólo estarán en condiciones de generar su propia electricidad, sino que también podrán compartirla con otros, lo que supondrá una seria amenaza para el régimen energético vertical y unidireccional que se impone actualmente en todo el mundo. «La transformación de los usuarios pasivos de la energía en productores autónomos de energía», escribe Steve Silberman en la revista Wired, es «un proceso paralelo al desarrollo progresivo de la interactividad, los recursos compartidos [peer-to-peer sharing] y la autonomía» en la World Wide Web. Las consecuencias de conectar a todos los propietarios de miniplantas energéticas en una red de energía compartida serán tan amplias y profundas como las que tuvo el desarrollo de la World Wide Web en los años noventa.

Tras señalar los sorprendentes paralelismos que existen entre lo que ya ha ocurrido con la World Wide Web y lo que está a punto de ocurrir con la generación distribuida, el Instituto de Investigación sobre la Energía Eléctrica concluye en sus recientes «Perspectivas de futuro» que la GD se va a desarrollar

de forma muy parecida a como ha evolucionado la industria informática. Los grandes ordenadores centrales han dejado paso a pequeños dispositivos de escritorio o portátiles diseminados geográficamente e interconectados en redes plenamente integradas y extremadamente flexibles. En nuestra industria, las plantas centralizadas continuarán desempeñando sin duda un papel importante. Sin embargo, cada vez será más necesario instalar generadores distribuidos, más pequeños y limpios [...] con el apoyo de tecnologías de almacenamiento de energía. Un requisito básico de un sistema como éste serán los sofisticados controles electrónicos: su papel será absolutamente esencial para gestionar el inmenso tráfico de información y energía que supondrá una interconexión tan compleja.

Muchas de las consideraciones e inquietudes que llevaron al desarrollo de la World Wide Web también se hallan presentes en el incipiente desarrollo de la HEW. El Pentágono creó el precursor de Internet a finales de los años sesenta. El Departamento de Defensa [Department of Defense, DOD] estaba interesado en recortar los costes que suponía instalar grandes superordenadores para los centros de investigación académicos y de defensa, y comenzó a explorar formas de compartir los ordenadores entre personas separadas por grandes distancias. El ejército también estaba preocupado por el carácter potencialmente vulnerable de un control central de las comunicaciones ante los ataques externos u otras formas de interferencia. Estaban buscando un nuevo medio de comunicación descentralizado en el que todas las partes pudieran generar y transmitir la información entre ellas, así como recibir información de las demás, de tal modo que el sistema siguiera funcionando aunque se viera parcialmente bloqueado o destruido. La solución llegó con ARPANET, un sistema desarrollado por la Agencia de Proyectos Avanzados de Investigación del DOD. El primer ordenador anfitrión [host] entró en funcionamiento en 1969. En 1988 se habían conectado más de 60.000 ordenadores anfitriones. La Fundación Nacional de la Ciencia [National Science Foundation, NSF] creó poco después su propia red para conectar a los investigadores universitarios de todo el país. Cuando en 1990 ARPANET dejó de funcionar, la red del NSF pasó a ser el vehículo principal de conexión entre ordenadores y terminó por convertirse en Internet.

Dos compañías de Denver (Colorado), Encorp y Celerity, han conectado cinco grandes generadores distribuidos en centros comerciales e industriales para formar una de las primeras minirredes energéticas. Juntas, pueden producir hasta 5 megavatios de potencia. Celerity y una empresa llamada Sixth Dimensión están organizando una minirred parecida en Albuquerque, Nuevo México, que conectará doce generadores. La red generará 25 megavatios de potencia. En el futuro, las «plantas energéticas virtuales» podrán conectar miles de pilas de combustible y generar la misma electricidad que una planta eléctrica centralizada de 1.000 megavatios.

Las compañías de servicios de treinta Estados permiten actualmente que los clientes generen su propia electricidad y la vendan en la red general. Y en 2001, el Senado de Estados Unidos introdujo un proyecto de ley según el cual todas las centrales eléctricas estarían obligadas a permitir que los clientes con generadores propios alimentados con recursos renovables pudieran vender electricidad a la red general.

La mayoría de las plantas eléctricas de GD sirven todavía como generadores de emergencia para la red principal y sólo se ponen en funcionamiento cuando se produce un corte en el suministro. Esto significa que se mantienen inactivas la mayor parte del tiempo. Si fuera posible integrarlas de forma eficiente en la red general podrían ser productivas y suministrar electricidad suplementaria en las horas punta a unas compañías eléctricas que se hallan ya al límite de su capacidad y no pueden dar respuesta a un aumento de la demanda.

A largo plazo, la capacidad de generación eléctrica de los usuarios finales conectados a través de la red energética superará la capacidad de las centrales eléctricas de las compañías de servicios públicos. Guando esto suceda, significará una revolución en la forma en que se produce y distribuye la energía. Tan pronto como el cliente, el usuario final, se convierta en el productor y proveedor de la energía, las compañías eléctricas de todo el mundo se verán obligadas a redefinir su propio papel si quieren sobrevivir. Algunas compañías eléctricas ya están comenzando a explorar nuevas funciones, como la de ofrecer paquetes de servicios energéticos o coordinar la incipiente actividad de la red energética. Dentro de este nuevo modelo, las compañías eléctricas se convertirían en proveedoras de «servicios públicos virtuales», y su servicio a los usuarios finales consistiría en conectarlos entre sí y ayudarles a compartir sus excedentes de energía de una forma eficiente y beneficiosa. Coordinar el contenido en lugar de producirlo será el eslogan de las compañías eléctricas en la era de la generación distribuida y America Online (AOL) se convertirá en el modelo a imitar.

Sin embargo, antes de que la HEW pueda realizarse plenamente, será necesario introducir una serie de modificaciones en la red eléctrica existente para asegurar la posibilidad de acceder directamente a ella, así como su capacidad para permitir un flujo ágil de servicios energéticos. Conectar miles y más tarde millones de pilas de combustible a las redes generales exigirá mecanismos sofisticados de gestión y control para dirigir el tráfico energético durante los períodos de alto y bajo consumo. Encorp ya ha desarrollado un programa de control remoto capaz de conectar los generadores locales de forma automática a la red eléctrica cuando se necesite energía auxiliar en los momentos de mayor consumo. Se estima que adaptar los sistemas actuales cuesta alrededor de 100 dólares por kilovatio/hora, lo cual sigue siendo más barato que aumentar la capacidad de generación.

El problema que tiene la red eléctrica actual es que fue diseñada para garantizar un flujo energético unidireccional, desde la fuente central hasta los usuarios finales. No es ninguna sorpresa que Kurt Yeager, el presidente del EPRI, comentara recientemente que «la actual infraestructura eléctrica es tan incompatible con el futuro como un camino de carros para los automóviles». En muchos sentidos, la red actual se halla en un estado parecido al de la industria mediática antes de la llegada de la World Wide Web, cuando las conexiones funcionaban en una única dirección, del medio emisor a la audiencia.

Convertir la red eléctrica en una red interactiva de miles o millones de pequeños proveedores y usuarios supone todo un reto. Los sistemas actuales de transmisión no están diseñados para dirigir cantidades determinadas de energía a partes específicas de la red. El resultado es que la electricidad circula por toda la red, lo cual provoca congestiones y frecuentes pérdidas de energía. El EPRI ha desarrollado una nueva tecnología llamada «sistema flexible de corriente alterna» [flexible alternative current transmission system, FACTS], que permite a las compañías de transmisión «distribuir cantidades precisas de electricidad a áreas específicas de la red». En un artículo publicado en la revista Wired, Silberman propone una buena analogía con la World Wide Web cuando dice que debemos «pensar en los controladores FACTS como los routers de la red energética». American Electric Power (AEP) adquirió en 1998 los primeros controladores FACTS para sus operaciones en Kentucky y en la actualidad hay nueve compañías que los utilizan.

La integración de la última generación de tecnologías de hardware y software transformará la red centralizada en una red energética inteligente y plenamente interactiva. Los sensores y los agentes inteligentes integrados a lo largo del sistema pueden proporcionar información actualizada sobre las condiciones energéticas y hacer que la corriente fluya exactamente cuando y donde se necesite y al precio más barato posible. Sage Systems, por ejemplo, ha creado un programa de software que permite a las compañías de servicios públicos «descongestionar instantáneamente» el sistema cuando se encuentra presionado por un pico de consumo mediante técnicas como «bajar un grado los termostatos de unos cuantos miles de clientes [...] [con] un simple comando dictado desde Internet». Otro producto nuevo, el Aladyn, permite a los usuarios controlar e introducir cambios desde un navegador de Internet en la energía consumida por la luz, el aire acondicionado y algunos aparatos domésticos.

En un futuro muy cercano todos los aparatos o máquinas alimentadas con electricidad —neveras, aparatos de aire acondicionado, lavadoras, alarmas de seguridad— llevarán sensores que proporcionarán información actualizada sobre las tarifas energéticas, así como sobre la temperatura, la luz y otras condiciones ambientales para que las fábricas, las oficinas, los hogares, los barrios y las comunidades enteras puedan ajustar de forma automática y continua su consumo energético a las necesidades de los demás y a la cantidad de energía que fluye por el sistema.

Un estudio realizado por el Departamento de Energía de Estados Unidos y publicado en 2000, titulado «Making Connections», concluía que el principal obstáculo para la creación de una red energética interactiva son las «viejas regulaciones y los incentivos diseñados para apoyar un suministro en régimen de monopolio y unos costes homogéneos entre los clientes». El estudio encargado por el gobierno dejaba claro que, en el entorno regulativo actual,

las compañías de servicios públicos reciben escasos o nulos incentivos para pasar a la generación distribuida. Al contrario, los incentivos de la regulación las empujan a defender el monopolio frente al mercado que se podría abrir con las tecnologías de generación distribuida.

En los dos años que han pasado desde la emisión de este informe, la situación ha comenzado a cambiar. Tal como se ha señalado antes, cada vez son más las compañías de servicios públicos que llegan a la conclusión de que las ventajas de fomentar la generación distribuida son muy superiores a las de mantener su control monopolista sobre la red y comienzan a trabajar en colaboración con propietarios y operadores independientes de plantas eléctricas locales de generación distribuida para crear redes energéticas interactivas.

Una de las necesidades más acuciantes en la actualidad consiste en establecer criterios uniformes para garantizar un acceso igualitario a la red para los productores de generación distribuida. El Departamento de Energía del gobierno de Estados Unidos, el Instituto de Ingenieros Eléctricos y Electrónicos, las compañías eléctricas, los productores independientes de pilas de combustible y otras tecnologías relacionadas con la generación distribuida y los usuarios finales están lidiando actualmente con las barreras regulativas y las prácticas monopolistas de algunas compañías eléctricas que impiden que los sistemas de generación distribuida tengan un acceso justo y equitativo a la red.

La fusión de la generación distribuida con la inteligencia distribuida cambiará para siempre el mercado energético. Por primera vez existe la posibilidad de reemplazar el modelo energético vertical tradicional por un modelo horizontal, una democratización de la energía que permitirá que todo el mundo sea a la vez productor y consumidor.

La generación distribuida y la creación de una red energética regional y finalmente mundial es la consecuencia lógica de la creación de una red mundial de comunicaciones. La interactividad en el campo de las comunicaciones y de la energía se estimulan y alimentan entre sí. La fusión progresiva de ambas revoluciones tecnológicas sentará las bases para un nuevo tipo de economía y de sociedad, en el cual el incremento del flujo energético puede verse acompañado, al menos en teoría, por un nuevo tipo de complejidad que por primera vez en la historia tendría un carácter descentralizado y una forma verdaderamente democrática.

Convertir el coche en una planta eléctrica

Es probable que la revolución de la generación distribuida despegue en los próximos años gracias a la introducción de automóviles, camiones y autobuses alimentados por pilas de combustible. Los principales fabricantes de coches del mundo han anunciado planes para introducir en el mercado automóviles con pilas de combustible. En 1997, Daimler-Benz puso en marcha un proyecto de 350 millones de dólares para desarrollar motores con pilas de combustible de hidrógeno en el que también participaba Ballard Power Systems, una firma canadiense líder en el desarrollo de pilas de combustible. La empresa asegura que a finales de esta década habrá producido 100.000 coches equipados con pilas de combustible, la séptima parte de su producción actual. Ford se sumó poco después al proyecto de Daimler-Chrysler y Ballard Power Systems, lo que permitió aumentar la inversión hasta más de 1.000 millones de dólares. Toyota espera tener en la calle los primeros coches con pilas de combustible antes de que termine la década. General Motors ha prometido tenerlos preparados para el año 2010. Nissan, Honda y Mitsubishi han anunciado también planes para producir coches de este tipo y, entre las tres, han comprometido otros 1.000 millones de dólares en el proyecto.

Aunque los consumidores han oído hablar muy poco o nada de los coches de hidrógeno, los principales fabricantes de automóviles del mundo se están preparando entre bastidores para lo que significará la mayor revolución en la forma de utilizar la energía desde que hace cien años se introdujo el motor de combustión interna. Bill Ford, el bisnieto de Henry Ford y actual presidente de Ford Motor Company, ha llegado a decir que está convencido de que «las pilas de combustible terminarán finalmente con los cien años de reinado del motor de combustión interna».

El entusiasmo de los fabricantes de automóviles se ha visto emulado en los últimos años por las principales compañías energéticas del mundo. Según ha dicho Chris Fay, director ejecutivo de Shell U.K., con sede en Londres: «En Shell creemos que los coches con pilas de combustible alimentadas con hidrógeno serán probablemente la gran novedad en el mercado de la automoción de toda Europa y Estados Unidos en 2005». Fay afirma que «esta tendencia representa todo un reto para una compañía como Shell, que deberá desarrollar nuevos productos y tecnologías y también preparar e informar a sus clientes para los cambios que nos esperan en el futuro».

Las implicaciones de este cambio son inmensas. Hay 750 millones de vehículos en las carreteras de todo el mundo, un número que se espera que se duplique en los próximos veinticinco años. Todos ellos funcionan con combustibles fósiles. Tan sólo en Estados Unidos el transporte absorbe el 54% del petróleo que se consume anualmente. El transporte consume más del 20% de la energía primaria global. Además, según la Agencia Internacional de la Energía, el 17% de las emisiones mundiales de dióxido de carbono proceden de la quema de combustibles fósiles para el transporte en carretera.

Imaginemos ahora, dicen los observadores de la industria, lo que sucedería si toda la flota de automóviles, autobuses y camiones funcionara con pilas de combustible de hidrógeno en lugar del motor de combustión interna. Naturalmente, el hidrógeno tendría que ser producido al principio mediante la reformación del metanol con vapor. Lamentablemente, hay compañías automovilísticas que incluso proponen usar gasolina. Sin embargo, a largo plazo comenzarán a emplearse recursos renovables —energía fotovoltaica, eólica, hidroeléctrica, geotérmica y de biomasa— para electrolizar el agua de forma barata y eficiente, con el objetivo de separar el hidrógeno que será utilizado directamente como combustible, lo que permitirá evitar el uso de combustibles fósiles para la producción de hidrógeno. Las pilas de combustible de hidrógeno no producen emisiones. Tal como se ha señalado antes, los únicos productos residuales del proceso son el calor y el agua pura. El largo reinado de la energía de los hidrocarburos llegaría a su fin, y también el incremento de la factura de la entropía que suponen las emisiones de CO2 derivadas de la quema de combustibles fósiles. El calentamiento global se frenaría hasta volver a unos niveles sólo dos veces superiores a los de la época preindustrial y el riesgo ecológico a largo plazo que representa el aumento de las temperaturas del planeta quedaría sustancialmente mitigado.

Otro dato igualmente importante es que en la nueva era de la pila de combustible de hidrógeno el automóvil mismo se convierte en una «central eléctrica sobre ruedas», con una capacidad generadora de 20 kilovatios. Si tenemos en cuenta que el coche medio se pasa el 96% del tiempo aparcado, durante las horas de inactividad podría conectarse a la casa, a la oficina o a una red eléctrica general interactiva, con el correspondiente incremento del suministro energético para la red. Los beneficios obtenidos con la venta de energía a la red podrían contribuir a sufragar los costes del arrendamiento o la adquisición del vehículo. Sólo haría falta que un pequeño porcentaje de los conductores utilizaran sus vehículos como plantas eléctricas y vendieran energía a la red para que se pudieran eliminar la mayor parte de las centrales eléctricas del país. El motivo es que una flota de 200 millones de vehículos de transporte con pilas de combustible alimentadas con hidrógeno tiene una capacidad de generación eléctrica cuatro veces mayor que la del conjunto de la red eléctrica nacional. La transición hacia los coches de hidrógeno pone a nuestro alcance una cantidad increíble de energía potencial. Bertrand Dusseiller, de Asea Brown Boveri, calcula que la potencia estimada de todos los automóviles que se construyen al año supera la capacidad estimada de todas las centrales eléctricas del mundo.

El principal problema al que se enfrenta la industria automovilística para realizar la transición hacia los vehículos con pilas de combustible es el de cómo producir, distribuir y almacenar el hidrógeno a un precio que resulte competitivo con la gasolina de las estaciones de servicio. Algunos estudios calculan que crear la infraestructura necesaria a escala nacional para producir y distribuir hidrógeno en grandes cantidades tendría un coste superior a los 100.000 millones de dólares. El «problema del hidrógeno» es el clásico dilema del huevo y la gallina. Las compañías automovilísticas son reacias a fabricar coches alimentados directamente con hidrógeno por miedo a que las compañías energéticas no inviertan los fondos necesarios para crear miles de estaciones para repostar hidrógeno. Éste es el motivo de que las compañías automovilísticas estén apostando por desarrollar coches con pilas de combustible equipados con reformadores capaces de transformar la gasolina y el gas natural en hidrógeno. Las compañías energéticas, a su vez, tienen miedo de comprometer miles de millones de dólares en la creación de una infraestructura de estaciones para repostar hidrógeno a escala nacional, por si no llega al mercado la suficiente cantidad de vehículos que funcionen directamente con hidrógeno.

Los críticos argumentan que si la industria automovilística apuesta por los coches con pilas de combustible equipados con un procesador de carburante —una planta termoquímica portátil— para convertir la gasolina o el metanol en hidrógeno, se estarían embarcando en una estrategia cara y a largo plazo innecesaria, cuyo coste podría llegar al billón de dólares en el caso de la próxima generación de coches.

En un artículo escrito para The International Journal of Hydrogen Energy, C. E. (Sandy) Thomas y otros antiguos miembros de Directed Technologies, Inc. —empresa asesora de Ford Motor Company en cuestiones relacionadas con la tecnología de las pilas de combustible— sostienen que tras un exhaustivo estudio realizado sobre los costes comparativos de utilizar procesadores de combustible para convertir el metanol o la gasolina en hidrógeno, frente a los de alimentar el vehículo directamente con hidrógeno, la conclusión es que lo segundo resulta más ventajoso. Según los autores, el motivo de que todos los cálculos realizados por la industria prevean unos costes tan elevados para la transición hacia el hidrógeno es que dichos estudios asumen que debería construirse un sistema de gasoductos a escala nacional como los que existen para el gas natural, con un coste de miles de millones de dólares. Sin embargo, los autores de un estudio encargado por Ford Motor Company y el Departamento de Energía de Estados Unidos concluían que «la distribución del hidrógeno hasta las pilas de combustible de los vehículos se puede resolver de forma más barata mediante la producción de pequeños reformadores de metano con vapor o de pequeños electrolizadores que podrían instalarse en las estaciones de servicio locales o en los garajes de las compañías de transporte», con lo que ya no habría necesidad de construir un gigantesco sistema de gasoductos. En el período de transición, se podría producir el hidrógeno «donde y cuando fuera necesario en cantidades proporcionadas al crecimiento de las ventas de vehículos con pilas de combustible [fuel cell vehicle, FCV], lo cual minimizaría la necesidad de realizar inversiones multimillonarias antes de que la cantidad de FCV introducida fuera suficiente como para garantizar un rendimiento adecuado de las inversiones». El proceso de conversión podría aprovechar además los gasoductos de gas natural existentes para obtener el combustible necesario para producir el hidrógeno, o bien la electricidad de la red general para generar hidrógeno por electrólisis del agua.

Cuando los investigadores compararon el coste de reformar el metanol y la gasolina a bordo del vehículo con el de repostar directamente hidrógeno en las estaciones de combustible existentes, que lo obtendrían a partir de pequeños electrolizadores y reformadores de metano con vapor, confirmaron que esto último resultaba más barato. Además, un vehículo alimentado directamente con hidrógeno sería más barato que uno equipado con un reformador de metanol o de gasolina. Según el estudio, un FCV de metanol costaría entre 550 y 1.600 dólares más que un vehículo alimentado directamente con hidrógeno, y un FCV de gasolina costaría entre 1.600 y 4.500 dólares más.

Por último, queda abierta todavía la cuestión de si es viable almacenar hidrógeno en los vehículos. Un estudio anterior dirigido por el Consejo de Recursos del Aire de California había concluido que el hidrógeno comprimido ocupa más volumen que los depósitos de gasolina y metanol, y que restaría tanto espacio para pasajeros y equipajes que el vehículo resultaría inviable. Sandy Thomas, actual presidente de H2Gen, responde, sin embargo, que Ford ha resuelto el problema del almacenamiento. La compañía ha rediseñado el vehículo de tal modo que el depósito de hidrógeno encaja bien y proporciona una autonomía de 610 kilómetros.

En un informe de 1999, el Panel Técnico Asesor sobre el Hidrógeno del Departamento de Energía de Estados Unidos coincidía con las estimaciones de Thomas y otros en el Journal of Hydrogen Energy. Respecto a la cuestión de asegurar una adecuada capacidad de almacenamiento de hidrógeno en los vehículos, el panel concluyó que «no hace falta ningún avance importante en la tecnología de almacenamiento del hidrógeno para que los vehículos dispongan de una cantidad de líquido o gas comprimido que les permita una autonomía aceptable entre una carga y otra de combustible, y sin pérdida de espacio para los pasajeros o el equipaje».

El Panel Asesor del Departamento de Energía también argumentaba que «sobre una base de coste por vehículo —que incluye tanto el coste del vehículo como el de parte de la infraestructura de suministro de combustible destinada a éste—, el coste de un FCV alimentado directamente con hidrógeno y el de uno equipado con un procesador de combustible pueden llegar a ser comparables cuando haya una cantidad de vehículos suficiente como para amortizar el coste de la estación de servicio».

El panel estaba preocupado por si la industria automovilística optaba en una primera fase por introducir vehículos equipados con procesadores para convertir la gasolina o el metanol en hidrógeno, con la esperanza de que las ventas justificaran económicamente la transición hacia los vehículos alimentados directamente con hidrógeno una década más tarde o así, lo cual «podría retrasar varias décadas la llegada de los vehículos alimentados directamente con hidrógeno y negar a la sociedad los superiores beneficios a largo plazo de esta clase de vehículos». Dichos beneficios incluyen la eliminación de las emisiones de CO2 y la reducción de la dependencia respecto al petróleo extranjero. El Panel del Departamento de Energía también coincidía con el estudio de C. E. Thomas y otros en que durante los primeros estadios las estaciones locales de servicio podrían producir y almacenar el hidrógeno con la ayuda de pequeños generadores locales de hidrógeno, que podían consistir en reformadores conectados a la red nacional de gasoductos o bien en electrolizadores conectados a la red eléctrica. El panel sugería que cuando aumentara el número de vehículos con pilas de combustible se podrían instalar grandes plantas generadoras centrales que distribuyeran el hidrógeno entre las estaciones de servicio por medio de gasoductos de hidrógeno, o bien mantener la producción local de hidrógeno basada en pequeños generadores.

El 12 de enero de 1999, Alemania abrió en Hamburgo la primera estación comercial de combustible de hidrógeno de Europa. En la ceremonia de inauguración, el alcalde de la ciudad, Ortwin Runde, pidió a sus conciudadanos que trataran de imaginar lo que podía significar el transporte impulsado con hidrógeno para la calidad de vida en ciudades como Hamburgo. Sus reflexiones fueron las siguientes:

Las calles serán tranquilas. El paso de los coches ya no irá acompañado por el rugido de los tubos de escape, sino que sólo se dejará sentir el ruido de los neumáticos y una ráfaga de aire. La ciudad estará limpia, puesto que prácticamente no habrá emisiones. Los peatones que caminen por las aceras no tendrán que ir con la nariz levantada y los visitantes no tendrán que encerrarse en los cafés para escapar de los olores de las calles porque ahora podrán tomar sus refrescos al aire libre.

Amory B. Lovins y Brett D. Williams, del Instituto Rocky Mountain, han sugerido una forma algo distinta de resolver el problema del abastecimiento de hidrógeno. Su plan se basa en usar pilas de combustible fijas en los hogares y las oficinas para obtener el hidrógeno en los primeros estadios de la transición hacia la flota de vehículos de hidrógeno. Lovins y Williams destacan que cada vez son más los edificios que tienen previsto instalar pilas de combustible en los próximos años. Su propuesta consiste en emplear estos dispositivos en los hogares y las oficinas para producir hidrógeno en períodos de bajo consumo. «Aparcas tu hipercoche con pilas de combustible en el trabajo (o en tu casa o apartamento...) y lo conectas por un lado a la red y por otro a un cargador automático de combustible de la planta de hidrógeno del edificio.»

Todavía queda por examinar otra cuestión relacionada con el uso del hidrógeno como combustible, a saber, la extendida percepción de que puede ser peligroso. Buena parte de la desconfianza pública con relación al hidrógeno procede de un hecho que ocurrió en el año 1937: el incendio de la aeronave alemana Hindenberg, que se produjo cuando intentaba aterrizar en Lakehurst, Nueva Jersey, y en el que murieron treinta y seis pasajeros. Contrariamente a lo que pretende la versión más extendida, el Hindenberg no explotó, y el hidrógeno no fue la causa del incendio. Addison Bain, anterior director del programa del hidrógeno en el Centro Espacial Kennedy, presentó en 1997 los resultados de una década de investigaciones sobre el accidente. Según las investigaciones de Bain, la causa probable del incendio fue una chispa provocada por la electricidad estática del aire, la cual incendió el tejido de algodón que cubría la aeronave. El fuego se extendió posteriormente hasta inflamar el hidrógeno, pero éste no fue la causa inicial del incendio.

Los estudios realizados a lo largo de los años han concluido que el hidrógeno no es más peligroso que otros combustibles. En algunas situaciones puede ser incluso más seguro, porque cuando es liberado se evapora rápidamente, en lugar de extenderse por el suelo como la gasolina. Un estudio encargado por el Bundestag alemán en 1993 y elaborado por la Oficina Alemana de Evaluación de las Consecuencias de la Tecnología concluyó que «los riesgos técnicos de los distintos procesos de un sistema energético basado en el hidrógeno, desde la producción hasta la utilización, parecen en principio controlables». La Organización Internacional para la Estandarización, con sede en Ginebra, Suiza, trabaja actualmente en la elaboración de unos criterios internacionales para garantizar la seguridad en la producción, el almacenamiento, el transporte y el uso del hidrógeno.

Aunque probablemente nos esperan todavía numerosos pleitos y debates, y también unos cuantos falsos comienzos, todo el mundo está de acuerdo en que la próxima década marcará el principio del fin del motor de combustión interna y el nacimiento del coche propulsado con hidrógeno. En la Exposición Norteamericana Internacional de Automóviles celebrada en Detroit en enero de 2002, General Motors entusiasmó al público con la presentación de su nuevo prototipo de coche de hidrógeno, el «Autonomy». El coche es una revolución en el diseño automovilístico. Su lustroso cuerpo futurista es articulado y modular, lo que permite al usuario cambiar a su gusto la forma y el estilo del chasis, que está diseñado para durar más de veinte años. El vehículo funciona con un sistema informático y prescinde de todos los sistemas mecánicos de los coches convencionales, como el motor, la barra de dirección, los pedales para el freno, el embrague y el acelerador, y el cambio de marchas. Dichas funciones pasan a estar dirigidas por un sistema informático que el conductor controla con un mando único. GM afirma que el escaso número de piezas hará que los vehículos de hidrógeno sean más baratos y seguros que los vehículos con motor de combustión interna. En otoño de 2002, GM presentó en el Salón de Paris una versión más avanzada de su coche de hidrógeno, el Hy-wire, y prometió tener un vehículo de hidrógeno producido en serie en los pabellones de la exposición para el año 2010. La empresa destina actualmente más de 100 millones de dólares anuales al desarrollo de una versión comercial del Hy-wire, y el director general de la compañía, Richard Wagoner, ha dicho repetidas veces que el objetivo de la empresa es ser la primera marca de fabricación de coches que produzca un millón de vehículos de hidrógeno.

La misma semana en que General Motors presentaba su nuevo prototipo, el Departamento de Energía de Estados Unidos sorprendió a todo el mundo al anunciar que abandonaba un proyecto de ocho años y 1.500 millones de dólares llevado a cabo con GM, Ford y Daimler-Chrysler para desarrollar vehículos de gasolina de gran duración y se comprometía en cambio a contribuir al desarrollo de vehículos con pilas de combustible alimentados con hidrógeno. Según el secretario de Energía, Spencer Abraham, el nuevo programa del Departamento de Energía, llamado «el coche de la libertad», «tiene su origen en el llamamiento que hizo el presidente Bush en el Plan Nacional de Energía del pasado mes de mayo para reducir la dependencia norteamericana respecto al petróleo extranjero».

Posteriormente, en abril de 2002, el gobernador de Michigan, John Engler, hizo público un plan de desarrollo económico a largo plazo para convertir el Estado en el líder mundial en el desarrollo, la producción y la comercialización de pilas de combustible y otras tecnologías, bienes y servicios relacionados con el hidrógeno. Su objetivo, según dijo, era trabajar en colaboración con los fabricantes de automóviles para garantizar que Detroit y el Estado de Michigan mantuvieran su estatus preeminente como capital mundial de la industria automovilística. El plan de Michigan, llamado «NextEnergy», incluye el establecimiento de un Centro NextEnergy, que servirá como campo de pruebas para las tecnologías del hidrógeno, así como una Zona NextEnergy, un campus público de unas 280 hectáreas donde se creará un centro de alta tecnología para la innovación en esta clase de tecnologías. El campus será declarado «Renaissance Zone» libre de impuestos, con la esperanza de atraer a compañías de todo el mundo para que instalen allí sus centros de investigación y desarrollo sobre el hidrógeno. Las compañías también recibirán descuentos en los impuestos por los nuevos empleos generados en la zona.

El plan llama a una estrecha colaboración con el sistema educativo estatal para diseñar un nuevo currículo y transmitir a la próxima generación la capacitación técnica necesaria para llevar adelante las investigaciones más avanzadas y para producir y comercializar tecnologías, productos y servicios relacionados con el hidrógeno.

El plan de Michigan propone la creación de un Programa Nacional de Energía Alternativa, financiado en parte por el gobierno federal. El programa serviría como un laboratorio de apoyo para mejorar los estándares industriales y los sistemas de certificación.

El gobernador Engler anunció la intención del Estado de eximir de los impuestos de venta y uso a los individuos e instituciones que adquirieran pilas de combustible fijas o móviles para promover su difusión y acelerar la transición hacia una economía del hidrógeno. También dijo que el Estado trabajaría en colaboración con la industria de los servicios públicos para crear minirredes energéticas y demostrar la viabilidad de la energía del hidrógeno.

El gobernador comparó este ambicioso esfuerzo con el que supuso la creación de Silicon Valley, que dio un nuevo impulso a la revolución del software durante el último cuarto del siglo XX.

El gobernador Engler creía que la economía del hidrógeno podría mover 100.000 millones de dólares en 2010 y generar cientos de miles de nuevos puestos de trabajo, por lo que expresó su esperanza de que Michigan se convirtiera en el centro global de la transición hacia la era del hidrógeno.

Otros Estados, como Ohio y California, han lanzado también iniciativas propias para capturar el emergente mercado de la energía del hidrógeno.

La Unión Europea da un salto adelante

En octubre de 2002, la Unión Europea (UE) cogió al mundo por sorpresa cuando Romano Prodi, presidente del organismo rector de la UE, la Comisión Europea, realizó el espectacular anuncio de que Europa pretendía convertirse en la primera superpotencia del hidrógeno enteramente integrada y basada en energías renovables del siglo XXI. (El autor sirve actualmente como asesor personal del presidente Prodi y en calidad de tal preparó el informe estratégico que llevó a la iniciativa europea sobre la energía del hidrógeno.)

La UE estaba ya previamente comprometida con la transición de la dependencia de los combustibles fósiles al futuro de la energía renovable. Los objetivos de la UE en el campo de la energía renovable son los más ambiciosos del mundo. Para el año 2010, el 22% de la electricidad y el 12 % de toda la energía producida en la UE deben proceder de fuentes energéticas renovables. La UE se daba cuenta de que esa clase de futuro de energía renovable sería imposible sin el empleo del hidrógeno como medio de almacenamiento de energía.

Para hacer realidad un futuro basado en el hidrógeno combinado con energías renovables, la Unión Europea ha creado una estructura de colaboración sin precedentes con el tejido empresarial y la sociedad civil europeos. Juntos, los tres sectores han sentado las bases de un plan operativo para una transición progresiva hacia un régimen energético del hidrógeno y han creado grupos de trabajo para la investigación, el desarrollo y la introducción comercial de las nuevas tecnologías de las células de combustible. La UE también ha aumentado enormemente su compromiso financiero con el desarrollo de la energía sostenible, que ha pasado de los 127 millones de euros de los últimos cuatro años a 2.100 millones de euros entre 2003 y 2006.

En el anuncio del nuevo plan energético, el presidente Prodi subrayó que la transición a una era del hidrógeno y de una distribución descentralizada de la energía por toda Europa señala un nuevo paso fundamental en la integración europea, después del éxito de la introducción del euro. Prodi comparó este esfuerzo hercúleo con el programa espacial norteamericano de las décadas de 1960 y 1970, que logró poner al hombre en la Luna y contribuyó al surgimiento de la economía de la alta tecnología en las décadas de 1980 y 1990. El presidente Prodi sabe que Inglaterra se convirtió en la principal potencia económica mundial en el siglo XIX gracias a que fue el primer país en emplear sus vastas reservas de carbón para alimentar el motor de vapor. De modo parecido, Estados Unidos se convirtió en la primera potencia económica en el siglo XX porque empleó sus vastas reservas de petróleo para alimentar el motor de combustión interna. El señor Prodi piensa que la Unión Europea podría convertirse en la principal superpotencia mundial en el siglo XXI gracias al empleo de las células de combustible de hidrógeno y está convencido de que el futuro éxito de Europa depende en buena medida de su capacidad de realizar a tiempo la transición a la nueva era energética.

La iniciativa europea con el hidrógeno estimuló los esfuerzos de las empresas estadounidenses para establecer un marco de colaboración público-privado en Norteamérica. En otoño de 2002, una coalición de empresas estadounidenses tanteó a la Administración Bush para obtener un compromiso de inversión por parte del gobierno federal de 5.500 millones de dólares a lo largo de los diez años siguientes para la investigación y el desarrollo de la tecnología y la infraestructura del hidrógeno. En su discurso sobre el Estado de la Unión de enero de 2003, el presidente Bush dijo que su administración estaba comprometida con un futuro basado en el hidrógeno y que pensaba presentar una serie de propuestas ante el nuevo Congreso para poner a Estados Unidos al frente de la carrera del hidrógeno. La factura energética presentada posteriormente al Congreso por la Casa Blanca y el Partido Republicano, sin embargo, reveló una orientación bien distinta de la promovida por la Unión Europea. La factura energética de 1.700 millones de dólares para los próximos cinco años presentada por los republicanos proponía un aumento de los fondos destinados a las industrias del carbón, el gas natural y la energía nuclear para la investigación y el desarrollo de la obtención de hidrógeno a partir de dichas fuentes energéticas tradicionales, al tiempo que destinaba escasos fondos adicionales a la investigación y al desarrollo del uso de energías renovables para la obtención del hidrógeno. Muchos observadores, sobre todo desde el campo ecologista, se quejaron de que la Administración Bush estaba usando el hidrógeno como un caballo de Troya para favorecer los intereses de sus amigos en las industrias nuclear y del petróleo, en perjuicio de un futuro basado en la energía del hidrógeno a partir de fuentes renovables. Parecía como si la Casa Blanca se propusiera promover un futuro del hidrógeno sin superar el pasado del combustible fósil o, al menos, eso pensaban los individuos y las organizaciones que trabajan en el campo del ecologismo.

Aunque sigue viendo los combustibles fósiles como una parte importante de la factura energética para las próximas décadas, la Unión Europea está comprometida con lo que llama una estrategia de doble vía. En la primera vía, la UE está comprometida con la conservación del combustible fósil existente a lo largo de los próximos años mediante la aplicación de unos criterios estrictos de eficiencia energética en los vehículos y de otras prácticas conservadoras, así como con un seguimiento firme de los objetivos del Protocolo de Kioto sobre el cambio climático global. En la segunda vía, la UE está comprometida con la promoción de un rápido desarrollo de las tecnologías renovables y de una infraestructura basada en la energía del hidrógeno, de modo que Europa pueda superar progresivamente su dependencia de los combustibles fósiles y sentar las bases para una nueva era energética.

* * *

La economía del hidrógeno está a nuestro alcance. El tiempo que tardemos en llegar a ella depende de lo decididos que estemos a emanciparnos del petróleo y los demás combustibles fósiles. Si nos limitamos a jugar con la idea y retrasamos la transición, en la creencia de que queda todavía suficiente petróleo barato como para cubrir nuestras necesidades hasta mediados del siglo XXI, podría ocurrir que no estuviéramos en condiciones de realizar la transición a tiempo en caso de que la producción global de petróleo tocara techo en los próximos años. Construir una nueva infraestructura para mantener una economía del hidrógeno maduro será una tarea cara y complicada, aunque con el suficiente celo y entusiasmo podría levantarse en menos de una década. Después de todo, la infraestructura necesaria para la economía de Internet y la World Wide Web fue creada en menos de una década, al menos en el mundo desarrollado, donde significó un cambio fundamental en la forma de hacer negocios y en el modelo de comunicación entre las personas. Muchas de las principales revistas de negocios predicen que la próxima gran revolución comercial será la economía del hidrógeno y la red energética mundial. Convertir las predicciones en realidad, sin embargo, requiere que tanto el sector público como el empresarial se comprometan con el futuro del hidrógeno y que se avancen propuestas prácticas sobre la forma de llegar hasta él.

Por primera vez en la historia de la humanidad, tenemos a nuestro alcance una forma de energía omnipresente; sus partidarios la llaman «el combustible eterno». El hidrógeno terminará siendo tan barato como los ordenadores personales, los teléfonos móviles y los palm pilots. Cuando esto suceda, se abrirá la posibilidad de democratizar verdaderamente la energía y ponerla al alcance de todos los seres humanos de la Tierra.

Capítulo 9

LA REGLOBALIZACIÓN DESDE ABAJO

Nuestro futuro reside en el hidrógeno. ¿Pero quién va a controlar el «combustible eterno»? Por su propia universalidad, el hidrógeno abre la posibilidad de democratizar la energía y dar acceso al poder a todos los seres humanos de la Tierra. Pero el hecho de que exista tal posibilidad no garantiza que el hidrógeno vaya a ser compartido de forma justa y equitativa entre todos los pueblos. La cuestión depende en buena medida de cómo «valoremos» el hidrógeno. ¿Será visto como un recurso compartido, igual que los rayos del Sol y el aire que respiramos? ¿Como una mercancía que se compra y se vende en el mercado? ¿O tal vez como algo intermedio?

Las lecciones de la World Wide Web

El «estatus» que se atribuya al hidrógeno marcará la tendencia básica de la economía del hidrógeno en el futuro y tendrá profundas consecuencias para las instituciones políticas y sociales que se desarrollen dentro del nuevo régimen energético. La cuestión del estatus del hidrógeno se parece en muchos sentidos a la cuestión del estatus de la «información» en Internet. Los primeros valedores de Internet y la World Wide Web defendían apasionadamente la idea de que «la información debería circular libremente». Según ellos, la propia arquitectura del nuevo medio de comunicación favorecía que las personas compartieran libremente la información. Después de todo, la World Wide Web no pertenece a nadie y está abierta a todo el mundo. No es otra cosa que la conexión de cualquier usuario informático del mundo con cualquier otro, con el objetivo de establecer una conversación. «La economía del futuro —exclamaba el teórico del ciberespacio John Perry Barlow— se basará en las relaciones más que en la posesión.» El argumento es que la comunicación y el intercambio de información entre personas no deberían encontrar limitaciones o verse sometidos a cuotas de acceso, licencias o permisos de ninguna clase. Los puristas no ven ningún problema en la posibilidad de descargar música, hacer copias y compartirlas con otras personas en la red de forma gratuita. ¿Por qué no habríamos de poder hacer copias de artículos y añadir reflexiones propias a sus relatos e historias, y luego enviarlos gratuitamente a amigos e incluso a extraños? La cuestión de qué es lo que debería considerarse información gratuita y qué debería estar sujeto a derechos de autor o de acceso ha sido el debate central desde el nacimiento de este nuevo medio de comunicación global e interactivo.

Aquellos que preferirían mantener el acceso a la información en Internet tan libre como fuera posible basan sus reivindicaciones en algunas de las premisas fundamentales de su funcionamiento. En primer lugar, la red está diseñada para que todos los usuarios puedan convertirse en proveedores de contenidos. Internet elimina la vieja jerarquía centralizada de los medios de comunicación de masas en los que la comunicación circulaba en una única dirección, desde los grandes centros creadores de contenidos hasta los consumidores individuales pasivos. En segundo lugar, el hecho de que Internet sea una red supone que los usuarios pueden mantener entre sí relaciones individualizadas o colectivas. Esto significa que cualquier persona puede tener acceso literalmente a cualquier otra persona que se encuentre en la red. Esta capacidad de comunicación individual no tiene precedentes en la historia. De golpe, todas las personas pueden comunicarse entre sí, en una especie de democratización instantánea de la comunicación. En una red descentralizada de comunicaciones sin ninguna instancia central de control, no hay nadie en principio que tenga el poder, sino todos. La idea de obtener permiso de una autoridad central o un «vigilante del portal» [gatekeeper] para poder comunicar ideas, mensajes e informaciones de todo tipo con otras personas de la red cuestiona la naturaleza misma de la red. Lo que tenemos aquí es un medio en el que todo el mundo puede dirigir su propio centro de comunicaciones. Cualquier nodo de la red se convierte en una instancia para la creación en común. En resumen, la arquitectura de la red confiere a las personas un mayor control potencial sobre la información que reciben y envían. Resulta fácil de comprender, por lo tanto, la irritación o incluso el enfado de los entusiastas ante la perspectiva de que se les niegue una plena libertad de expresión por medio de censores, propietarios de derechos de autor o gatekeepers.

La gran promesa de Internet se ha visto comprometida a cada paso, sin embargo, por intereses comerciales decididos a implantarse en el medio. Compañías como Microsoft y AOL Time Warner no han cejado en sus esfuerzos por domar la World Wide Web y hacer que dejara de ser un canal abierto al libre flujo de la comunicación y la información para convertirse en un dominio privado en el que la conexión, el acceso y la recepción y el envío de información fueran de pago. Cuando Napster y otras empresas de nueva creación comenzaron a distribuir software para que millones de personas pudieran copiar y compartir entre ellas sus bibliotecas musicales —archivos compartidos «de igual a igual» [peer-to-peer]— de forma gratuita, las principales compañías discográficas respondieron con demandas judiciales en las que argumentaban que la música estaba protegida por un copyright y que debía pagarse por su uso. Los gigantes de la música ganaron en los tribunales y Napster ha tenido que reinventarse como un medio comercial que da acceso a la vasta biblioteca musical de Bertelsman Napster a cambio de una cuota de suscripción mensual, un porcentaje de la cual va a parar a las compañías musicales y a los artistas para cubrir los derechos de autor. Otros renegados del ciberespacio, sin embargo, continúan con la lucha y se amparan en el argumento de que la música debería circular libremente para esquivar la ley con nuevos programas informáticos que permitiesen a todos los usuarios compartir libremente sus archivos musicales.

La batalla de los activistas del ciberespacio que defienden el libre acceso e intercambio de la información contra los intereses comerciales que promueven la privatización y la mercantilización del ciberespacio se libra actualmente en numerosos puntos de la red. Uno de los frentes más calientes es la posibilidad de compartir el código entre usuarios, el llamado «movimiento del código fuente abierto». Por ejemplo, Linux es un sistema operativo que se ofrece de forma gratuita. Además, todo el mundo puede acceder libremente al código fuente para examinarlo y modificarlo. Los programadores del sistema operativo comparten su trabajo con todo el mundo y animan a los demás usuarios a resolver colectivamente los problemas y crear un código nuevo. Si un usuario del sistema operativo Linux tiene una duda o un problema que necesita resolver, puede enviar su pregunta a la página web de ayuda de Linux y normalmente en pocos minutos recibe la asistencia de otros usuarios que le ofrecen soluciones prácticas. Linux afirma tener entre 10 y 20 millones de usuarios y es el sistema operativo que crece más rápidamente en el mundo. Algunos observadores de la industria predicen que, con el tiempo, Linux podría incluso amenazar el dominio de Microsoft sobre el mercado de los sistemas operativos.

Aunque la batalla por el control de los contenidos en Internet se ha inclinado en los últimos años en favor de los intereses comerciales, el enfrentamiento está lejos de haber terminado. El nuevo medio continuará estando marcado por el debate sobre qué recursos deben ser de libre acceso y cuáles no. En una red distribuida, en la que todo el mundo es a la vez un consumidor y un productor potencial de contenidos, y en la que todos los usuarios tienen acceso al conjunto de la red —que pronto integrará a uno de cada seis habitantes de la Tierra—, sólo se puede esperar una escalada del conflicto entre aquellos que defienden la libre circulación de la información y los intereses comerciales que pretenden imponer algún tipo de gravamen.

Hay que esperar que se produzca un conflicto similar en relación con la red energética del hidrógeno (HEW). Ciertamente puede argumentarse que del mismo modo que buena parte de las informaciones y las conversaciones entre las personas deberían circular libremente —aunque evidentemente no todas las comunicaciones e informaciones—, el hidrógeno también debería considerarse un recurso gratuito o compartido, accesible para todos. Después de todo, es el elemento más básico y universal del cosmos.

El hidrógeno como patrimonio común

La cuestión de si los pensamientos humanos o la energía elemental del universo deberían ser considerados recursos libremente compartidos o mercancías privadas nos lleva al fondo de una de las cuestiones, más profundas a las que ha tenido que enfrentarse la humanidad en el curso de la historia: ¿a quién pertenece todo aquello que forma parte de la esencia de la vida?

A finales de la Edad Media se suscitó un gran debate entre la Iglesia y la incipiente clase comerciante sobre la cuestión de si el tiempo mismo era un regalo universal y, por lo tanto, gratuito o si era algo que podía ser propiedad de alguien y sobre lo que se podían imponer unos intereses. Los comerciantes argumentaban que «el tiempo es dinero» y que cobrar unos intereses era una compensación legítima por permitir que alguien utilizara el dinero del prestamista durante un cierto período de tiempo. La Iglesia sostenía que la usura era un pecado mortal, pero no sólo por su naturaleza explotadora. Las autoridades eclesiásticas cuestionaban más bien la legitimidad misma del acto. ¿Acaso podían los comerciantes sacar un beneficio de vender tiempo, preguntaban los líderes de la Iglesia, cuando el tiempo no les pertenece a ellos, sino a Dios, que lo entrega libremente a los seres humanos como un regalo para que puedan preparar su salvación? Según escribió Thomas Chobham:

El usurero no vende nada al prestatario que sea propiedad suya. Lo único que vende es el tiempo, que es propiedad de Dios. En consecuencia, no puede sacar provecho de la venta de la propiedad de otro.

En la Inglaterra del siglo XVI surgió un segundo gran debate acerca de lo que debía ser patrimonio común y lo que podía reclamarse como propiedad privada. En la Europa medieval se consideraba que la tierra era un fideicomiso de Dios, que la entregaba a los hombres para que la labraran y la cultivaran. Aunque los señores feudales ejercían derechos de propiedad sobre la tierra y la arrendaban a los campesinos bajo diversos tipos de figuras jurídicas, no era fácil dividir, vender o comprar la tierra en sí. Además, buena parte de las tierras eran comunales y los campesinos las gestionaban colectivamente. A principios del siglo XVI, durante el reinado de los Tudor en Inglaterra, el Parlamento aprobó una serie de leyes que permitían a los terratenientes cercar las tierras comunales, es decir, adquirirlas como propiedad privada y eliminar con ello cualquier derecho que pudieran poseer previamente los arrendatarios como consecuencia de la explotación colectiva de las tierras. Los grandes cercamientos, que comenzaron en Inglaterra y se extendieron más tarde por el continente bajo diversas formas, terminaron con seiscientos años de dominio feudal durante los cuales los campesinos pertenecían a la tierra. A partir de este momento, los grandes feudos medievales serían divididos en forma de bienes inmuebles de titularidad privada, propiedad de la aristocracia local y los campesinos ricos. La tierra que antes era explotada en común como un bien compartido quedó reducida a un conjunto de parcelas de propiedad privada que podían comprarse y venderse en el mercado.

En el siglo XVIII, los gobiernos comenzaron a establecer divisiones territoriales en las aguas del océano y reivindicaron su soberanía sobre una zona costera que se extendía hasta cinco kilómetros mar adentro, la distancia que podía alcanzar la artillería de la época. Al término de la Segunda Guerra Mundial tuvo lugar una campaña todavía más agresiva de territorialización de las aguas oceánicas, que se inició con el anuncio del presidente Truman de que Estados Unidos extendía sus aguas territoriales hasta incluir la «jurisdicción y el control» de los depósitos de petróleo, gas y minerales situados en el lecho de la plataforma continental. La proclamación de Truman disparó una avalancha de reclamaciones parecidas por parte de otras naciones, interesadas en territorializar «sus» plataformas continentales y caladeros marítimos. A principios de los años setenta, diecisiete países habían reclamado la soberanía sobre sus aguas territoriales, que se extendían hasta 320 kilómetros mar adentro.

En 1982 se preparó finalmente, bajo los auspicios de la ONU, una convención de derecho marítimo en la que se garantizaba a los países signatarios la soberanía sobre 20 kilómetros mar adentro y los derechos económicos exclusivos sobre 320 kilómetros en océano abierto. Estas zonas de derechos económicos exclusivos conferían a los países «derechos de soberanía para la exploración y la explotación, la conservación y la gestión de los recursos pesqueros y minerales de los océanos, los lechos marinos y el subsuelo». La gran campaña de apropiación de los océanos convirtió efectivamente en aguas nacionales el 36% de las áreas oceánicas del mundo, con el 90% de los recursos pesqueros explotables y el 87% de las reservas marinas de petróleo de las plataformas continentales previstas. Aunque todavía tiene que ser ratificada por muchos países, la convención ha establecido los nuevos parámetros de la soberanía estatal y ha puesto buena parte de las aguas internacionales bajo dominio territorial.

A comienzos del siglo XX, el espacio aéreo que había sobre los países quedó dividido en una serie de corredores aéreos que se convirtieron en dominio soberano de los Estados. En la actualidad éstos cobran una cuota a las aerolíneas comerciales por el derecho a utilizar dichos corredores. Los gobiernos han fragmentado todavía más el espacio aéreo al permitir que las aerolíneas vendan y compren estos «derechos aéreos». En los últimos años ha surgido un debate en Washington sobre si las frecuencias radiofónicas que configuran el espectro electromagnético, que todavía constituyen un patrimonio común gestionado por los gobiernos y arrendado a las emisoras, deberían venderse a las empresas comerciales y convertirse en «bienes inmuebles electrónicos» negociables en el mercado global.

Otro debate igualmente apasionado se ha planteado acerca de si el genoma —el legado de millones de años de evolución biológica, durante mucho tiempo considerado un patrimonio común— debería privatizarse y convertirse en una propiedad intelectual de las empresas de biotecnología. Hasta la fecha, Estados Unidos y la mayor parte de los países europeos han declarado que los genes y las proteínas que éstos codifican, así como las células, los tejidos, los órganos y los embriones vivos y especies enteras son susceptibles de ser patentados. Otros países y la mayoría de las organizaciones no gubernamentales defienden que el genoma es por naturaleza un «patrimonio común» no reducible a ningún tipo de propiedad política ni comercial.

¿Qué decir de los pensamientos? ¿Pueden venderse como artículos de consumo o son libremente compartidos como patrimonio común? Antes de la era moderna, la idea misma de que un pensamiento pudiera ser propiedad de alguien o que se pudiera hacer pagar a otra persona por escuchar o utilizar las propias ideas hubiera resultado chocante. En las culturas orales, por ejemplo, donde las ideas y los relatos pasaban de boca en boca y eran alterados y embellecidos con cada nueva repetición, nadie podía reclamar una propiedad absoluta. La introducción del texto impreso, sin embargo, hizo más fácil poseer ideas y plantear pretensiones de propiedad sobre ellas. En la actualidad, según dicen los abogados, las nuevas tecnologías de la comunicación —los ordenadores, el software y la World Wide Web— acercan de nuevo la comunicación a la cultura oral porque permiten una forma más genuina de participación colectiva en el intercambio de ideas y pensamientos del que era posible durante el reinado de la imprenta y de los medios de comunicación de masas.

¿Y qué decir del uso del hidrógeno distribuido en una red energética? ¿Deberíamos considerar el hidrógeno como un patrimonio común que debe ser compartido colectivamente, o como un recurso privado sujeto a explotación comercial en el mercado? ¿Tal vez una combinación de ambos? Naturalmente, si el hidrógeno existiera en estado libre por todos los rincones de la naturaleza, igual que el aire que respiramos, y estuviera por lo tanto inmediatamente disponible para que pudiéramos aprovecharlo sin ningún coste, lo consideraríamos sin duda un bien gratuito. Pero el hidrógeno no se nos presenta en una forma fácilmente aprovechable. Debe ser extraído de alguna otra cosa de la naturaleza —de un combustible fósil, o de la biomasa, o del agua— y luego bombeado a una pila de combustible para que sea utilizable como electricidad.

Así pues, aunque es cierto que el hidrógeno está en todas partes y que por lo tanto no es un recurso escaso, el ingenio humano debe arrancarlo de su entorno y explotarlo para generar electricidad. El proceso de extracción requiere cierta inversión de tiempo, trabajo y capital, al igual que su almacenamiento y utilización. Sin embargo, a medida que baje el coste de producir energía a partir del hidrógeno —y sin duda bajará— su estatus como patrimonio común seguirá ganando terreno, porque está distribuido de forma homogénea por todo el mundo y porque, a diferencia de lo que ocurre con los combustibles fósiles, los recursos de hidrógeno son ilimitados. Es posible imaginar un futuro, tal vez sólo dentro de un centenar de años, en el que producir cantidades ilimitadas de hidrógeno tenga un coste cercano a cero. Todas las fuentes previas de energía no renovable en forma de combustibles fósiles se ajustan a la curva de campana de Hubbert, es decir, al principio resultan caras de procesar, luego bajan sus costes a medida que las tecnologías de explotación se hacen más baratas y sofisticadas y, finalmente, el proceso se encarece de nuevo cuando las reservas comienzan a escasear. La producción de hidrógeno, en cambio, es una línea recta que se proyecta indefinidamente hacia arriba, es decir, cada vez resulta más barato producir mayores cantidades de hidrógeno. Al cabo de un tiempo, el único coste significativo es el de mantener y mejorar las redes energéticas inteligentes por las que el hidrógeno fluye alrededor del mundo.

Si cada vez va a ser más barato producir hidrógeno, hasta que llegue un momento en que su coste sea virtualmente cero y se convierta en un recurso «casi» gratuito, con la única reserva de los elevados costes de construcción y mantenimiento de las redes inteligentes por las que circulará, debemos reflexionar seriamente, al comienzo de la era del hidrógeno, sobre el tipo de estructura institucional que puede reflejar mejor el carácter de la fuente de energía que estamos utilizando. La HEW y la economía del hidrógeno construida a partir de ella requieren un diseño arquitectónico radicalmente nuevo que ponga las actividades públicas y privadas, lucrativas y no lucrativas en una relación simbiótica que refleje tanto el aspecto privado como el aspecto comunitario del nuevo régimen energético.

La democratización de la energía

Los primeros esfuerzos dirigidos a democratizar el flujo de la información en la World Wide Web tuvieron cierto éxito, aunque, tal como se ha señalado antes, quedaron rápidamente eclipsados por «vigilantes del portal» [gatekeepers] corporativos como AOL. A finales de los años ochenta y principios de los noventa se extendieron por Estados Unidos las redes ciudadanas, cuyo objetivo era promover la participación en la vida de las comunidades a las que servían. Las redes ciudadanas [free-nets] inducían a la participación mediante la oferta de libre acceso. Dichas redes no eran comerciales y no contenían publicidad ni cobraban cuotas de suscripción. Las redes ciudadanas ayudaron a despertar el interés del gran público por Internet y atrajeron a muchos de los «usuarios pioneros» [early adopters], en buena medida porque el acceso era libre. Lamentablemente, el contenido de las redes ciudadanas resultaba a menudo inconsistente y aburrido, y no conservó demasiado tiempo el interés de los usuarios. Mientras tanto, compañías como AOL comenzaron a llenar este vacío con la oferta de un acceso barato y sencillo a Internet y unos contenidos más entretenidos y atractivos. La mayoría de las redes ciudadanas basadas en Internet han dejado de funcionar, aunque todavía hay más de cien que siguen operativas y tienen bastante éxito en algunas comunidades.

Hoy en día ya no son las comunidades geográficas, sino las comunidades de interés, las que tienen mayor éxito en Internet. En particular, las organizaciones civiles [civil society organizations, CSO] utilizan la red como instrumento organizativo para unir a personas de intereses parecidos en proyectos comunes dentro de áreas tan diversas como las reformas agrarias, los derechos de los animales, la justicia social, los derechos humanos, la reforma económica, los derechos de las mujeres, la salud pública y las artes. Internet se ha convertido en el escenario de una «democratización de la información» donde las comunidades de interés comparten sus ideas, tareas y objetivos para impulsar agendas sociales y construir capital social. Resulta interesante ver cómo Internet se ha convertido en un poderoso foro global de construcción de comunidades a pesar de haber sido víctima de los intereses comerciales y de verse reducido, en parte, al dominio corporativo. En efecto, Internet es ahora un instrumento de comunicación híbrido, a la vez social y comercial, parcialmente privatizado y mercantilizado, aunque también parcialmente abierto a servir como vasto espacio social donde se pueden compartir libremente ideas e intereses.

La generación distribuida y la HEW se hallan en los primeros estadios de su desarrollo, igual que Internet en los años ochenta. La estructura que adopte la generación distribuida a lo largo de los próximos cinco años determinará probablemente el tipo de infraestructura energética que evolucionará y madurará finalmente dentro de diez o quince años.

Lo primero que debe tenerse en cuenta es que la generación distribuida propiciará que todas las familias, las empresas, los barrios y las comunidades del mundo se conviertan potencialmente en productores, consumidores y vendedores de su propio hidrógeno y electricidad. Las pilas de combustible estarán ubicadas geográficamente en los mismos lugares donde el hidrógeno y la electricidad serán producidos y en parte consumidos —los excedentes de hidrógeno serán vendidos como combustible y los excedentes de electricidad reenviados a la red energética—, razón por la cual la capacidad de integrar grandes cantidades de productores-usuarios en asociaciones es fundamental para la descentralización del poder energético y para la promoción de la concepción democrática de la energía.

La integración de la generación distribuida tiene mucho en común con la integración de los trabajadores en el incipiente movimiento sindical de principios del siglo XX. Los trabajadores industriales eran demasiado débiles individualmente para negociar los términos de sus contratos laborales con la dirección. Sólo si se organizaban colectivamente en un bloque dentro de una fábrica, una oficina o una industria entera podían reunir la fuerza suficiente para regatear con la dirección. La capacidad de negarse a trabajar colectivamente mediante la «huelga» ponía en manos de los trabajadores una herramienta poderosa en su campaña para recortar la duración de la semana laboral, mejorar las condiciones de trabajo e incrementar tanto los salarios como los beneficios sociales.

Cuando se organizan colectivamente en asociaciones de generación distribuida [distributed generation associations, DGA], los operadores particulares pueden negociar mejor con los distribuidores comerciales de pilas de combustible los términos de sus contratos de arrendamiento, venta u otros acuerdos de uso. La capacidad de las DGA para integrar a los operadores de pilas de combustible en vastas plantas energéticas con una gran capacidad generadora también les da cierta ventaja sobre las compañías de servicios energéticos, tanto comerciales como no comerciales, encargadas de ayudar a dirigir y coordinar el flujo del hidrógeno y la electricidad por la red energética hasta los consumidores potenciales.

Es improbable que las DGA lleguen a entrar en el negocio del desarrollo, la producción y la distribución de las pilas de combustible y del equipamiento complementario que requiere la generación distribuida, aunque tal vez podrían entrar en sociedades de participación [joint ventures] o bien adquirir acciones y asegurarse posiciones de poder en los consejos directivos de las empresas comerciales. Lo más probable es que en los primeros estadios de la construcción de las HEW locales, regionales y globales las DGA tengan que colaborar con las compañías eléctricas existentes, y ello por dos razones: por un lado, las compañías eléctricas y las compañías de transmisión controlan la red de infraestructuras energéticas actuales y, por otro lado, tienen la experiencia necesaria para actuar como «centrales eléctricas virtuales» encargadas de coordinar el flujo de la energía y los diversos servicios de las redes.

Las compañías eléctricas van a tener que adaptarse al hecho de que millones de pilas de combustible de operadores locales pueden producir más electricidad y de forma más barata que las mayores centrales eléctricas actuales. Cuando los usuarios finales también se conviertan en los productores de su propia energía, el único papel que les quedará a las centrales eléctricas actuales será el de convertirse en «centrales eléctricas virtuales» encargadas de manufacturar y comercializar pilas de combustible, ofrecer servicios energéticos y coordinar el flujo de la energía por las redes eléctricas actuales.

Llevar la teoría a la práctica

Algunos de los modelos organizativos que existen actualmente podrían ayudar a configurar las DGA. En Estados Unidos los más adecuados parecen ser las corporaciones de desarrollo comunitario, las uniones de crédito y los servicios gestionados por entes públicos. En otros países, tal vez sean las cooperativas el modelo más fuerte que existe para el desarrollo de las DGA. En el mundo en vías de desarrollo las encargadas de crear las DGA podrían ser las cooperativas rurales, que trabajan con bancos de microcrédito y reciben el apoyo de subvenciones y préstamos gubernamentales, así como de las ayudas para el desarrollo. Todos estos modelos son organizaciones sin ánimo de lucro o entidades públicas que responden ante sus miembros o bien, en el caso de los entes públicos, ante los ciudadanos. Si las DGA se basaran en modelos organizativos no lucrativos, la nueva economía del hidrógeno estaría comprometida desde el principio del nuevo régimen energético con una concepción semipública de la energía del hidrógeno y de las redes de generación distribuida.

Si se utilizaran los modelos organizativos no lucrativos existentes para configurar las DGA y la nueva infraestructura energética para los siglos XXI y XXII, se obtendría un segundo efecto: las DGA promoverían un modelo institucional adecuadamente descentralizado y horizontal para la organización del nuevo régimen energético, el cual podría servir de base para muchas otras instituciones económicas, sociales y culturales que irán asociadas a la economía del hidrógeno.

Uno de los muchos modelos organizativos que podrían servir para configurar las DGA son las corporaciones de desarrollo comunitario [Community Development Corporations, CDC], que existen desde hace treinta años en Estados Unidos. Se trata de organizaciones no lucrativas, habitualmente situadas en barrios y comunidades urbanas pobres —aunque también existen CDC rurales—, cuya misión consiste en estimular el desarrollo económico y aumentar el poder y la capacidad de control en manos de los residentes locales. El senador Robert Kennedy, que participó en 1967 en la fundación de la primera gran CDC en el barrio de Bedford Stuyvesant, en Brooklyn, Nueva York, concebía las CDC como vehículos para combinar «lo mejor de la acción comunitaria con lo mejor del sistema de las empresas privadas». En la actualidad existen entre 3.000 y 4.000 CDC en Estados Unidos. Su financiación proviene, en parte, de fundaciones e instituciones comerciales, así como de subvenciones federales, estatales y locales. Las CDC están gobernadas por un consejo de dirección integrado principalmente por residentes locales de la comunidad a la que sirve la organización.

En los primeros años, las CDC concentraban sus esfuerzos en la construcción de viviendas a precios asequibles y crearon más de medio millón de viviendas. En la actualidad, las CDC han extendido sus actividades para incluir una amplia gama de iniciativas comerciales. Por ejemplo, la New Community Corporation de Newark, Nueva Jersey, es propietaria de un centro comercial y de dos tercios de las acciones de un supermercado Pathmark, y gestiona varias guarderías, un centro asistencial, un centro de atención médica para la tercera edad, un restaurante, un periódico y una unión de crédito. Sus activos inmobiliarios están valorados en 500 millones de dólares. La CDC de Newark genera 200 millones de dólares en ingresos anuales y es la entidad privada que da trabajo a un mayor número de residentes de la ciudad.

Es virtualmente imposible encontrar alguna de las 133 principales ciudades de Estados Unidos que no tenga al menos una CDC activa. Las hondas raíces de las CDC en los sectores de la vivienda y las superficies comerciales de algunos de los barrios más pobres del país hacen que se encuentren en una posición ideal para movilizar a los consumidores domésticos y a las industrias locales para que compren o alquilen sistemas y creen asociaciones de generación distribuida.

La financiación del alquiler o la compra de pilas de combustible para hogares y empresas podría estar cubierta, en parte, por las uniones de crédito de desarrollo comunitario [community development credit unions, CDCU]. En Estados Unidos operan actualmente alrededor de 1.500 CDCU. Las CDCU son organizaciones sin ánimo de lucro cuyos depositantes son también sus miembros. Estas instituciones comunitarias ofrecen servicios bancarios tradicionales, como el mantenimiento de depósitos de ahorro y cuentas corrientes, así como la concesión de préstamos personales para hipotecas, renovaciones del hogar y compra de automóviles. La misión de las CDCU consiste en ofrecer créditos accesibles para trabajadores y ciudadanos con ingresos bajos, mantener servicios financieros en barrios pobres y reintroducir los depósitos de los miembros en la comunidad local. Conceder préstamos para la compra de pilas de combustible estaría de acuerdo con el objetivo propuesto de promover el capital social de sus respectivas comunidades.

Las compañías de servicios de titularidad pública [publicly owned Utilities, POU], de carácter no lucrativo, también podrían tener un papel determinante en el establecimiento de asociaciones de generación distribuida. Las primeras POU aparecieron hace más de cien años en Estados Unidos con el objetivo de producir electricidad en regiones rurales del país que los inversores privados habían dejado de lado porque consideraban que como mercado no justificaban la inversión. En la actualidad, uno de cada siete norteamericanos —40 millones de personas— obtiene su electricidad de alguna de las 2.000 compañías de titularidad pública que operan en comunidades rurales y urbanas. Las POU representan el 12% de la capacidad generadora total instalada en el país. Aunque las tres cuartas partes de las POU sirven a comunidades rurales de 10.000 habitantes o menos, algunas de las principales ciudades del país, como Los Ángeles, San Antonio, Sacramento, Nashville, Jacksonville y Memphis obtienen su electricidad de compañías de servicios públicos de titularidad municipal. Por el hecho de ser instituciones públicas sin ánimo de lucro, las POU han ofrecido tradicionalmente un servicio de calidad a un precio más reducido que las compañías comerciales. Los consumidores domésticos pagan de media un 30% más si reciben la electricidad de compañías comerciales que si se la suministra una POU. Los clientes comerciales y residenciales juntos pagan un 9% más de media si compran la electricidad a compañías privadas. Además, sólo el 20% de la ventaja del suministro público procede de la financiación vía exenciones fiscales. El resto deriva de una mejor gestión y eficiencia.

Las compañías de servicios de titularidad pública, antes en decadencia, han experimentado un pequeño renacimiento en los últimos años como consecuencia del cansancio de las empresas y los consumidores privados ante las subidas de precios, los apagones y los cortes de electricidad. En las dos últimas décadas se han establecido cuarenta y cinco nuevos sistemas municipales, veintitrés de ellos en comunidades anteriormente servidas por compañías de titularidad privada.

Por el hecho de ser instituciones sin ánimo de lucro al servicio de comunidades enteras, las compañías de servicios de titularidad pública podrían movilizar más rápidamente a su actual base de clientes para crear una red de generación distribuida, primero como complemento o reserva auxiliar del sistema eléctrico actual y, finalmente, como una fuente alternativa de energía eléctrica. Las POU podrían comprar e instalar pilas de combustible en los hogares y las oficinas de sus clientes de forma «gratuita» y llegar a un acuerdo por el cual el exceso de electricidad generado por los operadores locales durante los períodos de bajo consumó fuera reenviado a la red, lo que podría repercutir en una rebaja de sus facturas eléctricas. Las POU podrían vender entonces los excedentes energéticos creados en otros lugares para ayudar a sufragar los costes de la compra e instalación de las pilas de combustible a domicilio del cliente, así como los del mantenimiento de la red energética.

Todavía queda otra institución inequívocamente norteamericana que, a pesar de tener carácter lucrativo, podría desempeñar un papel crucial en la difusión de la generación distribuida y sentar las bases para la creación de la red energética nacional. En la actualidad, más de 30 millones de norteamericanos —el 12% de la población— residen en alguna de las 150.000 urbanizaciones de interés común [common interest developments, CID]. Los residentes de las CID tienen la propiedad exclusiva de sus propias viviendas y comparten la propiedad de las «áreas comunes», que incluyen los parques, jardines, carreteras, aparcamientos, pistas de tenis, piscinas y centros recreativos. Todos los residentes pertenecen a una asociación de miembros de la comunidad y están obligados a pagar cuotas mensuales o anuales para la dirección y el mantenimiento de la misma.

Cada año se construyen entre 4.000 y 5.000 nuevas urbanizaciones de interés común. Si se mantiene la tasa de crecimiento actual —y todos los indicios apuntan hacia un aumento todavía mayor en las dos próximas décadas—, las CID podrían convertirse en un rival para los gobiernos municipales actuales, tal como señala Robert H. Nelson, un economista del Departamento de Interior de Estados Unidos.

Los promotores de las CID podrían establecer acuerdos con las compañías eléctricas locales antes de empezar las obras para que instalaran gratuitamente una pila de combustible en cada casa o unidad, a cambio de lo cual el excedente de electricidad generada sería vendido de nuevo a la red principal a un precio rebajado, o incluso entregado de forma gratuita para cubrir los costes de instalación del sistema de pilas de combustible y gestión de la red que habría asumido la compañía eléctrica. Alternativamente, podría ser un tercer agente el que actuara como central eléctrica virtual y financiara el coste de la adquisición e instalación de las pilas de combustible, a cambio de un contrato a largo plazo con la asociación de miembros de la comunidad que atribuyera a la central virtual la gestión del flujo del excedente de energía hacia la red eléctrica general. Las CID actuales podrían decidir instalar generadores de emergencia y financiarlos con cuotas anuales, como forma de garantizar el flujo ininterrumpido de la electricidad de su servicio eléctrico. El coste inicial de financiación de los generadores y el equipo complementario quedaría justificado por sí mismo, e incluso podría proporcionar unos ingresos sustanciales para las CID derivados de la venta del excedente energético a la red durante los períodos de máximo consumo.

Globalmente, las cooperativas constituyen tal vez el mejor vehículo organizativo para el establecimiento de las DGA. El movimiento de las cooperativas nació en el siglo XIX, cuando veintiocho hombres de negocios de Rochdale, Inglaterra, indignados ante los elevados precios impuestos por los comerciantes locales, se unieron para comprar alimentos al por mayor a precios más baratos y luego revenderlos a sus miembros a precios más bajos. La llamada Sociedad de los Pioneros Equitativos de Rochdale redactó una declaración de objetivos, más tarde conocida como los Principios Rochdale, que todavía hoy sigue siendo utilizada en una versión más moderada por las cooperativas de todo el mundo. Según la Alianza Internacional de Cooperativas [International Cooperative Alliance, ICA], con sede en Ginebra, Suiza, los principios de las cooperativas incluyen la apertura a nuevos miembros, la participación democrática, la distribución equitativa de los recursos, la autonomía, la educación, la cooperación entre cooperativas y la preocupación por la comunidad. La ICA afirma que representa a 230 organizaciones integradas por 750.000 grupos empresariales y de consumidores que operan en 100 países distintos, lo que representa una afiliación total de 730 millones de personas en todo el mundo.

Al sector corporativo de Estados Unidos le gusta pensar que la empresa privada es prácticamente el único actor dentro de la economía norteamericana. Las cosas no son exactamente así. Hay más de 48.000 cooperativas en Estados Unidos y su actividad económica mueve más de 125.000 millones de dólares anuales. Según la Asociación Nacional de Empresas Cooperativas [National Cooperative Business Association, NCBA], más de 75 millones de norteamericanos depositan su dinero en uniones de crédito, 50 millones contratan seguros con entidades afiliadas o integradas en cooperativas y 34 millones compran la electricidad a precio de coste en las cooperativas eléctricas rurales. El 30% de todos los productos agrícolas se comercializa a través de cooperativas. Las cooperativas también proporcionan seguros médicos a 1,4 millones de personas y más de un millón de hogares forman parte de alguna cooperativa. Marcas nacionales como Ace Hardware, Land O'Lakes, ShopRight, Ocean Spray, Sunkist y REI son cooperativas.

Las cooperativas son populares porque hacen posible la integración de productores o consumidores individuales para negociar de forma más efectiva con los proveedores. Los productores se sienten especialmente atraídos por las cooperativas porque pueden repartir los riesgos y las inversiones y establecer unos canales más eficientes de distribución y comercialización. Las cooperativas también transmiten a sus miembros un sentimiento de participación directa y de control sobre el negocio. «Un miembro, un voto» sigue siendo el principio operativo general en la gestión de las cooperativas, a diferencia del que rige en las empresas privadas, en las que el número de votos queda fijado por la cantidad de acciones que se poseen.

En muchos países, las cooperativas sin ánimo de lucro se hallan entre las empresas comerciales más grandes y poderosas. La cooperativa española Mondragón está integrada por 160 explotaciones controladas por sus empleados que operan en varios países, con unas ventas conjuntas anuales de más de 5.000 millones de dólares y con unos activos de más de 10.000 millones de dólares (cifras correspondientes a 1999). Italia cuenta con 250.000 cooperativas no lucrativas de trabajadores, integradas por pequeñas y medianas empresas artesanales e industriales. El Cooperative Group del Reino Unido integra más de 1.100 supermercados, un banco, una casa de seguros, una empresa de servicios agrarios, la mayor empresa de servicios funerarios de Gran Bretaña, un grupo minorista que comercializa sus propias marcas y gestiona más de 1.900 tiendas, la cuarta agencia de viajes más grande del país y 57 consultas de oculistas. La cooperativa de consumidores más grande de Japón tiene 19 millones de miembros, lo que representa el 20% de los hogares japoneses.

Con sus 730 millones de miembros repartidos por cien países distintos, las cooperativas están en una buena posición para ayudar a realizar la transición hacia la era del hidrógeno mediante la fundación de asociaciones de generación distribuida en miles de comunidades. Las cooperativas encajan bien con los requisitos de una red energética de generación distribuida, ya que tienen una base geográfica muy amplia, están formadas por un conjunto de productores o consumidores individuales integrados en una organización común y son entidades sin ánimo de lucro. Presentan una estructura organizativa horizontal adecuada para la infraestructura energética descentralizada y proporcionan además un modelo de gobierno democrático que contribuiría a promover la democratización de la energía en la nueva era del hidrógeno.

Touchstone Energy, una alianza nacional de 550 cooperativas eléctricas gestionadas por los consumidores que suministra electricidad a 16 millones de clientes en treinta y nueve Estados, ha establecido por primera vez una red para conectar todas las cooperativas eléctricas de Estados Unidos. La nueva red se anuncia en la televisión y en los medios impresos, y proporciona una factura integrada y un programa de gestión energética para las cooperativas integradas en ella, así como centrales de asistencia telefónica que cubren todo el país para que las cooperativas puedan compartir recursos entre zonas horarias distintas y proporcionar un servicio de 24 horas a sus clientes. Esta clase de redes a nivel nacional podrían contribuir a aumentar la popularidad de la generación distribuida y movilizar a la opinión pública en favor de la creación de una red energética nacional para el hidrógeno.

El recientemente creado Programa de Desarrollo de Cooperativas Energéticas, dirigido por la Comisión de la Energía de California, es uno de los nuevos modelos organizativos que podrían contribuir a configurar las DGA de todo el mundo. El objetivo del programa es ayudar a los consumidores a organizarse en cooperativas energéticas y crear una red energética a nivel regional y nacional. El sector al que se dirige inicialmente son las empresas pequeñas, las pequeñas explotaciones agrícolas, los hogares de rentas bajas y moderadas, los cultivadores de temporada, los arrendadores y las personas mayores con ingresos fijos, así como los nativos norteamericanos y otras minorías cuyo primer idioma no es el inglés.

En la actualidad ya se han creado dos cooperativas de usuarios eléctricos, o «servicios públicos sin cables», una en California y otra en la ciudad de Nueva York. La Cooperativa de Usuarios Eléctricos de California es una federación de dieciocho cooperativas agrarias que han llegado a un acuerdo para obtener un completo servicio de suministro eléctrico por parte de la empresa New West Energy, de Phoenix, Arizona. El acuerdo es un principio en el negocio de la energía. La Primera Cooperativa Rochdale [1st Rochdale Cooperative Group Inc.], de Nueva York, es la primera cooperativa de usuarios eléctricos creada para servir a una de las grandes ciudades de Estados Unidos. A mediados de los años noventa, a la vista de que las tarifas eléctricas de Nueva York estaban entre las más altas del país y constituían una partida importante dentro de los presupuestos de las cooperativas de viviendas, la Primera Cooperativa Rochdale buscó apoyo en la experiencia de la Asociación Nacional de Cooperativas Eléctricas Rurales [National Rural Electric Cooperative Association, NRECA] para establecer una alianza mutuamente beneficiosa, aprovechando que la reestructuración de la industria eléctrica abría nuevas oportunidades para las iniciativas independientes. En la actualidad, la Primera Cooperativa Rochdale suministra electricidad a bajo precio a 50,000 apartamentos de Nueva York. Si una parte de las 500.000 familias de Nueva York que viven en apartamentos de cooperativas se sumaran al proyecto, la Primera Cooperativa Rochdale podría llegar a convertirse en un poderoso actor dentro del mercado de la energía a nivel nacional y en un modelo para programas parecidos en otros lugares. La Primera Cooperativa Rochdale ha dado el primer paso hacia la generación distribuida y la creación de una red energética. Su iniciativa de la Manzana Verde promueve tanto el uso de las energías sostenibles como la generación distribuida.

En Chicago, la recién creada Cooperativa Comunitaria de la Energía se ha unido al Instituto de Investigación sobre la Energía Eléctrica en un programa piloto para llevar la generación distribuida a algunas familias del área de Chicago. La primera fase consiste en instalar pequeñas pilas de combustible en los hogares para cubrir las necesidades de electricidad de los residentes. Según Dan Rastler, responsable del área de recursos distribuidos en el EPRI, este esfuerzo pionero «es un paso hacia las minirredes descentralizadas que podrían cubrir parcialmente las necesidades de las comunidades con fuentes distribuidas domésticas o de mayor capacidad».

La Cooperativa Comunitaria de la Energía es el resultado de un proyecto original del Centro de Tecnología para los Barrios, una organización sin ánimo de lucro con sede en Chicago y destinada a la promoción de comunidades sostenibles. El lanzamiento de la cooperativa recibió el apoyo de ComEd, una de las mayores compañías eléctricas de Estados Unidos, radicada en Chicago. ComEd contribuyó a la puesta en marcha del proyecto con un programa de ayuda económica de tres años. El objetivo de la cooperativa es ayudar a sus miembros a reducir el consumo energético durante los períodos de máxima demanda. ComEd, a su vez, paga a la cooperativa por el ahorro energético obtenido. La cooperativa utiliza parte del dinero para ofrecer descuentos a sus miembros en la compra de aparatos y tecnologías energéticamente eficientes, un programa que en él futuro incluirá la compra, el arrendamiento y la instalación de pilas de combustible en los hogares y las oficinas de sus miembros. El resto del dinero se destina a programas juveniles, parques infantiles, centros para la tercera edad y otros proyectos comunitarios. La cooperativa espera invertir en las comunidades locales a las que sirve más de un millón de dólares derivados del ahorro energético antes del final de 2003.

El esfuerzo conjunto realizado por la Cooperativa Comunitaria de la Energía de Chicago, ComEd y el EPRI es un ejemplo del tipo de asociaciones que se formarán probablemente en el futuro entre las compañías eléctricas nacionales, las cooperativas y otras organizaciones no lucrativas de base local. Actualmente ya se han puesto en marcha proyectos para establecer cooperativas energéticas parecidas a la Primera Cooperativa Rochdale en otras grandes ciudades estadounidenses, como Washington, D.C., y Filadelfia.

Muchos de los elementos estructurales necesarios para hacer viable una HEW ya están presentes en Estados Unidos y otros países. Si la infraestructura eléctrica se hace cada vez más vulnerable a las interferencias, los apagones y los cortes de electricidad, y si continúa subiendo el precio de la electricidad, hay que esperar la aparición de nuevos actores en el mercado de la energía dispuestos a aprovechar la brecha abierta, en especial cooperativas sin ánimo de lucro, servicios públicos de titularidad municipal, asociaciones vecinales, corporaciones de desarrollo comunitario, uniones de crédito comunitarias y urbanizaciones de interés común, los cuales podrían poner en marcha proyectos de colaboración entre sí y con los proveedores comerciales para crear asociaciones de generación distribuida y comenzar a desplegar las redes energéticas del hidrógeno.

Dar poder a los pobres

El 65 % de la población mundial no ha hecho siquiera una llamada telefónica y una tercera parte de la humanidad no tiene acceso a la electricidad ni a ninguna otra forma de energía comercial. La distancia entre los conectados y los desconectados es profunda y amenaza con ampliarse todavía más durante el próximo medio siglo, en el que se espera que la población mundial aumente desde los 6.200 millones actuales hasta los 9.000 millones. Buena parte de este incremento de la población tendrá lugar en el mundo en vías de desarrollo, donde se halla concentrada la pobreza.

Aunque muchos norteamericanos acomodados viven bajo la ilusión de que la distancia entre los ricos y los pobres se está recortando cada vez más gracias a los continuos avances de la ciencia, la tecnología y las innovaciones comerciales, la situación es más bien la contraria. En más de cien países —con un volumen total de población de 1.600 millones de personas— la economía se halla en recesión. Y en ochenta y nueve países la renta per cápita actual es inferior a la de hace diez años. La familia media africana consume actualmente un 20% menos que hace veinticinco años. Por otro lado, la Organización Internacional del Trabajo estima que una tercera parte de los 3.000 millones de trabajadores del mundo se encuentran actualmente desempleados o subempleados.

La pobreza extrema es la realidad habitual en buena parte del mundo. En la actualidad, 600 millones de personas no tienen casa o viven en condiciones poco seguras, y el Banco Mundial estima que, alrededor del año 2010, cerca de 1.400 millones de personas vivirán sin agua potable y sin sistemas de alcantarillado. Mientras que el 20% de las personas con mayores ingresos del mundo cubren el 86% del consumo privado actual, el 20% más pobre consume menos del 1,3% de la producción económica global. Un estudio dirigido por el Programa de Desarrollo de las Naciones Unidas ha revelado un dato todavía más increíble: el valor de los activos en manos de las 358 personas más ricas del mundo supera la suma de los ingresos anuales de aproximadamente la mitad de la población mundial.

La falta de acceso a la energía, sobre todo a la electricidad, es un factor clave para la perpetuación de la pobreza en todo el mundo. A la inversa, el acceso a la energía significa mayores oportunidades económicas. En Sudáfrica, por ejemplo, se crean entre diez y veinte negocios por cada cien nuevos hogares que reciben electricidad. La electricidad emancipa el trabajo diario de las exigencias básicas de supervivencia. La simple tarea de encontrar suficiente madera o estiércol para calentar una casa o cocinar en países con escasos recursos puede ocupar varias horas cada día. La electricidad permite adquirir maquinaria agrícola y poner en marcha pequeñas fábricas y talleres artesanales, así como iluminar los hogares, las escuelas y los negocios.

Tal como se ha señalado antes, la cantidad de energía consumida per cápita es una prueba de la capacidad de ir más allá de la mera supervivencia que ha demostrado la humanidad a lo largo de la historia. En la actualidad, el consumo energético per cápita en el mundo en vías de desarrollo apenas representa una quinceava parte del consumo de Estados Unidos. La media global de consumo energético per cápita en todos los países del mundo sólo llega a la quinta parte del nivel de Estados Unidos.

En su estudio sobre la relación entre las categorías sociales y el consumo eléctrico, Chauncey Starr, presidente emérito del Instituto de Investigación sobre la Energía Eléctrica, observa que por debajo de un umbral crítico de ingresos y consumo energético per cápita anuales, las personas se ven forzadas a dedicar la mayor parte del día a la «supervivencia», es decir, a encontrar un refugio temporal y una cantidad suficiente de agua y alimentos. Una vez que las personas se aseguran un mínimo acceso al empleo y a la electricidad, alcanzan una «calidad de vida básica» que incluye la alfabetización, una mejor higiene, seguridad personal y una mayor expectativa de vida. A medida que aumenta el consumo energético y los ingresos per cápita, la persona accede a un estilo de vida con una serie de «prestaciones» que incluyen la educación, el ocio y la inversión intergeneracional. En el escalón más alto de las categorías sociales, en el nivel de lo que Starr llama la «colaboración internacional», las personas disfrutan de un acceso ilimitado a la electricidad y pueden crear comunidades de interés de escala auténticamente global. El problema es que en el próximo medio siglo más del 90% de la población mundial nacerá dentro de las categorías sociales de la «supervivencia» o la «calidad de vida básica».

Sí un tercio de la población mundial no tiene acceso a la electricidad, la situación no es mucho mejor para aquellos que sí lo tienen. En este momento, la mitad de la humanidad vive con apenas unos cientos de kilovatios/hora de electricidad per cápita al año, una cantidad que apenas representa diferencia alguna en su calidad de vida. Alcanzar el objetivo marcado por las agencias internacionales de desarrollo de conseguir la electrificación universal para el año 2050 supondría llevar la electricidad a 100 millones de personas cada año, una cifra dos veces y media superior al número de usuarios que se introducen actualmente por año. Proporcionar a estos 100 millones de usuarios nuevos un consumo medio per cápita equivalente al que disfrutaron los consumidores estadounidenses en 1950 requeriría la creación de 10 millones de MW de capacidad de generación eléctrica para el año 2050, lo que significa multiplicar por cuatro el consumo actual. El Instituto de Investigación sobre la Energía Eléctrica estima que para alcanzar este objetivo debería entrar en funcionamiento una nueva central eléctrica de 1.000 MW cada 48 horas durante los próximos cincuenta años. Por si esto no fuera suficiente reto, el EPRI añade que, para cumplir con las regulaciones ecológicas globales, el 50% de la nueva capacidad de generación no debería estar alimentada con carbono. Para cumplir este objetivo haría falta una inversión de capital de entre 100.000 y 150.000 millones de dólares anuales. La Agencia Internacional de la Energía calcula que tan sólo la capacidad de generación que debería instalarse en los países en vías de desarrollo supone una inversión del orden de 1,7 billones de dólares entre 1995 y 2020.

La cuestión es: ¿hasta qué punto es realista el objetivo de la electrificación universal, cuando el consumo energético per cápita global ya ha tocado techo y está bajando por el lado negativo de la curva de campana, y se espera que el petróleo y el gas natural lo hagan en algún momento antes de 2020? Incluso el EPRI reconoce que el esfuerzo «no será fácil». El motivo es que la transición al gas natural que se está realizando en Estados Unidos y otros países no puede mantener la producción de electricidad mucho más allá de 2030. Tal como se ha señalado antes, el EPRI estima que la mayor dependencia de unas reservas cada vez más escasas de gas natural podría provocar una escalada de los precios, con subidas que podrían llegar al 50%. Después de 2030, según el EPRI, tendremos que buscar otras estrategias. La única estrategia alternativa viable que nos queda es emplear los recursos de las energías renovables para crear un régimen energético basado en el hidrógeno.

Incluso en este caso, la magnitud de la tarea es desalentadora. Para crear un futuro sostenible en el que fuera posible generar la suficiente energía como para garantizar una calidad de vida decente para todos los seres humanos de la Tierra, al tiempo que se evitaran emisiones perjudiciales para el calentamiento global, haría falta conseguir un aumento anual de la productividad de un 2%. Podemos darnos cuenta de lo difícil que será conseguir este objetivo si pensamos que Estados Unidos ha conseguido efectivamente un aumento acumulado de la productividad del 2% a lo largo del pasado siglo —el doble de la media mundial—, pero que lo ha logrado en buena medida gracias a que ha tenido acceso a grandes reservas de petróleo crudo barato.

La única forma de sacar a miles de millones de personas de la pobreza consiste en realizar la transición hacia un régimen energético basado en el hidrógeno —en el que se utilicen tecnologías y recursos renovables para producir el hidrógeno— y crear redes energéticas de generación distribuida capaces de conectar comunidades de todo el mundo. Recortar la distancia que separa a los que tienen recursos de los que no los tienen significa antes que nada recortar la distancia entre los conectados y los desconectados.

Las pilas de combustible y los equipos complementarios serán cada vez más accesibles a medida que bajen los precios gracias a las innovaciones y las economías de escala, igual que sucedió con los transistores, los ordenadores y los teléfonos móviles. El objetivo debería ser instalar pilas de combustible fijas en todos los barrios y pueblos del mundo en vías de desarrollo. Las pilas de combustible individuales pueden proporcionar energía de forma rápida y barata en áreas rurales donde no se han instalado aún líneas eléctricas comerciales por resultar una inversión excesiva. Una vez que se hayan vendido o alquilado las suficientes pilas de combustible, se podrán crear minirredes energéticas para conectar zonas urbanas y rurales en redes energéticas cada vez más extensas. La construcción de la HEW podría ser un proceso orgánico que acompañara a la extensión del uso de la generación distribuida. Las pilas de combustible de hidrógeno de mayor tamaño tienen la ventaja añadida de producir agua potable pura como producto derivado, una consideración nada insignificante en aquellas comunidades pequeñas donde el acceso a aguas limpias suele constituir un problema crucial.

Los países del Tercer Mundo ya no tendrán que depender del flujo de petróleo crudo. Recordemos que, en los años setenta, las subidas de los precios impuestos por la OPEP tuvieron un efecto mucho más perjudicial en los países pobres del mundo que en los países industrializados. En los años posteriores al embargo petrolero de la OPEP de 1973, el precio del petróleo se multiplicó por cuatro en los mercados mundiales —el precio del barril pasó de 3 a 13 dólares—, lo que causó estragos en los países en vías de desarrollo. Los países del Tercer Mundo se vieron forzados a solicitar préstamos a los bancos occidentales y a instituciones financieras internacionales como el Banco Mundial y el Fondo Monetario Internacional (FMI) para cubrir la subida del coste de sus importaciones de petróleo, sin las cuales sus proyectos de desarrollo económico quedarían bloqueados. Los préstamos de la banca comercial al Tercer Mundo aumentaron un 550% entre 1973 y 1980.

La segunda subida de los precios del petróleo impuesta por la OPEP en 1979 provocó una recesión a nivel global y el colapso de los precios de los productos básicos, lo que debilitó aún más las economías de los países en vías de desarrollo, ya agobiadas por las deudas. La escalada de los costes de las importaciones de petróleo, en combinación con la caída de los precios de sus productos en los mercados mundiales, obligó a los países en vías de desarrollo a solicitar nuevos préstamos, la mayor parte de los cuales servían únicamente para comprar petróleo y pagar los intereses de deudas pasadas. Hacia 1985, la deuda del Tercer Mundo superaba el billón de dólares. Como la mayor parte del dinero de los préstamos se utilizaba para comprar petróleo y pagar deudas pasadas, quedaba poco dinero para promover un desarrollo económico real. El resultado fue que los países del Tercer Mundo perdieron impulso económico y se hundieron todavía más en la pobreza. El crédito comercial e institucional comenzó a retirarse progresivamente y los países del Tercer Mundo entraron en una espiral descendente. En 1988 muchos países en vías de desarrollo registraban una pérdida neta de dinero.

Las condiciones no han hecho más que empeorar en los últimos años, ya que los países en vías de desarrollo se han vuelto todavía más dependientes de las importaciones de petróleo para alimentar su sector industrial en expansión, proporcionar luz y calefacción para unas poblaciones urbanas cada vez más numerosas y cubrir sus crecientes necesidades de transporte. En los años setenta, los países en vías de desarrollo sólo representaban el 26% de la demanda mundial de petróleo. En la actualidad su cuota supera el 40% y no para de aumentar. La subida de los precios del petróleo en el año 2000 se tradujo en un aumento de 6.000 millones de dólares en el balance de importaciones de la India. La factura de las importaciones de petróleo de Brasil era un 150% más alta en 2000 que en el año anterior. China experimentó un aumento del 250% en la factura de sus importaciones petroleras en el mismo año. En muchos países, el aumento de los precios del petróleo había anulado los beneficios de la asistencia internacional al desarrollo. Según la EIA, en países como China y Tailandia el coste adicional del petróleo importado supera en dos veces y media el volumen total de las ayudas internacionales.

El secretario general de las Naciones Unidas, Kofí A. Annan, ha advertido recientemente que en la mayoría de los países en vías de desarrollo «es probable que aumenten los costes de mantenimiento de la deuda si la subida de los precios del petróleo lleva a un aumento de los tipos de interés internacionales» en los próximos años. La deuda del Tercer Mundo ha alcanzado ya proporciones de crisis antes incluso de que la producción global de petróleo toque techo. A finales de 1999, 47 países —con una población conjunta de 1.100 millones de personas— debían más de 422.000 millones de dólares a entidades extranjeras. La deuda media per cápita en estos países es de 380 dólares, una cifra que equivale aproximadamente a la media del producto nacional bruto per cápita. Lo más grave, en los países más pobres —aquellos cuyo PNB está por debajo de los 885 dólares anuales per cápita—, es que 83 centavos de cada dólar que reciben de los nuevos préstamos se destinan meramente a pagar sus antiguas deudas, lo que deja poco margen para promover el desarrollo y mejorar el nivel de vida. La dimensión humana de la crisis de la deuda externa es dramática. Muchos de los países más pobres del mundo dedican una parte más importante de los ingresos nacionales a cubrir la deuda externa que a la salud, la educación y otros servicios sociales básicos.

No es ninguna sorpresa, pues, que los activistas antiglobalización que participan en los foros para el desarrollo mundial hayan convertido la cuestión de la cancelación de la deuda externa en su principal consigna. Aunque las instituciones acreedoras han accedido a cancelar una pequeña parte de la creciente deuda, los críticos afirman que no será suficiente para invertir la grave situación económica de la mayoría de los países del Tercer Mundo. Estos países sólo conseguirán salir de su postración y mejorar las condiciones económicas de sus habitantes si se liberan de la dependencia de las importaciones de petróleo y gas natural.

Es necesario establecer asociaciones de generación distribuida por todo el mundo en vías de desarrollo. Las organizaciones de la sociedad civil, las cooperativas (allí donde existan), las instituciones de microcrédito y los gobiernos locales tendrían que ver las redes energéticas de generación distribuida como la estrategia básica para construir comunidades sostenibles y autosuficientes. La «distribución del poder» adquiere un significado distinto en este contexto. Aquellas personas que no tengan acceso a la energía, y particularmente a la electricidad, seguirán sin poder controlar su destino personal. Para conseguir romper el círculo de la dependencia y la desesperación —y acceder verdaderamente al «poder»— hay que tener primero el control sobre la energía.

Es necesario presionar e influir sobre los gobiernos nacionales y las instituciones, internacionales de crédito para que den apoyo financiero y logístico a la creación de una infraestructura energética basada en el hidrógeno. También es importante que se aprueben nuevas leyes que faciliten la adopción de la generación distribuida. Será necesario exigir a las compañías públicas y privadas que garanticen el acceso de los operadores de la generación distribuida a la red eléctrica general, así como el derecho a vender energía o intercambiarla por otros servicios.

La era de los combustibles fósiles trajo consigo una infraestructura energética altamente centralizada y una infraestructura económica paralela que favorecía a la minoría sobre la mayoría. La creciente distancia que ha separado durante el último siglo a los que tienen de los que no tienen, y recientemente a los conectados de los desconectados, es atribuible en buena medida a la naturaleza del régimen energético de los combustibles fósiles.

Ahora bien, en el momento de máximo desarrollo de la era del hidrógeno, podemos imaginar una infraestructura energética descentralizada, capaz de promover una democratización de la energía y permitir a los individuos, las comunidades y los países reivindicar su propia independencia, al tiempo que asumen también la responsabilidad de su interdependencia.

A principios de los años noventa, cuando la era de Internet daba sus primeros pasos, la necesidad de un «acceso universal» a la información y a las comunicaciones se había convertido en la principal consigna de toda una generación de activistas, consumidores, ciudadanos y líderes políticos. Ahora que iniciamos el viaje hacia la era del hidrógeno, debería surgir una nueva generación de activistas inspirados por la reivindicación de un acceso universal a la energía que contribuyera a poner los cimientos para la creación de comunidades sostenibles. Si eso sucediera, se podría redefinir la estructura global del poder sobre bases enteramente distintas. Esta vez el poder circularía horizontalmente, de hogar a hogar, de barrio a barrio y de comunidad a comunidad, hasta crear una vasta infraestructura energética descentralizada que promovería los valores de la autosuficiencia y la interdependencia.

Si todos los individuos y comunidades del mundo se convirtieran en productores de su propia energía, el resultado sería un cambio fundamental en la configuración del poder: su estructura ya no sería vertical, sino horizontal. Las comunidades locales estarían menos sujetas a la voluntad de los lejanos centros del poder y podrían producir una parte importante de sus bienes y servicios y consumir localmente los frutos de su propio trabajo. Sin embargo, como también estarían conectadas a través de las redes mundiales de energía y de comunicaciones, se encontrarían en condiciones de compartir sus habilidades comerciales, servicios y productos únicos con otras comunidades de todo el mundo. Esta clase de autosuficiencia económica se convertiría en el punto de partida de una interdependencia comercial global y su resultado sería una realidad económica muy distinta de la de los regímenes coloniales del pasado, donde las comunidades locales se hallaban en una posición subordinada y dependiente respecto a las potencias exteriores.

La creación de comunidades locales económicamente sostenibles haría posible algo más que el mero bienestar material. Dar poder a las comunidades locales también contribuye a preservar la rica diversidad cultural de la familia humana. La autosuficiencia económica proporciona la seguridad material que necesitan los pueblos para mantener la cohesión social y preservar sus riquezas culturales. Al mismo tiempo, la integración en redes globales de energía y de comunicaciones liberaría a las personas de la xenofobia que tradicionalmente ha acompañado a aquellas formas de vida más aisladas geográficamente. En este nuevo contexto global, las culturas locales dejan de ser una posesión que defender y se convierten en un don que compartir con el mundo. El intercambio cultural volvería a ganar importancia y se convertiría en una expresión de la interacción entre las personas tan poderosa como el intercambio comercial. El capital social prosperaría también junto al capital mercantil y el poder político emanaría del interior de la cultura y no de las esferas comerciales o gubernamentales.

Dentro de este nuevo orden de cosas, la globalización comienza por el pleno reconocimiento de la esfera de poder de todos los seres humanos, que irradia posteriormente a la familia, la comunidad, el país, las redes de intereses comerciales y la biosfera misma. La redistribución del poder hacia todas las personas hace posible establecer las condiciones para un reparto verdaderamente equitativo de las riquezas de la Tierra. Ésta es la esencia del proyecto político de la reglobalización desde abajo.

Replantearse la seguridad

La economía del hidrógeno va a traer consigo una nueva forma de entender la sociología de nuestra existencia, igual que sucedió con la era de los combustibles fósiles. Las grandes transformaciones que han experimentado los regímenes energéticos a lo largo de la historia han provocado siempre una reformulación de las principales categorías de la existencia humana. A pesar del carácter único de cada una de las culturas de cazadores-recolectores, todas ellas comparten un espíritu común, igual que sucede con las sociedades agrícolas e industriales. La forma que tienen los seres humanos de captar, transformar y utilizar las formas dominantes de energía que tienen a su disposición —sean animales y plantas silvestres, cultivos artificiales, esclavos humanos o máquinas alimentadas con carbón, petróleo y gas natural— queda reflejada en sus distintas ideas sobre la seguridad personal y colectiva.

Comparemos, por ejemplo, la idea de seguridad que tenían los cristianos en la Baja Edad Media con la de la burguesía capitalista del siglo XX. Estas dos formas tan distintas de entender la seguridad dicen mucho acerca de la diferente naturaleza de los regímenes energéticos de los que dependían las personas para su supervivencia.

Tal como hemos señalado antes en este capítulo, la Europa medieval estaba difusamente organizada alrededor de la Iglesia católica, con el acuerdo de los señores de la guerra locales: los reyes, los príncipes y los señores feudales. La sociedad era vista como un microcosmos de la grandiosa creación divina, que descendía desde el cielo en forma de una gran escala o cadena del ser cuya cima era ocupada por Dios, tras el cual venían sus emisarios en la Tierra, el Papa y los sacerdotes, y luego las figuras menores, entre las que se hallaban los reyes, los nobles, los caballeros, los granjeros, los campesinos, los siervos y nuestro prójimo hasta llegar a las regiones más bajas e incluir «todo cuanto se arrastra sobre la superficie de la Tierra». Según explicaba santo Tomás de Aquino, cada peldaño de la escalera divina estaba ocupado por alguna de las criaturas de Dios y todos ellos estaban llenos, sin que hubiera espacio para la sorpresa, el cambio o la innovación dentro del plan magistral de Dios. El mundo de la Iglesia era una estructura firmemente trenzada a partir de unas jerarquías cuidadosamente establecidas y un detallado catecismo de reglas que gobernaban las obligaciones entre las personas. La seguridad dependía de que los seres humanos realizaran las tareas que la Providencia les había asignado y aceptaran su papel y sus responsabilidades dentro de la jerarquía natural. Mediante un estricto cumplimiento de sus deberes y obligaciones en este mundo los hombres y las mujeres medievales conseguían un cierto grado de seguridad en la Tierra, así como una seguridad eterna en el mundo por venir.

Dentro del orden medieval de las cosas, la tierra tenía una importancia especial a la hora de definir la seguridad, pues era allí donde uno prestaba su servicio a la creación divina. En una época en que la agricultura constituía el régimen energético dominante, la seguridad emanaba de forma natural de la tierra. Los lazos de unión con la tierra se hacían más fuertes todavía, porque en el orden feudal la mayor parte de las personas pertenecían a ella, más que al revés. La seguridad era por lo tanto una cuestión vertical, que comenzaba en este mundo con una sensación de vinculación al hogar ancestral en el que se había nacido y ascendía hasta el mundo por venir, donde se recibía el premio de la vida eterna.

La economía del carbón cambió fundamentalmente la ecuación de la seguridad. Ahora eran las máquinas de vapor las que permitían a la sociedad superar la capacidad de los esclavos humanos y los animales con sustitutos mecánicos. Los nuevos esclavos mecánicos hacían que el «hombre» se sintiera menos dependiente de Dios y de las fuerzas de la naturaleza para su bienestar. La autonomía ocupó gradualmente el lugar de la salvación divina, al menos para aquellos que controlaban y recibían los beneficios de lo que el historiador Arnold Toynbee llamaría más tarde la «Revolución Industrial».

La economía del carbón también aumentó enormemente el ritmo, la velocidad, el flujo, la diversidad y la intensidad de la vida humana. Si durante la Edad Media poca gente se alejaba a más de un día de camino de su propia casa, a finales del siglo XIX millones de personas atravesaban regularmente océanos y continentes enteros en sólo irnos pocos días gracias a los trenes y los barcos de vapor. El ferrocarril y el telégrafo acortaron las distancias, comprimieron el tiempo y añadieron una nueva dimensión a la vida humana: la «movilidad». Millones de granjeros, desplazados de sus tierras ancestrales y liberados de sus ataduras feudales, emigraron a las bulliciosas ciudades de la época, en pleno proceso de expansión descontrolada, donde se convirtieron en «mano de obra libre» destinada a trabajar en las fábricas junto a las máquinas de vapor. La nueva era urbana estaba cada vez más caracterizada por la inquietud y el cambio constante. La innovación se convirtió en el sello propio de la economía industrial. Ser moderno significaba ser vanguardista, experimentar constantemente con ideas, modas y estilos de vida nuevos. La generación más joven acusaba a sus mayores de estar «pasados de moda».

La creciente sensación de autoconfianza y el ritmo acelerado de vida que caracterizaban a la era que se estrenaba se tradujeron en una concepción completamente nueva de la seguridad. La visión medieval de la seguridad, íntimamente asociada a la pertenencia a la tierra en este mundo y a la salvación en el otro, fue reemplazada de forma gradual por un nuevo sentido de la seguridad, basado en la idea de la autonomía y la movilidad personal. Estos dos valores iban a convertirse en las virtudes dominantes de la clase burguesa emergente en todos los países industriales.

La autonomía se convirtió en sinónimo de libertad. Ser autónomo significaba controlar el propio destino y no depender de otros. La clave para conseguir la propia autonomía era la propiedad. La nueva función del gobierno, a su vez, consistía en defender y ampliar los mercados y asegurar con ello el derecho de las personas a acumular propiedades para que pudieran ganarse la autonomía y ser verdaderamente libres.

En la nueva era, la movilidad significaba mucho más que la posibilidad de ir de un lugar a otro. Tener movilidad era disponer siempre de nuevas opciones y oportunidades. Del mismo modo que la autonomía se convirtió en sinónimo de libertad, la movilidad se convirtió en sinónimo de oportunidad. La era de los combustibles fósiles liberó a los seres humanos de los lentos ritmos estacionales de la agricultura y, con ello, de las limitaciones impuestas por la naturaleza y la intervención divina. Cada nuevo converso a la modernidad estaba marcado por un espíritu prometeico. Armado con su autonomía y su movilidad, el ser humano podía convertirse en un dios menor y construirse un pequeño paraíso propio en este mundo material.

La clase media, cuyo sentido de la seguridad estaba tan íntimamente asociado a la autonomía y la movilidad, encontró el objeto supremo de sus afectos a comienzos del siglo XX con la invención del automóvil. He aquí una máquina —impulsada con gasolina— que en un mismo acto proporcionaba al conductor y los pasajeros una sensación completa de autonomía y movilidad. No es ninguna casualidad que el automóvil se convirtiera rápidamente en la principal metáfora de la época y en la pieza clave de la economía industrial. Henry Ford comentó en una ocasión que su coche debía ser visto como un «salón con ruedas». A qué joven no le invade una sensación de libertad y anticipación la primera vez que toma el asiento del conductor y pone las manos sobre el volante y el pie sobre el acelerador. Cuando uno va en coche por la carretera se siente a la vez autónomo y móvil, y tan seguro como se puede estar en la era moderna... y todo ello gracias a los combustibles fósiles.

La concepción moderna de la seguridad no sólo marcaba el comportamiento de los individuos, sino también el de los países. La nueva «ciencia» de la geopolítica, definida por primera vez en 1882 por Friedrich Ratzel, un profesor alemán de geografía, se convirtió en el reflejo perfecto de la nueva sensibilidad burguesa. Tomando prestadas muchas ideas de la teoría darwinista del origen y el desarrollo de las especies, sobre todo en el aspecto de la competición biológica por un espacio y unos recursos escasos, Ratzel argumentaba que en su esfuerzo por ampliar sus oportunidades comerciales y su dominio militar, el Estado-nación no hacía otra cosa que cumplir con su destino biológico. Las naciones tratan de asegurarse la autonomía mediante una expansión constante de sus dominios y la clave de su éxito reside en su movilidad. Ratzel pensaba que la mejor forma de hacer efectiva la movilidad era tener una presencia dominante en los océanos. «Alemania debe ser fuerte en el mar», escribió Ratzel, «para cumplir su misión en el mundo».

El geógrafo británico sir Alfred Mackinder estaba de acuerdo con Ratzel en que el control de los océanos, que cubren las nueve décimas partes de la superficie de la Tierra, era lo que daba a las naciones la movilidad necesaria para defenderse de los agresores y desarrollar sus propios planes expansionistas. Tras la Segunda Guerra Mundial, los teóricos de la geopolítica extendieron su planteamiento para incluir también la movilidad en el aire.

Los arquitectos de la nueva geopolítica pronto se dieron cuenta de la importancia estratégica del carbón, el petróleo, el hierro y otros minerales, sin los cuales la guerra moderna o el modelo de vida industrial eran impensables. Nicholas Spykman, una de las principales figuras de la geopolítica de principios del siglo XX, escribió que el control sobre los recursos naturales es un factor crucial en el diseño de la política exterior. Tal como hemos visto antes, los líderes políticos y militares de ambos bandos en las dos guerras mundiales tuvieron ocasión de comprobar la importancia del acceso al petróleo y otros recursos vitales para lograr la autonomía, la movilidad y la victoria.

Las nociones modernas de la seguridad y la geopolítica tenían sentido en un mundo en el que el tiempo seguía siendo lineal y el espacio seguía siendo extenso. El terreno de juego espacial y temporal era todavía lo suficientemente amplio como para poner el énfasis en la autonomía y la movilidad.

Sin embargo, en un mundo donde la tecnología hace posible que las personas interactúen entre sí y desarrollen sus negocios y su vida social a la velocidad de la luz, 24/7, la duración se acorta hasta convertirse casi en simultaneidad y las distancias prácticamente desaparecen. La Tierra deja de ser un espacio de encuentro y pasa a convertirse en una unidad indivisible. El primer anuncio de la nueva realidad llegó en 1945, cuando el ejército norteamericano lanzó dos bombas atómicas sobre las ciudades japonesas de Hiroshima y Nagasaki. De golpe, la humanidad comprendió que tenía el poder suficiente para destruirse a sí misma. Por primera vez comenzamos a hablar de un mundo unificado en el que la seguridad era algo que ganábamos o perdíamos entre todos. La idea misma de encontrar algún medio de garantizar la autonomía individual o nacional en un mundo nuclear resultaba anticuada y completamente ingenua.

Las primeras fotografías de la Tierra tomadas desde el espacio exterior en los años sesenta también contribuyeron a cambiar nuestras ideas sobre la seguridad. Las imágenes causaron una honda impresión e hicieron que nos viéramos como los habitantes de una pequeña esfera que giraba alrededor de una estrella menor perdida en la inmensidad del universo. Si la autonomía se basa en la afirmación del individuo frente a la colectividad y la movilidad supone un vasto espacio en el que maniobrar, la nueva perspectiva de la Tierra vista desde la distancia transmitía más bien la idea de una única comunidad planetaria confinada en un espacio extraordinariamente reducido dentro del teatro cósmico.

Tal vez el hecho que mayor influencia ha tenido a la hora de obligarnos a reformular nuestras nociones convencionales de seguridad es el aumento de las temperaturas terrestres causado por la quema de combustibles fósiles durante la era industrial. Hemos dedicado la mayor parte de la era de los combustibles fósiles a someter a los ecosistemas de la Tierra para adaptarlos a la actividad comercial, con el objetivo de hacer realidad nuestros valores individuales y colectivos de autonomía y movilidad. Ahora la Tierra nos somete a nosotros con el resultado de nuestro propio consumo energético y socava la autonomía y la movilidad que tan desesperadamente hemos tratado de alcanzar durante toda la era moderna.

Hoy nos encontramos embarcados en una nueva revolución científica, tecnológica y comercial que podría tener como resultado, al menos en teoría, la creación de una red única e indivisible que conectara a la humanidad entera y al resto de las criaturas del planeta. Sin embargo, nuestras ideas sobre la seguridad todavía están ancladas en una época que pensaba de acuerdo con las estrechas premisas darwinistas, según las cuales cada organismo es una isla que sólo busca optimizar su movilidad y asegurar su autonomía.

Existen signos de que la primera generación que ha crecido entre videojuegos, ordenadores, palm pilots y teléfonos móviles, cuyos integrantes han pasado buena parte de su tiempo conectados unos a otros y compartiendo información a través de Internet y la World Wide Web, podría estar desarrollando otro tipo de conciencia y, con ella, una nueva concepción de la seguridad. Para la «generación dot.com», la idea misma de autonomía que tanto influyó sobre la concepción de la seguridad que mantenía la generación de sus padres resulta sospechosa, cuando no directamente irrelevante para sus necesidades en el nuevo mundo interconectado. Si las generaciones anteriores definían la libertad en términos de autonomía y exclusividad —cada persona es una isla cerrada en sí misma—, los hijos de Internet han crecido en un entorno tecnológico muy distinto, en el que la autonomía es un valor ignorado o asimilado al aislamiento y la muerte, y en el que la libertad es vista más bien como el derecho a ser incluido en múltiples relaciones. Sus identidades están mucho más entrelazadas con las redes de las que forman parte. Para ellos, el tiempo es virtualmente simultáneo y las distancias apenas importan. Cada vez están más conectados con las demás personas y realidades a través de un sistema nervioso central electrónicamente mediado que se extiende por toda la Tierra y pretende abarcarlo virtualmente todo. Y cada día que pasa están más profundamente imbricados en un organismo social superior, para el que ciertas nociones, como la de autonomía personal, tienen escaso sentido, y el sentimiento de movilidad ilimitada proviene de la propia densidad e interactividad de los estrechos vínculos que les unen.

Las redes comerciales jerarquizadas que surgieron de la mano del régimen energético de los combustibles fósiles y del modelo de vida industrial, caracterizadas por una dirección centralizada y los sistemas de control, estaban diseñadas de acuerdo con la idea moderna de seguridad. Las empresas corporativas que han dirigido la economía global en los últimos estadios de la era de los hidrocarburos son afines a la idea de que la mejor forma de garantizar la seguridad institucional consiste en promover siempre una mayor autonomía y movilidad.

La red de comunicaciones mundial y la perspectiva de una red energética del hidrógeno, sin embargo, abren la posibilidad de reorganizar la vida comercial. En la nueva era de los mecanismos descentralizados de dirección y control, y de la gestión potencialmente democrática de la energía y de las comunicaciones, una era en la que las personas tienen lazos cada vez más estrechos con los demás y con el resto de la realidad a través de múltiples redes de relaciones que se mueven a la velocidad de la luz, podemos empezar a reformular nuestras ideas sobre la seguridad.

De la geopolítica a las políticas de la biosfera

Los nuevos avances en la comprensión científica del funcionamiento de la Tierra proporcionan un marco unificado para reformular la cuestión de la seguridad en la era del hidrógeno. El primer científico que concibió la Tierra como un «organismo vivo» fue el ruso Vladimir Vernadsky. En 1926, Vernadsky escribió un libro que posteriormente se ha hecho famoso, titulado Biosfera, en el que proponía la hipótesis, contraria a la teoría de Darwin, de que los procesos geoquímicos evolucionaron primero y crearon un entorno atmosférico adecuado para la aparición de los organismos vivos, es decir, que en realidad los procesos geoquímicos y biológicos evolucionaron juntos, en una relación simbiótica. Vernadsky creía que el ciclo químico de la materia inerte de la Tierra está influido por la cualidad y la cantidad de materia viva existente y que la materia viva influye a su vez sobre la cantidad y la cualidad de la materia química inerte del planeta.

La biosfera es una delgada capa superficial de entre 50 y 65 kilómetros que se extiende desde las profundidades oceánicas hasta la parte superior de la estratosfera y que contiene todas las formas de vida que existen en la Tierra. Dentro de esta estrecha banda vertical las criaturas vivas y los procesos geoquímicos de la Tierra interactúan para sostener la vida.

En los años setenta, el científico inglés James Lovelock y el biólogo norteamericano Lynn Margulis retomaron la tesis de Vernadsky con la publicación de la hipótesis Gaia. Dicha hipótesis afirma que la Tierra funciona como un organismo vivo capaz de autorregularse. Según su teoría, la biota (la flora y la fauna de una determinada región) y la composición geoquímica de la atmósfera mantienen una relación simbiótica dirigida a mantener unos niveles climáticos relativamente estables y favorables a la vida en la Tierra.

Lovelock propone la regulación del oxígeno y el metano como principales ejemplos de cómo el proceso cibernético entre la materia viva y el ciclo geoquímico permite mantener un régimen climático homeostático en la Tierra. Lovelock recuerda que los niveles de oxígeno del planeta deben estar comprendidos entre unos márgenes extraordinariamente estrechos. Un aumento del 1% en los niveles de oxígeno incrementaría un 70% el riesgo de incendios. Un aumento del 4% pondría el planeta entero en llamas y provocaría la destrucción de toda la materia viva de la superficie emergida. La producción de oxígeno se mantiene gracias a la fotosíntesis. Los cloroplastos verdes de las células vegetales convierten la energía solar en energía química para alimentar a la planta y en el proceso transforman el dióxido de carbono y el agua en oxígeno. Los animales, a su vez, absorben el oxígeno para mantenerse con vida y emiten dióxido de carbono al entorno. Buena parte del dióxido de carbono se recicla de nuevo a través del ciclo de las plantas, y así sucesivamente.

Aunque hace tiempo que los científicos conocen la interacción existente entre los ciclos del oxígeno y el dióxido de carbono, todavía no se explican cómo es posible que los niveles de oxígeno se mantengan tan estables, a pesar de los cambios drásticos que tienen lugar en las emisiones solares y en las clases y el número de seres vivos que habitan el planeta. Para comprender por qué el nivel de oxígeno se mantiene estable en el 21% es necesario comprender cómo reacciona con otros gases atmosféricos. Lovelock recurre al metano para ofrecer una explicación parcial del fenómeno.

Hace menos de treinta años los científicos descubrieron que el metano era un subproducto biológico derivado de la fermentación bacteriana. Los microorganismos que viven en el interior de los animales rumiantes, en las termitas y en las turberas, producen más de 900 millones de toneladas de metano al año. El metano va a parar a la atmósfera, donde actúa como regulador de la cantidad de oxígeno que hay en el aire. En cuanto llega a la estratosfera, el metano se oxida y se transforma en dióxido de carbono y vapor de agua. El agua, a su vez, se descompone en oxígeno e hidrógeno. El oxígeno desciende hacia la Tierra, mientras que el hidrógeno se escapa hacia el espacio exterior. De este modo, el metano puede incrementar los niveles de oxígeno en la atmósfera superior. En la atmósfera inferior, la oxidación del metano consume oxígeno, en una cantidad de mil megatoneladas al año. Lovelock hace notar que «en ausencia de la producción de metano, la concentración de oxígeno aumentará hasta un 1% en sólo 24.000 años: un cambio muy peligroso y demasiado rápido dentro de la escala temporal geológica».

Lovelock y Margulis creen que cuando la cantidad de oxígeno de la atmósfera supera los niveles tolerables se dispara algún tipo de señal de alarma que hace que las bacterias microscópicas aumenten su producción de metano. El exceso de metano se libera en la atmósfera y reduce los niveles de oxígeno hasta que se vuelve a una situación estable. La constante interacción entre los seres vivos y los elementos y los ciclos geoquímicos actúa como una unidad orgánica que mantiene estable el clima y el medio ambiente de la Tierra y hace posible la vida:

Resulta fascinante pensar que sin la ayuda de esta flora anaeróbica que vive en los malolientes lodos de los lechos marinos, los lagos y los estanques no sería posible la lectura o la escritura. Sin el metano que ellos producen, las concentraciones de oxígeno aumentarían inexorablemente hasta alcanzar niveles en los que cualquier fuego sería el holocausto, y la vida en las tierras emergidas sería imposible, a excepción de la microflora de algunos lugares húmedos.

Así pues, buena parte de los elementos que componen la biosfera proceden de los seres vivos o son modificados por ellos. Lovelock nos recuerda que el oxígeno y el hidrógeno del aire proceden directamente de las plantas y los microorganismos. Los depósitos de piedra caliza y creta derivan de las conchas y los huesos de antiguos animales marinos. Los arrecifes coralinos y numerosas islas no son otra cosa que los cementerios de una cantidad inimaginable de animales microscópicos. Lovelock cita éstos y otros ejemplos en apoyo de su tesis de que la vida no es simplemente un añadido a un mundo estático e inerte de materia «determinado por la mano muerta de la química y la física». Al contrario, dice Lovelock, «la evolución de las rocas y del aire no es separable de la evolución de la biota».

Por lo tanto, el planeta es algo así como un ser vivo, una entidad orgánica que se autorregula para mantenerse en un estado estable conducente a la preservación de la vida. De acuerdo con la hipótesis Gaia, la adaptación y la evolución de las criaturas individuales forma parte de un proceso más amplio: la adaptación y la evolución del planeta mismo. Lo que garantiza la supervivencia tanto del organismo planetario como de las especies individuales que viven en su biosfera es la constante relación simbiótica que se establece entre los seres vivos y los procesos geoquímicos.

Muchos otros científicos han estudiado desde entonces la hipótesis Gaia y han contribuido a desarrollar, cualificar y moderar el trabajo de Lovelock y Margulis. La idea de que la Tierra funciona como un organismo vivo ha sido un punto de partida crucial para reformular las relaciones entre la biología, la química y la geología durante las dos ultimas décadas.

Si la Tierra funciona realmente como un organismo vivo, entonces las actividades humanas que interfieren en la bioquímica de este organismo pueden tener graves consecuencias, tanto para la vida humana como para el conjunto de la biosfera. La quema masiva de combustibles fósiles es el primer ejemplo de una actividad humana, a escala global, que amenaza con provocar un cambio radical en el clima de la Tierra y poner en peligro la biosfera que sostiene todas las formas de vida.

La incipiente conciencia que tenemos del funcionamiento de la Tierra como un organismo vivo indivisible nos exige que reformulemos nuestras ideas sobre la seguridad. Si todos los seres humanos, la especie en su conjunto, y las demás criaturas de la Tierra están entrelazados con la geoquímica del planeta en una coreografía rica y compleja que hace posible la vida misma, entonces todos y cada uno de nosotros somos dependientes y responsables de la salud del conjunto del organismo. Saber llevar esta responsabilidad significa vivir nuestras vidas individuales, en nuestros vecindarios y comunidades, de una forma que promueva el bienestar general de la biosfera en la que vivimos.

El científico francés René Dubos se hizo eco de este nuevo sentido de la seguridad que emerge de nuestra nueva concepción científica del funcionamiento de la Tierra cuando llamó a todos los seres humanos a «pensar globalmente y actuar localmente». Internet y la World Wide Web nos ofrecen un «sistema nervioso central» electrónicamente mediado que nos permite conectar a todas las personas y los elementos de la biosfera para que, por primera vez, podamos realmente «pensar globalmente y actuar localmente». Al mismo tiempo, la red del hidrógeno nos ofrece un nuevo régimen energético no contaminante que descentraliza y democratiza la energía, y permite que la población humana viva en comunidades más pequeñas y dispersas, menos capaces de presionar a la biosfera más allá de sus límites. La despoblación masiva del campo y la emigración hacia megaciudades con poblaciones que se cuentan por millones —el rasgo social más definitorio de la era de los combustibles fósiles— es simplemente insostenible desde una perspectiva termodinámica. La energía del hidrógeno está repartida por todas partes y las pilas de combustible pueden ser instaladas en cualquier lugar y luego conectadas en vastas redes de energía, por lo que estamos en condiciones de superar la arquitectura jerarquizada y centralizada de la era del petróleo. En la economía del hidrógeno, la actividad industrial y comercial puede desarrollarse de una forma más sostenible desde el punto de vista ecológico y hacer posible una distribución más equilibrada de los asentamientos humanos.

Por otro lado, la disponibilidad universal del hidrógeno, así como la ligereza, la flexibilidad y, con el tiempo, los bajos costes de las tecnologías de generación distribuida plantearán inevitablemente la cuestión de si deberíamos replantearnos las instituciones políticas y las fronteras existentes. Después de todo, el Estado-nación es una creación exclusiva de la era de los combustibles fósiles. Tal como vimos en el capítulo 4, la introducción del telégrafo y de la máquina de vapor alimentada con carbón hizo posible el desarrollo del comercio y la vida social en territorios mucho más extensos y dio origen al Estado-nación, una especie de «organización política a escala» capaz de controlar vastas regiones geográficas. Por desgracia, la redefinición de las fronteras políticas para crear los Estados-nación tuvo muy poco en cuenta la dinámica de los ecosistemas e hizo difícil que la población humana pudiera vivir de forma sostenible.

En la economía del hidrógeno, con su red energética descentralizada y democratizada, se pueden establecer asentamientos humanos adaptados a las bio-regiones, eco-regiones y geo-regiones que reflejan los patrones de asentamiento de muchas de las comunidades bioquímicas del propio planeta Tierra. Entrelazar las comunidades humanas con las biocomunidades genera un nuevo y profundo sentido de la seguridad, inseparable de la salud y el bienestar de la Tierra.

La creación de una arquitectura económica y social que constituya un microcosmos de la propia fisiología de la Tierra abre un nuevo mundo lleno de posibilidades de naturaleza afirmativa y regeneradora para nuestra especie. Finalmente se acaba el prolongado y bárbaro reinado de la geopolítica y comienza un nuevo peregrinaje para crear una forma de gobierno estable sobre la biosfera.

En el curso de la historia hay algunos momentos singulares en los que una generación de seres humanos recibe un nuevo don que les permite reconstruir las relaciones que mantienen entre ellos y con el mundo que les rodea. Éste es uno de tales momentos. Se nos ha entregado el poder del Sol. El hidrógeno es una señal prometedora para el futuro de la humanidad sobre la Tierra. Depende de nosotros si echamos a perder esta promesa a base de proyectos frustrados y oportunidades perdidas, o si la usamos sabiamente en beneficio de nuestra especie y de las demás criaturas del planeta.

BIBLIOGRAFÍA

Adams, Richard Newbold, Energy and Structure: A Theory of Social Power, Austin, Texas, University of Texas Press, 1975.

Ahmad, Q. K. y otros, Summary for Policy Makers, Climate Change 2001: Impacts, Adaptation, and Vulnerability, informe del Grupo de Trabajo II del Intergovernmental Panel on Climate Change, IPCC, febrero de 2001.

Albritton, Daniel L. y otros, Summary for Policy Makers: Climate Change 2001: The Scientific Basis, informe del Grupo de Trabajo I del Intergovernmental Panel on Climate Change, IPCC, febrero de 2001.

Anderson, Robert O., Fundamentals of the Petroleum Industry, Norman, Oklahoma, University of Oklahoma Press, 1984.

Armstrong, Karen, Islam: A Short History, Nueva York, The Modern Library, 2000 (trad. cast.: El islam, Barcelona, Mondadori, 2001).

Banuri, Tariq y otros, Summary for Policy Makers, Climate Change 2001: Mitigation, informe del Grupo de Trabajo III del Intergovernmental Panel on Climate Change, IPCC, febrero-marzo de 2001.

Barber, Benjamin R., Jihad Versus McWorld, Nueva York, Bailantine Books, 2001.

Bateson, Gregory, Steps To An Ecology Of Mind: Collected Essays in Anthropology, Psychiatry, Evolution, and Epistemology, Northvale, NJ, Jason Arson, 1987 (trad. cast.: Pasos hacia una ecología de la mente, Buenos Aires, Lumen, 1997).

Beniger, James R., The Control Revolution, Cambridge, MA, Harvard University Press, 1986.

Blum, Harold F., Times Arrow and Evolution, Princeton, NJ, Princeton University Press, 1968.

Borbely, Anne-Marie y Jan F. Kreider (comps.), Distributed Generation: The Power Paradigm for the New Millennium, Washington, D.C., CRC Press, 2001.

Catton, William R., Jr., Overshoot: The Ecological Basis of Revolutionary Change, Urbana, University of Illinois Press, 1980.

Chambers, Ann, Distributed Generation: A Nontechnical Guide, Tulsa, PennWell, 2001.

Chandler, Alfred D., Jr., The Visible Hand: The Managerial Revolution in American Business, Cambridge, Massachusetts, Belknap Press of Harvard University Press, 1977 (trad. cast.: La mano visible: revolución en la dirección de la empresa norteamericana, Madrid, Ministerio de Trabajo y Asuntos Sociales, 1988).

Chomsky, Noam, Rogue States: The Rule of Force in World Affairs, Cambridge, MA, South End Press, 2000 (trad. cast.: Estados canallas: el imperio de la fuerza en los asuntos mundiales, Barcelona, Paidos, 2001).

Clark, Wilson, Energy for Survival, Garden City, Nueva York, Doubleday/Anchor Books, 1975.

Cochcaisg, Willard, Development of American Agriculture: A Historical Analysis, 2ª ed., Minneapolis, University of Minnesota Press, 1993.

Condorcet, Marqués de, Outlines of an Historical View of the Progress of the Human Mind, Londres, J. Johnson, 1795 (trad. cast.: Bosquejo de un cuadro histórico de los progresos del espíritu humano, Madrid, Editora Nacional, 1980).

Corn, Joseph J. (comp.), Imagining Tomorrow: History Technology and the American Future, Cambridge, MA, MIT Press, 1986.

Davidson, Lawrence, Islamic Fundamentalism, Westport, Connecticut, Greenwood Press, 1998.

Deffeyes, Kenneth S., Hubbert's Peak: The Impending World Oil Shortage, Princeton, NJ, Princeton University Press, 2001.

Dunn, Seth, Micropower: The Next Electrical Era, Worldwatch Paper n° 151, Washington, D.C., Worldwatch Institute, julio de 2000.

—, Hydrogen Futures: Toward a Sustainable Energy System, Worldwatch Paper n° 157, Washington, D.C., Worldwatch Institute, agosto de 2001.

Economides, Michael y Ronald Oligney, The Color Of Oil: The History, the Money and the Politics of the World's Biggest Business, Katy, Texas, Round Oak Publishing, 2000.

Falkenrath, Richard A., Robert D. Newman y Bradley A. Thayer, America's Achilles Heel: Nuclear, Biological, and Chemical Terrorism and Covert Attack, Cambridge, MA, MIT Press, 1998.

Farb, Peter, Humankind, Boston, Houghton Mifflin, 1978.

Fergosi, Paul, Jihad in the West: Muslim Conquests from the 7th to the 21st Centuries, Amherst, Nueva York, Prometheus Books, 1998.

Frank, Tenney, An Economic Survey of Ancient Rome, vol. V, Rome and Italy of the Empire, Baltimore, Johns Hopkins Press, 1940.

Gause III F. Gregory, Oil Monarchies: Domestic and Security Challenges in the Arab Gulf States, Nueva York, Council on Foreign Relations Press, 1994.

Gever, John, David Skole y Robert K. Kaufmann, Beyond Oil: The Threat to Food and Fuel in the Corning Decades, Niwot, CO, University Press of Colorado, 1991.

Gibbon, Edward, The Decline and Fall of the Roman Empire, Nueva York, Modern Library, 1776-1788 (trad. cast.: Historia de la decadencia y ruina del Imperio Romano, Madrid, Hyspamérica, 1988).

Goodman, David y otros, From Farming to Biotechnology: A Theory of Agro-Industrial Development, Nueva York, Basil Blackwell, 1987.

Hawken, Paul, Amory Lovins y L. Hunter Lovins, Natural Capitalism: Creating the Next Industrial Revolution, Boston, Little, Brown and Company, 1999.

Hirsh, Richard F., Power Loss: The Origins of Deregulation and Restructuring in the American Electric Utility System, Cambridge, MA, MIT Press, 1999.

Hoffmann, Peter, Tomorrow's Energy: Hydrogen, Fuel Cells, and the Prospect for a Cleaner Planet, Cambridge, MA, MIT Press, 2001.

Houghton, John, Global Warming: The Complete Briefing, Cambridge, MA, Cambridge University Press, 1997.

Hoveyda, Fereydoun, The Broken Crescent: The Threat of Militant Islamic Fundamentalism, Westport, CT, Praeger, 1998.

Huband, Mark, Warriors of the Prophet: The Struggle for Islam, Boulder, CO, Westview Press, 1998.

Hughes, Donald J., Ecology in Ancient Civilizations, Albuquerque, NM, University of México Press, 1975.

Hughes, Thomas P., Networks of Power: Electrication in Western Society, 1880-1930, Baltimore, Johns Hopkins University Press, 1983.

Huntington, Samuel P., The Clash of Civilizations and the Remaking of World Order, Nueva York, Simón and Schuster, 1996 (trad. cast.: El choque de las civilizaciones y la reconfiguración del orden mundial, Barcelona, Paidós, 1997).

Johnson, Chalmers, Blowback: The Costs and Consequences of American Empire, Nueva York, Metropolitan Books, 2000.

Jones, A. H. M., The Roman Economy: Studies in Ancient Economic and Administrative History, Oxford, Basil Blackwell, 1974.

Juergensmeyer, Mark, Terror in the Mind of God: The Global Rise of Religious Violence, Berkeley, CA, University of California Press, 2000 (trad. cast.: Terrorismo religioso: auge global de la violencia religiosa, Madrid, Siglo XXI, 2001).

Khavari, Farid A., Oil and Islam: The Ticking Bomb, Malibu, CA, Roundtable Publishers, 1990.

Kranzberg, Melvin y Carroll W. Pursell, Jr. (comps.), Technology in Western Civilization: The Emergence of Modern Industrial Society Earliest Times to 1900, vols. I y II, Nueva York, Oxford University Press, 1967.

Landes, David S., Revolution in Time, Cambridgev, MA, Harvard University Press, 1983.

Levy, Jean-Philippe, The Economic Life of the Ancient World, Chicago, University of Chicago Press, 1967.

Lovins, Amory B. y L. Hunter Lovins, Brittle Power: Energy Strategy for National Securityv, Snowmass, CO, Rocky Mountain Institute; Andover, MA, Brick House Publishing Co., Inc., 2002.

Marcuse, Herbert, One-Dimensional Man: Studies in the Ideology of Advanced Industrial Society, Boston, Beacon Press, 1964 (trad. cast.: El hombre unidimensional: ensayo sobre la ideología de la sociedad industrial avanzada, Barcelona, Ariel, 2000).

McNeill, William H., Plagues and Peoples, Garden City, Anchor/Doubleday, 1976 (trad. cast.: Plagas y pueblos, Madrid, Siglo XXI, 1984).

Melko, Matthew, The Nature of Civilizations, Boston, Porter Sargent Publisher, 1969.

Miller, G. Tyler, Jr., Energetics, Kinetics and Life, Belmont, CA, Wadsworth, 1971.

Miller, Steven E., Civilizing Cyberspace: Policy, Power, and the Information Superhighway, Nueva York, Addison Wesley, 1996.

Moaddel, Mansoor y Kamran Talattof (comps.), Contemporary Debates in Islam: An Anthology of Modernist and Fundamentalist Thought, Nueva York, St. Martin's Press, 2000.

Motavalli, Jim, Forward Drive: The Race to Build «Clean» Cars for the Future, San Francisco, Sierra Club Books, 2000.

Mowbray, A. Q., Road to Ruin, Filadelfia, Lippincott, 1969.

Mumford, Lewis, Technics and Civilization, Nueva York, Harcourt, Brace, 1934 (trad. cast.: Técnica y civilización, Madrid, Alianza, 2000).

Noble, David F., America By Design: Science, Technology and the Rise of Corporate Capitalism, Nueva York, Alfred A. Knopf, Inc., 1977 (trad. cast.: El diseño de Estados Unidos: ciencia, capitalismo y aparición del capitalismo corporativo, Madrid, Ministerio de Trabajo y Asuntos Sociales, 1987).

—, Forces of Production, Nueva York, Oxford University Press, 1984.

Odum, Howard T., Environment, Power, and Society, Chapel Hill, NC, University of North Carolina Press, 1971 (trad. cast.: Ambiente, energía y sociedad, Barcelona, Blume, 1980).

Philander, S. George, Is the Temperature Rising? The Uncertain Science of Global Warming, Princeton, NJ, Princeton University Press, 1998.

Pipes, Daniel, In the Path of God: Islam and Political Power, Nueva York, Basic Books, Inc., 1983 (trad. cast.: El islam: de ayer a hoy, Madrid, Espasa-Calpe, 1987).

Ponting, Clive, A Green History of the World: The Environment and the Collapse of Great Civilizations, Nueva York, Penguin Books, 1991 (trad. cast.: Historia verde del mundo, Barcelona, Paidós, 1992).

Quigley, Carroll, The Evolution of Civilizations, Indianápolis, Liberty Press, 1961.

Rashid, Ahmed, Taliban: Militant Islam, Oil and Fundamentalism in Central Asia, New Haven, Connecticut, Yale University Press, 2000.

Redman, Charles L., Human Impact on Ancient Environments, Tucson, AZ, University of Arizona Press, 1999.

Rifkin, Jeremy, Time Wars: The Primary Conflict in Human History, Nueva York, Henry Holt and Company, Inc., 1987 (trad. cast.: Las guerras del tiempo, Buenos Aires, Editorial Sudamericana, 1989).

—, Entropy Into the Greenhouse World, Nueva York, Bantam Books, 1989 (trad. cast.: Entropía: hacia el mundo invernadero, Barcelona, Urano, 1990).

—, Biosphere Politics: A New Consciousness for a New Century, Nueva York, Crown Publishers, Inc., 1991.

—, The End of Work: The Decline of the Global Labor Force and the Dawn of the Post-Market Era, Nueva York, Tarcher/Putnam, 1995 (trad. cast.: El fin del trabajo, Barcelona, Paidós, 1997).

—, The Biotech Century: Harnessing the Gene and Remaking the World, Nueva York, Tarcher/Putnam, 1998 (trad. cast.: El siglo de la biotecnología, Barcelona, Crítica, 1999).

—, The Age of Access: The New Culture of Hypercapitalism Where All of Life Is a Paid-For Experience, Nueva York, Tarcher/Putnam, 2000 (trad. cast.: La era del acceso, Barcelona, Paidós, 2000).

Romm, Joseph J., Cool Companies: How the Best Businesses Boost Profits and Productivity by Cutting Greenhouse Gas Emissions, Washington, D.C., Island Press, 1999.

Rusk, David, Inside Game, Outside Game, Washington, D.C., Brookings Institution, 1999.

Schneider, K. R., Autokind v. Mankind, Nueva York, Schoken, 1972.

Schrodinger, Erwin, What ls Life?, Nueva York, Macmillan, 1947 (trad. cast.: ¿Qué es la vida?, Barcelona, Tusquets, 1984).

Shadid, Anthony, Legacy of the Prophet: Despots, Democrats and the New Politics of Islam, Boulder, CO, Westview Press, 2001.

Shapiro, Andrew L., The Control Revolution: How the Internet is Putting Individuáis in Charge and Changing the World We Know, Nueva York, Public Affaírs, 1999.

Soddy, Frederick, Matter and Energy, Londres, Oxford University, col. «Home University», 1912.

Soto, Hernando de, The Mystery of Capital, Nueva York, Basic Books, 2000 (ed. cast.: El misterio del Capital, Bogotá, Norma, 1999).

Spengler, Oswald, The Decline of the West, Nueva York, Modern Library, 1962 (trad. cast.: La decadencia de occidente: bosquejo de una morfología de la historia universal, Madrid, Espasa-Calpe, 1999).

Stern, Jessica, The Ultimate Terrorists, Cambridge, MA, Harvard University Press, 1999.

Swarztrauber, Sayre A., The Three-Mile Limit of Territorial Seas, Annapolis, MD, Naval Press Institute Press, 1972.

Tainter, Joseph A., The Collapse of Complex Societies, Cambridge, MA, Cambridge University Press, 1988.

Taylor, Frederick, The Principies of Scientific Management, Nueva York, W. W. Norton, 1947.

Thomas, William L. (comp.), Man's Role in Changing the Face of the Earth, Chicago, Wenner-Gren Foundation for Anthropological Research and the National Science Foundation by the University of Chicago Press, 1956.

Tibi, Bassam, The Challenge of Fundamentalism, Berkeley, CA, University of California Press, 1998.

Tichi, Cecelia, Shifting Gears, Chapel Hill, NC, University of North Carolina Press, 1987.

Verne, Julio, The Mysterious Island, Nueva York, Garamond Press, Limited Editions Club, 1959 (trad. cast.: La isla misteriosa, Madrid, Gaviota, 2001).

Victor, David G., The Collapse of the Kyoto Protocol and the Struggle to Slow Global Warming, Princeton, NJ, Princeton University Press, 2001.

White, Leslie A., The Science of Culture: A Study of Man and Civilization, Nueva York, Farrar, Straus and Company, 1949 (trad. cast.: La ciencia de la cultura: un estudio sobre el hombre y la civilización, Barcelona, Paidós, 1982).

—, The Evolution of Culture, Nueva York, McGraw Hill Company, Inc., 1959.

Whitehead, Alfred North, Science and the Modern World, Nueva York, New American Library, 1925.

Wilkinson, Richard G., Poverty and Progress: An Ecological Perspective on Economic Development, Nueva York, Praeger Publishers, 1973.

Winkless, Neis, III y Iben Browning, Climate and the Affairs of Men, Nueva York, Harper's Magazine Press, 1975.

Yergin, Daniel, The Prize: The Epic Quest for Oil, Money and Power, Nueva York, Simón and Schuster, 1992 (trad. cast.: La historia del petróleo, Barcelona, Plaza & Janés, 1992).

Youngquist, Walter, GeoDestinies: The Inevitable Control of Earth Resources Over Nations and Individuals, Portland, OR, National Book Company, 1997.
