Enrique Dans

TODO VA A CAMBIAR

Tecnología y evolución:

adaptarse o desaparecer

Conocer y entender los cambios que la tecnología motiva en el comportamiento de los consumidores es crucial para que las empresas sepan cómo adaptar correctamente sus modelos de negocio y mantener con ello su competitividad. Negarse a aceptar dichos cambios y obstinarse en mantener estrategias válidas en su momento pero ineficaces en el presente y obsoletas en el futuro supone la mejor receta para conseguir una lenta pero inexorable desaparición.

Enrique Dans nos explica en este libro qué trasformaciones ha supuesto la irrupción de Internet en los modelos de negocio de sectores relacionados con el entretenimiento y la cultura (tales como la industria discográfica, la audiovisual, los medios de comunicación o el sector editorial) y nos describe cómo y en qué medida las disrupciones tecnológicas van a seguir manifestándose en los próximos años, al tiempo que nos indica cuáles son las referencias a observar y los movimientos del mercado que debemos monitorizar.

Igualmente, nos explica qué política y legislación debe aplicarse a la propiedad intelectual, por qué no tiene sentido luchar contra las mal llamadas descargas ilegales, y en qué sentido nos favorece a todos preservar la llamada "neutralidad de la red".

Fue un placer que me pidieran que escribiese un prólogo para el primer libro de Enrique Dans, Todo va a cambiar. Enrique tiene toda la razón: Todo va a cambiar. Pero, como dicen los franceses: «cuanto más cambia una cosa, más se convierte en lo mismo». Fijémonos, por ejemplo, en el cloud computing. En cierto modo, se trata de una extensión natural del time-sharing, inventado en los años sesenta. De hecho, por aquel entonces ya se oía hablar del término computing utility. Se imaginaban ordenadores enormes situados en edificios inmensos, a los que se podía acceder a través de líneas telefónicas desde terminales remotos. Hoy, el «terminal» es el navegador y la computing utility está compuesta por miles de procesadores apilados en racks en edificios inmensos. ¿Y qué hay de interconectar las nubes? Google tiene una red de cloud computing (centros de conexión de datos); Amazon, IBM, Microsoft y otros ofrecen varios servicios a través de la nube. ¿Qué ocurre si intentamos interconectar esas nubes? La realidad con respecto a esto en 2010 es prácticamente la misma que en 1973, cuando Bob Kahn y yo empezamos a investigar modelos y arquitecturas para la interconexión de redes basadas en paquetes de datos. Las redes existían, pero no se interconectaban entre sí, y los servidores de una red no podían intercambiar información con los servidores de otra. Lo mismo se puede decir de los actuales sistemas en la nube: si bien es cierto que los navegadores pueden interactuar con una serie de nubes que, a su vez, están conectadas a Internet, las transacciones entre hosts de nubes específicas no son normalmente posibles. Transferir datos de una nube a un portátil u ordenador de sobremesa y luego de estos a otra nube suena a algo parecido al concepto de sneaker net de los años setenta. No me cabe la menor duda de que las interacciones entre nubes van a ser necesarias y de que habrá que desarrollar nuevos protocolos, estructuras y modelos de referencia para producir el efecto deseado: la posibilidad de transferir datos entre nubes y de hacer cálculos, posiblemente utilizando varios sistemas de nubes al mismo tiempo. Se trata sin duda de un área en la que hay un gran potencial de investigación y de desarrollo de nuevos productos y servicios.

A medida que Internet y sus componentes vayan evolucionando, se irán repitiendo las mismas ideas, aunque con nuevos formatos. Los teléfonos móviles son una novedad en el entorno de la red de datos y nada podría ser más indicativo del ritmo del cambio que la rapidez con la que está evolucionando este campo. Probablemente, el subtítulo de Enrique Dans, «adaptarse o desaparecer», venga especialmente al caso en este mercado. Las nuevas funcionalidades para la telefonía móvil están emergiendo a un ritmo espectacular. Una de las razones es que las propias plataformas se están abriendo más (por ejemplo, el Android de Google) y las tiendas de aplicaciones ofrecen a los usuarios la posibilidad de descargarse nuevas aplicaciones sin necesidad de comprarse otro móvil. El resultado es una impresionante proliferación de nuevas funcionalidades procedentes no sólo de los fabricantes de teléfonos o de las compañías de servicios de comunicaciones, sino también de los propios usuarios. Este fenómeno refleja la apertura de Internet, que sigue evolucionando como consecuencia del desarrollo de nuevas aplicaciones de sus usuarios. La World Wide Web es en gran parte responsable de dicho desarrollo al proporcionar un entorno que facilita la aportación de contenido y funcionalidad por el usuario. El asombroso crecimiento de YouTube ilustra muy bien este punto. Asimismo, resulta cada vez más evidente que estamos evolucionando hacia convertirnos en una sociedad online. Alrededor del 25% de la población mundial dispone de conexión a Internet y en torno a un 5% más tiene acceso a la red exclusivamente a través del móvil. Los países con una mayor penetración de Internet tienen del orden del 75 al 80% de su población conectada. En estos países está aumentando el uso de Internet para la distribución de todo tipo de información y crece también la actividad colaborativa en la red. Todos los medios de información del pasado se están recreando y reinventando en Internet. Formatos impresos, radio, televisión, telefonía y comunicación a través del ordenador se mezclan y conviven en ella.

Esta convergencia tiene muchas consecuencias. Las economías de Internet ponen de manifiesto la gran diferencia de costes que supone el almacenamiento y la distribución digital respecto al almacenaje y la distribución en papel, película o DVD. Las empresas construidas alrededor del uso de medios físicos necesitarán introducir cambios significativos en sus modelos de negocio para adaptarse a la nueva economía. La publicidad online está cambiando claramente el rostro de los modelos de negocio de las empresas que hacen publicidad en periódicos, revistas, radio y televisión. La publicidad dirigida a un target concreto puede ser mucho más precisa, potencialmente, en el mundo online que su contrapartida offline. Aparecerán empresas nuevas y las más antiguas tendrán que adaptarse o, como dice Enrique, desaparecer.

Un ejemplo de empresa nueva e inesperada nos lo ofrece eBay, que permite realizar subastas en la red que serían totalmente imposibles en el mundo real. Se generan espacios para comercializar lo que las personas tienen en sus garajes y trasteros de maneras que el típico mercadillo norteamericano jamás podría ofrecer. Aparecen nichos minúsculos que se agregan por la interconexión de las redes e inyectan una nueva energía al negocio de las subastas, los intercambios o el trueque. Otro ejemplo nos lo ofrecen los juegos multijugador online. Aparecen emprendedores inquietos y dinámicos que se ganan la vida creando «moda» virtual y «cirugía» estética para avatares virtuales, mientras otros se ganan la vida jugando para mejorar los resultados de otros jugadores más perezosos o con menor disponibilidad de tiempo para desarrollar sus personajes ellos mismos.

Es frecuente observar, al menos en el mundo de la alta tecnología, que muchos de los empleos actuales ni siquiera existían hace cinco años. El ritmo de cambio y evolución del mercado de trabajo seguirá sorprendiéndonos y cuestionando nuestras capacidades y nuestra preparación para nuevos trabajos. Esta observación pone de manifiesto la importancia de la formación continua y el aprendizaje a lo largo de toda la vida. De nuevo, Internet puede contener las semillas necesarias para superar este desafío al facilitar el acceso a una educación continuada en la red a escala global.

Por último, está claro que Internet, que ha provocado una notable revolución social en las conexiones personales, también va a ser escenario de la «Internet de las cosas»: miles de millones de dispositivos conectados unos a otros a través de la red. Tanto las redes de sensores como las redes de suministro eléctrico inteligentes (Smart Grid) formarán parte de este entorno. La demanda de conectividad generará un interés cada vez mayor por el acceso inalámbrico a Internet y por el incremento de su capacidad. La gran cantidad de instrumentos y aparatos conectados forzará la adopción del nuevo protocolo iPv6, que permite acomodar 340 trillones de trillones de trillones de direcciones únicas.

Que Internet seguirá evolucionando es algo que nadie pone en duda. En qué dirección lo hará es, sin embargo, una pregunta completamente abierta para la que no tenemos predicciones garantizadas. ¡Tendremos que vivirlo y pasar por ello para averiguar hasta dónde podremos llegar!

VINT CERF

Woodhurst, enero de 2010

INTRODUCCIÓN

En 1986, un billete de lotería de Navidad premiado dio a mi padre la oportunidad de cumplir unos cuantos caprichos, entre otros el de regalarme un ordenador. En realidad, el capricho no era mío, sino de él: yo, con mi carné de conducir recién obtenido, suspiraba por un coche. Pero el coche tuvo que esperar. Por alguna misteriosa razón que nunca me ha explicado, mi padre se mostró completamente convencido de que aquello era mucho más importante, y nos plantamos directamente en una tienda de informática.

En aquella época, prácticamente ningún particular en España adquiría un ordenador. Recuerdo perfectamente la charla del vendedor, imagino que extrañado ante lo que tenía delante, y cómo nos fue ofreciendo diferentes alternativas hasta centrar la decisión en un clónico de IBM PC de marca Elbe (Electrónica Bertrán, fabricado en Barcelona), con procesador Intel 8088 a 4,77 MHz, dos unidades de disco de 5 1/4 pulgadas y un monitor monocromo de fósforo verde. Recuerdo también perfectamente cómo el vendedor nos habló de una gran novedad ya disponible, el disco duro: tenía uno de IBM capaz de almacenar diez megas, que nos ofreció montar en nuestro ordenador. Tras unos cuantos cálculos mentales, y aterrado ante la posibilidad de salir de la tienda sin el ordenador en ese mismo instante, decidí que no quería aquel disco duro, pensando para mí: «aunque teclee toda mi vida, voy a ser completamente incapaz de llenar diez megas de información». Hoy, diez megas no dan ni para dos canciones en MP3, y cualquiera de las presentaciones que utilizo en mis clases o conferencias ocupan en torno a diez o veinte veces más... ¡un solo archivo! Como bautismo para aprender a reírme de mí mismo y de los visionarios tecnológicos, la verdad es que no estuvo nada mal.

Aquel ordenador fue completamente operativo hasta el año 1990. Me sirvió, primero, para impresionar a los profesores de mi carrera (un trabajo pasado por un procesador de textos podía ser igual de bueno o de malo que el de cualquiera de mis compañeros, pero su sola presentación le proporcionaba unos cuantos puntos extra), para aprender a usar y programar bases de datos, para descubrir que con una hoja de cálculo, era capaz de mover el mundo, y para pasar tantas horas de juegos, que todavía hoy me pregunto cómo es que no se me pusieron los ojos verdes por el reflejo del fósforo del monitor. Aquel ordenador me acompañó horas y horas de noches larguísimas preparando trabajos en mis años de carrera en A Coruña y Santiago, y después durante mi MBA en Madrid. Gracias a él, cuando llegué en 1989 a clase de informática en las primeras semanas de mi MBA, me di cuenta de que «me lo sabía todo», y que podía dedicar el tiempo de clase a explicar a aquellos que se perdían intentando seguir al profesor. Aquel profesor, José Mario Álvarez de Novales, fue quien me ofreció, tras terminar mi MBA, quedarme en el mismo Instituto de Empresa dando clases de informática: me había visto explicar aquello, había visto que «no hablaba raro» y que no debía de hacerlo del todo mal, y pensó que tal vez aquello podría gustarme.

La toma de la decisión fue larga: por un lado, tras haberme negado a hacer el CAP (Curso de Adaptación Pedagógica) al salir de mi carrera, opción que habían escogido un gran porcentaje de mis compañeros, la idea de dar clases me atraía más bien poco. Con un MBA recién terminado y con buena nota en el año 1990, el mercado de trabajo era sumamente atractivo. Sin embargo, la idea de quedarme en el sitio donde había pasado el último año de mi vida, unida a la oferta económica y a la idea de trabajar con un José Mario que por alguna razón parecía confiar en mí más que yo mismo, terminaron por convencerme. En pocas semanas había hecho las correspondientes pruebas y me encontraba listo para empezar a dar clases a personas que, en muchos casos, eran mayores y con más experiencia que yo.

Bastaron unas pocas clases para que me diese cuenta de que aquello no tenía nada que ver con la idea tradicional de una escuela. Alumnos motivadísimos, buenos medios, ideas ambiciosas... Comparado con el concepto que se suele asociar a la idea de dar clase, hacerlo en el Instituto de Empresa era como hospedarse en el Ritz. Al llegar las primeras valoraciones de los alumnos, quedó bastante claro que había encontrado algo —o mejor, que José Mario me había dirigido hacia algo— que me encajaba muy bien. En aquellas aulas, con José Mario y con mi amigo Julián de Cabo, que entró prácticamente al mismo tiempo que yo, pudimos experimentar la evolución de los distintos programas: Lotus, WordPerfect, Harvard Graphics y Framework dieron paso a Windows y Office mientras dábamos innumerables horas de clase que intentaban enseñar a generaciones de directivos cómo sacar partido de aquellas herramientas. Miles de anécdotas, la llegada de los primeros virus, la evolución cada vez más rápida... En pocos años, hacia 1993 o 1994, presenciamos en primera fila la llegada de internet: vivimos la época de las BBS, empezamos con cuentas individuales de CompuServe, pero en seguida empezamos a llevarnos el ordenador ya no a un aula de informática, sino a una clase normal, a tratar de que los alumnos viesen las posibilidades de aquella incipiente red. Finalmente, tras seis años de clases y de intensa colaboración con muchos profesores en proyectos de consultoría de todo tipo, decidí aceptar la oferta del Instituto de Empresa para completar mi formación con un doctorado en Sistemas de Información. Tras analizar varias universidades norteamericanas, me decidí por UCLA. En Los Angeles viví desde 1996 hasta 2000, cuatro años de intensa actividad, cuatro años ideales para pasarlos en una California en la que estaba ocurriendo prácticamente de todo: nuevas empresas, nuevos modelos de negocio, nuevas formas de hacer las cosas... Cuando, en el año 2000, volví a España con mi familia, la sensación fue la de retornar a un mundo distinto: España, en esos cuatro años, también había cambiado muchísimo: el horizonte tecnológico conformaba un escenario efervescente, había muchísimas cosas que contar, y un académico con un título norteamericano, con aspecto de neutralidad y de ausencia de intereses comerciales parecía ideal para hacerlo. Mis clases también habían cambiado: ya no daba clase en aulas de informática, y en su lugar me dedicaba a explicar a los alumnos temas como sistemas de información, CRM o, en general, los efectos de la tecnología sobre las personas, las empresas y la sociedad. De la noche a la mañana, bien dirigido desde el Instituto de Empresa, me encontré colaborando con varios diarios económicos y generalistas: mis primeras secciones en Expansión, las llamadas de periódicos como El Mundo o El País para hablar de temas relacionados con 1a. tecnología. La prensa daba una cierta sensación de vértigo, pero me gustaba: trabajar con periodistas no era muy distinto a trabajar con alumnos, se trataba de ser didáctico y explicar conceptos aparentemente complejos de manera que tanto ellos como sus lectores pudiesen entenderlos.

En el año 2003, me encontré con una nueva herramienta: el blog. Aunque llevaba tiempo leyendo blogs de otras personas, fue la noticia de que Google adquiría Pyra Labs, una pequeña startup creadora de un servicio llamado Blogger, el 21 de febrero de 2003, la que me hizo decidirme: el comentario de esa noticia fue, de hecho, la primera entrada en mi blog después de saludar. Cuando a principios de 2003, deslumbrado por la facilidad y versatilidad del formato blog, escribí en Expansión una de las primeras columnas que mencionaban en un diario español la palabra blog y terminaba con la frase «en el futuro, seguramente, habrá un blog en su vida» parece que no andaba del todo desencaminado: hoy, los blogs forman parte de la forma de comunicar de millones de personas, empresas y medios de comunicación de todo el mundo.

El blog fue un auténtico descubrimiento: en pocas semanas, pasó a tener varias veces más visitas que la página personal en la que reseñaba mis artículos de prensa, además de darme algo mucho más valioso: retroalimentación inmediata en forma de comentarios y citas en otras páginas. Tras un primer año irregular, en febrero de 2001 empecé a escribir diariamente, en ocasiones varias veces al día. En realidad, el blog encajaba perfectamente en mi metodología de trabajo de profesor: me obligaba a leer noticias todos los días para mantenerme al día, y el esfuerzo extra de escoger alguna de ellas y comentarla, acostumbrado a escribir para las habitualmente cortas fechas límite de la prensa, era muy escaso. Además, el blog me permitía enriquecer mi opinión con las de otras personas en muchos casos muy bien informadas, que aportaban puntos de vista sumamente interesantes: en muchas de mis entradas; los comentarios eran indudablemente mejores que lo que yo puse originalmente encima de la mesa para la discusión. Era como lo que me ocurría en mis clases con las intervenciones de mis alumnos, pero, además, quedaba escrito. Y finalmente, podía utilizarlo como repositorio para mi propio trabajo (artículos, ideas a medio desarrollar, etc.), una función fundamental para una persona desordenada como yo: pronto, me convertí en el usuario más frecuente de la caja de búsquedas de mi blog.

El blog, en muchos sentidos, se convirtió en el centro de mi ecosistema informativo: a él recurro para preparar charlas y clases, para consultar mis publicaciones en medios, para revisar mis fuentes... Es un complemento perfecto para mi trabajo como profesor. De hecho, me ha servido también para introducirlo como parle de mi metodología docente: muchos de mis alumnos optan por un blog para entregarme sus trabajos y análisis de clase, para sus reflexiones personales, o para crear un perfil profesional de cara al mercado de trabajo. En muchos sentidos, el blog ha representado la maravillosa posibilidad de pasar a mantener una interlocución directa con muchos miles de personas, con una calidad muy superior que la que tenía cuando únicamente publicaba en medios de comunicación unidireccionales. Antes de 2003, una de mis columnas o artículos en prensa podía ser leída por mucha gente, pero mis lectores eran eso, «gente»: una masa indiferenciada de personas indistinguibles, de las cuales únicamente emergía alguna de vez en cuando si, al cruzarme con alguien por un pasillo, me decía eso de «te he leído, y me ha parecido muy interesante» (o «te he leído, y no me ha gustado nada», que no todo iban a ser parabienes). Gracias al blog, una parte importante de esa hasta entonces masa indiferenciada de gente tomó carta de realidad, se convirtieron en personas con las que interaccionar, discutir, comentar; en personas que enriquecían en muchas ocasiones lo que yo había comentado aportando datos o análisis originales y diferentes, demostrando una de las grandes verdades de los medios sociales: siempre hay alguien ahí fuera que sabe más que tú de cualquier cosa. Nunca en mi blog he pretendido ser «el que más sabía de un tema», y creo que el no hacerlo me ha funcionado muy bien: el blog es un libro vivo y en movimiento de cuya marcha estoy enormemente orgulloso.

En muchos casos, las ideas que encontrará en este libro se han beneficiado también de cientos de discusiones en mis clases en IE Business School, o en las conferencias que habitualmente imparto sobre estos temas. La idea de escribir un libro así me rondaba la cabeza desde hacía mucho tiempo: para que se materializase, ha sido clave el impulso de Roger Domingo, un editor activo e implicado hasta el límite, con el que ha sido un verdadero placer trabajar. El libro que tiene entre manos es el intento de llevar a papel muchas de las reflexiones habituales en ese blog, tras enriquecerlas con comentarios. No ha sido tarea sencilla: las entradas en un blog tienen mucho de coyuntural, de secuencia, de aplicación a noticias del día o a reflexiones momentáneas motivadas por cuestiones diversas. Exprimir y estructurar esos contenidos, destilarlos de kilómetros de entradas y horas de charlas o conferencias tiene, para alguien sin experiencia en este tipo de cuestiones, una cierta dificultad. El blog me permite lanzar mis contenidos sin necesidad de estructurarlos, el libro obliga a hacerlo desde un primer momento, a escribir de otra manera, a sintetizar, a sujetarse a un esquema. Al lector corresponderá juzgar si el esfuerzo ha valido la pena. Si le gusta lo que lee, ya sabe: en el blog hay mucho, mucho más, y además podrá participar, que para eso es bidireccional.

En este libro encontrará algo sumamente parecido a lo que intento hacer todos los días en mi blog o en mis clases: poner encima de la mesa elementos de reflexión, para que sirvan de base para la discusión, ese food for thought o «alimento para el pensamiento» que dicen los norteamericanos. En ningún caso piense que intento «venderle» nada: no vivo de vender tecnologías, ni tengo interés en dedicarme a ello. En la mayor parte de los casos, expondré hechos de manera rigurosa, sin definir si me parecen buenos o malos, sin juicios de valor, situaciones que, independientemente de lo que podamos pensar sobre ellas, son ya por sí mismas una realidad. Mi «contrato» con usted, lector, es el de intentar aportar ideas e interpretaciones para estructurar el sentido común sobre un campo de actuación nuevo, un campo en el que, se ponga como se ponga, le tocará moverse. Y un campo en el que, además, ha expresado usted mismo su interés mediante la adquisición de este libro. Veremos cómo se nos da.

Me gustaría terminar esta introducción con algunos muy necesarios agradecimientos:

Nada de lo que hago sería posible sin Susana, mi mujer y mi cómplice en absolutamente todo desde hace más de veinticinco años. Ni sin Claudia, mi hija, de la que aprendo cosas nuevas todos los días. Si no estuviesen ahí y compartiesen muchas de mis pasiones, todo esto sería infinitamente más aburrido.

Mis padres, que me enseñaron fundamentalmente que educar era de esas «tareas 2.0» que no funcionan de manera unidireccional, sino que son, o deben ser, un constante intercambio. Y mis amigos «de primera necesidad» como Juan Freire, Kiko Rial, Wicho, Julián de Cabo, Julio Alonso, Olga Palombi, José Holguín, Beatriz Blanco, Carlos Carretero, Julio Rodríguez, Añil de Mello...

Soy de los que piensan que lo mejor que le puede ocurrir a una persona es tener un trabajo que de verdad disfrute haciendo. Llegué a él gracias a la confianza de una de las personas a las que más he admirado en mi vida y a la que más echo de menos desde que se fue: José Mario Álvarez de Novales. El próximo año hará veinte que disfruto intensamente de lo que hago, los veinte vinculado a la misma empresa. Una empresa que no dudó en apoyarme cuando me fui a hacer el doctorado a California durante cuatro años o cuando decidí tomarme unos meses para escribir este libro, o cuando, en numerosas ocasiones, personas que están en desacuerdo con algunas de las ideas que manifiesto en el blog creen que la mejor manera de callarme es dirigirse a mi empresa para intentar que me echen de ella. En todos esos casos, IE Business School ha estado ahí con su apoyo, y personas como Diego del Alcázar, Santiago Iñiguez, Julián de Cabo y muchos más que no podría nombrar han sido capaces de entender que la labor y el compromiso de un profesor con las cosas que enseña no deben terminar cuando este sale del aula. Otros muchos compañeros de trabajo, como Víctor Torre de Silva, David Alien, Rafael Pampillón, Javier Vega o Gildo Seisdedos me han servido de inspiración, apoyo o fuente de conocimientos, y han conseguido que me sintiese mucho más seguro al tocar determinados temas de los que sabían mucho, muchísimo más que yo. O han soportado mis constantes olvidos, despistes y desastres cotidianos con paciencia y cariño infinitos, como tantas personas que podría citar del personal administrativo de IE Business School: Julia Ortega, Aielí Castrejón, Begoña Sanz, Pilar Urbón, Angelines Armenteros, Elia Gil, Pilar Zamora, Elena Díaz, Dani Rivas, Leticia Fuentes, entre otras.

A Fernando Serer y a todo su equipo en Blogestudio, a Luis Rull por estar siempre en todo y ser enormemente constructivo, a Burt Swanson por despertar en mí la pasión por la investigación y, por supuesto, a muchas personas a las que leo todos los días en la red, a quienes ya en casi todos los casos conozco y aprecio, y que me aportan tantas opiniones e ideas nuevas: Eduardo Arcos, J. J. Merelo, Bernardo Hernández, Mariano Amartino, José Luis Orihuela, Antonio Ortiz, Fernando Tricas, Genis Roca, Jesús Encinar, Manuel M. Almeida, los Microsiervos, Diego Martín Lafuente, Antoni Gutiérrez-Rubí, Pedro Jorge, Pepe Cervera, Ricardo Galli, Rodolfo Carpintier, y tantas otras personas a las que no leo todos los días, pero en las que en tantas ocasiones descubro entradas interesantes.

Y, por supuesto, a mis alumnos, a mis lectores y comentaristas en el blog, a los periodistas que me citan y me dejan espacio en sus medios, y a tantas personas de las que aprendo cosas todos los días. Este libro y mi trabajo de todos los días se apoyan en ellos, y serían completamente imposibles si no estuviesen ahí. En muchas, muchísimas de sus opiniones me he basado para construir argumentos y puntos de vista que reflejo en este libro. Los errores, por supuesto, son todos míos, y para discutirlos, va sabe dónde estoy: soy muy fácil de encontrar.

Nos leemos.

ENRIQUE DANS

CAPÍTULO 1. MÚSICA, PELÍCULAS, MENTIRAS E INTERNET

«Tratando a Napster como al anticristo del copyright, la industria asegura que el vector del desarrollo tecnológico en Internet creará rápidamente una herramienta distribuida, gratis y a prueba de juicios —exactamente lo que los dueños de la propiedad intelectual quieren evitar. ¿Cómo de estúpido puedes llegar a ser?»

SCOTT ROSENBERG

en Salon.com, 2000

A poco diestro que sea en el uso de Internet, seguro que alguna vez se ha bajado alguna canción de una red P2P.{1} De hecho, el uso de la red para descargarse música o películas es uno de los hábitos más generalizados, aceptados y extendidos entre la población, y sin duda una de las maneras más convenientes y sencillas de obtener rápidamente un contenido. Mediante la instalación de un programa completamente gratuito y sencillo, o simplemente mediante una búsqueda, cualquier usuario normal de la red puede localizar un contenido y descargarlo con total facilidad. Sin embargo, existen todavía personas que, a pesar de bajarse habitualmente y con total normalidad contenidos de la red (ellos o sus hijos), se niegan a reconocerlo en público, así como otras que directamente criminalizan uno de los comportamientos más habituales en la sociedad de hoy en día. En algunos países, como Francia o el Reino Unido, se cuestionan pilares tan básicos de la democracia como la privacidad de las comunicaciones, y se intenta obligar a compañías privadas a espiar a sus usuarios con el fin de descubrir si se bajan obras sujetas a derechos de autor. Decididamente, la interacción entre los derechos de autor y la red es una de las más violentas y contradictorias de la historia actual: la confrontación entre compañías que explotan dichos derechos y la sociedad en su conjunto es cada día más insostenible, y sin ningún lugar a dudas, vamos a vivir muchos cambios en este sentido. Pero estudiemos el tema desde su origen para intentar, de alguna manera, esclarecer los hechos.

La historia de la industria de la música se encuentra íntimamente vinculada a la de la tecnología: la música se originó en la prehistoria, en forma de canciones acompañadas rítmicamente por palmas e instrumentos de percusión. En sus orígenes, la música era un evento, algo que sucedía vinculado a un momento y lugar determinado, pero que no resultaba recogido en ningún sitio, ni codificado en modo alguno para su repetición. Sí existía la posibilidad de repetir, de memoria, sucesiones de sonidos, siempre que las condiciones y la disponibilidad de instrumentos lo permitiesen, pero cada interpretación era única: si estabas presente en ese mismo momento y lugar, podías disfrutarla. Si no, tenías que conformarte con el recuerdo que otras personas que sí estaban allí presentes podían compartir contigo. La repetición de la música estaba condicionada al recuerdo, y era sumamente frágil: la desaparición de una persona podía conllevar la pérdida de muchas obras musicales. Con la tecnología de la escritura, la música pasó a tener la posibilidad de trascender el año tiempo y el espacio: el registro más primitivo de música codificada que se conserva corresponde al año 800 a.C., y es un himno sumerio en escritura cuneiforme que quedó conservado al cocerse de forma accidental; unos invasores incendiaron el templo en el que la tabla de arcilla estaba almacenada. Hacia el año 700 a.C., aparecen en Grecia los primeros rapsodas, músicos itinerantes que vivían de interpretar música en diferentes lugares: posiblemente el primer registro histórico de la música como negocio. Un rapsoda podía, si así lo deseaba, acudir a los Juegos Pitios, precursores de las Olimpiadas, y si ganaba, recibía una corona de laurel e incrementaba su prestigio, con lo cual era llamado a tocar música en más lugares y podía incrementar su caché.

El primer registro de un artista famoso corresponde a Píndaro, que vivió entre el 522 y el 443 a.C. Sus odas componen un enorme repertorio de diecisiete libros, entre los que se encuentran himnos, lamentos, música de victoria, teatro y hasta música para bailar. Las odas de Píndaro eran pagadas por clientes que deseaban utilizarlas para motivos diversos, y su casa era visitada por sacerdotes, personajes de todo tipo y hasta reyes como Alejandro Magno. A partir de Píndaro, la música se convirtió en un fenómeno de difusión cada vez mayor, y vivir de ella suponía diferentes modelos de negocio que nos comienzan a resultar familiares: podías componer, y recibir, como Píndaro, un pago por tus obras, que podían interpretar otros. Podías interpretar, viajando de un lugar a otro, y recibir un pago por tu interpretación, contribuyendo además a la difusión de las obras. Y podías enseñar el arte de tocar instrumentos a otros que deseaban aprenderlo y te pagaban por tus lecciones, como quien enseña cualquier otra materia.

En 1506, medio siglo después de la popularización de una nueva tecnología, la imprenta de Johannes Gutenberg, aparecen registros históricos de ejemplares impresos de A Lytel Geste of Robyne Hood, el primero de los llamados broadsides o broadsheets, hojas impresas con la letra e indicaciones de la música de una canción. En 1520, un comerciante inglés vendió la fastuosa cantidad de 190 hojas de una obra, dando lugar a otro modelo de negocio: la venta de copias de partituras de música. Finalmente, en 1566, se promulgó la obligación, para cada impresor que desease hacer una tirada, de registrarse en la Stationers Company de Londres y, a partir de 1567, pagar cuatro peniques por canción. Esto se considera el origen del modelo que hoy conocemos como copyright. Este modelo, basado en la necesidad de controlar las obras impresas, estuvo vigente hasta 1709, cuando la fuerte presión social que pedía libertad de prensa se hizo efectiva. Curiosamente, el modelo de negocio en la época dorada de la Stationers Company era diferente al actual, sobre todo en cuanto a la localización del poder y el reparto del margen: desarrollado inicialmente para los libros, pero aplicado también a la música, el modelo consistía en que el autor vendía todos los derechos de su obra a cambio de un precio fijo a un impresor, el cual retenía el derecho perpetuo de explotación de la obra, incluso si esta resultaba ser enormemente popular. Las obras, de hecho, se registraban en Stationers Company con el nombre del editor, no con el del autor.

En 1877, llega otra nueva tecnología: Thomas Alva Edison fabrica el fonógrafo, la primera máquina capaz de grabar sonido. Edison, conocido como «el genio de Menlo Park», está considerado el rey de la patente. Su habilidad a la hora de industrializar la innovación fue paralela al desarrollo de su pericia para el registro de patentes, incluso de inventos, como la bombilla, que simplemente no eran suyos. Su creación, el fonógrafo, provocó la traslación del modelo de copyright de las hojas de música a los soportes de audición (cilindros primero, y discos después). El mantenimiento del modelo de copyright evolucionó en lo que conocemos actualmente, con alguna leve corrección: las compañías discográficas se encargaban de conseguir artistas de talento para llevar a cabo la producción, fabricación, comercialización y distribución de las grabaciones, y el artista obtenía un 15% del precio en mayorista de cada copia en concepto de derechos de autor. Este modelo, ya familiar para nosotros, se ha llegado a complicar bastante con variaciones como los sistemas de costes recuperables, en los que un artista puede, por ejemplo, pactar el no ganar nada hasta que los ingresos de la discográfica le han permitido recuperar una gran parte de las inversiones iniciales en producción y marketing, y múltiples modelos afines sujetos a negociación.

Con el tiempo y la sucesivas generaciones de tecnología, las compañías discográficas llegaron a tener un control verdaderamente férreo de todo el proceso: al descubrir los principios del marketing masivo y su capacidad de manipulación de los gustos del público por mecanismos como la repetición, las discográficas podían seleccionar el talento en virtud de criterios puramente comerciales, y hacerlo llegar de manera masiva al público a través de los medios de comunicación, En general, más inversión en el canal conllevaba una mayor popularización, lo que en muchos casos generaba un hit o superventas.

La venta de copias pasó, más allá de la calidad, a representar la métrica del éxito. Pero la actividad de creación artística es distinta del mecanismo económico implicado en su sostenimiento o viabilidad. Johann Sebastian Bach era indudablemente un artista dotado de enorme capacidad creativa. El Duque Juan Ernesto III de Sajonia-Weimar (1664-1707) no lo era, pero jugó un papel fundamental en que Bach pudiera llegar a serlo. Mientras lo primero depende de factores como la capacidad de crear, la inspiración y la sensibilidad artística, lo segundo depende del nivel de recursos ociosos disponibles, del modelo de negocio o de la apreciación del público. El arte lo crea el artista, no los que desarrollan el modelo de negocio para incentivar su creación. Lícito es que el artista quiera obtener lo más posible a cambio de su creación, pero el proceso por el cual lo haga no es arte, sino simple explotación de un modelo de negocio, sujeto como todos a las condiciones de viabilidad del libre mercado.

La idea de copyright, o «derecho a hacer copias», emerge en 1710 en el Reino Unido, con el llamado «Estatuto de la Reina Ana», que protegía a los autores contra aquellos impresores que llevaban a cabo tiradas de las obras de estos autores sin su consentimiento. La idea era, precisamente, consagrar el derecho de un autor a percibir una compensación cada vez que alguien generaba un ingreso mediante una obra de él. En 1886, el copyright obtuvo reconocimiento y validez internacional al firmarse el Convenio de Berna, cuya última revisión data de 1989.

La clara separación entre la creación artística y la comercialización de la obra creada resulta fundamental a la hora de entender la evolución subsiguiente. Hace cierto tiempo, la tecnología inventó métodos para que la creación de un artista dejase de ser un acontecimiento único ligado al momento de su interpretación. Se inventó la capacidad de capturar el tiempo, de «meter al genio en la botella», para liberar la creación cada vez que se quisiera disfrutar de ella. Por supuesto, esto impactó el modelo de negocio. Pero los artistas siguieron siendo los artistas: los que grababan o comercializaban no se convirtieron en artistas. Esos son meramente empresarios que explotan un modelo de negocio.

Con el tiempo, los empresarios descubrieron, además, que el modelo era perfectamente manipulable: que podían seleccionar a los artistas no por la calidad de su obra, sino por las posibilidades que les otorgaban de enriquecerse con la venta de copias. Desarrollaron todo un sistema para proporcionar oportunidades a aquel artista que maximizase el número de copias vendidas, convenciendo a los clientes de que la calidad de la creación dependía del número de personas dispuestas a pagar por ella. Dejaron fuera del sistema a miles de artistas, interesados solo en los que, a cambio de unos costes de producción bajos (series largas de productos uniformes), les permitiesen llegar a un mercado lo más amplio posible. Inventaron modas y fans, mediatizaron la voluntad popular gracias a mecanismos como la publicidad y la payóla,{2} completamente ajenos a la creación artística. Un proceso industrial destinado no a la optimización de la creación artística, sino a la maximización del modelo de negocio ligado a la misma. En virtud de ese interés, la industria fue apoyándose en la tecnología para desarrollar soportes cada vez más baratos en su producción, pero por los que pedían al público que pagase cantidades cada vez mayores. En su apogeo, se calcula que esa industria daba acceso únicamente al 3% del total de la creación artística de la Historia: el 97% restante, simplemente, no les interesaba. No es que no fuese arte, sino que no les proporcionaba el margen que ellos querían. Según las métricas de la industria, no era un hit, sino un flop, y no debía, por tanto, ser producido, de acuerdo con la estructura de costes y comercialización disponibles.

Pero fue el mismo progreso tecnológico que les trajo hasta aquí el que determinó el fin de esa época. Llegar hasta el culmen de su desarrollo, evolucionar hasta convertirse en una industria enormemente popular con muchísimos millones de clientes fue algo que llevó a la industria de la música varios cientos de años, según donde estimemos su inicio. Pero, de repente, por alguna razón, las cosas se aceleraron: en junio de 1999, Shawn Fanning, un joven estudiante de primer año de la Northeastern University, crea Napster, y se convierte en uno de los fenómenos tecnológicos de más rápida difusión de la historia. En febrero de 2001, en menos de dos años, alcanzaba ya a más de veintiséis millones de usuarios verificados en todo el mundo, y se calcula que llegó a tener unos noventa millones. Las múltiples batallas legales emprendidas por la industria de la música, a pesar de conseguir el cierre de Napster en 2002 y de llevar a juicio a más de treinta mil usuarios por descargar música de Internet, nunca llegaron a conseguir que el fenómeno de las descargas disminuyese su cadencia. Hoy, prácticamente toda la música está disponible en redes P2P, y los usuarios son perfectamente libres para descargarla de acuerdo con las leyes que rigen en España. El valor de selección imperfecta aportado por la industria desaparece, al establecerse espacios en los que cualquier artista puede autoproducirse con un nivel de calidad razonable, y llegar directamente a su público mediante la distribución digital de sus obras. El valor aportado por el proceso de fabricación de soportes plásticos para la distribución de las obras desaparece también, de manera que las cifras de venta de discos caen de manera alarmante: la música se ha liberado de su soporte físico, y ahora circula libremente en la red mediante esquemas de todo tipo, desde modelos de pago hasta P2P, pasando por streaming o simples búsquedas que conducen a descargas directas.

De repente, la posibilidad de hacer una copia de una obra, que era precisamente lo que se suponía protegía el copyright, era algo al alcance de cualquiera. Tan al alcance de cualquiera, que resultaba de imposible protección. Copiar en Internet, era tan natural como respirar: cada vez que un usuario hace clic sobre un vínculo, genera una copia. Copias son las páginas que leemos, las fotografías que vemos y absolutamente cualquier cosa que nuestro navegador descarga. Todo en Internet es copia de un original almacenado en un servidor. Por tanto, proteger el derecho del autor de percibir una remuneración por cada copia, cuando generalmente esas copias realizadas no generan ningún tipo de transacción económica, pasa a resultar completamente absurdo y sin sentido. E intentar perseguir a quienes copian lo es, obviamente, mucho más. La gama de esquemas mediante la que un usuario puede obtener una canción u obra determinada es creciente, y la práctica se ha extendido de tal manera que su aceptación social es absoluta. Las reacciones de la industria de la música, que ha traducido su poder económico en una fortísima capacidad de manipulación de la política y las leyes, no han conseguido en ningún momento que el número de usuarios y el volumen de lo descargado se reduzca, y ha llevado, además, a que toda una generación de jóvenes crezca pensando que las leyes son una soberana estupidez que merece ser ignorada y que las discográficas o las entidades de gestión de derechos de autor son poco menos que la personificación de la peste negra.

Al día de hoy, la música es un producto de facto completamente gratuito, que se descarga en cualquier momento y se intercambia con absoluta normalidad mediante P2P, Bluetooth, tarjetas de memoria, mensajería instantánea, mensajes de correo electrónico, páginas de descarga directa, etc. Una enorme gama de opciones que, además, se incrementan con el tiempo y el avance de las tecnologías, dando forma a un agujero en el modelo de negocio tradicional que resulta completamente imposible taponar. La evidencia de que la música, obviamente, es un producto que cuesta esfuerzo y dinero crear y producir, solo debe llevarnos a la búsqueda de modelos de negocio válidos que lo permitan, modelos que deberán competir con la disponibilidad ubicua de métodos gratuitos. El reto, por supuesto, no es sencillo, y mucho menos cuando choca con una industria empecinada en seguir obteniendo los mismos rendimientos que obtenía cuando Internet no existía y su aporte de valor como únicos productores de copias era elevado. El panorama actual de la industria de la música es calificable, por tanto, como de extremadamente confuso. Con una industria y unas sociedades de gestión de derechos anclados en un marco legal obsoleto y sin sentido, parece sumamente difícil que se llegue a una solución que pueda satisfacer a todas las partes implicadas. Sin embargo, existen opiniones que creen en una ralentización progresiva del uso de plataformas de descarga P2P: aunque el tráfico provocado por su uso aún sigue creciendo, todo parece indicar que se debe a la incorporación de nuevos usuarios, mientras que los habituales parecen tender a una saturación que deriva en un uso cada vez más ocasional. Por otro lado, la disponibilidad de un ancho de banda cada vez mayor y dotado de una ubicuidad progresiva parece llevarnos hacia un escenario en el que los sistemas basados en streaming crecen con tasas muy superiores a las que disfrutan aquellos basados en la descarga de redes P2P. Los sistemas basados en streaming se distinguen por dar cabida de manera mucho más sencilla a modelos basados en el pago por reproducción, a menudo mediante esquemas que no requieren un pago al usuario. Por otro lado, el hecho de que gran parte del consumo se redirija hacia formatos de streaming no conlleva necesariamente que la industria de la música llegue a sobrevivir: por el momento, la industria se ha caracterizado por dificultar las iniciativas de streaming intentando aplicarles los mismos esquemas económicos del modelo anterior, no por mantener una actitud constructiva hacia ellas ni por facilitarlas.{3}

Mención especial merece el fenómeno del DRM, Digital Rights Management o Gestión Digital de Derechos, un término genérico utilizado para describir una serie de tecnologías de control de acceso desarrolladas por fabricantes de hardware, software y creadores de contenidos, con el fin de intentar imponer restricciones (sus detractores lo denominan Digital Restrictions Management, o Gestión Digital de Restricciones) en el uso de determinados dispositivos y contenidos. Durante muchos años, el DRM se consideró «la gran esperanza» para una industria que invirtió miles de millones de dólares en su desarrollo e implantación: mecanismos para impedir la copia, números de serie, servidores de autenticación, etc., que, en la práctica, han sido siempre convenientemente anulados en cuanto se ha planteado suficiente interés por hacerlo.

La historia del DRM refleja una de las verdades más evidentes dentro de Internet: los bits son libres, su circulación no puede ser restringida, únicamente dificultada en circunstancias puntuales. En realidad, el comportamiento de Internet se asemeja al de un cuerpo orgánico, que reacciona a las agresiones externas mediante mecanismos de aislamiento y bloqueo, de enquistamiento, con el fin de mantener su funcionamiento: una restricción es interpretada por la red como una agresión, y termina invariablemente siendo «arreglada» o «cicatrizada», aunque para quienes la desarrollaron parezca precisamente lo contrario. Programadores geniales como el noruego Jon Lech Johansen, el mítico DVD Jon, se han hecho mundialmente famosos y han visto sus hazañas llevadas al cine debido a su habilidad para la ingeniería inversa y la anulación de estos sistemas de protección.

En la lucha por intentar desarrollar un DRM verdaderamente funcional se han vivido todo tipo de batallas: desde inversiones millonarias que resultaban inútiles cuando simplemente se pintaba con un rotulador indeleble sobre una parte determinada del disco, hasta violaciones de la intimidad de los usuarios con discos que instalaban programas de forma oculta en los ordenadores en los que eran reproducidos y «llamaban a su casa», a la discográfica de turno, cuando su propietario los reproducía. Como norma general, es preciso entender que el DRM supone la restricción de los derechos de un usuario a la hora de utilizar un producto que ha adquirido de manera legítima, y que, por lo tanto, crea un incentivo precisamente en el sentido opuesto al esperado: aquellos que obtienen el producto de manera irregular y con su DRM convenientemente violado, pasan a tener más derechos que quienes lo obtuvieron por los canales que la empresa consideraba adecuados. Tras muchos años de dolorosos fallos, la industria comenzó, a partir de 2008, a alejarse del DRM como apuesta estratégica fundamental. Actualmente, los esquemas de DRM únicamente se mantienen como apuesta en el mundo de la distribución de música a plataformas móviles, debido solo a que, al no estar éstas suficientemente homogeneizadas, no existe todavía suficiente escala como para que los hackers se planteen el objetivo u obtengan suficiente fama por el hecho de habérselas saltado.

Todos los elementos analizados parecen indicar una necesaria revisión de la cadena de valor de la industria: el papel de selección de talento ya no parece necesario en un mundo en el que todo artista, por el hecho de autocalificarse como tal, puede poner su obra frente a los ojos y oídos del público en la red. La producción del soporte físico y su distribución pierden asimismo su valor de manera acelerada, lo que lleva a las discográficas a convertirse en una especie de agencias de servicios que gestionan la carrera de los artistas de su cartera con criterios de marketing. Parece lógico pensar que la relativa disminución del papel de estas empresas vaya a determinar un consecuente redimensionamiento de las mismas, aunque seguirán existiendo, a través de la innovación en las técnicas de marketing, formas en las que añadir valor al proceso creativo. El número de posibilidades en manos del artista crecerá para conformarse como un continuo entre la gestión completamente individual e independiente, y la gestión completa en contratos de tipo 360° con una discográfica, con variaciones intermedias en función del nivel de implicación y el poder de negociación de ambas partes.

A medida que se incremente el ancho de banda disponible y la ubicuidad tanto de las conexiones como de la reproducción, los clientes perderán el síndrome de escasez que les lleva a actuar como «coleccionistas» de obras, y procederán a utilizar sistemas basados en consumo inmediato vía streaming. En esas circunstancias, tras consolidarse una «economía de la abundancia», el problema para el usuario no provendrá ya de la obtención de la música, sino de la elección de la misma con un criterio razonable y sencillo: los sistemas de recomendación y descubrimiento tienen, en este sentido, un enorme potencial. Corresponderá a las sociedades de gestión de derechos auditar y controlar la reproducción de las obras cuando esta se produzca con ánimo de lucro, con el fin de estimar el pago correspondiente a derechos en las diferentes plataformas, y abonarlo a los derecho habientes cuando así proceda, conformando un nuevo equilibrio y nuevos esquemas de explotación comercial. Cabe la duda, obviamente, de si los actores implicados en la industria de la música seguirán en ella tras un cataclismo semejante. Sin duda, cuando el fenómeno de la disrupción aparece, lo hace rápido y sin tiempo a que los actores implicados sepan reaccionar: en todo el proceso desde la aparición de Napster hasta el momento en que se edita el presente libro, han transcurrido aproximadamente unos diez años.

¿En dónde estamos, por tanto, a día de hoy? En este momento, puede usted descargarse lo que buenamente quiera y desee en cada momento. No se preocupe ni tenga remordimiento alguno: nadie, por descargarse una canción o una película, va a calificarle a usted de delincuente, porque el delito simplemente no existe como tal en el ordenamiento jurídico español. El delito comenzaría, hipotéticamente, si usted llevase a cabo un aprovechamiento económico de la obra descargada: si la utilizase para reproducirla en un bar (que consecuentemente puede incrementar su clientela y obtener un beneficio económico gracias a dichas obras), para venderla a sus amigos o para, incorporarla a otra obra que de alguna manera, comercializase. Las presiones de los lobbies y grupos de presión de la propiedad intelectual para cambiar el sentido de las leyes, por el momento, no han dado resultado.

Pero más allá de lo que establezca nuestro ordenamiento jurídico, que de hecho podría llegar a cambiar si los poderosísimos grupos de presión de la industria consiguen torcer el brazo de los políticos de turno para que estos legislen a favor de su negocio en lugar de hacerlo a favor de los derechos de los ciudadanos, pensemos en lo que es justo y de sentido común: el que usted descargue una obra de la red gratuitamente no perjudica a su autor, todos los estudios realizados hasta el momento establecen claramente que la difusión que la red proporciona a estas obras redunda, en todos los casos, en un incremento de popularidad que permite a estos autores obtener más dinero por otros conceptos, tales como ingresos por conciertos, merchandising, etc.

En el caso de las películas, el hecho de que haya personas que prefieran verlas en su casa en una pantalla pequeña en lugar de desplazarse al cine, donde pueden disfrutar de una experiencia con un nivel de inmersión mucho mayor nos dice, únicamente, que la inmensa mayoría de estas personas, en cualquier caso, no iban a desplazarse al cine, lo que convierte ese hipotético «lucro cesante» reclamado por las empresas productoras en una de las mayores falacias conocidas. No, no existe equiparación posible entre número de descargas y pérdidas de ingresos. Cada vez que lo escuche, puede directamente echarse a reír.

Tampoco existe correspondencia alguna entre la descarga de un material sujeto a derechos de autor y el robo de un objeto físico, como pretenden consagrar cientos de torticeras campañas en países de todo el mundo del tipo de «no robarías un coche»: es la llamada «falacia de la propiedad exclusiva». Mientras la propiedad de un bien físico, tal como un coche o un jamón, es exclusiva y no puede, por tanto, ser disfrutada de manera igual por dos personas a la vez (si te llevas mi coche o te comes mi jamón, yo invariablemente dejaré de tener mi coche o no podré comerme el mismo jamón), la propiedad de un archivo electrónico no lo es: la persona que hace una copia mantiene el archivo original en su sitio, y lo único que hace es generar una copia idéntica a un coste virtualmente de cero. Pretender aplicar la misma consideración a una cosa y a la otra es, simplemente, una estupidez simplista procedente de mentes o bien muy débiles, o bien extremadamente interesadas.

El problema, por supuesto, surge del propio concepto de copyright, de una legislación de derechos de autor que necesita urgentemente ser revisada, pero cuya revisión choca con los intereses de una de las industrias más poderosas del mundo. Urge una redefinición que consagre que el derecho de autor debe estar condicionado a la obtención de un lucro. Sobre todo, por una razón fundamental: lo contrario no puede ser objeto de protección, porque carece de toda posibilidad de control. Ni siquiera en la ahora reaccionaria Francia, que ha pretendido fiscalizar los intercambios de bits por la red en pleno delirio de grandeza de su presidente, las descargas han mostrado el más mínimo signo de desfallecimiento. Las descargas, le parezca lo que le parezca a discográficas, productoras y políticos, están aquí para quedarse, y únicamente descenderán cuando se idee un método para obtener esos mismos productos de una manera más cómoda y sencilla. Lo cual, además, no quiere decir que deba ser gratuita. Está perfectamente demostrado que se puede competir con lo gratuito: solo es necesario ofrecer un precio razonable y una experiencia de compra superior. La tienda iTunes, de Apple, es a día de hoy el mayor vendedor de música del mundo, a pesar de tener un precio que proviene no del coste de la canción, sino de lo que las discográficas, en su torpe miopía, pretenden seguir obteniendo a pesar de no tener ya que imprimir discos, comercializarlos y distribuirlos. Alternativas como Spotify, que posibilitan que sus usuarios escuchen canciones mediante streaming y crecen rápidamente en popularidad a pesar de que, de nuevo, se ven lastradas por las demandas de ingresos de unas discográficas absurdamente codiciosas que obligan a la imposición de unos precios excesivos y no justificados más que por su interés en mantener sus cuentas de resultados lo más a flote posible.

A pesar de las quejas de las discográficas que afirman erróneamente que «la música morirá», cualquiera puede comprobar, con un simple vistazo, que no es en absoluto así: la música, de hecho, está ahora mejor que nunca, al remover las barreras artificiales que precisamente estas discográficas pretendían imponer. El consenso hoy en día es denominar a la época actual como «la época dorada de la música infinita», lo que convierte los llantos de plañideras del estilo de «la música va a morir» en una solemne estupidez. No muere la música, mueren los que pretenden seguir viviendo de vender copias en un mundo en el que una copia la hace cualquiera. Los datos recogidos entre los años 2004 y 2008 por The Times dejan meridianamente claro que mientras los ingresos de las compañías discográficas descienden de manera notoria y con ritmo constante, los ingresos percibidos por los artistas ascienden fuertemente debido a conceptos como música en directo, merchandising o pago de royalties. Durante el año 2009, los ingresos percibidos por los artistas, que en modo alguno se ven perjudicados por las descargas a través de la red, superarán el total de facturación de las discográficas en concepto de venta de discos. No. las descargas no son el problema: el problema es querer seguir ganando dinero vendiendo productos sin sentido y vinculando el éxito a unas copias en plástico que cada vez menos gente quiere.

Hoy en día disfrutamos de más música, de una variedad infinita, y llega a nosotros de muchísimas más maneras que antes. Obviamente, quienes antes controlaban los escasos canales de distribución e imponían en ellos sus condiciones, no están demasiado contentos, y posiblemente no vuelvan a estarlo hasta que toda una generación de directivos haya pasado a la jubilación y manejen esas compañías —si es que todavía existen— sus hijos y nietos. Pero eso, para usted, es y debe ser completamente irrelevante. Descárguese de la red lo que buenamente quiera, por el método que prefiera, pagando o sin pagar. Si le llaman «pirata», no solo estarán equivocados (los «piratas» son personas que atacan barcos en alta mar y que habitualmente secuestran a sus ocupantes, los desposeen de sus pertenencias o los asesinan, absolutamente nada que ver con el inofensivo hecho de descargar una canción), sino que merecerán generalmente insultos mucho peores. No haga caso de «cuentos de viejas», y disfrute de la mayor herramienta de difusión cultural que la Humanidad ha tenido nunca entre sus manos: la red.

CAPÍTULO 2. LAS EVIDENCIAS DEL CAMBIO

«Supe que todo había terminado cuando me descargué Skype. Cuando los creadores de KaZaA distribuyen gratis un pequeño programa que puedes usar para hablar con cualquier otra persona, la calidad es fantástica, y además es gratis, es el fin. El mundo va a cambiar inevitablemente.»

MICHAEL POWELL,

director de la Federal Communications

Commission (FCC), en Fortune.

Febrero de 2004

Si hay una afirmación que no precisa prueba en los tiempos que vivimos es la de que la velocidad con la que transcurren las cosas se ha incrementado hasta niveles próximos a la histeria. En poquísimo tiempo, hemos podido ver cómo empresas que hace unos pocos años únicamente existían en la imaginación de sus fundadores, se convertían en imperios económico-tecnológicos capaces de marear tendencias y definir escenarios. Hemos visto cómo industrias de toda la vida caían víctimas del avance de nuevas tecnologías completamente imparables, y cómo marcas absolutamente consolidadas desaparecían o se convertían en casi irrelevantes siguiendo ciclos en ocasiones extremadamente cortos. La impresión resulta especialmente llamativa cuando comparamos nuestra vida con la de hace una o dos generaciones: para que una persona de hace no tantos años pudiese llegar a ver cambios de una magnitud tal como la que nosotros vivimos hoy en nuestro día a día, de informativo en informativo, de clic en clic, habría tenido que vivir varias vidas seguidas. Para los nacidos en la primera mitad del siglo pasado o anteriormente, la vida cotidiana era un lugar sometido a una gran estabilidad. Una persona podía mantener un trabajo en una empresa durante toda su vida laboral, y manejar en él prácticamente la misma tecnología, sometida en algunos casos a algunas mejoras incrementales. Mi padre, ingeniero de profesión pero no relacionado con el mundo del automóvil más allá de conducir habitualmente uno, presumía con bastante fundamento de ser capaz de diagnosticar la mayor parte de los problemas mecánicos en función de un somero análisis de prácticamente cualquier vehículo. Conociendo de manera no profesional una sola tecnología, la del motor de explosión, mi padre podía sentirse seguro circulando en su automóvil si este, que como media permanecía en su poder entre diez y quince años, sufría cualquier contratiempo en el medio de un viaje. Sin embargo, en sus últimos automóviles, la cara de mi padre si pretendía inspeccionar el motor era todo un poema: se quedaba mirando a aquel compartimento cerrado, en el que prácticamente lo único que se podía hacer para diagnosticar un problema era conectar el coche a un ordenador y pedir la ejecución de un programa de diagnóstico, algo que estaba únicamente a1 alcance de técnicos especializados y dotados de una tecnología determinada. Hoy en día, revisar un automóvil ya no requiere únicamente conocimientos de mecánica, sino también el manejo de programas especializados y de electrónica, que si no han dejado a toda una generación de mecánicos fuera de juego es porque las marcas han optado por una simplificación de la tecnología que reduce la necesidad de conocimientos específicos al leer la información que ofrece un programa e intercambiar la pieza afectada por una nueva.

En la sociedad tradicional, la que vivieron nuestros padres y abuelos, una generación formaba a la siguiente en el uso de las tecnologías necesarias para el desempeño de sus actividades habituales: aprendías a utilizar el teléfono de la mano de tus padres, mientras que los entresijos del trabajo te los enseñaba un empleado veterano. El incremento de velocidad del progreso tecnológico ha determinado, sin embargo, que el sentido de ese aprendizaje se invierta: si mi padre tiene un problema con su ordenador, me llama a mí. Pero si el problema afecta a su teléfono móvil (o habitualmente, a su comprensión de los menús o elementos de dicho teléfono móvil), ni siquiera: llama a mi hija. El impenitente avance de la tecnología relega a muchos al papel de «generación perdida»: observan los cambios en otras personas, generalmente más jóvenes, los critican como si fuesen la razón de todos los males del universo, y se sienten demasiado alejados de ellos como para atreverse a probarlos.

¿Cuántas de las cosas que hacemos con total naturalidad a día de hoy resultarían absolutamente increíbles si se las contásemos, por ejemplo, a nuestro abuelo o bisabuelo? ¿En cuántos casos éste nos miraría con cara de perplejidad, incluso preocupándose por nuestra salud mental? Mi abuelo no se sentía en absoluto un extraño con respecto a la tecnología: ingeniero especializado en electricidad, con carrera desarrollada en el extranjero, pertenecía sin duda a la parte superior de la distribución de formación de su época. Sin embargo, si pudiese dirigirme a él para explicarle que voy a intentar conocer de primera mano los trabajos de un profesor de una universidad norteamericana, probablemente se levantaría de su sempiterna butaca orejera para ayudarme a hacer la maleta. Si tras convencerle de que no es necesario, me viese sentarme ante una pantalla y acceder en cuestión de segundos a lo que este profesor ha publicado en toda su vida, empezaría a pensar que detrás de aquello se encontraba algo sobrenatural. Y si de repente me encontrase al susodicho profesor conectado a GTalk o a Skype e iniciase una charla con él utilizando vídeo, me temo que mi abuelo exclamaría algo así como «istu é causa de meigas»{4} (mi abuelo era, efectivamente, tan gallego como yo).

Pero no hace falta retrotraerse una o dos generaciones. Si cuando yo tenía la mitad de la edad que tengo ahora hubiese observado a una persona sentada sola en su coche en pleno atasco, con las manos sobre el volante y hablando sin parar, o incluso gesticulando puntualmente, habría pensado con total convencimiento que esa persona manifestaba rasgos de clara peligrosidad social y estaría mejor internada en un manicomio. Hoy, por supuesto, asocio la imagen de manera automática con la telefonía móvil y con el uso de un dispositivo manos libres integrado en su coche. Y si pretendiese explicarle a mi abuelo que en un pequeño dispositivo en mi bolsillo llevo la posibilidad de comunicarme telefónicamente, de recibir correos electrónicos y de acceder a todos los que he escrito y recibido en los últimos cuatro o cinco años, necesitaría armarme de mucha, mucha paciencia y resignarme a ver a mi abuelo, una persona de gran autoridad y presencia intelectual para mí, transformado en una especie de primo cercano de los restos humanos fósiles encontrados en Atapuerca. Y es que como bien reza la tercera ley enunciada por el escritor y científico británico Arthur C. Clarke, «cualquier tecnología suficientemente avanzada es indistinguible de la magia». O, citando al gran programador norteamericano Alan Kay, «tecnología es cualquier cosa que no existía cuando naciste».

Pero la tecnología no es magia, ni aparece de la noche a la mañana, por mucho que en ocasiones nos lo parezca. El desarrollo tecnológico es muchas veces lento y costoso, rodeado de importantes condicionantes. La tecnología no «se desarrolla y ya está», sino que suele ser objeto de mejora continua, de generaciones sucesivas, de aplicaciones nuevas y de usos inesperados. Además, el proceso que sigue al desarrollo tecnológico, el de la difusión y adopción, resulta especialmente fascinante y ha sido objeto de estudio por parte de académicos como Everett Rogers o Frank Bass, que han intentado ajustarlo a modelos matemáticos explicativos o predictivos. Si su trabajo tiene que ver con la innovación o con la puesta en el mercado de productos innovadores, es posible que conozca perfectamente el trabajo de estos dos profesores (o si no es así, debería conocerlo). La difusión de una innovación en la sociedad divide a las personas en función de su velocidad de adopción, y define fenómenos de sustitución que van desde la coexistencia pacífica y progresiva de tecnologías, hasta la llamada disrupción o innovación disruptiva, definida por Clayton Cliristensen como una innovación capaz de crear un mercado nuevo o inesperado mediante el desarrollo de un conjunto diferente de valores, de convertir un hasta entonces pacífico y próspero sector industrial en un ramillete de ejecutivos vociferantes, desesperados y, en muchos casos, patéticamente ridículos, enfrentados con una realidad que nunca tiene vuelta atrás. Como de manera genial enuncia Seth Godin: «los ejecutivos de las compañías no tienen el poder: la competencia y el mercado son como el agua... van donde quieren».

Imaginemos, por ejemplo, una situación real de hace algunos años: una página en eBay, el sitio de subastas más popular de la red, en el que un usuario bajo la dirección camcurtis@aol.com ofrece, por el módico precio de salida de dos millones y medio de dólares, un riñón perfectamente funcional. Tan perfectamente funcional como que, de hecho, lo llevaba puesto: el usuario intentaba obtener un dinero, que supuestamente donaría a la entidad de caridad de su elección, a cambio de su propio riñón, aprovechando el hecho de que, salvo en casos de afecciones renales, un ser humano normal puede sobrevivir con un solo riñón con una pérdida relativamente escasa de calidad de vida. Aunque los detalles se desconocen debido al anonimato del usuario, la imaginación nos lleva a imaginarnos a alguien con problemas de dinero que, tras vender la mayoría de sus pertenencias, decide desprenderse ni más ni menos que de una parte de su cuerpo, de un órgano vital. La pregunta, por supuesto, es de qué manera debería enfrentarse la Justicia a una situación así. Por un momento, evite escandalizarse: no intentamos caer en el amarillismo, sino analizar una situación. De entrada, la primera pregunta es si una situación así es claramente legal o ilegal, algo que antes de recibir respuesta, exigiría plantear otra pregunta: legal o ilegal... ¿dónde? La estructura de la Justicia está intensamente condicionada por la territorialidad, por el ámbito geográfico de aplicación de las leyes, e Internet sobrepasa completamente estas barreras para convertirlas muchas veces en absurdas: en los Estados Unidos, donde presuntamente podríamos suponer que se encuentra nuestro usuario, la donación con contrapartida económica de fluidos corporales (sangre, semen, ¡hasta orina para engañar en los análisis de detección de sustancias estupefacientes que practican muchas empresas!) es legal, pero la de órganos vitales no lo es. Pero, ¿y si nuestro usuario se encontrase en un país en el que la legislación fuese, en este tipo de temas, laxa o directamente inexistente? Parece evidente que la territorialidad, una variable esencial en nuestra forma de entender los negocios, las leyes o muchísimas cuestiones de nuestra vida cotidiana, encuentra verdaderos problemas a la hora de representarse en una red que, más allá de los tópicos, es universal por naturaleza.

Por simplificar, eliminemos el factor territorial. Supongamos que tanto nuestro usuario que desea subastar su riñón como el hipotético comprador del mismo se encuentran en los Estados Unidos, y que el acto de compraventa resulta, por tanto, claramente ilegal. ¿Sobre quién debería recaer la responsabilidad? ¿Sobre el vendedor? ¿Sobre el comprador? ¿Sobre una eBay que sirve de medio para la transacción? ¿Sobre todos ellos? La respuesta a esta pregunta es de todo menos simple: comprador y vendedor incurren en una ilegalidad al intentar comerciar con algo prohibido, pero ¿en qué situación se encuentra eBay? En este caso, eBay no es más que el lugar en el que tiene lugar la transacción, papel que desempeña a cambio de una comisión por listado y otra comisión sobre el importe de la misma. ¿Deberíamos considerar responsable a un mercado en una plaza pública por el hecho de que alguna de sus tiendas de golosinas venda, por ejemplo, drogas además de golosinas a aquellos clientes que se identifiquen de un modo especial? ¿Y al Ayuntamiento que expidió la correspondiente licencia? O peor... ¿es una compañía telefónica responsable por los delitos que sus clientes traman a través de sus líneas? La respuesta, en muchos casos, depende de la capacidad para supervisar las transacciones: el Ayuntamiento debe tener policías que impidan delitos, mientras que la compañía telefónica tiene estipulado que únicamente puede intervenir las comunicaciones de sus usuarios en caso de petición judicial. ¿Cuál es la respuesta de eBay cuando se le demanda que «controle» las transacciones que ocurren en su mercado? Como cabía esperar, la respuesta es que en un sitio como eBay, en el que tienen lugar, aproximadamente, unos quince millones de subastas cada día y cambian de manos 1.900 dólares cada segundo, resulta verdaderamente difícil controlar nada por medios humanos.

Ante la palmaria evidencia de la imposibilidad de controlar fehacientemente un sitio como eBay, ¿recomendaríamos a sus gestores optar por la construcción de filtros de palabras consideradas «peligrosas», opción escogida por algunos jueces que intentaron enfrentarse a problemas similares?{5} La respuesta, mucho me temo, es que no. Que ante tal prohibición, los infractores se dedicarían a utilizar variantes de los términos clasificados como tabú (se han calculado 600.426.974.379.824.381.952 formas alternativas de deletrear la palabra Viagra de manera que pueda ser interpretada por el ojo humano, variaciones que son utilizadas para intentar evitar los filtros antispam), a inventarse códigos o sinónimos, o a idear mecanismos como el denominado pig latín, utilizado en la primera época de los bloqueos de Napster y consistente en desplazar la primera letra de la palabra prohibida a la última posición de la misma (etallicaM en lugar de Metallica, por ejemplo, para identificar las canciones de la conocida banda de rock, e ir aumentando el número de caracteres en rotación a medida que era necesario). Pero las consecuencias —y esto es algo que veremos repetirse en infinidad de ocasiones a lo largo del libro— serían peores aún: el problema ya no es únicamente que los infractores puedan, mediante artimañas de todo tipo, seguir llevando a cabo sus acciones, sino que los intentos de prevención de las mismas provocan que quienes quieren llevar a cabo transacciones legítimas no puedan hacerlo sin sufrir molestos contratiempos: ¿cómo subastaría un estudiante de tercer año de Medicina su tratado de Anatomía del riñón sin mencionar la palabra riñón»?

La tesitura, por tanto, se establece entre desarrollar patéticos intentos de control infructuosos que no detienen el problema y sí, en cambio, molestan a los usuarios legítimos, u optar por otro tipo de aproximaciones. En el caso de eBay, la solución proviene de apalancar el trabajo colectivo: si bien sería imposible y antieconómico crear un sistema de vigilancia dimensionado con el personal suficiente para examinar manualmente todas las transacciones, sí resulta en cambio posible y eficiente poner las herramientas para que cualquier usuario que se cruce con una subasta que levante de algún modo sus alarmas la denuncie mediante un simple clic de su ratón. Un sistema de control descentralizado aprovechando los millones de ojos de los usuarios presenta varias ventajas: por un lado, tiene infinitamente más alcance que uno creado específicamente para ello. Por otro, resulta muchísimo más barato, porque preselecciona las transacciones sospechosas, que son las que finalmente se someten al escrutinio de la compañía. Y por otro, recompensa a los participantes con un sitio de mejor calidad percibida, algo en lo que definitivamente tienen interés por su condición de usuarios. Pero ¿qué es lo que hace que el sistema funcione bien? Paradójicamente, el hecho de que funcione al margen de la ley en paralelo con esta.

Pocas semanas después de la transacción del ya famoso riñón de camcurtis, una pareja de estudiantes con problemas económicos intentó subastar por la misma vía algo que provoca sensaciones todavía más espeluznantes: un feto en pleno proceso de gestación. Tras haberse quedado embarazada, y visto que sus convicciones morales les impedían abortar, habían llegado a la conclusión de que sería mejor entregar a su hijo en adopción al mejor postor, para lo que pasaron a añadir a la página de la subasta vínculos a las últimas ecografías, los análisis de sangre en los que se demostraba que estaban sanos y libres de drogas, etc. Como en el caso del riñón, resulta evidente que el mercado para este tipo de transacciones existe: el número de personas en busca de un donante de riñón es muy elevado, como lo es el de parejas buscando un bebé en adopción. En ambos casos hay fuertes incentivos y excusas para saltarse el canal habitual regulado y recurrir al irregular, pero igualmente en ambos casos, las subastas no llegaron a tener lugar: en cuanto algunos usuarios se encontraron con las páginas y las denunciaron mediante el mecanismo de marcado diseñado para ello, las páginas fueron dadas de baja, no sin antes pasar fugazmente por todos los boletines de noticias y convertirse en conversaciones de café. El sistema, como vemos, funciona mejor cuando se autorregula mediante mecanismos desarrollados por el propio sistema, en lugar de intentar someterlo a directrices completamente externas al mismo.

Pero no solo la ley se encuentra con situaciones y cambios difíciles de manejar. En los últimos años, hemos tenido oportunidad de presenciar la disrupción en una creciente variedad de industrias. Pero sin duda, la palma de los llamados «premios Darwin», otorgados a aquellos que contribuyen a mejorar la especie eliminándose a sí mismos de ella, la obtiene la industria de la música, tal y como hemos podido ver en el capítulo anterior: un auténtico compendio de «historias para no dormir» que hacen las delicias de cientos de profesores y alumnos en cursos de management de todo el mundo. Estudiar el contexto que llevó a la industria de la música a evolucionar de la manera en que lo hizo es, de hecho, una buena manera de entender el tipo de cosas que bajo ningún concepto se deben hacer cuando uno se encuentra cara a cara con el fenómeno de la disrupción.

Pero, a pesar de la terrible experiencia todavía no concluida de la industria de la música y de su lucha contra sus propios clientes y contra el sentido común, aunque sea especialmente interesante debido a lo enfermizamente erróneo de sus reacciones, la industria de la música no ha sido la única en sufrir este tipo de disrupción brusca. En otros casos, las reacciones han sido mucho más mesuradas, pero tampoco han servido para evitar el duro impacto del proceso disruptivo.

Pensemos, por ejemplo, en la industria de los periódicos: en este caso, nos encontramos con una actividad que no es la primera vez que, en sus más de cuatrocientos cincuenta años de existencia, siente en sus carnes el impacto de la disrupción. En realidad, la profesión periodística tiene su origen en los llamados avvisi o fogli a mano, documentos manuscritos nacidos en la floreciente economía de la Venecia del siglo xv. Aunque no eran estrictamente periódicos, los avvisi eran un recurso de altísimo valor utilizado por banqueros y comerciantes para obtener información acerca del cambiante entorno sociopolítico de la época. Era un servicio muy caro y exclusivo que pocos podían pagar y que tenía, además de un valor práctico, uno simbólico, como elemento de estatus que, de hecho, todavía persiste de una manera muy interesante en algunos entornos (estar suscrito a determinados periódicos en algunas ciudades es algo que sirve en muchos casos casi como un elemento de ostentación). Los periodistas, conocidos como menanti, eran personajes influyentes y conocidos en la sociedad de la época. Con la aparición y rápida popularización de la imprenta, la situación cambió: de repente, resultaba muy fácil imprimir un gran número de copias de cualquier documento, de manera que los originales avvisi se convirtieron en las gazetta, cuyo nombre provenía del de una moneda veneciana de escaso valor que era precisamente su precio. El proceso, sin embargo, no fue inmediato ni rápido: las noticias manuscritas y las impresas coexistieron todavía durante buena parte del siglo xvi e incluso más tarde. La invención del telégrafo en 1837 y la del teléfono en 1875 representaron también ejemplos sucesivos de cómo la llegada de innovaciones tecnológicas afectaban de manera importante a la forma de llevar a cabo el periodismo, transformando en muchas ocasiones de manera drástica los factores estratégicos o las barreras de entrada implicadas en el mismo.

La llegada de Internet, sin embargo, a pesar de ser interpretada originalmente como un proceso similar, ha resultado tener un impacto notablemente distinto. En principio, la gran mayoría de los periódicos optaron con mayor o menor velocidad por el desarrollo de una edición online, pero rápidamente cayeron en la cuenta del problema que esto representaba: la idea de tomar unas noticias que tanto costaba producir —sueldos de periodistas, coste de papel y tinta, proceso de impresión, distribución, equipos de ventas, etc. y por las que los clientes pagaban para llevárselas del quiosco o para recibirlas en su casa, y ponerlas en Internet para que pudieran ser consumidas gratis resultaba poco menos que anatema para muchos editores. Sin embargo, la posibilidad de cobrar por ellas demostró muy pronto ser un camino sin retorno: salvo el honroso caso del The Wall Street Journal, con una demanda ampliamente globalizada de usuarios no demasiado sensibles al precio y que otorgaban un alto valor a la mayor velocidad de actualización del medio en la red, todo el resto de los intentos se toparon con la total indiferencia de los usuarios, que al encontrarse el acceso a su cabecera favorita dificultada tras una barrera de pago, optaron simplemente por irse a otra diferente. Pero se toparon, además, con algo mucho peor: al parapetar sus contenidos tras esa misma barrera y dejarlos, por tanto, inaccesibles a las «arañas» indexadoras de los motores de búsqueda, los periódicos desaparecían también de las páginas de resultados que los usuarios recibían tras hacer una búsqueda, con lo que perdían, además, toda su influencia. Casos como el de El País, primer diario en España que no fue capaz de lograr una posición similar en la red precisamente por intentar llevar a cabo una estrategia errónea de convertirse en un medio accesible únicamente por suscripción, han sucedido en muchos otros países, demostrando que la posibilidad de pedir a los usuarios que depositasen el importe correspondiente solo era una estrategia válida para unos muy escasos elegidos. Y a pesar de las experiencias del pasado, los periódicos todavía parecen disponerse, como indica el texto de la llamada «Declaración de Hamburgo» del 26 de junio de 2009, a repetir uno por uno los mismos errores que otros cometieron anteriormente. En este caso, los más de quinientos años de historia del periodismo parece que han tomado el mismo camino de «velocidad absurda hacia la nada» que sufrieron anteriormente las discográficas: en su última iteración, los periódicos y las agencias de prensa parecen decididos a intentar regular quién tiene derecho a hablar y comentar las noticias y quién no lo tiene.

Lo sucedido con los periódicos deja claro que si bien el camino de la negación, el emprendido por las discográficas, no es el adecuado, el de creer que los mismos modelos son válidos antes y después de una innovación disruptiva tampoco lo es. Al encontrarse con Internet, los periódicos empezaron por intentar hacerlo mismo que hacían en el papel: seguir siendo periódicos —entendida tal palabra como «dotado de periodicidad»— y mantener la misma estructura, tanto en lo concerniente a la publicidad como en el formato de interacción con los lectores. Así, los usuarios fuimos obteniendo una prueba detrás de otra de que los periódicos no habían entendido el medio: en la red, conceptos como la periodicidad o el cierre de edición perdían su sentido, la publicidad no se podía vender siguiendo las mismas reglas que en el papel, los lectores no querían únicamente leer noticias, sino también comentarlas, reenviarlas o reutilizarlas como si estuviesen pasando un rato en la máquina del café.

Pero el fenómeno de la disrupción no afecta únicamente a empresas de sectores que parecen oponerse al progreso. Lo hace también con otros que incluso podríamos considerar que sustentan el mismísimo desarrollo de la sociedad de la información. Veamos, por ejemplo, el caso de las telecomunicaciones: una industria que, en principio, debería verse enormemente favorecida por el hecho de que los usuarios incrementen su nivel de consumo, pero que se encuentra, de repente, con un proceso disruptivo que afecta a uno de sus productos más rentables, las comunicaciones de voz. Con la aparición de Internet, aparece la posibilidad de utilizar su protocolo para la transmisión de voz a través de la red. El primer desarrollo comercial de voz sobre IP (VoIP) corresponde a VocalTec en 1995, pero no es en realidad hasta la aparición de Skype en 2003 cuando las llamadas a través de la red alcanzan una popularidad apreciable.

Para las empresas de telecomunicaciones, era la más inesperada de las situaciones: acostumbradas a cobrar, en la mayoría de los países, en función de variables tan conocidas y tangibles como espacio y tiempo —distancia entre los puntos que se conectaban y duración de la llamada— se encontraban de repente ante la paradoja de que cualquier cliente podía, utilizando Skype, conectarse con otro en cualquier lugar del mundo, y hablar durante el tiempo que quisiese sin pagar nada más que el importe de su tarifa plana. De la noche a la mañana, las dos variables que regían la tarificación de las empresas de telecomunicaciones perdían su sentido, ¡y lo hacían además a manos de una empresa que utilizaba precisamente las infraestructuras de las propias empresas de telecomunicaciones! El protocolo desarrollado por la compañía hacía uso de dos tecnologías especialmente interesantes: VoIP, por un lado, y Peer-to-peer (P2P), por el otro. Cuantos más usuarios se conectaban a Skype, más nodos prestaban una fracción de su ancho de banda para el enrutamiento de paquetes, y mejor funcionaba la red en su conjunto.

Las alarmas saltaron relativamente rápido: si las llamadas entre usuarios eran completamente gratuitas y la empresa adquiría el compromiso de que siguieran siéndolo siempre, ¿cuál era el modelo de negocio de Skype? Mientras sus críticos afirmaban que en realidad Skype era una empresa típica de la burbuja tecnológica cuya única posibilidad era la de venderse a otra empresa, sus creadores consiguieron, basándose en unos bajísimos costes de explotación, organizar un interesante modelo de negocio basado casi exclusivamente en los ingresos colaterales: los usuarios rellenaban su cuenta de Skype para utilizar el crédito haciendo llamadas a teléfonos convencionales (SkypeOut) y generaban un interesante flujo de dinero flotante para Skype, contrataban servicios adicionales como el contestador automático, adquirían auriculares y otros productos generando comisiones, o incluso contrataban números de teléfono convencionales en otros países que transferían las llamadas a su cuenta de Skype (Skypeln). Para algunas empresas, como es el caso de muchas PYME, algunos de los productos eran ideales: podían empezar a ofrecer productos en cualquier lugar del mundo, disponiendo siempre de un número de contacto local para las posibles llamadas de sus clientes.

La reacción de las empresas tradicionales de telecomunicaciones ante la llegada de Skype fue bastante más mesurada que en otros casos: en lugar de intentar combatir la tecnología, intentaron simplemente limitar su propuesta de valor. En aquellos países en los que las operadoras no ofrecían todavía tarifas planas para llamadas de voz locales o nacionales, empezaron a ofrecerlas rápidamente. La cuenta era relativamente simple: en caso de no existir tarifa plana, la opción de utilizar Skype resultaba en una muy interesante propuesta de valor para un amplio segmento de la población, lo que habría conllevado una popularización rápida del servicio. Estableciendo la tarifa plana, las compañías telefónicas disminuían en gran medida la inclinación de un cliente a adoptar Skype, relegando el interés a aquellos clientes que tenían un volumen elevado de llamadas internacionales. Sacrificar un cierto volumen de ingresos en llamadas nacionales a cambio de mitigar la adopción masiva de Skype permitía un cierto «control de daños colaterales», que ocurría al tiempo que el ADSL se consolidaba como una de las grandes fuentes de ingresos de la industria.

En conjunto, la erosión de márgenes provocada por la VoIP ha sido, por el momento, mucho más reducida que la que tuvo lugar en otras industrias con tecnologías de similar poder disruptivo: de hecho, la adopción de una tecnología con tanto potencial como la VoIP todavía puede calificarse de testimonial, en gran medida gracias a las «medidas paliativas» adoptadas. Sin embargo, un vistazo al panorama de la industria de las telecomunicaciones antes de la adopción de un servicio como Google Voice y otros similares ofrece un panorama desolador: con los servicios telefónicos digitales de nueva generación, que algunas compañías telefónicas como AT&T intentan detener como si se pudiera detener el avance del mar en una tormenta, los usuarios disponen de posibilidades jamás imaginadas en la telefonía: redirecciones sucesivas en función de múltiples criterios como la hora del día o el número que llama, transcripción de los mensajes de voz a correos electrónicos, manejo de números de distintos sitios como si fueran uno solo, y todo lo que la aplicación de la tecnología digital permite imaginar. Servicios que permiten imaginar, realmente, lo que deberían ser a día de hoy los servicios de telefonía, si no estuviesen gestionados por una serie de fósiles llamados compañías telefónicas, que, además, hacen lo que hacen rematadamente mal: en prácticamente todos los países desarrollados las operadoras telefónicas son las protagonistas de la inmensa mayoría de las reclamaciones de los clientes a las oficinas de consumo.

Otro mercado interesantísimo que ha notado los efectos de la disrupción en varias fases sucesivas es el de las enciclopedias. Un segmento de la industria editorial caracterizado por el dominio de un competidor durante más de doscientos años, Encyclopædia Britannica. La historia tiene su parte de ironía amarga: se inicia en los años ochenta con la visita de Microsoft a este líder de mercado histórico con el fin de solicitar una licencia de sus contenidos para soporte CD-ROM. La respuesta de la compañía que había liderado el mercado mundial de enciclopedias a lo largo de los últimos doscientos años fue clara y contundente: NO. Los contenidos de su enciclopedia eran suyos, y la idea de empaquetarlos sobre un soporte como el CD-ROM sonaba pretenciosa o absurda, cuando no una amenaza. A los ojos de Britannica, la idea de digitalizar contenidos y ponerlos en CD-ROM sonaba a auténtica blasfemia: era algo que podía tener la potencialidad de dañar las venta de su línea de producto principal, una enciclopedia que costaba unos $250 producir, pero que tenía un precio de entre $1500 y $2200, dependiendo no de la información que contenía, que era invariablemente la misma, sino de la calidad de los materiales utilizados en su encuadernación, entre guáflex sintético y cuero verjurado de alto gramaje. La comisión media que un vendedor se llevaba por vender una enciclopedia estaba entre los $500 y los $600, lo que permite entender en muchos casos su inveterada persistencia en el proceso de venta.

Ante la negativa, Microsoft se dedicó a buscar otros candidatos. Lo encontró en la enciclopedia Funk-Wagnalls, propiedad de la conocida editorial Readers Digest: una modesta obra típica de hogares humildes, y que se vendía en supermercados. Tras licenciar de forma no exclusiva sus contenidos, Microsoft se planteó el reto de convertir aquella modesta enciclopedia en un producto diferente: adquirió a otra editorial, McMillan, los derechos de otras dos enciclopedias, la Collier's y la New Merit Scholar, y llevó a cabo un importantísimo trabajo de documentación y enriquecimiento: el producto final fue un CD-ROM con unos treinta mil artículos, en el que bastaba con empezar a teclear una palabra para obtener sugerencias, y en el que una búsqueda llevaba a un conjunto de contenidos entre los que frecuentemente se hallaban fotografías, ilustraciones, mapas, diagramas, vídeos o infografías. Como enciclopedia, Encarta no podía compararse con Britannica en términos de calidad, pero había logrado algo importantísimo: cambiar los atributos percibidos y valorados por el usuario. Mientras una búsqueda en Britannica suponía recorrer un montón de palabras sin sentido escritas en los lomos de los libros, pasar páginas hasta localizar la definición, y encontrarse finalmente con un texto y, solo con mucha suerte en tres de cada veinte casos con una única ilustración; la búsqueda en Encarta era una experiencia sencilla, cómoda, entretenida y que ofrecía una amplia gama de contenidos audiovisuales de todo tipo. Encarta, con un precio de $60, había desbancado a todo un líder histórico de mercado, perplejo al ver que algo que a sus ojos no era una enciclopedia (y en caso de serlo, era en su opinión muy mala), llevaba prácticamente a la quiebra a su respetadísimo, histórico y prestigioso producto estrella. El análisis de la compañía fue atribuir la popularidad de Encarta a su bajo precio, despreciarla, e intentar remarcar los atributos de calidad de su producto. Pero no funcionó. El precio era únicamente uno de los atributos, y no el más importante, a la hora de decidirse por una enciclopedia. En 1996, la compañía fue malvendida muy por debajo de su precio de mercado debido a sus dificultades financieras, y a pesar de la enorme popularidad de la marca, ha seguido una estrategia errática y muy poco brillante desde entonces.

Las sucesivas ediciones de Encarta fueron creciendo en popularidad y calidad, incorporando contenido y convirtiéndose en una línea de negocio interesante para Microsoft. Sin embargo, la llegada de Wikipedia en 2001 y su fortísimo crecimiento posterior en visitas y en número de artículos hicieron que Microsoft tuviese que tomar medidas drásticas: casi al mismo ritmo que Wikipedia crecía, Encarta disminuía sus ventas. El refuerzo de los contenidos de la web, los sucesivos recortes de precio —llegó a costar $29,35 en su última época— y su constante inclusión en ofertas con otros productos no lograron salvar la marca: de nuevo, como había ocurrido en el caso de Britannica, se interpretaba la gratuidad de Wikipedia como la fuente de su popularidad, cuando la realidad era diferente: el concepto de enciclopedia había vuelto a cambiar. Intentando enderezar la marcha de su producto, Microsoft llegó al punto, en abril de 2005, de solicitar la colaboración de sus usuarios para actualizar y completar sus definiciones. Sin embargo, el sistema nunca funcionó: con el Encarta Feedback System, Microsoft imponía una supervisión editorial a las sugerencias de los usuarios para diferenciarse de aquellas open-content encyclopedias etiquetadas como poco fiables. Los usuarios se veían trabajando gratis para una Microsoft considerada una especie de encarnación del mal que, además, no tenía necesariamente que hacer el menor caso a sus sugerencias. Ante el no funcionamiento del sistema, Encarta siguió languideciendo: en su última edición, sus 65.000 artículos no eran nada comparados con los más de 2,7 millones de la Wikipedia. Al final, Microsoft se dio cuenta del cambio de concepto, y decidió dar carpetazo al proyecto anunciando su cierre en marzo de 2009. ¿El mejor epitafio para el mismo? A las pocas horas de su anuncio, la noticia del cierre aparecía ya en la definición de Encarta... en Wikipedia.

La Wikipedia constituye un caso completamente inexplicable y paradójico para todo aquel que es incapaz de entender la economía de la información y el papel de la tecnología en el mundo actual. El profesor de mi hija, por ejemplo, tuerce el gesto cuando ve que sus alumnos recurren a Wikipedia, y se lo recrimina tildando la fuente de «poco rigurosa». El hecho patente y comprobado de que la Wikipedia sea, a día de hoy, la mejor, más completa y rigurosa enciclopedia del mundo resulta para muchas personas tan incomprensible, que están dispuestos directamente a negarlo, aunque se les pongan las pruebas delante de las narices. En un artículo de la prestigiosa revista Nature, un grupo de especialistas revisaron detalladamente una muestra de entradas escogidas aleatoriamente de la Wikipedia y de la Encyclopædia Britannica, y encontraron una media de cuatro errores u omisiones por cada artículo de la Wikipedia, mientras que en la Britannica la media era de tres. En los ocho casos de errores encontrados considerados por los investigadores como «serios», hubo un empate: cuatro vinieron de cada una de las fuentes. Con estas cifras, la fiabilidad de la Wikipedia quedaba fuera de duda, dado que la diferencia no era estadísticamente significativa. Sin embargo, hay un factor más que se añade al estudio: las entradas en la Wikipedia son, como media, 2,6 veces más largas que en Britannica, lo que mejora el ratio de conversión texto/errores claramente en favor de la Wikipedia. Considerando que Britannica mantiene un consejo editorial de expertos que escriben y revisan los artículos, mientras que Wikipedia somos cualquiera de nosotros, el resultado es para echarse a pensar, Y no desde el punto de vista de que los editores de Britannica hagan mal su trabajo, sino de lo contrario: el trabajo de la inteligencia distribuida es completamente insuperable.

¿Ha probado Wikipedia? No, no me refiero a si ha buscado en ella: eso, perdóneme, no tiene el más mínimo mérito. Me refiero a si ha probado a editarla. Abandone un rato la lectura de este libro, y haga la prueba: diríjase a la Wikipedia en español —la Wikipedia en inglés es mucho más completa y puede que le resulte más complicado encontrar algo que decir. Ahora, busque un término del que usted considere que sabe bastante: puede ser una de sus aficiones, algo que maneje habitualmente en su entorno profesional... cualquier cosa de la que crea razonablemente saber más que la media de la población. No escoja un término polémico, la biografía de un líder político o religioso: esos son objeto de una protección especial. Cuando encuentre un tema, lea cuidadosamente la definición del término, no como para enterarse —hemos quedado en que de ese tema ya sabe mucho— sino como quien se lo leería corrigiendo un examen. Sea pejiguero, y localice desde un error tipográfico, que puede haberlos, hasta algún tema que le parezca que no está bien explicado, o en el que pueda aportar más sin recurrir a la copia de una segunda fuente.

Cuando localice un punto en el que hacer su aportación, hágala. Para ello, tendrá que crear una cuenta haciendo clic en uno de los vínculos de la esquina superior derecha, proporcionar una dirección de correo electrónico a la que tenga acceso — no se preocupe, no le van a inundar con spam— y validándola desde su bandeja de entrada. En un momentito, con su cuenta ya creada, haga clic en «Editar». Se encontrará con un lenguaje sumamente sencillo, en el que simplemente tiene que escribir y, si quiere que algún término de los que ha escrito vincule a su vez a otra página de definición del mismo en Wikipedia, tendrá que ponerlo entre dos pares de corchetes. No estropee ni rompa nada, no sea vándalo. Teclee su cambio, previsualícelo y utilice el botón de grabar. Sienta lo que siente alguien cuando se convierte en autor de una enciclopedia, cuando contribuye, aunque sea modestamente corrigiendo un error tipográfico, cuando aporta su grano de arena a la creación de una obra común. Exactamente así es como se construye Wikipedia: con las pequeñas aportaciones de miles de autores que tienen algo que decir sobre un tema concreto, y que quieren contribuir a una obra común. Sobre eso, simplemente unos mínimos sistemas de supervisión colectiva, una pirámide de editores que corrigen el ámbito de temas en los que se consideran expertos, y que revierten el posible daño causado a la obra con un simple clic.

Ahora, cada vez que vea en un informativo una de esas noticias que pretenden ridiculizar la Wikipedia poniendo de manifiesto sus errores, ya sabe lo que ha sucedido en realidad: uno de esos periodistas que no dejan que la realidad les estropee una buena noticia ha entrado en Wikipedia, ha introducido un cambio absurdo en ella y ha hecho una captura de pantalla antes de que alguno de los editores de la categoría tuviesen tiempo de corregirlo. Cualquiera puede hacerlo, pero generalmente, la gran mayoría de los errores son corregidos con cierta velocidad por el sistema de editores. La Wikipedia, pese a quien pese, es la única enciclopedia en la que podemos consultar la biografía completamente actualizada de una persona. Sin ánimo de ser macabro, haga la prueba en cuanto oiga la noticia del fallecimiento de algún famoso: lo normal será que en cuanto llegue a la página en Wikipedia del finado, en ésta ya figure la fecha de su muerte. Más allá de la anécdota, Wikipedia es uno de los grandes éxitos de la sociedad colaborativa, aunque para quienes no participan en ella más que como usuarios, les pueda resultar complejo entender las motivaciones que llevan a una persona a dedicar parte de su tiempo a contribuir a ella.

Finalmente, un caso particularmente paradójico: el efecto de la disrupción sobre la industria del software. Aunque inicialmente, uno habría esperado un nivel de preparación mayor con respecto a este tipo de temas en los directivos de una industria tan propia del mundo tecnológico como la del software, resulta interesante constatar que no ha sido en absoluto así. El software ha sufrido el impacto de la disrupción de una manera tanto o más virulenta que otras industrias mucho más tradicionales, y ha reaccionado a la misma exactamente igual de mal.

Antes de Internet, los programas típicos instalados en un ordenador eran desarrollados por una empresa que, tras invertir un considerable esfuerzo, pasaba a compilar el código para hacerlo inaccesible al usuario o a posibles competidores, y lo vendía, generalmente metido en una caja de cartón envuelta en celofán, siguiendo un modelo muy parecido al de la industria discográfica. El comprador no era propietario de lo que había adquirido, sino que disfrutaba de una licencia de uso. Los costes de producción correspondían fundamentalmente a los salarios de los equipos de desarrollo, que trabajaban codo con codo en equipos situados habitualmente bajo un mismo techo, con el fin de facilitar la coordinación. Una vez desarrollado el producto, se sometía a pruebas, se corregían los errores o bugs,{6} se pasaba a un departamento de control de calidad, y se lanzaba al mercado. Una vez puesto en el mercado, los usuarios se limitaban a su papel, el de usar el producto. La empresa, simplemente, vendía y proporcionaba soporte a los usuarios, y monitorizaba tanto las gráficas de ventas como las prestaciones de los productos de la competencia. Cuando las ventas se estancaban, es decir, cuando se había vendido el producto a la gran mayoría del público considerado objetivo, o bien cuando un competidor lanzaba un producto que amenazaba al nuestro, se procedía al lanzamiento de una nueva versión, con el fin de volver a dinamizar el mercado.

En este contexto, la aparición de un mensaje de un ingeniero de software finlandés en un foro de usuarios del sistema operativo Minix, en 1983, en el que solicitaba colaboraciones desinteresadas para crear una versión de dicho sistema operativo capaz de correr en ordenadores personales sencillos, pasó completamente desapercibida para las grandes compañías de software. El ingeniero de software se llamaba Linus Torvalds, y a partir de su primer y aparentemente inofensivo mensaje se generó todo un movimiento de colaboración que dio lugar a Linux, hoy uno de los grandes sistemas operativos existentes y que ha marcado la pauta para el desarrollo de muchos más. El proyecto de Linux y su evolución hasta convertirse en un sistema operativo completo, sólido, seguro y eficiente venía a demostrar el poder de la red para organizar tareas distribuidas: antes de Internet, la colaboración entre programadores de todo el mundo para un proyecto similar habría resultado completamente imposible. Pero mediante Internet, y con el uso de unas pocas herramientas de coordinación sencillas, el proyecto no solamente era posible, sino que el nivel de eficiencia que alcanzaba podía superar en varios órdenes de magnitud las metodologías de desarrollo tradicionales utilizadas por las compañías de software: al incorporarse al proyecto una variedad más amplia de experiencias y de ojos capaces de supervisar cooperativamente el trabajo colectivo, el resultado evolucionaba más rápidamente y mostraba una capacidad de respuesta más rápida a los problemas. De repente, afloraba la evidencia: la mejor manera de construir software ya no era dentro de una gran corporación reuniendo a una serie de programadores para crear un código que después se ocultaba a los ojos del resto del mundo, sino organizándolo como proyecto abierto para que cualquier interesado en contribuir pudiese hacerlo. La superioridad del método fue expresada de manera magistral por Eric S. Raymond en su libro La catedral y el bazar, con la frase «dado un número suficientemente elevado de ojos, todos los errores son superficiales»: cuanto más público y disponible estuviese el código de una aplicación para pruebas, escrutinio y experimentación, más rápidamente serían descubiertos todos los errores en él.

Algunas de las creaciones del software de código abierto, tales como el sistema operativo Linux, el motor de base de datos MySQL, el navegador Firefox o el servidor web Apache, son hoy en día líderes o competidores muy destacados en su segmento. El desarrollo mediante herramientas LAMP (Linux, Apache, MySQL y Perl/PHP/Python) representa una de las grandes tendencias para la creación de aplicaciones, y, sin duda, el conjunto de herramientas más demandado por el mercado de programadores. En la base del éxito del software de código abierto están las posibilidades de Internet, de cara a posibilitar la coordinación entre múltiples agentes encargados de una tarea compleja, así como la no dependencia de ciclos comerciales: mientras que una empresa de software convencional condiciona la siguiente versión de sus programas a factores como la penetración en el mercado o las reacciones de la competencia, las comunidades de desarrollo en código abierto optan por una mejora gradual y constante, lo que genera un ritmo de progreso superior

El nuevo modelo supone una fuerte disrupción sobre el modelo tradicional: el código abierto convierte en complejo cobrar por licencias» dado que cualquiera puede, en principio, obtener el programa de manera gratuita. Sin embargo, el hecho de que el código se pueda obtener de manera gratuita no impide otros modelos de negocio: en muchos casos, el usuario está dispuesto a pagar una licencia a cambio de, por ejemplo, acceso a servicios de mantenimiento, funcionalidades que no se encuentran incorporadas en la versión básica, formación, adaptaciones, etc. El hecho de que empresas como Red Hat, que comercializa productos en código abierto, haya llegado a incorporarse al selecto club del Fortune 500 indica claramente que dedicarse al software de código abierto no es necesariamente algo incompatible con el negocio. Además, la constatación de la superioridad del código abierto como método de desarrollo ha llevado a muchas empresas a incorporarlo en un grado mayor o menor. Actualmente, empresas como IBM, Apple o Google recurren al código abierto para una gran mayoría de sus desarrollos, y obtienen gracias a ello un nivel de productividad de su investigación y desarrollo netamente superior. De hecho, resulta profundamente simplista considerar las aplicaciones de código abierto como el producto de un montón de locos con mucho tiempo libre que dedican parte de su jornada a trabajar de manera altruista: una gran cantidad de empresas contribuyen con dinero y horas-hombre al desarrollo de productos de código abierto, y muchos programadores independientes obtienen sustanciosos beneficios del hecho de destinar esfuerzos a ello.

Como en tantos otros casos, el código abierto supone una fuerte disrupción en la industria del software, dado que impone modelos de negocio alternativos a los que las empresas tradicionales no desean adaptarse. Aquel que percibe la disrupción siempre piensa que vivía mejor antes de que esta llegase, e intentará evitarla como si tal cosa fuera posible, como si fuese posible detener algo que ya no está en sus manos detener.

Para terminar este capítulo, vamos con una reflexión importante usando algunos de nuestros ejemplos: piense en la Encyclopædia Britannica. Su precio, como hemos comentado anteriormente, estaba entre los 1.500 y los 2.200 dólares en función de su encuadernación. Su sustituta en el mercado, Encarta, costaba tan solo $60, mientras que la iniciativa que reemplazó (y eventualmente eliminó del mercado) a esta, tenía un precio de cero dólares, era un producto gratuito. De 2.200, a 60 dólares, y finalmente a 0 dólares. ¿Estamos hablando de un proceso de destrucción de valor debido al impacto de la tecnología? Piénselo bien: de la respuesta a esta pregunta depende en gran medida su nivel de aprovechamiento de este libro cuya lectura está comenzando. Si usted ve en ese proceso una destrucción neta de valor (y no es usted Jacqui Safra, propietario de Encyclopædia Britannica, o William H. Gates III accionista de referencia de Microsoft), tenemos un problema. Porque si es usted una de esas dos personas o es accionista de alguna de esas dos empresas, puedo entenderlo: obviamente, usted ha visto cómo se destruía valor. Ha podido observar casi físicamente, cómo los títulos de propiedad de sus acciones entraban en un proceso de combustión espontánea y quedaban reducidos a cenizas, cómo los ambiciosos planes trazados para esos productos desaparecían víctimas de un cambio tecnológico. Pero... ¿dónde ha ido ese dinero? ¿Se ha convertido en humo? ¿Ha desaparecido? No, no es así. En estos casos, el dinero actúa siguiendo algo parecido al primer principio de 1a. termodinámica, el de la conservación de la energía: no se crea ni se destruye, solo cambia, de forma o de lugar. En realidad, el valor se ha generado en los usuarios, que ahora disponen de una enciclopedia mejor, infinitamente más actualizada y completa, y, además, de uso gratuito. El incremento de valor en los bolsillos de unos pocos accionistas se ha transformado en un incremento de valor en muchos millones de usuarios. No existe destrucción de valor, a lo sumo una migración del mismo.{7}

¿Podría alguien diferente del patético Steve Ballmer, el histriónico director general de Microsoft, argumentar seriamente que el software libre provoca una fortísima generación de valor en empresas de todo el mundo, que gracias al mismo son capaces de obtener herramientas adecuadas, baratas, en permanente proceso de mejora y que pueden ser adaptadas y modificadas por los propios usuarios o por empresas de servicios? Afirmar, como hizo Ballmer, que el software libre «destruye riqueza y puestos de trabajo» no es más que carecer de la inteligencia suficiente como para ver que no todo ocurre a dos palmos de su nariz: el software libre destruye, efectivamente, «su» riqueza y los puestos de trabajo de «su» compañía (Microsoft ha despedido ya a varios miles de empleados, a pesar de tener más de treinta y siete mil millones de dólares en caja y enormes beneficios), pero genera mucha más riqueza en otros sitios en los que no existen las patentes ineficiencias que se producen en Microsoft.

No, la tecnología no destruye valor. Pero a lo largo de la historia de la Humanidad, hemos tenido constantes ejemplos de personas que afirmaban que así era. Si la tecnología destruyese valor, ahora seríamos infinitamente más pobres que en la época de la Revolución Industrial, y fíjese usted por donde, no es así. Por supuesto, en la Revolución Industrial pudimos ver cómo muchas empresas se iban a pique debido a las innovaciones tecnológicas que otras adoptaban. Las empresas de transporte de hielo se fueron a pique cuando la tecnología permitió tener una nevera en casa. Pero todas estas innovaciones terminaron, al cabo de muy poco tiempo, provocando una inmensa generación de valor que ha incrementado el nivel de vida medio de la mayoría de la Humanidad por un factor descomunal.

Los clichés son la manera en la que los seres humanos nos enfrentamos a razonamientos cuya aparente complejidad excede nuestro razonamiento o las ganas que tenemos de recurrir al mismo. La próxima vez que se enfrente a ese cliché que afirma irresponsablemente que la tecnología destruye valor, racionalícelo un poco: la tecnología genera sistemas más eficientes, y aniquila a los que no son capaces de adaptarse a esas ganancias de eficiencia.

CAPÍTULO 3. LA DISRUPCIÓN TECNOLÓGICA (CUANDO LO VEMOS VENIR)

«En este entorno sorprende ver cómo todavía hay empresas con ejecutivos ocupando altos cargos directivos simplemente porque son expertos en lo que era importante ayer.»

JONAS RIDDERSTRALE,

autor de Funky Business

¿Qué lecciones podemos extraer de la enumeración de casos del capítulo anterior? La gran mayoría de directivos de empresas de sectores distintos a los mencionados, al ver la descripción de las situaciones acaecidas, tiende a quedarse con cara de «esto no me va a pasar a mí» y a buscar argumentos por los cuales «su caso no es como todos esos». Pero obviamente, lo mismo pensaban en su momento los directivos de los sectores afectados y lo mismo piensan a día de hoy los que están empezando a ver las orejas al lobo, en una especie de ciclo sin fin. ¿Existe alguna manera de entender lo experimentado hasta el momento «en cabeza ajena», de proyectar el impacto del proceso de disrupción tecnológica sobre nuestros propios negocios?

Para entender el impacto de la disrupción tecnológica, es indispensable racionalizar el comportamiento de algunas variables. La primera de ellas es la denominada fricción, entendida desde un punto de vista económico. Podríamos definir la fricción como el conjunto de circunstancias que se interponen entre un agente económico y el bien que este pretende obtener, derivadas de imperfecciones en el mercado como costes de adquisición de información, comunicación, transporte, logística, etc. En cualquier transacción, la fricción viene representada por todo eso que impide que lo que deseamos adquirir aparezca ante nuestros ojos con simplemente chasquear los dedos. Si nos levantamos una mañana queriendo comprar un libro determinado, la fricción hace que para obtenerlo tengamos que asearnos, vestirnos, salir a la calle, caminar hasta la librería más próxima, buscar en las estanterías, localizar el libro deseado en caso de que se encuentre en ellas, pagarlo y transportarlo de vuelta. En el caso de la obtención de una hipoteca, la fricción determina que tengamos que ir hasta el banco, explicar nuestras circunstancias a un empleado de la sucursal, rellenar un montón de formularios, solicitar una peritación del inmueble, esperar a que se reúna el comité de riesgos, etc. En general, la fricción supone un conjunto de circunstancias no necesariamente relacionadas con el bien que se desea obtener o con la empresa que lo vende, pero que asociamos con el proceso de una manera casi natural, como algo inherente al mercado: de hecho, algunos competidores juegan con elementos de reducción de la fricción en algunas ocasiones para hacer su oferta más atractiva.

A efectos puramente didácticos, imagínese su negocio, ese en el que tiene experiencia, que conoce perfectamente y en el que ha desarrollado su actividad profesional, y piense qué ocurriría si de la noche a la mañana apareciese algún primo lejano del mago Merlín y, agitando su varita, provocase una disminución total de la fricción. ¿Qué tipo de cosas ocurrirían a partir de ese momento? ¿Podría competir su empresa en un mundo sin fricción? En esa tesitura debieron sentirse, por ejemplo, gigantes del mundo de la distribución editorial norteamericana como Borders o Barnes&Noble cuando empezaron a experimentar la competencia con Amazon. De repente, si te levantabas por la mañana y querías un libro determinado, todo lo que tenías que hacer era ponerte delante de la pantalla, del ordenador —lo de vestirse era plenamente opcional en función de la distancia entre este y una ventana sin cortinas— y mover el ratón durante un rato. Todos los procesos de fricción anteriormente enumerados, desde el salir a la calle hasta la búsqueda por las estanterías, sustituidos por la escasa fricción entre el ratón y su alfombrilla. Nótese aquí que no pretendo afirmar que un sistema sea inherentemente mejor que el otro, ni que la fricción disminuya de manera absoluta: el paseo hasta la librería podría llevarse a cabo en una deliciosa mañana de primavera, parando para degustar un Martini con su aceituna en una terracita mientras conversamos con un amigo, para después hablar un rato con ese librero encantador que nos conoce desde que llevábamos pantalones cortos. Pero eso, a efectos de teoría económica, tiene entre poca y ninguna relevancia. Podría ocurrir que el libro que quisiésemos obtener estuviese prominentemente expuesto en el escaparate de la librería, reduciendo notablemente el esfuerzo de encontrarlo, y que volviésemos a casa con él debajo del brazo, mientras todos sabemos que en el caso de la librería electrónica, nadie nos va a quitar una espera de unos días hasta que el operador logístico llame a nuestra puerta con el libro en cuestión. De acuerdo. Nada es perfecto. Para analizar cuál de los dos procesos tiene una fricción menor, necesitaríamos dar a dos personas una lista de, por ejemplo, cien libros escogidos al azar, y enviar a ambos a obtener los libros, uno exclusivamente a través de la librería de su barrio y otro exclusivamente a través de una tienda en la web. ¿Cuál de los dos conseguiría obtener antes la totalidad de los libros?

A efectos económicos, la web supone seguramente el mayor reductor de fricción inventado por el hombre desde que el mundo es mundo. No es perfecto y, sobre todo, choca con infinidad de cuestiones culturales o derivadas de los usos y costumbres desarrollados durante generaciones. Pero la tarea de imaginarnos nuestro negocio en ausencia de fricción, como quien hace el ejercicio de imaginarse cómo serían sus quehaceres diarios si estuviese a bordo de la Estación Espacial Internacional en situación de gravedad cero, nos puede ayudar a aproximarnos a la idea. Intente aislar el papel que en su negocio juega la fricción: si es usted un intermediario comercial, la ausencia de fricción determinaría en muchos casos que su papel simplemente desapareciese: en ausencia de fricción, los fabricantes hacen llegar sus productos a los clientes de manera directa, como a golpe de varita mágica. Por supuesto, eso no es algo que la web pueda conseguir —todavía— y sí en cambio protagoniza, en algunos casos, procesos de reintermediación, con el desarrollo de nuevos papeles necesarios para las transacciones, pero recuerde dos cosas: una, estamos haciendo un simple ejercicio académico. Y dos, nadie está intentando venderle nada, no es preciso que se ponga a la defensiva.

Intentemos ahora hacer un ejercicio causal: ¿qué tipo de cosas se alteran de manera fundamental cuando ese concepto visto anteriormente, la fricción, desaparece o se reduce de manera notable? A poco que lo pensemos, nos daremos cuenta de que uno de los factores más directamente afectados son los llamados costes de búsqueda. Los costes de búsqueda representan el esfuerzo que un cliente debe hacer para obtener información sobre la oferta, de un producto determinado. A la hora de adquirir, por ejemplo, un automóvil, los costes de búsqueda representarían el esfuerzo de acercarse por todos los concesionarios de la ciudad para preguntar el precio del modelo escogido o de aquellos que hayamos introducido en nuestro conjunto de selección. ¿Hasta qué punto estamos dispuestos a incurrir en costes de búsqueda? Lógicamente, hasta el momento en que el trabajo adicional de buscar es mayor que el diferencial de ahorro que puede llegar a obtenerse de manera razonable.

La existencia de los costes de búsqueda permite construir, por ejemplo, muchos modelos de negocio basados en supremacías locales, en los que un competidor ofrece el mejor balance de calidad o precio para el espacio en el que resulta razonable que los clientes objetivo lleven a cabo su búsqueda. Desde un punto de vista puramente intuitivo, la disminución de los costes de búsqueda debida a Internet resulta sumamente fácil de entender: donde antes un cliente recorría tres o cuatro bancos preguntando por las condiciones de una hipoteca, o varias tiendas de televisores apuntando el precio del modelo deseado, ahora puede simplemente entrar en una página de comparación, bien con datos suministrados por las propias tiendas u obtenidos de manera social preguntando a otros clientes, y decidir en unos golpes de ratón a que tienda acudir, o incluso no acudir, y poner en marcha el proceso logístico desde la misma página web.

Su segundo ejercicio, por tanto, es llevar a cabo un análisis mental rápido de su negocio desde el prisma de los costes de busqueda: ¿qué pasaría con su negocio si cada uno de sus clientes pudiese, de manera instantánea, obtener una comparación fidedigna de sus precios y calidades, comparada con todo el resto de la oferta? ¿Sería capaz su negocio de sobrevivir en tales circunstancias?

El tercer elemento interesante de la disrupción que conviene analizar es la naturaleza de su producto, o más concretamente, la tangibilidad del mismo: como bien escribió Nicholas Negroponte en su Mundo Digital allá por 1995, se trata de entender qué componente de nuestros productos caen dentro de la categoría de «productos átomo» y cuáles dentro de la de «productos bit». Aunque aparentemente obvia, la división merece una explicación: un producto átomo es, claramente, aquel que al dividirlo en sus unidades más pequeñas posibles, acaba dando lugar a átomos de algún tipo de elemento. Un pedazo de hierro está en último término compuesto por átomos de hierro, mientras que una pieza de fruta lo está por átomos de carbono, hidrógeno, oxígeno y algunos elementos más. Un producto bit, en cambio, carece de naturaleza física como tal, y al reducirlo a sus componentes más primigenios, da lugar a ceros y unos encadenados de tal manera que lo dotan de sentido. La información, por ejemplo, es claramente un producto bit. Las consecuencias de ser un producto bit o un producto átomo son muy distintas en lo que a disrupción tecnológica se refiere: entre otras cosas, los bits se transportan de manera inmediata por la red mientras que los átomos no son susceptibles de pasar a través do los cables y precisan de la participación de algún tipo de operador logístico para moverse de un sitio a otro.

Pero aplicando este análisis, ¿en qué categoría pondríamos, por ejemplo, un periódico? Resulta claro que si bajamos al quiosco, compramos un periódico y lo reducimos a sus componentes elementales, obtendríamos primero pulpa de papel, y finalmente los átomos que la componen, probablemente carbono, hidrógeno, oxígeno y otros en cantidades menores. ¿Indica esto que un periódico debería ser incluido en la categoría de producto átomo? Si fuésemos un editor, ¿nos quedaríamos tranquilos tomando decisiones estratégicas bajo la consideración de que nuestro producto pertenece al ámbito de los átomos? No cabe duda: la afirmación de que un periódico está hecho de papel y que el papel está hecho de átomos es irrefutable, tan irrefutable que miles de editores de todo el mundo la han considerado cierta durante muchos años, pero ¿es ese el punto de vista que debemos aplicar? ¿Qué más componentes forman parte de un periódico? ¿Es razonable, a la hora de conceptualizar el producto, hacerlo únicamente en función de la composición del material que le sirve de soporte? ¿Qué componente del producto lleva a los clientes a pagar por él? En este caso, mucho me temo que los lectores de un periódico no pagan por el papel, a no ser que estén pintando en casa y necesiten ese papel para cubrir sus muebles. Si nuestro negocio como editores de prensa dependiese del número de personas que pintan su casa —que, además, no suelen adquirir periódicos nuevos para ello— sospecho que la prensa habría dejado de existir hace tiempo. Los clientes, en realidad, pagan por la información contenida en el papel, y la información no es un producto átomo, sino un producto bit. Y esa confusión tan aparentemente simple ha llevado a muchos editores a proteger absurdamente su negocio en función del soporte físico del mismo, olvidando que, en el momento en que otro soporte pudiese proporcionar mejores condiciones que el papel, este sería olvidado en, como máximo, una generación, el tiempo necesario para que se produjese un proceso de olvido colectivo. En realidad, muchos de los negocios enumerados en el primer o segundo capítulo de este libro han ganado su derecho a estar ahí por haber caído en errores conceptuales como este.

¿Qué parte de su producto, por tanto, son bits, y qué parte son átomos? En mi caso, el negocio de un profesor se podría conceptuar, en condiciones normales, como de producto átomo: mi negocio consiste en trasladar mis átomos de un lugar a otro, de un aula a otra, para impartir una sesión. Sin embargo, ¿qué ocurre en el momento en que alguien filma mi clase, me pide la presentación que utilicé en ella y la pone en la red? En ese momento, acabo de pasar de átomos a bits, y debo entender que mi negocio acaba de cambiar de arriba abajo, sin que me vaya a servir de nada oponerme a ello. Si me niego a aceptarlo, solo prolongaré la agonía, además de hacerla seguramente más dolorosa. En el caso del negocio de la producción y distribución musical, no entender el proceso de transición de átomos a bits les lleva a intentar por todos los medios defender el negocio de venta de galletas de plástico, claramente un soporte que Internet convierte en obsoleto, mientras renuncian o imposibilitan con tasas abusivas el desarrollo del negocio de la distribución a través de la red. En su descargo, habría que decir que el renunciar a utilidades conocidas para obtener otras por conocer y seguramente inferiores no resulta una decisión sencilla para nadie, y menos si eres una empresa cotizada en bolsa y se lo tienes que explicar a tus accionistas.

Hasta el momento, hemos revisado tres análisis que todo directivo debería intentar aplicar a su negocio, y que proporcionan resultados sorprendentes incluso con una aproximación somera: examinar su empresa desde la perspectiva de la fricción, desde la de la disminución o desaparición de los costes de búsqueda, y revisar cuidadosamente el balance entre átomos y bits. Pero nadie le dijo que la lectura de este libro iba a ser fácil; entender por qué todo va a cambiar requiere que haga todavía algunos deberes más. Así que pasemos a un cuarto ejercicio: el del modelo de interacción. El modelo de interacción se refiere a su modo habitual de interaccionar con los agentes relacionados con su empresa, entendidos de un modo amplio. En principio, se suele aplicar a la relación con los clientes, aunque también ofrece muy buenos resultados cuando se considera de cara a proveedores, empleados, o incluso en la relación entre empresa y sociedad en su conjunto, entrando en el complejo ámbito de la responsabilidad social corporativa. Pero en pro de la simplicidad, iniciaremos el análisis planteándonos la relación con los clientes.

¿Cómo se relaciona habitualmente con sus clientes? ¿Los visita? ¿Los llama por teléfono? ¿Les envía cartas? ¿Correos electrónicos? Como veremos en el capítulo siguiente, la elección de un medio de comunicación conlleva un proceso de toma de decisiones que afecta a la relación con los clientes. El paso de las comunicaciones orales a las escritas produjo un cambio importantísimo, con un efecto que encontraremos en numerosos análisis a lo largo de este libro: alteró drásticamente el balance síncrono-asíncrono, y por tanto, cambió el componente de intrusividad. Este principio de nombre complejo y que utiliza varias palabras inexistentes como tales en nuestro diccionario esconde, en realidad, una cuestión de gran simplicidad y sentido común: una comunicación síncrona es toda aquella en la que, para producirse, emisor y receptor tienen que coincidir en el tiempo. En la comunicación oral, resulta evidente que es así, y que solo un dispositivo de grabación, que, por tanto, convierte lo síncrono en asíncrono, puede evitarlo. El medio escrito, por contra, constituye el caso opuesto: utilizarlo para la transmisión síncrona suele carecer de sentido o ser muy poco eficiente salvo excepciones, como el uso de una pizarra o encerado para exponer una idea a un grupo de gente, y lo normal es usarlo para una transmisión asíncrona, es decir, que otra persona lo lea en otro momento o lugar.

Hasta aquí, perfectamente obvio. Lo que no lo es tanto, o sí lo es, pero requiere una dosis de introspección, es cómo afecta el balance síncrono/asíncrono a la ecuación de intrusividad, es decir, a nuestra libertad de elegir cuándo queremos recibir la comunicación. En el caso de una comunicación oral, la elección no existe. En el momento en que una persona nos habla, un vendedor llama a nuestra puerta y abrimos, o un teléfono suena y descolgamos, nuestra libertad de elección se limita severamente. No hemos elegido el inicio de la comunicación, y esta nos puede perfectamente suponer un engorro en ese momento, nos puede suponer una interrupción, una intrusión en cualquier tarea que estemos realizando en ese momento, sea una entretenida conversación, una película o una siesta. Una carta, por contra, supone el caso contrario: puedo recogerla en el buzón, dejarla en la mesita del recibidor, abrirla cuando buenamente quiera, y contestarla si lo estimo oportuno en cualquier otro momento dentro de un plazo considerado razonable. Un correo electrónico provoca el mismo efecto, aunque en algunas empresas se utilice como si fuera un chat. Con un SMS ocurre lo mismo, y en este caso el hecho de que los jóvenes los usen a modo de conversación no deja de ser un hecho que hace las delicias de las operadoras. En el balance síncrono-asíncrono de la comunicación y en el manejo de la ecuación de la intrusión radican, como veremos más adelante, muchas cuestiones importantes en comunicación que hemos visto cambiar en los últimos años.

Para muchos, en el balance síncrono-asíncrono se encuentra una de las claves fundamentales del éxito de Internet. Un ejemplo personal: cuando en 1996 llegué a los Estados Unidos tras haber recibido durante toda mi vida una educación en inglés británico, el acento californiano me suponía ciertos problemas de comprensión. Si a ello le unimos una cierta timidez natural, no es difícil imaginar algunas de mis peripecias a la hora de amueblar una casa o comprar una fregona para el suelo, por no mencionar la conversación con un profesor o compañero de universidad sobre un trabajo académico. ¿Qué hacer en un caso así? Simplemente, desplazar a Internet la mayor cantidad de comunicación posible. En Internet, a través de correo electrónico o de las páginas web de las diversas tiendas, podía tranquilamente disfrutar del tiempo necesario para asimilar los mensajes, tomar decisiones, contestarlos, o simplemente llegar a la tienda y decir «quiero eso» mientras señalaba con el dedo. El modelo de interacción de Internet no es humano, es decir, no conlleva molestar a una persona cada vez que la otra no entiende o precisa una repetición. Podemos hacer clic diez veces seguidas en un vínculo, que bajo ningún concepto nos encontraremos una mirada de censura o una contestación en voz tensa que nos recrimine nuestra insistencia. No molestamos a nadie solicitando diez veces con sendos clics de nuestro ratón el mismo documento, y eso tiene unas implicaciones interesantísimas.

En el caso de la mayoría de las empresas, y especialmente en mercados tan estudiados como el del gran consumo, la respuesta a preguntas relacionadas con el comportamiento de elección de medios nos llevaría a analizar, entre otras cosas, los mecanismos de la publicidad. La publicidad suele ser el vehículo a través del cual las empresas se relacionan con sus clientes: mensajes publicitarios emitidos a través de diversos canales, con la intención de influir sobre el comportamiento de compra, de provocar un sesgo positivo que lleve al cliente a escoger nuestro producto sobre las ofertas de los competidores.

Un modelo publicitario pulido a lo largo de siglos y que, como veremos en el siguiente capítulo, se apoya en una asunción fundamental: el uso de canales unidireccionales. De hecho, la asunción de un canal unidireccional ha provocado, a lo largo de los años, que terminemos sufriendo un curioso fenómeno: si un empresario hablase a las personas que lo rodean de la misma manera en que lo hace a través de la publicidad, estas se darían la vuelta y se irían, o bien, siguiendo una frase del genial Hugh MacLeod (Gapingvoid.com), le darían directamente un puñetazo en la cara.{8} La publicidad no solamente nos grita, sino que sigue mecánicas que, examinadas con cierta frialdad, parecen diseñadas para generar el consumo en idiotas y débiles mentales. Con el avance de los medios sociales y participativos, la publicidad como la conocemos ha caído de manera progresiva en su credibilidad: tanto el que la produce como el que la ve «asume directamente que el contenido de la misma es absurdamente sesgado y no fiable por definición, lo cual la relega al mero papel de recordatorio más o menos molesto. De hecho, en la interacción entre la publicidad e Internet, hemos vivido situaciones que podrían sin dificultad ser incluidas en una obra del llamado «teatro del absurdo», con usuarios instalándose aplicaciones para bloquear la insoportable publicidad que amenazaba con reducir la propuesta de valor del medio a mínimos inaceptables mediante molestísimos pop-up, layers, y demás inventos diseñados para resultar a cada cual más intrusivo y molesto.

Pruebe, por ejemplo, a leer un periódico en Internet: salvo alguna honrosa excepción, se encontrará un anuncio a toda pantalla nada más teclear la dirección del periódico, que pretenderá mantenerse ahí durante varios interminables segundos. Tras eso, en muchos casos, la pantalla se convierte en una especie de «campo de minas»: cada vez que pasa con su ratón por determinados espacios, algo se despliega, se mueve, canta o efectúa alguna acción sumamente molesta, que dificulta lo que usted realmente quería hacer: leer las noticias. Pero lo más increíble es que, con el tiempo, algunos, en pleno «síndrome de Estocolmo», han llegado a ver esto como algo natural, a justificarlo en la necesidad de los periódicos de ganar dinero: claro, como no ganamos suficiente, vamos a convertir la experiencia de leer el periódico en una basura miserable e insoportable... no, decididamente, así no vamos a ningún lado. Y lo que debería ocurrir es que cada persona que se encontrase publicidad intrusiva en su periódico favorito (pop-ups, intersticiales a toda pantalla, sonido o vídeo preactivado, formatos extensibles, etc.) se hiciese la promesa de no volver a ese periódico hasta que renunciase a semejante aberración.

En realidad, la «guerra de los pop-up» se inició con cosas mucho más inocentes. El primer anuncio publicitario, o banner; vendido por una publicación, Hotwired (hoy Wired) en una página en Internet pertenecía a la compañía telefónica AT&T, y decía premonitoriamente «¿Has hecho alguna vez clic en un banner? Lo vas a hacer». Para el anunciante, los anuncios en Internet suponían una gran diferencia con respecto a los medios convencionales: a pesar de su en aquel momento escasa difusión, que hacía implanteable utilizarlos para campañas masivas, el medio permitía cosas que el anunciante jamás podía haber imaginado baste entonces, tales como mediciones exactas y fiables, seguimiento, caracterización del visitante o establecimiento de vínculos causales con el comportamiento de compra. El banner se planteaba como «publicidad con esteroides»: si cuando te anunciabas en un periódico únicamente podías saber el número de copias vendidas por este, pero nunca cuántas personas lo habían visto realmente o si les había resultado interesante de alguna manera, cuando ponías un banner eras capaz de conocer con exactitud cuántas personas lo habían visto (impresiones servidas), cuántas habían hecho clic sobre él (clickthrough), cuántas habían llegado hasta tu página, e incluso cuántos habían visualizado o adquirido el producto o servicio en cuestión. De manera prácticamente inmediata podías ensayar diferentes diseños, posicionamientos, combinaciones de colores o elecciones de medios, comprobando prácticamente al instante los cambios en volumen y tipo de respuesta provocada,

La capacidad de medirlo todo provocó rápidamente un auge en el desarrollo de metodologías de medición, con una métrica, el clickthrough, a la cabeza. El clickthrough representaba en términos porcentuales el número de personas que, habiendo visto el anuncio, habían decidido hacer clic en él. Los primeros banners además provocaron, por su novedad, magnitudes de clickthrough notables: como nadie sabía qué era aquello y adonde llevaba, lo natural era hacer clic para satisfacer la curiosidad. En una red en la que por entonces predominaba claramente el texto y había pocas fotos, los banners destacaban notablemente. Llevados por esos valores iniciales elevados, los anunciantes pasaron a lanzar campañas de todo tipo en la web, sin tener en cuenta que el entusiasmo inicial pasaría rápidamente hasta entrar en valores convergentes con los de la publicidad tradicional. Pero al ver cómo, pasado el efecto de la novedad, los valores de clickthrough retornaban a la lógica de magnitudes habitualmente muy inferiores a un punto porcentual (un 2% es considerado un enorme éxito), los anunciantes escogieron una curiosa manera de evitarlo: «¿que no quieren hacer clic? ¡Pues hagamos que los anuncios bailen!». De la noche a la mañana, la web se pobló de rectángulos en furioso movimiento que provocaban casi el estrabismo en los visitantes de las páginas, mientras intentaban pacíficamente leer un periódico o consultar la información de estas. Gracias a la inclusión del movimiento, sin embargo, se recuperó algo del «efecto novedad», y el clickthrough manifestó una cierta recuperación temporal. Pero lo bueno no podía durar, y la siguiente caída llevó a los anunciantes a exigir a los creativos nuevas formas de mantener el clickthrough elevado, aunque fuese a costa de destrozan la experiencia de navegación y de consumo de contenidos de los usuarios: del movimiento pasamos al insoportable sonido preactivado, y de ahí, a los pop-ups que se desplegaban inmisericordes ante los ojos del visitante de una página, impidiéndole, precisamente, ver su contenido, la razón que en función de toda lógica le había llevado hasta allí. El acoso llegó a términos insoportables: a finales de los años noventa, muchas páginas lanzaban pop-up de manera constante, introducían mecanismos para evitar su cierre (ventanas que «huían» del ratón mientras el usuario se afanaba en perseguirlas por toda la pantalla, o engaños que presentaban un falso botón que servía supuestamente para cerrar la ventana) o, de una manera u otra, llevaban a cabo actividades que suponían para el usuario molestias que iban entre lo leve y lo directamente insufrible.

Los abusos de una publicidad que amenazaba con destrozar la propuesta de valor de la publicidad en Internet terminaron con la incorporación de mecanismos de bloqueo en la mayoría de los navegadores y con el lanzamiento por parte de Google de una barra de navegación que servía, además, para bloquear los pop-up: en el año 2000, todos los navegadores excepto Microsoft Internet Explorer permitían el bloqueo de pop-up, y la barra de Google había alcanzado una rapidísima popularidad llevada por la necesidad de los usuarios de protegerse frente al ataque. En realidad, se trató del inicio de una escalada armamentística: aún en nuestros días, siguen existiendo anunciantes irresponsables que intentan buscar métodos para saltarse las restricciones y seguir molestando a sus usuarios por encima de los mecanismos que estos utilizan para intentar defenderse de ellos. Momento en el cual convendría, seguramente, detenerse un momento y pensar de manera reposada: ¿qué lleva a una empresa que pretendía dar a conocer sus productos o presentarlos ante sus clientes potenciales a convertirse en una fiera acosadora que los persigue, molesta e incómoda hasta el punto en que estos sienten la necesidad de instalar herramientas para protegerse de ella?

Lo ocurrido con la publicidad en Internet es, en realidad, algo digno de uno de los más conocidos pasajes de una de las obras de teatro más representadas de todos los tiempos, la famosa Muerte de un viajante de Arthur Miller, cuando el vecino de Willy Loman, Charley, se da cuenta de que Willy era un vendedor en el más puro sentido de la palabra: se vendió a sí mismo hasta que el mundo dejó de comprar. Durante años, las empresas se han dedicado a agotar canales de comunicación con sus clientes hasta convertirlos en muchos casos en completamente inservibles: hace años, abríamos la puerta cuando alguien llamaba con intención de vendernos algo. Hoy en día, el canal de la venta a domicilio vive unas cotas de popularidad tan bajas que su eficiencia ha descendido muchísimo, y su uso se limita a unas muy pocas industrias con una gran tradición en este sentido, a los pedigüeños, a determinadas religiones en busca de prosélitos, o a los niños vendiendo lotería de fin de curso. Y lo ocurrido con la venta a domicilio es un proceso que hemos visto repetirse de manera idéntica con otros canales, como el correo postal o el teléfono. ¿Debía de alguna manera Internet ser necesariamente diferente en ese sentido?

La respuesta, como en todos los casos anteriores, es que sí. Que Internet estaba destinado a ser diferente en ese sentido. Del mismo modo en que lo ha llevado a cabo con la fricción, los costes de búsqueda y el componente de átomos frente a bits, revise ahora los mecanismos de interacción con sus clientes: si están basados en métodos predominantemente unidireccionales, o si se desarrollan de tal manera que le daría vergüenza dirigirse así a sus familiares o amigos, se dispone a presenciar una drástica caída de su eficiencia, cuando no un efecto directamente negativo derivado de su uso. Un cambio en el modelo de interacción que está provocando, entre otras muchas cosas, la crisis en los negocios sostenidos por la publicidad convencional, como periódicos, revistas o televisiones, en una espiral de decrecimiento que solo puede empeorar con el tiempo. Si su negocio consiste en interrumpir a una serie de clientes que quieren obtener un contenido, bombardeándoles de manera sistemática con otro contenido que no han solicitado ni desean ver, olvídelo: su negocio está destinado a desaparecer.

¿Cómo reconocer, por tanto, los procesos de disrupción? La respuesta, por supuesto, es que no resulta en absoluto sencillo. Las variables que determinan el arranque de un proceso disruptivo se inician con manifestaciones a veces difíciles de percibir, con cambios de hábitos que, en muchas ocasiones, tienen lugar en subconjuntos de clientes extremadamente poco representativos. El análisis de las variables citadas en este capítulo puede enseñarle a reconocerlos, pero lo que sin duda lo hará es otra variable: la velocidad. Si prestamos atención a los casos comentados en capítulos anteriores, observaremos una característica común: todos ellos tuvieron lugar a gran velocidad. En el caso de la música, el desarrollo de Napster por parte de aquel diecisieteañero llamado Shawn Fanning tuvo lugar en junio de 1999. En febrero de 2001, tenía más de veintiséis millones de usuarios. En el de las enciclopedias, los primeros contactos de Microsoft con el líder histórico del sector, Encyclopædia Britannica, tuvieron lugar a mediados de la década de los ochenta: en 1996 la compañía fue malvendida muy por debajo de su precio de mercado debido a sus dificultades financieras, y a pesar de la enorme popularidad de la marca, ha seguido una estrategia discreta y gris desde entonces, mientras su verdugo, Encarta, caía pocos años después, en 2009, por acción y efecto de Wikipedia, una enciclopedia nacida tan solo ocho años antes. No cabe duda: cuando hablamos de procesos disruptivos, hablamos casi siempre de sucesos que tienen lugar muy, muy deprisa.

La esencia de los procesos de disrupción subyace en la curva de adopción de los clientes. Preste atención a estos procesos: cuando detecte cambios de hábitos rápidos en determinados segmentos de clientes, tenga mucho, muchísimo cuidado: bajo ningún concepto los considere raros, extravagantes o, siguiendo la terminología actual, «frikis».{9} Póngase en su lugar, pruebe a hacer lo que ellos hacen, y lleve a cabo el análisis que hemos comentado, porque la curva de adopción tiene una forma sigmoidea, y tras la fase de la primera adopción, suele emprender un rápido camino ascendente que explica los ejemplos citados anteriormente. Y una vez que la curva empieza su camino ascendente, nada ni nadie puede pararla. El proceso de adopción no lo toma su empresa, por muy líder que sea o crea ser. La decisión de adopción, en el momento en que a usted le resulta por primera vez visible, ya ha sido tomada en otro sitio. Lo único que puede hacer es intentar entenderla, y adaptarse a un cambio que en ningún caso depende de usted ni de su empresa: depende del entorno que los rodea.

Terminemos este capítulo con la última y definitiva prueba de que se encuentran usted o su empresa dentro de la espiral de un proceso disruptivo: padecer una extraña incapacidad para darse cuenta de ello. Efectivamente, para el directivo que presencia uno de estos procesos, todo tiene explicaciones mucho más sencillas, asentadas en experiencias anteriores o en comportamientos que deben ser corregidos. A menudo visualizan oscuras conspiraciones destinadas a hundirles, en lo que podría ser calificado casi como de proceso esquizofrénico: culpan a otras industrias, pretenden compensar sus pérdidas mediante subvenciones y mecanismos inexplicables ante las leyes del mercado, o deducen hábilmente que «los clientes se equivocan» o que «deben ser perseguidos o castigados por no seguir comportándose como lo hacían antes». Vistos desde fuera, o a través del prisma de la historia, las reacciones de los directivos de industrias en declive son tan claramente aberrantes, que no dejan lugar a dudas. Pero por alguna razón en la que seguramente interviene el instinto de conservación, las cosas no se ven igual desde dentro. Por lo tanto, no olvide una última precaución: por buenos, o sobre todo, por bienintencionados que sean sus análisis, es muy posible que sus impresiones estén peligrosamente sesgadas, y que sea incapaz de percibir la disrupción. Si su percepción del proceso se separa drásticamente de la de sus clientes, si observa la creación de bandos a veces fuertemente radicalizados, o si se encuentra con análisis y puntos de vista radicalmente diferentes a los suyos, no piense simplemente que «todos los demás están locos», como el que va por la autopista en dirección contraria y cree encontrarse ante una epidemia de conductores suicidas. Seguramente esté ante un proceso disruptivo. Y sobre todo, recuerde: la pregunta no es si lo va a sufrir o va a dejar de hacerlo: la pregunta es, simplemente, cuándo.

CAPÍTULO 4. LA EVOLUCIÓN DE LA COMUNICACIÓN

«La televisión no podrá con ningún mercado después de los primeros seis meses. La gente se aburrirá en seguida de mirar todas las noches la misma caja de madera.»

DARRYL F. ZANUCK,

director de 20th Century Fox (1946)

La evolución de la comunicación en la especie humana está ligada, precisamente, a lo que define a nuestra especie como humana. La comunicación, en principio, no es un rasgo diferencial de nuestra especie. Los expertos en etología nos dirían que una amplísima variedad de especies se comunican gracias a todo tipo de mecanismos, en algunos casos muy interesantes: las manadas de lobos cuentan con sistemas de comunicación sofisticados que les permiten cazar coordinadamente o señalar categorías que marcan quién tiene derecho a comer primero y quién debe respeto a quien mediante la posición de la cola. Las abejas utilizan complejas danzas para indicar la situación de alimentos, en las que codifican elementos tales como la dirección y la distancia. Sin embargo, la comunicación en especies animales se reduce, en general, a aspectos relacionados con las necesidades fisiológicas, tales como alimentación y reproducción, los más elementales en la jerarquía de necesidades. Hace unos cincuenta mil años, algunas agrupaciones del género Homo empezaron a desarrollar la capacidad de comunicarse verbalmente, de transmitir información mediante un código compartido, un lenguaje. El desarrollo del lenguaje trajo consigo una ventaja evolutiva que la especie humana aprovechó muy bien: además de soportar necesidades elementales como alimentación y reproducción como en el resto de las especies, el lenguaje permitía transmitir información de una manera eficiente de generación en generación, crear un acervo de conocimientos compartidos, coordinarse de manera eficiente para el desarrollo de tareas, organizarse...; el desarrollo del lenguaje supone, como decíamos, una de las principales señas de identidad de nuestra especie. En perspectiva evolutiva, cabe pensar que determinadas agrupaciones humanas fueron capaces de disfrutar de ventajas evolutivas gracias al conocimiento del lenguaje: eran capaces de cazar de maneras más eficientes y sofisticadas, de transmitir el aprendizaje de todo tipo de cuestiones, etc. Una ventaja que, a lo largo de las generaciones, conllevó la consolidación progresiva del lenguaje y el desarrollo de un cerebro más adecuado para su procesamiento y almacenamiento.

El lenguaje verbal, sin embargo, tiene serias y evidentes limitaciones. Para transmitir un mensaje, el emisor y el receptor deben estar en el mismo lugar y en el mismo momento: la distancia solo puede salvarse de manera limitada elevando el tono de voz, una solución que no va más allá de unas cuantas decenas de metros, mientras que el factor tiempo resulta directamente imposible de salvar. Por mucho que yo vaya a un sitio a una hora determinada y pronuncie un discurso o emita un mensaje determinado, para una persona que llegue una hora después resultará imposible captarlo, a no ser que alguien lo haya grabado y lo reproduzca para ella. La comunicación verbal no puede, obviamente, superar tiempo ni distancia, lo que plantea limitaciones de cara a la transmisión de la información.

Precisamente para superar estas limitaciones, el ser humano desarrolló, hace unos cinco mil años, el lenguaje escrito. Los primeros registros de mensajes escritos que se conservan corresponden a textos religiosos en escritura cuneiforme sobre tablillas de arcilla que se cocieron accidentalmente al arder un templo sumerio a manos de invasores que practicaban el pillaje. Durante la mayor parte de la historia antigua, la escritura estuvo únicamente en manos de un estrato social relacionado directamente con el poder, bien con la corte o con los sacerdotes. Esta asimetría en el acceso a los medios de comunicación resulta sumamente interesante, porque se ha mantenido con la evolución de estos: cuando, hace unos quinientos años, Gutenberg diseñó y popularizó el uso de la imprenta, tener acceso a esta para realizar una tirada con un nivel razonable de popularidad era algo relativamente accesible para cualquiera que tuviera el equivalente en términos corrientes a unos diez mil dólares. Sin embargo, el paso del tiempo produjo una asimetría cada vez mayor: a finales del siglo XIX y principios del XX; se calcula que para acceder a una imprenta y llevar a cabo la impresión de un periódico era preciso disponer de alrededor de unos dos millones y medio de dólares. La prensa tal y como la conocemos se había convertido en lo que en gran medida sigue siendo hoy: un medio unidireccional. La base de lectores hace únicamente eso, leer, mientras el canal de retorno se restringe, como mucho, a escribir una carta al director, que además únicamente llegará a publicarse si el editor lo estima oportuno.

Si continuamos con la historia de los medios de comunicación, este componente unidireccional se incrementa: la radio, en sus orígenes, era una tecnología relativamente popular: cualquiera con conocimientos no demasiado sofisticados y herramientas tan simples como un soldador de estaño podía montar una radio en su casa y ponerse a recibir o a emitir sin demasiada dificultad. La proliferación de emisoras de radio era elevada: había estaciones de radio vecinales, de clubs deportivos, de asociaciones, de grupos de amigos, etc., fundamentalmente porque el Departamento de Comercio no podía denegar una licencia de radio a cualquiera que solicitase una. Sin embargo, esta abundancia y permisividad duró poco: en la Radio Act de 1927 se estableció un comité de cinco personas con la responsabilidad de asignar o denegar frecuencias y establecer niveles de potencia. En pocos años, la práctica totalidad del espectro radioeléctrico había sido asignado o subastado, la radio se había convertido en un medio prácticamente unidireccional, y los ciudadanos únicamente podían acceder a unos determinados canales en la banda de los 27 MHz, donde podían, básicamente, mantener conversaciones con otras personas, pero sin la posibilidad de convertirse como tales en medios de comunicación.

La televisión ni siquiera llegó a pasar en su evolución por esa etapa de libertad. Nacida con posterioridad a la regulación del espectro radioeléctrico, las emisiones de televisión provinieron siempre de grupos mediáticos, de poderosas compañías que podían obtener una licencia de emisión. En muchos países, como es el caso de España, la concesión de dichas licencias ha sido en ocasiones polémica, con elementos claramente derivados de favores políticos a cambio de apoyos al gobierno de turno. En muchos países, la existencia de una televisión estatal se interpreta como una herramienta en manos del gobierno para intentar influenciar a los ciudadanos y generar corrientes de opinión favorables a sus posturas, particularmente en época de elecciones. En otros, la gran mayoría, se han generado fuertes procesos de concentración, hasta llegar a acumular la práctica totalidad de las audiencias en un escaso número de cadenas, a veces tan solo dos o tres»

En los Estados Unidos, tres cadenas, ABC, NBC y CBS, acapararon la totalidad del panorama televisivo durante más de cincuenta años, hasta que Rupert Murdoch decidió desafiar su hegemonía demostrando que había sitio para una cuarta lanzando FOX. La televisión, como la radio, también es un medio unidireccional: los ciudadanos no tienen acceso a la emisión, se limitan a ser espectadores o audiencia, un papel claramente pasivo que se refleja perfectamente en la denominación que los televidentes reciben en los Estados Unidos: couch potatoes, o «patatas de sofá». El televidente se sitúa en su salón, en actitud completamente pasiva, y recibe una información ya validada, sancionada: lo que sale en la televisión es cierto, no admite discusión. El mundo del as seen on TV «anunciado en televisión», como forma de garantizar la veracidad, la autoridad, la calidad de cualquier producto. Sin duda, gozar de una de esas licencias proporcionaba una enorme cuota de poder porque daba al emisor el control de un medio no discutido, apenas contestado. Y el negocio era evidente: obtenida la atención de millones de espectadores, todo consistía en interrumpir en contenido que querían ver con breves franjas de contenido que no querían ver, pero que aceptaban sumisos: el fenómeno de la publicidad de interrupción se trasladó desde la radio o desde los periódicos sin ningún tipo de interrupción: distinto medio, pero exactamente el mismo modelo.

La unidireccionalidad de los medios proviene, como vemos, tanto de limitaciones de la tecnología como de la regulación desarrollada por diversas razones, bien prácticas —evitar una saturación caótica de emisiones—, económicas —vender o subastar las licencias generando los consiguientes ingresos para las arcas públicas— o incluso políticas —control de la opinión. Y esa unidireccionalidad se halla profundamente impregnada en nuestra forma de relacionarnos con la información: cuando abrimos un periódico, cuando encendemos la radio o cuando nos sentamos ante la televisión llevamos a cabo actividades invariablemente unidireccionales, anónimas. Nadie en su sano juicio intenta discutir con el locutor de radio o con el presentador de televisión, y si lo hiciese llevado por algún súbito sentimiento, sabe que no tendría la más mínima consecuencia más allá de ser considerado un excéntrico o un loco por aquellos que le oyesen: el canal de retorno simplemente no existe.

La situación se mantuvo así, en la más estricta unidireccionalidad, hasta la llegada de Internet. Desarrollada a partir del trabajo de Robert Kahn, Lawrence Roberts y Vinton Cerf como una solución capaz de generar una red altamente robusta, recibió el más fuerte impulso para su popularización y crecimiento de la mano de Tim Berners-Lee, un científico del CERN que en 1989 propuso y liberó el conjunto de protocolos que daban origen a la llamada World Wide Web, y que derivó en la masiva popularización de Internet durante la segunda mitad de la década de 1990. El origen de Internet, un proyecto financiado con fondos del Departamento de Defensa (DoD), tenía como uno de sus objetivos construir una red robusta capaz de transmitir mensajes entre dos nodos, aunque el camino directo entre ellos hubiese sido destruido, pero, en realidad, todos los integrantes del proyecto supieron desde el primer momento que estaban construyendo algo cuya proyección superaba con mucho el ámbito de lo militar, algo que era susceptible de cambiar muchas cosas.

Son muchos años de medios unidireccionales los que han dado a nuestra sociedad los usos y costumbres que hoy tiene, muchos años de recibir información sin tener la posibilidad de contestar a la misma a través del mismo canal por el que había sido recibida. La producción de información estaba limitada a unos pocos, que eventualmente, dada la asimetría de la situación (pocos emisores, muchísimos receptores), llegaban en muchos casos a convertirse en celebridades, a ejercer una gran influencia y a ser reconocidos por la calle. Mandaban las audiencias enormes, los hits, la popularidad masiva: un evento importante en la televisión, como el momento en que los Beatles actuaron por primera vez en una cadena norteamericana, fue capaz de congregar a un 75% de los estadounidenses delante del programa de Ed Sullivan. En nuestra época, el arquetipo de los eventos mayoritarios, la final de la Superbowl llega, en un año bueno, a un 45%, y es una cifra que desciende cada año.

Siguiendo esta pauta, la primera época de Internet, la de los años noventa, posee todavía un marcado carácter unidireccional: los encargados de producir información eran aquellos capaces de manejar un lenguaje, el HTML, que, aunque simple desde el punto de vista de sus estándares técnicos, no era manejado por el común de los ciudadanos. Los programas habitualmente utilizados para simplificar o gestionar el manejo de HTML, tales como DreamWeaver o FrontPage, tampoco eran de uso común, y mucho menos las habilidades necesarias para controlar, por ejemplo, el almacenamiento remoto en un proveedor de hosting. En estas condiciones, se calcula que en torno al año 1996, Internet estaba compuesto por unos cuarenta y cinco millones de usuarios, que navegaban a través de unos doscientos cincuenta millones de sitios web propiedad, en general, de empresas e instituciones de diversos tipos. La actividad de esos cuarenta y cinco millones de usuarios se limitaba, en la mayor parte de los casos, a consultar información. Heredando el comportamiento desarrollado durante décadas delante de la televisión, los usuarios hacían clic en su ratón para pasar de un sitio a otro como quien cambiaba de canal, y simplemente leían o consumían información en diversos formatos, sin plantearse más interacción con ella. Un escaso número de usuarios de la red creaban contenido en lugares como Usenet, foros, etc., donde permanecía sin tener una vocación demasiado clara de convertirse en referencia o de ser fácil de encontrar. En estas condiciones, la inmensa mayoría de los usuarios utilizaban Internet para acceder a información, mientras eran únicamente unos pocos, generalmente relacionados con empresas y medios de comunicación, los que la producían. El propio diseño de la conectividad en Internet reflejaba el hecho de que la inmensa mayoría de los usuarios bajaban mucha más información de la red de la que subían a ella: la A de las conexiones ADSL proviene precisamente de la palabra «asimétrica»: los usuarios, por norma general, consumían información de la red, pero enviaban a ésta únicamente sus peticiones de información, comandos muy ligeros que se generaban al hacer clic en un vínculo. El camino aguas arriba, en cambio, no era habitual, y estaba únicamente al alcance de empresas que daban empleo a personas con conocimientos tecnológicos.

Pero esta primera etapa unidireccional no representa en absoluto lo que Internet es hoy. Para llegar a la fortísima vocación democratizadora de la red de hoy, que la convierte en un fenómeno completamente diferente al resto de los medios de comunicación, la red vivió, por un lado, un crecimiento exponencial en su número de usuarios, pero, por otro, una serie de importantísimos cambios en su composición, Hacia el año 2006, el número de usuarios en Internet superaba ya los mil millones en todo el mundo, separados no por barreras geográficas, sino por los lenguajes que eran capaces de leer o entender. De los escasos doscientos cincuenta mil sitios web que había en el año 1996, habíamos pasado a más de ochenta millones, con una densidad media de páginas por sitio inmensamente superior.

Pero el principal cambio no era dimensional, sino de naturaleza. El cambio más importante, el que de verdad redefinía Internet como medio y convertía en obsoletos los conocimientos de todos aquellos que habían desarrollado algún tipo de actividad en la web «del siglo pasado», era de otro tipo, y estaba relacionado con el progresivo y rápido desarrollo de herramientas al alcance de todo el mundo, independientemente de sus conocimientos técnicos. Empezando aproximadamente en el año 2000, empezaron a aparecer en la web un cierto tipo de herramientas que permitían que cualquier persona generase una página en la red: en quince minutos, una persona podía crearse una cuenta con herramientas como Blogger o TypePad y, tras señalar unas cuantas opciones, generar una página con un aspecto relativamente sofisticado y profesional en la que empezar a incluir contenido; texto, fotografías, etc., contenido que era, además, hospedado en los servidores del propio proveedor. Las complicaciones y la complejidad eran mínimas, lo que conllevó, a partir de febrero de 2003, con la adquisición de Blogger por Google, una auténtica explosión del fenómeno blog: los usuarios de Internet pasaron a tomar conciencia del tipo de cosas que podían hacer con la red. En muy poco tiempo, se desarrollaron herramientas capaces de hacer que cualquier persona pudiese, independientemente de sus conocimientos de tecnología, utilizar la web para publicar textos, fotografías o vídeo, que alojaba en servicios en muchos casos gratuitos y de manejo absolutamente simple. Esta etapa, que describiremos posteriormente con mayor detalle, fue denominada «Web 2.0», un término que hacía referencia al hecho de asemejarse a una segunda edición, corregida y mejorada, de la Web que Tim Berners-Lee había definido originalmente.

Para muchos, la irrupción de esa Web 2.0 resultó completamente traumática. Es bastante habitual en muchos campos de la actividad humana que los expertos de un tema reaccionen de manera desagradable cuando su conocimiento y saber hacer resulta vulgarizado y puesto al alcance de cualquiera. Y así fue: de la noche a la mañana, cualquier persona podía poner cualquier contenido en Internet, y hacerlo con relativa solvencia si no en su esencia, sí en su aspecto, sin necesidad de saber cómo escribir ni un mísero comando HTML. Para muchos, esto era terrible, algo destinado a «colapsar» Internet, a hacer imposible que pudiésemos localizar información, que quedaría «enterrada» entre tantos contenidos creados por «cualquiera». La Web 2.0 representó una caída tan brusca de las barreras de entrada, que cambió completamente la naturaleza de la web, la convirtió en el primer medio verdaderamente democrático de la historia de la comunicación humana. Y con ello, cambió la manera en la que nos comunicábamos, dando lugar a toda una inmensa gama de posibilidades que aún estamos empezando a explorar.

Las predicciones de los agoreros, las que anunciaban el colapso de la red y la llegada de las diez plagas de Egipto, resultaron ser simplemente falsas, como la inmensa mayoría de los problemas achacados a la tecnología. En su mayoría, provenían de asimilar Internet a un lugar físico, a una especie de «almacén» en el que se guardaban las páginas web y los contenidos, algo incompatible con la naturaleza virtual de la web. El tamaño de Internet excede todos los límites del razonamiento humano: si fuésemos capaces de imprimir toda la información de la red en un libro de tamaño estándar, éste tendría más de tres kilómetros de grosor, pesaría por encima de los quinientos millones de kilos, y tardaríamos 57.000 años en leerlo si leyésemos las 24 horas del día, para encontrarnos con que, aun así, su tamaño se habría multiplicado por un factor impensable antes de terminar.{10} Por otro lado, la hiperabundancia de contenidos se probó como el mejor aliado para los nuevos motores de búsqueda como Google: en una red plagada de contenidos enlazados entre sí, Google era capaz de encontrar una lógica y priorizar su visibilidad de una manera intuitiva, que todos los usuarios podían entender. Transcurridos ya unos cuantos años desde el salto a la popularidad del término Web 2.0, resulta evidente que las historias apocalípticas que se contaron en su inicio eran simplemente eso, cuentos de viejas plañideras, y que la web forma cada día más una parte activa e inseparable de la vida de un número cada vez mayor de personas.

Su nueva dimensión social y participativa dotó a Internet de un dinamismo y un crecimiento todavía mayor que el que había vivido anteriormente. En realidad, a finales de la primera década del siglo XXI, resulta completamente evidente que el éxito de la web como fenómeno comunicativo convierte en inútil cualquier discusión sobre su naturaleza: se trata de un fenómeno que supera todo planteamiento o juicio de valor. De hecho, el crecimiento de la web ha empezado ya a eclipsar a los medios convencionales unidireccionales con una marcada virulencia: en el Reino Unido, durante el primer semestre del año 2009 la inversión publicitaria en Internet superó a la realizada en televisión por primera vez, poniendo fin a una hegemonía que había durado más de cuarenta años. Un crecimiento del 4,6% llevó la inversión publicitaria en Internet hasta los 1.750 millones de libras, un 23,5% del total invertido, superando a una televisión que, con una caída del 17%, se situaba en los 1.600 millones, un 21,9%. En Dinamarca, ese mismo evento había tenido lugar durante el segundo semestre de 2008. La cuestión, en realidad, es tan sencilla como la elección entre un canal unidireccional y otro bidireccional. El que el segundo creciese más que el primero era simplemente una cuestión de penetración de uno frente al otro —la publicidad en Internet no garantizaba la universalidad ni la llegada a determinados segmentos demográficos— y de madurez del mercado publicitario.

A medida que el uso de Internet alcanza un porcentaje cada vez mayor de la población, los anunciantes van dándose progresivamente cuenta de que un canal ofrece más posibilidades que el otro en todos los sentidos, y operan en consecuencia: la publicidad en la red permite segmentaciones infinitamente mejores, impactos de mucha mejor calidad, mediciones extremadamente más fieles y respuestas en muchos casos inmediatas, además de permitir la entrada de anunciantes de todo tipo, incluso los más pequeños: superar a una televisión cuyos formatos no han evolucionado prácticamente nada desde sus inicios y en la que tanto la cualificación de la audiencia como la medición de resultados se realizan mediante someros muestreos era, simplemente, una cuestión de tiempo y de lógica.

CAPÍTULO 5. INTRODUCCIÓN A LA RED. LA NEUTRALIDAD DE LA RED

«Si los proveedores empiezan a privilegiar unas aplicaciones o páginas sobre otras, las voces más pequeñas serán eliminadas, y todos perderemos. Internet es la red más abierta que hemos tenido nunca. Debemos mantenerla como tal.»

BARACK H. OBAMA (noviembre 2007)

Dejemos a un lado todas las historias que habitualmente se cuentan con respecto al origen de la red, a sus inicios como proyecto militar, y al hecho de que todos los participantes en el mismo supiesen desde un primer momento que estaban trabajando en algo mucho, muchísimo más grande que un simple proyecto militar. Desde sus orígenes en 1960 como proyecto de construcción de una red robusta, distribuida y tolerante con los fallos, hasta el resultado actual de su difusión y participación en la vida cotidiana de más de una cuarta parte de la población mundial, Internet ha recorrido un muy largo camino. Un camino en el que, además, no estaba solo: la idea de una red de conexión entre diferentes actores incluyendo proveedores de información, consumidores, comerciantes, etc., estaba ya en la cabeza de muchos en torno a la misma época: CompuServe, por ejemplo, tiene sus orígenes en el año 1969, AOL en 1983, y BITNET en 1981, todas ellas antes del inicio de la popularización real de Internet entre el gran público, que suele adscribirse a mediados de los años noventa. La francesa Minitel fue lanzada en 1982, y obtuvo un notable éxito en el país vecino. Sin embargo, ninguna alcanzó el crecimiento meteórico y la penetración de Internet. ¿Dónde están las claves que convirtieron este proyecto inicialmente poco orientado en un éxito capaz de cambiar el mundo tal y como lo conocemos?

El secreto, en realidad, hay que buscarlo en la esencia más primigenia de la red: los protocolos que la soportan. La palabra protocolo es de esas que tienden a intimidar al usuario no técnico; sugiere algo complejo, inextricable: en realidad, el significado de protocolo no va más allá de ser el de un conjunto de reglas que son utilizadas para la comunicación. En una fiesta en una embajada, por ejemplo, el protocolo incluye una serie de convenciones en el vestir, unos tratamientos y unos modales determinados. Si una persona aparece vistiendo vaqueros rotos y pretendiendo darle un abrazo al embajador, es posible que no pueda hacerlo, y que su ruptura del protocolo impida además sus posibilidades de comunicación en esa fiesta: muy probablemente no sea admitido en ella. En Internet, los protocolos gobiernan cómo se transmite la información: cómo se encapsula en forma de paquetes, cómo se transmiten éstos, cómo se comprueba su recepción, cómo se vuelven a enviar en caso de pérdida, etc. Y el atributo que proporciona a Internet su potencial es, ni más ni menos, que el hecho de que tanto estos protocolos como los de la World Wide Web (WWW), la capa de representación que lo recubre, fueron publicados en abierto y puestos a disposición de cualquiera que los quisiera utilizar. En muchos sentidos, Internet es uno de los proyectos exitosos más grandes del código abierto. De hecho, Tim Berners-Lee obtuvo la inspiración para el desarrollo de la WWW de la observación de un lector de páginas denominado Dynatext que el CERN había licenciado, pero que tenía no solo una licencia muy cara, sino que también requería un pago por cada documento ¡y por cada vez que este era editado! En la mentalidad del creador de la WWW, un sistema así jamás podría liberar el potencial de Internet como forma de crear y diseminar información al alcance de cualquiera. La visión de Tim Berners-Lee era la de una red en la que todos pudieran ser autores, crear documentos, establecer vínculos entre ellos y acceder a ellos con total sencillez.

La idea de Internet y de la WWW, por tanto, fue la de ofrecer sus protocolos para su uso absolutamente indiscriminado: una conexión, un navegador o un editor eran algo que prácticamente cualquiera podía conseguir, y que con el tiempo se fueron haciendo cada vez más ubicuos. Al principio, las conexiones a Internet estaban situadas en empresas y universidades, pero se fueron haciendo progresivamente más baratas y ofreciéndose más a particulares. Originalmente, los navegadores y, sobre todo, los editores de páginas eran relativamente complejos: los pioneros de Internet componían sus páginas directamente en HTML, que sin ser un lenguaje especialmente complejo, tampoco está al alcance de cualquier mortal. Tras la edición en HTML llegaron editores de los denominados WYSIWYG,{11} que acercaban algo más al usuario las tareas de edición tomando la responsabilidad de interpretar la apariencia y traducirla en líneas de código. Y tras éstos, llegaron los blogs, que acercaron verdaderamente la edición y el almacenamiento de páginas hasta ponerlas auténticamente a disposición de cualquier persona sin prácticamente ningún tipo de conocimiento técnico o de programación. Internet es una red en la que cualquiera con acceso a una conexión puede crear algo de manera sencilla, alojarlo gratis o por muy poco dinero, y ponerlo a disposición de cualquiera que pueda encontrarlo conociendo su existencia o a través de un motor de búsqueda. La red no discrimina absolutamente nada con respecto a los contenidos: trata exactamente igual lo escrito por un particular que la creación de una gran empresa. Ambos son paquetes de bits y ambos circulan por la red con los mismos privilegios.

La neutralidad es una característica definitoria y fundacional de la red. Internet es por naturaleza abierto y libre, y lo es para todos, porque estableció sus características en torno a un protocolo que consagraba la más absoluta y radical neutralidad, y que tiene como norma de identidad transmitir ciegamente un mensaje entre un origen y un destino. Si dos personas tienen contratado un acceso a Internet de unas características determinadas, deberán poder enviar y recibir cualquier tipo de contenido sin discriminación alguna a esas velocidades. El tráfico de datos recibido o generado en Internet no debe ser manipulado, tergiversado, impedido, desviado, priorizado o retrasado en función del tipo de contenido, del protocolo o aplicación utilizada, del origen o destino de la comunicación ni de cualquiera otra consideración ajena a la de su propia voluntad. Ese tráfico se trata como una comunicación privada, y únicamente podrá ser espiado, trazado, archivado o analizado en su contenido bajo mandato judicial, como correspondencia privada que es en realidad.

Este principio, la neutralidad de la red, ha estado embebido en el diseño de la misma desde sus orígenes, y ha jugado un enorme y relevante papel en su popularización. La red es lo que es hoy gracias sobre todo al hecho de ser neutral Sin embargo, existen una serie de fuerzas que tienden a intentar convertir la red en algo diferente: bajo el pretexto general del «control», existen actores tales como las empresas de telecomunicaciones, los grupos de presión de la industria de la propiedad intelectual y determinados políticos que intentan, de manera reiterada, alterar la naturaleza de Internet y transformarlo en algo más adecuado a sus propios intereses.

El triángulo de actores implicados en esta pretendida desnaturalización de la red empieza por las empresas de telecomunicaciones; propietarias de la gran mayoría de las infraestructuras por las que discurre Internet, las empresas de telecomunicaciones llevan años viendo como sus servicios se convierten en una gran commodity: mientras ellas se limitan a ofrecer el acceso a sus clientes, asisten al enriquecimiento de quienes hacen negocios en la red. Mientras una página cualquiera gana en función de sus ventas o de la publicidad que exista en ella, los operadores se limitan a cobrar una tarifa en función de un acceso determinado —más rápido o más lento independiente de lo que circula por sus cables.

Los intentos de entrar en el negocio de contenidos por parte de la gran mayoría de operadores han fracasado repetidamente: hablamos de un negocio completamente distinto al suyo, y cada contenido, además, es un contenido más en una amplísima panoplia de contenidos a disposición de los usuarios. Pero el desarrollo de infraestructuras tiene un elevado coste material, y la red demanda anchos de banda cada vez más elevados, obligando a poner fibra óptica donde antes solo había cobre, y a alcanzar, en virtud del principio de universalidad exigido a los operadores en muchos países, zonas geográficas de escasa densidad en las que la rentabilidad resulta claramente negativa. El hecho de que, en muchos casos, partes muy significativas de las infraestructuras se hayan construido en su momento mediante dinero público no deja de eclipsar que ser operador implica participar en un negocio de inversión: invertir en el tendido de cables y antenas no está al alcance de cualquiera. Y para un país, resulta fundamental tener unos operadores dispuestos a mantener infraestructuras capaces de albergar el progreso de la red, lo cual se sostiene únicamente proporcionándoles un nivel de rentabilidad adecuado a sus inversiones.

Ante esta tesitura, ¿cómo armonizar las necesidades de inversión en infraestructura con las de rentabilidad de los operadores, si estos deben limitarse a proveer lo que los norteamericanos llaman una dumb pipe, una «tubería tonta» y completamente insensible a lo que circula por ella? El razonamiento de los operadores en este sentido es «déjame privilegiar el tráfico en función de quien lo envíe, y eso me permitirá rentabilizar mis infraestructuras exigiendo a quien quiera ir rápido, un pago en consecuencia, o privilegiando mis propios servicios frente a los de terceros». Y eso es, precisamente, lo que introduciría un brutal elemento de distorsión en la red: ante un escenario como ese, todo aquel que no pudiese comprar su derecho a una autopista rápida quedaría relegado a que sus datos circulasen por un camino de cabras, e Internet se convertiría en un medio más en manos de las grandes empresas y conglomerados mediáticos que poseerían los canales privilegiados, con la voz de los usuarios escondida en un cajón de sastre de «contenidos que bajan a velocidades insufriblemente lentas».

En el escenario actual, Internet circula por las redes de los operadores, pero se ha convertido en algo demasiado importante como para dejar su gestión en manos de los operadores. Permitir a estos que tomen decisiones que afecten a la naturaleza de Internet supondría desnaturalizar Internet, convertirlo en algo completamente diferente: existen ya operadores que priorizan determinados tipos de tráfico frente a otros, o que penalizan el trafico vinculado a determinados protocolos, prácticas completamente inaceptables y que, de hecho, la Comisión Federal de las Comunicaciones sancionó como tal a principios del año 2009. La elección de Barack Obama como presidente de los Estados Unidos ha dado un fuerte impulso a la neutralidad en la red: como candidato, Obama hizo de la neutralidad de la red uno de los principales puntos de su campaña, llegando incluso a escenificar su fuerte apoyo en un acto en el mismísimo Googleplex, sede de Google, uno de los más fuertes abogados de las tesis de dicha neutralidad. El día 29 de mayo de 2009, en un discurso dedicado a la ciberestructura de la nación, el ya presidente renovó su apoyo con una frase contundente:

Permítanme también ser muy claro acerca de la que no haremos. Nuestra búsqueda de la ciberseguridad no incluirá —repito, no incluirá— la monitorización de redes privadas sectoriales o del tráfico de Internet. Preservaremos y protegeremos la privacidad de las personas y las libertades civiles cuya existencia celebramos como americanos. Es más, mantengo completamente firme mi compromiso con la neutralidad de la red para que podamos mantener Internet como debe ser —abierto y libre.

Las actuaciones del máximo mandatario norteamericano en este sentido tampoco han dejado lugar a dudas. Su elección de Julius Genachowski como director de la Comisión Federal de las Comunicaciones supone un refrendo importante para las tesis de la neutralidad de la red, que se hizo explícito en su primer gran discurso en la Brookings Institution. En él, Genachowski describió la situación actual, en la que ya ha habido ejemplos de ruptura de la tradicional neutralidad y apertura de Internet por parte de algunos operadores, como una encrucijada en función de tres factores fundamentales: por un lado, una competencia entre operadores cada vez más limitada debido a un proceso progresivo de concentración que reduce las opciones de los consumidores. Por otro, los incentivos económicos que dichos operadores tienen para alterar la situación en su favor. Y en tercer lugar, la enorme explosión de tráfico en la red, que impone tensiones en la gestión de las infraestructuras.

La elección en esta encrucijada es la de preservar por encima de todo la naturaleza abierta y libre de la red. Para ello, se añaden expresamente a las cuatro libertades enunciadas en su momento por la FCC en 2004 (los operadores de red no podrán impedir a los usuarios acceder a todo contenido legal, aplicaciones o servicios de su elección, ni podrán prohibir a los usuarios que conecten a la red dispositivos que no resulten perjudiciales para esta), otras dos más: el quinto principio establece la no discriminación, y establece expresamente que los operadores de red no podrán efectuar discriminación contra contenidos o aplicaciones específicas en Internet. Y el sexto, transparencia, que afirma que los proveedores de la red deberán ser transparentes acerca de las políticas de gestión de tráfico en sus redes. Una serie de provisiones que, a pesar de las protestas de los operadores, se impondrán en la gestión de redes tanto fijas como móviles, en las que las tensiones por la capacidad son todavía más fuertes, pero que, como todo en tecnología, están sujetas a mismo ritmo de progreso y desarrollo que procurará, en el futuro, capacidades mucho mayores.

Pero las empresas de telecomunicaciones no son, como decíamos, las únicas que intentan subvertir la naturaleza de la red para convertirla en algo diferente y más propicio a sus intereses. Un segundo poder, el de las industrias de la propiedad intelectual, pretende convertir Internet en un lugar fiscalizado, donde las comunicaciones de los ciudadanos son espiadas constantemente para averiguar si en alguna de ellas aparecen supuestamente obras de sus catálogos. Una pretensión que pone a los derechos de autor por encima de los derechos fundamentales del individuo, y que está siendo progresivamente rechazada por la gran mayoría do los países civilizados con la excepción de la Francia de Sarkozy. Para lograr tal fiscalización, los grupos de presión de la industria no dudan en calificar de delito lo que no lo es, o de relacionar algo tan inofensivo y generalizado como las descargas de materiales de la red con las diez plagas bíblicas que asolaron Egipto: en la interesada visión de la industria cultural, vigilar Internet es necesario para evitar terribles delitos como la pederastia, el terrorismo, la delincuencia o el fraude. Delitos que aunque puedan representar problemas puntuales, no son en absoluto privativos de Internet, ni tendrían solución alguna en el caso de implantarse una Internet en modo «Estado policial» como la industria cultural pretende. Está perfectamente demostrado que la vigilancia y la retención de datos no supone una solución a estos problemas, sino que únicamente provoca una búsqueda de nuevos mecanismos por parte de los delincuentes y se convierte en una manera de vigilar a quienes no lo son. Por muy execrable que pueda resultar un delito como la pornografía infantil, la solución al mismo no es vigilar las comunicaciones de los ciudadanos, porque aquellos que quieren incurrir en el delito buscarán métodos de cifrado más eficientes y la vigilancia servirá únicamente para controlar lo que hacen los inocentes, que carecen de un incentivo directo para intentar eludir dicha vigilancia. Para atacar un delito como la pornografía infantil habrá que hacer lo que se ha hecho siempre: perseguir a quienes la producen, monitorizar los flujos económicos que genera, y aplicar presión policial a los lugares donde se intercambia. Nada nuevo bajo el sol, salvo que las fuerzas del orden tendrán que cualificarse para trabajar en un escenario diferente. Cosa que, por otro lado y, como cabía esperar, hacen cada vez mejor.

La neutralidad de la red no significa pedir que sea gratuita. Significa pedir que siga siendo lo que es hoy, un vehículo de transmisión que mueve paquetes de bits de un lado para otro, a la velocidad de conexión que cada uno decida contratar en ella, pero de manera completamente independiente al contenido de esos paquetes de bits. Significa que los cables sean únicamente cables, sin que puedan tener ningún tipo de capacidad de decisión sobre lo que circula por ellos. Las turbias maniobras de las industrias culturales y la supuestas «coaliciones de creadores» intentando cercenar la libertad de la red se encuentran, por otro lado, con un aliado ideal para sus intereses: una clase política que se encuentra en una fortísima redefinición, y que en muchos casos no ve mal la posibilidad de escalar en su nivel de control. Para los políticos, que trataban antes con un puñado de periodistas y medios, encontrarse de repente inmersos en una red en la que todos tienen derecho a opinar y los tradicionales mecanismos de control resultan inservibles es una especie de pesadilla, por lo que en algunos temas se alinean con la industria cultural y con la de las telecomunicaciones en búsqueda del imposible de una Internet «más controlada». Enarbolando la falsa bandera de la lucha contra la delincuencia o el terrorismo, olvidan la máxima de Benjamin Franklin: «el que está dispuesto a sacrificar libertad a cambio de seguridad, no tiene ni merece tener ninguna de las dos».

La red es un entorno de relación e innovación cuya naturaleza debe ser preservada por todos los medios. Nunca, en la historia de la Humanidad, hemos contado con un arma tan poderosa y de naturaleza tan democrática: con el tiempo, los intentos de cercenar la libertad de la red serán vistos como algo tan grave como los atentados contra la democracia y los derechos humanos, porque de hecho, el acceso a Internet está progresivamente siendo considerado como un derecho del ciudadano. El primer país en consagrar el derecho de los ciudadanos al acceso a Internet mediante banda ancha, marcando incluso una velocidad mínima para ello fue Finlandia, que el 14 de octubre de 2009 estableció que a partir de julio de 2010 todos sus ciudadanos tendrán derecho a una conexión a Internet de al menos 1 Mbps, que pasará a ser de un mínimo de 100 Mbps para finales del año 2015.

En pleno siglo XXI, la red se ha configurado ya como una de las herramientas fundamentales en la interacción y la comunicación de las personas, empresas e instituciones de todo tipo. Para un país, el desarrollo de Internet es clave como motor de desarrollo económico, innovación y competitividad global. Para una persona, mantenerse fuera de la red alegando desconocimiento, miedo o falta de incentivos ha pasado de ser una postura tradicional, a ser directamente ridícula y cavernaria, y a provocar exclusiones cada vez mayores en perfiles personales y profesionales. Los tiempos han cambiado, Internet ha posibilitado el intercambio horizontal de información y de cultura entre todos los ciudadanos, y los medios de producción cultural deben adaptarse a esta nueva democracia y no al revés. Configurar el acceso a Internet como un derecho fundamental no es más que una señal, la que indica que en nuestra sociedad, la red juega un papel cada vez más importante y central. Por eso, por haberse convertido en un punto central de nuestra interacción como sociedad, la red tiene que mantenerse neutral, tiene que seguir manteniendo la característica fundamental que la ha llevado a ser lo que es. En muchos casos, ante los excesos de las empresas de telecomunicaciones, de las industrias culturales y de determinados políticos, la única respuesta será el activismo. Un activismo que estará, en ese caso, plenamente justificado como forma de defender una red que es la clave de nuestro desarrollo futuro como sociedad.

CAPÍTULO 6. LOS COSTES DE TRANSACCIÓN Y COMUNICACIÓN

«Si crees que lo tienes todo bajo control, es que no vas suficientemente deprisa.»

MARIO ANDRETTI,

campeón del mundo de Fórmula 1 (1978)

Entender el impacto de la tecnología, en general, y de Internet, en particular, resulta mucho más sencillo si entendemos su efecto en un aspecto absolutamente fundamental de la economía: los costes de transacción y comunicación. El análisis de estos costes es fundamental, hasta el punto de que fueron una de las claves fundamentales que redundaron en la concesión, en 1.991, del Premio Nobel de Economía al profesor británico Ronald Coase, fundamentalmente por una de sus obras más celebradas: The nature of the firm, que traduciríamos como «La naturaleza de la empresa», publicada en 1937. Un breve ensayo de tan solo dieciocho páginas publicado en Economica, una prestigiosa revista de investigación británica, y que constituye una interesante lectura: pocas veces se encontrará ante una obra de un Premio Nobel de Economía que pueda ser leída con tanta facilidad.

De una manera muy simple, las tesis de Ronald Coase vienen a decir que la razón por la cual constituimos empresas es, básicamente, para obtener una ventajosa reducción en los costes de transacción. Los costes de transacción son aquellos en los que incurrimos cada vez que intentarnos operar en un mercado: para llevar a cabo una transacción, es preciso generalmente incurrir en costes de búsqueda (localizar a los posibles proveedores para un producto o servicio), información (obtener la información adecuada sobre ellos, su registro anterior de transacciones, su historial con otros clientes, etc.), negociación (fijación de los términos de la transacción), decisión (evaluación interna de los términos y comparación con otras ofertas u oportunidades), aseguramiento (supervisar la ejecución de la transacción en los términos en que fue estipulada) o cumplimiento (resolución de contratos incumplidos). Todos estos costes llevan a que, para un determinado número de procesos, la opción preferente sea tratar de internalizar muchos de esos procesos, formalizando así las transacciones y evitando la gran mayoría de los costes: mediante el desarrollo de una organización, y agrupando tareas bajo un mismo techo o marca, se consiguen cosas como establecer mecanismos de autoridad, estandarización de procesos, etc., que logran reducir en gran medida los costes de transacción, permitiendo así la generación de un margen determinado. Un organigrama, por ejemplo, no es más que la expresión de una serie de mecanismos por los cuales se organizan y parcelan los sistemas de toma de decisiones para obtener así un descenso de los costes de transacción. Las divisiones funcionales son, en gran medida, una expresión de lo mismo.

Por supuesto, toda transacción exige un análisis riguroso: en ocasiones, los costes internos que podemos alcanzar en una actividad exceden a la suma de los que otra empresa puede obtener más los costes de transacción implicados en adquirirlo de ella, lo que nos lleva a no intentar hacerlo todo internamente. Las decisiones de tipo make or buy, fabricar internamente o adquirir fuera de la empresa, son una parte habitual del desempeño profesional de muchos directivos de empresas de todo el mundo. Sin embargo, durante años, los costes de transacción eran suficientemente complejos y elevados como para que se justificase prácticamente todo, dando lugar así a empresas con una gran tendencia a la autarquía: un ejemplo de empresa muy utilizado es Kodak, el auténtico paradigma de libro de la integración vertical, comentado habitualmente en todas las escuelas de negocios del mundo. Kodak era, sin duda, una gran empresa con una marcada vocación de gran empresa. En su momento de mayor esplendor, poseía desde minas de plata hasta plantas químicas dedicadas a la producción de los haluros de plata que depositaba sobre celuloide y sobre hojas de papel producidas en sus propias plantas, además de vender los correspondientes revelador, baño de paro, fijador, barnices, pigmentos, humectante y todos los reactivos precisos para el procesado de las fotografías, y sin olvidar las cámaras, los equipos para profesionales y los correspondientes servicios de revelado de negativos, de positivado sobre papel, de máquinas profesionales para tiendas de fotografía, publicaciones, etc. El bagaje de diversificación que esto supone incluye procesos de todo tipo, desde la minería y la metalurgia hasta la química o el manufacturado de todo tipo, introduciendo una complejidad de gestión absolutamente inimaginable.

Durante una buena parte de su historia, el lema de Kodak fue «You press the button, we do the rest»: «usted apriete el botón, nosotros hacemos todo lo demás». Sin embargo, no parece que la estrategia le haya ido a Kodak demasiado bien, ni que haya sido una empresa especialmente flexible o resistente al paso del tiempo: tras haber mostrado una patente lentitud e inadaptación ante el desarrollo de la tecnología digital, su historia reciente es una auténtica espiral de pérdidas y despidos masivos, una especie de crónica triste de una muerte anunciada desde hace mucho tiempo. Para muchos nostálgicos de la fotografía como yo mismo, sería una gran noticia que Kodak volviese a encontrar la ruta de los beneficios, pero, en realidad, todo indica que la empresa es el reflejo de un modelo de gestión, de una forma de hacer las cosas típica de compañías que intentan abarcar lo más posible, reducir hasta el límite las transacciones externas, englobar todo aquello que pueda ser considerado de alguna manera una ventaja competitiva dentro de los muros de la compañía. Un sistema que, en el entorno actual, resulta extremadamente poco eficiente, nada flexible y escasamente competitivo. En los tratados más básicos de estrategia empresarial se habla desde hace mucho tiempo de la llamada «diversificación relacionada»: en el caso de Kodak, la relación entre todas las actividades de la compañía era clara y evidente, pero no era suficiente como para justificar semejante derroche de recursos. Muchos de los negocios en los que se encontraba Kodak eran enormemente intensivos en capital, y tenían umbrales de rentabilidad intrínsecos que no podían ser alcanzados con la dedicación a una pequeña actividad. Pero en una época en la que los márgenes eran elevados, el mercado tenía poca competencia y los costes de transacción y comunicación eran altos, todo estaba permitido.

En su lugar, ¿cuál es el modelo que vivimos hoy en día? Vamos con un ejemplo que vivo muy de cerca: la mayor empresa de medios especializados online en español de Europa y Latinoamérica, Weblogs, S. L., fue fundada por un emprendedor, Julio Alonso, dejando atrás una carrera de diez años en consultoría estratégica. Su fundador había descubierto los blogs a finales de 2003, y al poco tiempo, se dio cuenta de que leer blogs y escribir en el suyo propio le resultaba más divertido y le suponía un mayor reto intelectual que su trabajo de consultor. A mediados de 2004, decidió preguntar en una entrada de su blog personal por posibles colaboradores para iniciar un blog comercial sobre electrónica de consumo, con la intención no de escribir en él, sino de dedicarse a la búsqueda de anunciantes: el resultado, bautizado como Xataka, resultó ser un éxito y alcanzó gran repercusión e importantes cifras de tráfico para la época en pocos meses. A partir de ahí, la empresa industrializó el proceso: partiendo de la inspiración que suponían los quioscos y las revistas especializadas, se dedicó a buscar temáticas verticales que reuniesen los requisitos de tener una cantidad adecuada tanto de lectores como de anunciantes interesados, y a buscar bloggers capaces de escribir con pasión sobre ello. El resultado fue una secuencia de blogs de temas variados: automóviles, cine, juegos, moda, televisión, bebés, vida sana... En cada uno de ellos, unas tres a ocho personas profundamente aficionadas a ese tema (en muchos casos venían de tener blogs personales dedicados a ello) buscaban contenidos y se organizaban para cubrir la actualidad, las cuestiones de interés y las noticias relacionadas.

Cuatro años después, Weblogs, S. L. reúne ya más de cuarenta blogs temáticos que cubren áreas de todo tipo, y factura varios millones de euros en publicidad y servicios a empresas. Su estructura, pasada ya la época de trabajar desde la casa del fundador en Pozuelo, cuenta con diez personas entre el equipo comercial (constituido desde 2008 como una empresa separada, Social Media, para poder así expandir las actividades de venta de publicidad a otros soportes) y el equipo administrativo, en una oficina céntrica en Madrid. La empresa cuenta con unos doscientos bloggers, que trabajan en su gran mayoría desde sus casas distribuidas por toda la geografía española y algunos países de Latinoamérica. Escriben sobre los temas que les apasionan y obtienen, en algunos casos, ingresos que permiten su dedicación plena a la actividad. Además, la empresa tiene un equipo de dirección editorial de cuatro personas distribuido entre Málaga, Mérida y A Coruña, que se encarga de ejercer una labor de coordinación laxa sobre los equipos de cada blog. Cuenta también con un director técnico, un holandés residente en Barcelona, con un equipo de cinco personas a caballo entre Barcelona, Murcia y Manchester, más un equipo de unos diez desarrolladores en Tiyderabad, India.

Cada equipo de bloggers usa diversas herramientas de comunicación, desde una lista de correo compartida hasta mensajería instantánea o telefonía IP. Existe un blog interno de comunicación entre los editores, un wiki para compartir y coordinar material y documentos compartidos en Google Docs. Para el lanzamiento de nuevos proyectos se usa un software de gestión denominado Basecamp y todo el desarrollo tecnológico está coordinado mediante una herramienta de generación de tickets llamada Jira y un wiki corporativo sobre Confluence. La práctica totalidad de las herramientas que se utilizan en la compañía son de código abierto, y las muchas modificaciones que la empresa desarrolla sobre estas para su propio uso son liberadas posteriormente. Y, por supuesto, costes bajos no implican necesariamente ingresos bajos: si bien mucho de los bloggers que trabajan para la compañía lo hacen simplemente con la intención de tener un punto de contacto con los temas que les apasionan y únicamente complementan sus ingresos de otras fuentes con los obtenidos a través de la empresa, no faltan casos de personas que, al encontrarse cómodos con el modelo, acaban dejando lo que se consideraba «su trabajo principal» y se centran en escribir para uno o varios blogs de la red, obteniendo en algunos casos ingresos perfectamente razonables para lo que sería eso que se ha dado en llamar «un trabajador del conocimiento»: algunos de esos casos alcanzan ingresos mensuales medios de varios miles de euros.

Weblogs S. L. es un ejemplo claro de empresa de la era Internet: las tareas derivadas de escribir un blog entre varios autores que trabajan desde sus casas, así como las de insertar en él publicidad de diversos anunciantes o generar nuevos desarrollos tecnológicos para su gestión y visualización requieren necesidades importantes de coordinación. Sin embargo, aunque en las tarjetas de los empleados de Weblogs, S. L. aparece la razón social de la céntrica plaza, de Madrid en la que se encuentran las oficinas, la empresa, en realidad, está distribuida por muchísimos lugares y genera, una estructura de costes infinitamente mis eficiente que la que resultaría de reunir a todas esas personas o a una gran parte de ellas en una o varias redacciones convencionales. La empresa puede mantener un elevado ritmo de creación de varios blogs al año y proyectos para empresas gestionados con casuísticas completamente diferentes, gracias fundamentalmente al hecho de apalancarse sobre una estructura de costes de transacción bajísimos: es una empresa de Internet, que dependió de Internet para su fundación, y depende de Internet tanto para su coordinación interna como para el consumo de sus productos.

Y Weblogs, S. L. no es un caso aislado: en todas las empresas, en mayor o menor medida según su disposición para abrazar el cambio, empiezan a aparecer procesos de este tipo, que aligeran la estructura y proporcionan una mayor flexibilidad a trabajadores, proveedores y clientes. En muchos casos, ni las estructuras de las empresas ni los propios esquemas mentales de los trabajadores están preparados para acomodar el cambio: la flexibilidad de una empresa creada hoy en torno a estas herramientas y entorno es muy superior a la que puede tener una empresa de toda la vida. A día de hoy, vivimos casos de personas que podrían hacer la totalidad o gran parte de su trabajo desde la comodidad de su casa, pero que no lo hacen debido a absurdos convencionalismos sobre la presencia en el lugar de trabajo. A medida que las telecomunicaciones vayan proporcionando experiencias de uso más próximas a la realidad, este tipo de presiones y desfases se irán intensificando. ¿Qué ocurrirá cuando la televisión plana de 50 pulgadas que muchas personas tienen en su casa pueda ser una ventana abierta para reunirse con cualquier otra persona en tiempo y tamaño prácticamente real, o cuando empiece a olvidar qué cosas habló con una persona por teleconferencia y cuáles en una conversación «real»? ¿Es una conversación a través de una pantalla menos «real» que una en la que las dos personas se encuentran a un metro de distancia?

A día de hoy, empresas como Weblogs, S. L. son capaces de organizar flujos complejos de trabajo de una manera hipereficiente gracias a la reducción de costes de transacción y comunicación, y esto ya no afecta simplemente a cuatro empresas vanguardistas, sino que empieza a extenderse entre empresas de todo tipo. Las metodologías que Weblogs, S. L. y un número cada vez mayor de empresas utilizan en su flujo productivo normal contradicen todos los mitos habituales acerca de la conjunción entre trabajo y tecnología: un trabajador de Weblogs, S. L. no tiene un despacho en la compañía, trabaja desde la comodidad de su casa, pero no sufre ataques de soledad, ni se siente desvinculado de la empresa. En lugar de eso, se encuentra en situación de hiperconexión: se coordina constantemente con los otros autores del blog, y tiene, en caso de necesitarlo por la razón que sea, a su director editorial o incluso al director general de la compañía a tan solo un clic de distancia. Comparemos esta situación con la habitual en una gran compañía, en la que el fundador de la empresa es una especie de mito viviente al que un empleado normal no tiene acceso de ningún tipo. La productividad, como es lógico, se incrementa no solo por el hecho de trabajar tranquilamente desde casa (o desde donde buenamente quiera siempre que haya una conexión), sino que evita los tiempos de desplazamiento. La tecnología, en este caso, permite que una serie de personas trabajen haciendo algo que les gusta, desde la comodidad de su casa, y que en lugar de «aislarse» o «sentirse solas», mantengan una vida social virtual —además, lógicamente, de la que mantienen con su familia y amigos— de lo más activa. Por supuesto, esto conlleva una alteración del balance ocio-trabajo, pero de todas las alteraciones que he visto en este sentido, creo que es una de las que más positivas me parecen.

En breve, aquellas empresas que sean capaces de aprovechar este tipo de tecnologías en sus procesos productivos serán, simplemente, más eficientes que aquellas que no lo hacen: ¿qué tal eso como expresión del más puro y descarnado darwinismo digital? La supervivencia, para aquel que mejor se adapta al entorno.

Muchos de los cambios que la tecnología trae consigo vienen, en gran medida, determinados por ese fortísimo cambio en los costes de transacción y comunicación. Empresas como eBay o Amazon establecen su ventaja competitiva sobre dichas reducciones de costes: solo eliminando gran parte de los costes implicados en la subasta o en el proceso de venta de un artículo pueden obtenerse rendimientos que justifiquen las rentabilidades de estas compañías. En el caso de eBay, resulta evidente que antes de la aparición de Internet habría sido completamente imposible llevar a cabo toda una subasta (imagínesela, con su señor impecablemente trajeado empuñando un mazo de madera y todos los posibles compradores presentes en la misma sala) para un artículo de un precio tan bajo como un dispensador de caramelitos Pez. Antes de que existiera Internet, resultaba imposible para una librería ofrecer la práctica totalidad de los libros del mundo, en un formato que cada vez incluye más libros en los que el cliente puede pasar páginas y curiosear, con información completa acerca de su precio y disponibilidad.

Pensemos, por ejemplo, en un periódico: un periódico servía, en cierto sentido, para hacer que la información de una serie de sucesos ocurrida en diferentes puntos salvase los costes de transacción y comunicación y llegase a una serie de personas dispuestas a pagar por el acceso a ella. Para poner en marcha un periódico, era necesario disponer de corresponsales, servicios de agencia, redactores, una imprenta y una distribución. Todos esos procesos se entrelazaban de la mejor manera posible intentando reducir al máximo los costes de transacción y comunicación internas, para que así los lectores, que no tenían acceso a esas reducciones de costes, pudiesen disfrutar de las noticias a un precio razonablemente bajo. Antes de la invención de la imprenta, cuando los costes implicados en la producción y la comunicación eran más elevados, los periódicos eran un producto de lujo, que únicamente ricos mercaderes y políticos podían pagar. Si la imprenta, como comentábamos en un capítulo anterior, supuso la posibilidad de generar muchas copias de manera inmediata y a bajo precio, así como el abaratamiento de los periódicos hasta costar únicamente una gazzetta, ¿qué esperamos que ocurra cuando la información pasa a fluir de manera inmediata y a coste prácticamente cero de un ordenador a otro a través de la web, mediante sistemas como Facebook, Twitter o simple correo electrónico? Creer que el negocio de la prensa o muchos otros va a permanecer inalterados y que los vastos imperios mediáticos constituidos en la segunda mitad del siglo XX van a seguir «manteniendo su papel» es, sencillamente, no tener ni idea de cómo nos afectan los costes de transacción. Y, por supuesto, equivocarse de parte a parte.

La disminución brusca de los costes de transacción y comunicación conforma el cambio más importante que hemos vivido en nuestro sistema económico desde la época de la Revolución Industrial de finales del siglo XVIII, un cambio comparable al de una glaciación. Cuando piense en los cambios que la tecnología provoca a personas y empresas, piense en esta dimensión: hemos vivido un cambio drástico en el entorno, y nos toca adaptarnos a él. Al menos, a aquellos que sobrevivan.

CAPÍTULO 7. LA GENERACIÓN PERDIDA: LA RESISTENCIA A LA TECNOLOGÍA

«No existe ninguna razón por la cual una persona podría querer tener un ordenador en su casa.»

KEN OLSON,

fundador, CEO y presidente

de Digital Equipment Corp. (1977)

Toda revolución, indefectiblemente, conlleva bajas. A veces, esas bajas lo son en toda la extensión de la palabra, y aparecen de una forma tan tangible como cadáveres ensangrentados en las calles, que convierten en afortunado a aquel que únicamente los tiene que ver desde la lejanía de la pantalla de su noticiero. En otras ocasiones, las bajas se escenifican como personas obligadas a un cambio de vida drástico, brutal, que tambalea todas sus referencias anteriores, como en el caso de los campesinos y labriegos de la revolución industrial que acabaron emigrando a los barrios periféricos de las ciudades tras tantas generaciones de vidas dedicadas al cuidado de los campos: de repente, las habilidades necesarias para proporcionar sustento a su familia eran otras que ellos no poseían, y la mecanización había hecho que en el campo sobrase una ingente cantidad de personas, abocadas a una redefinición dramática de sus vidas, a desenvolverse en un lugar inhóspito y con reglas completamente diferentes a las que habían conocido hasta ese momento. Ned Ludd, según algunas fuentes llamado Ned Ludlam o Edward Ludlam, era un operador de la industria textil inglesa que, en 1779, cegado por un ataque de rabia tras haber sido reprendido por su bajo nivel de productividad, rompió a martillazos dos grandes telares de la empresa en la que trabajaba. La historia, contada y exagerada en innumerables ocasiones, fue ganando cuerpo hasta que, alrededor de 1810, su figura fue utilizada para simbolizar los movimientos obreros en contra de una mecanización y unos avances tecnológicos que amenazaban con dejarlos sin trabajo o reducirlos al papel de autómatas. Estos movimientos tomaron en su honor el nombre de «luditas» y firmaban sus cartas con seudónimos como Capitán Ludd, General Ludd o Rey Ludd. En realidad, los luditas protestaban contra la introducción de un nuevo tipo de telar que permitía que personas sin ningún tipo de entrenamiento pudieran realizar la tarea de artesanos textiles especializados, un tipo de protesta recurrente a lo largo del tiempo y que caracteriza uno de los dos extremos de la resistencia al progreso de la tecnología: el de aquellos que sienten que la nueva tecnología permite que otros hagan algo que anteriormente a ella solo ellos podían hacer.

Históricamente, este tipo de resistencia se ha manifestado en un ingente número de ocasiones: casi todas las tecnologías cuentan con detractores de este tipo, con obvios y patentes conflictos de intereses, que menosprecian la nueva tecnología y a quienes la utilizan, y la convierten en responsable de todos los males de la Humanidad: los daguerrotipos ofendían a los retratistas del mismo modo que la imprenta a los copistas o el ferrocarril a los carreteros, porque amenazaba su trabajo y su modo de vida Dentro del mundo de la tecnología, el telégrafo fue criticado porque «demasiada velocidad haría que las personas no tuviesen nada que decirse», y el teléfono lo fue porque «nada era suficientemente importante como para no poderse comunicar mediante el telégrafo, que además dejaba un registro escrito, mientras que el teléfono no se podía grabar». Protestas tal vez comprensibles, pero completamente estériles: jamás en la Historia una tecnología se ha detenido debido a las protestas de quienes utilizaban o dominaban la tecnología anterior a la que sustituía. A pesar de esa obvia realidad histórica, toda tecnología cuenta con detractores y genera resistencias enconadas: por un lado, como afirma el refranero, no se aprende en cabeza ajena. Y por otro, conseguir un retraso, aunque sea leve o incluso percibido, en la difusión de determinadas tecnologías, puede suponer cuantiosas ganancias para algunos.

Un ejemplo patente es la actual resistencia de la industria de la música ante el avance de las tecnologías P2P, que permiten la distribución de copias de obras de cualquier tipo a través de la red independientemente de que se hallen protegidas o no por derechos de autor. La abundancia de recursos a disposición de la industria le permite armar y desarrollar un poderosísimo lobby de poder, capaz de influir en las decisiones de todo tipo de estamentos: la dialéctica de la industria, unida a la evidencia de que la industria dependiente de los derechos de autor es uno de los principales generadores económicos de un país como los Estados Unidos, han llegado a condicionar incluso el nombramiento del vicepresidente del Gobierno, Joe Biden, bajo cuyo mandato se sustituyó la cúpula del Departamento de Justicia con más de seis ex-abogados de la Recording Industry Ass. of America, lo que supuso un fuerte endurecimiento de las penas impuestas a las personas denunciadas por descargas de música. En una patente demostración del fortísimo poder de este tipo de asociaciones de defensa de intereses privados, la todopoderosa Organización Mundial del Comercio (WTO) llegó hasta el punto de condicionar la entrada de Rusia en la organización al cierre de la página web AllofMP3, dedicada a la venta de música en formato MP3, y que cumplía religiosamente con la legislación de su país. En otros países, como Francia, se ha llegado hasta el punto de legislar en contra de derechos fundamentales de los ciudadanos tan importantes como el secreto de las telecomunicaciones: en el país vecino, los proveedores de acceso a Internet espían a los usuarios, y los denuncian cuando detectan descargas sujetas a derechos de autor. En España, la fuerza de estas asociaciones se escenificó cuando unas semanas antes de la disolución de las Cortes previa a las elecciones del 9 de marzo de 2008, una serie de reuniones secretas en casa de un conocido miembro de la directiva de la Sociedad General de Autores y Editores (SGAE) determinó la aprobación por la vía rápida de un canon dedicado supuestamente a «compensar» a la industria y a los autores por las descargas de sus obras, canon que gravaba una amplia variedad de soportes físicos tales como CD, memorias USB o discos duros, y que suponía una infusión de miles de millones de euros desde el bolsillo de los consumidores en manos de una asociación que los repartía de manera completamente arbitraria.

Sin embargo, a pesar de la pujanza de estas asociaciones y de su proximidad al poder político, ninguna de sus acciones ha condicionado un descenso del uso de tecnologías P2P. En países como los Estados Unidos, en donde una persona puede ser sometida a demandas cuantiosas por descargarse una obra sometida a derechos de autor, el uso de redes P2P ha aumentado de manera consistente durante los últimos diez años, siguiendo una serie temporal completamente independiente a las acciones y esfuerzos de la industria, e igualmente ha ocurrido en prácticamente todo el mundo. En la actualidad, empieza a ganar un consenso cada vez mayor la idea de que la legislación que protege los derechos de autor no está adaptada a los tiempos actuales, y que es preciso una reforma de la misma que evite la criminalización de un acto, la descarga y generación de copias, que está dentro de los usos y costumbres de la práctica totalidad de los usuarios de la red.

En otras ocasiones, el destino de los ataques son los nuevos modelos de negocio que la red propone, que son invariablemente calificados de ruinosos, imposibles o engañabobos, a menudo recurriendo a la comparación con la denominada «burbuja de Internet» de principios de siglo. Indudablemente, lo ocurrido a principios de siglo supuso un colapso económico de primera magnitud en el que infinidad de inversores perdieron su dinero, pero interpretar esa circunstancia con una pérdida de validez de Internet como medio supone un cortoplacismo total: en todo ecosistema nuevo surgen excesos de confianza, abusos, planteamientos absurdos y extrapolaciones a veces arriesgadas, sin que ello suponga necesariamente una, quiebra del sistema.

Entender lo que Alan Greenspan calificó en su momento de «exuberancia irracional» resulta relativamente sencillo mirando los reportes y análisis producidos en la época, que predecían para escenarios de dos o tres años crecimientos que todavía hoy, casi diez años después, no se han producido. No estar en Internet o estar presente de manera «tibia» significaba una caída automática de la cotización de las acciones de una empresa, lo que generaba un mercado de falsos profetas dedicado a llevar a aquellas empresas necesitadas de pruebas de concepto a la web fuera como fuera. La aproximación del director que, tras un viaje en avión en el que se tropezaba con una revista que mencionaba alguna acción de un competidor en la red, llegaba a su despacho y convocaba a todas las fuerzas vivas al grito de «¡tenemos que estar en Internet inmediatamente!» se hizo tristemente famosa: muy pocas de las empresas que se iniciaron en la web con este tipo de principios lograron de verdad hacer algo mínimamente aprovechable. La comparación que mejor hace justicia a aquella época, seguramente, es la de la fiebre del oro californiana: todo el mundo sabía que había oro, aunque no sabía dónde, y los que realmente se hacían ricos eran los que vendían picos, palas o supuestos mapas para poder ir a buscarlo. Surgió toda una nueva casta de analistas que, amparados por metodologías de investigación completamente discutibles, daban al mercado aquello por lo que el mercado quería pagar: informes que esgrimir ante terceros para defender inversiones o compras necesarias para mantener el desmesurado crecimiento de un mercado que no existía, en realidad, más allá de las páginas de esos mismos informes.

A estas alturas, descubrir que las burbujas económicas existen y suponen en ocasiones un grave perjuicio económico para algunos sería realmente algo poco novedoso. Lo importante, en este caso, es ir un poco más allá en el análisis, y evitar el «efecto péndulo» que surgió en los años inmediatamente posteriores, que supuso una congelación casi total de todas las inversiones en tecnología. En la actualidad, cuando Internet supone ya una parte muy importante de la economía de muchísimos países, justificar la crítica a los modelos de negocio en la red con la excusa de la burbuja empieza a tener tintes nostálgicos, de recorte de periódico amarillento y ajado por el paso del tiempo. Sobre todo teniendo en cuenta que los modelos de negocio actuales ya no se utilizan para justificar inversiones millonarias ni desmesuradas cuentas de gastos suntuarios, sino para poner a funcionar empresas reales, con inversiones realizadas con un criterio completamente pegado al suelo. Los inversores actuales reclaman dosis de realismo muy superiores, no admiten excesos ni lujos, y exigen cumplimientos claros de objetivos pactados: la época de «el papel lo aguanta todo» parece ya una cosa del pasado. Sí existen, por supuesto, proyectos en los que la generación de ingresos se da de maneras muy poco convencionales: empresas que no venden nada, que viven de la publicidad (negocios grandes y de toda la vida como la televisión o la radio también lo hacen) o que exploran caminos interesantes dentro de la llamada «economía de la atención». Y que es bueno que así sea, porque en muchos casos representan la exploración de nuevas avenidas de generación de valor que serán aprovechadas más adelante.

Aquellos que se resisten al avance de la tecnología negando su importancia, en función de perjuicios a su modelo de ingresos o a su modo de vida, son, por razones evidentes, los opositores más virulentos y feroces. Se consideran objeto de un ataque injustificable, consideran las reglas de su modelo de negocio como algo inmutable, que debe ser protegido por encima de todo, como si la tradición fuese un modelo de negocio. Según ellos, quienes utilizan la tecnología merecen calificativos denigrantes, infamantes, cuyo uso se intenta extender al resto de la sociedad mediante el recurso a los medios de comunicación, en un reflejo propio de quienes se consideran en posesión de la verdad. Sus actitudes son violentas: no intentan comprender o justificar la actitud de los usuarios de la tecnología, sino que mantienen una postura descalificatoria, como cuando se reacciona contra algo que amenaza tu forma de vida. Para defenderse contra tal amenaza, están dispuestos a invertir importantes esfuerzos y recursos económicos: constituyen potentes lobbies y grupos de presión, recurren a los mejores abogados y asesores, y desarrollan accesos preferentes a los medios de comunicación. Para este grupo, sin duda, el fin justifica el uso de todos los medios, pero su actividad se enfrenta a la llamada Ley de Grove, enunciada por Andy Grove, uno de los históricos fundadores de Intel: «la tecnología siempre gana. Puedes retrasar la tecnología mediante interferencias legales, pero siempre huye alrededor de las barreras legales».

El segundo grupo refractario a la tecnología no proviene de la defensa de los propios intereses, sino de un enemigo mucho más peligroso: la falta de recursos. Se trata de la llamada «división digital», una línea imaginaria que separa a quienes pueden acceder a un ordenador y a una conexión de datos de aquellos que no pueden. Aunque toda tecnología suele tender al abaratamiento y a la generalización de su uso, en todas las sociedades existe un determinado estrato social que permanece al margen debido a la falta de recursos, con una incidencia lógicamente mayor en países con rentas per cápita más reducidas. En algunos casos, la falta de recursos se combina con una escasa disponibilidad de alguno de los factores imprescindibles para el uso de la tecnología: en algunos países con despliegues de conectividad importante y generalizado, es posible encontrar «zonas de sombra» que provocan marcadas exclusiones. Aunque Internet avanza hacia la consideración de servicio universal al que todos los ciudadanos deben tener acceso, en varios países abundan zonas en las que no es posible obtener una conexión sin optar por métodos especialmente onerosos, inalcanzables para el ciudadano medio.

La imposibilidad o la dificultad de acceso a la tecnología por falta de recursos económicos genera en muchos casos una sensación de ansiedad: la percepción «desde fuera» provoca que «la hierba parezca todavía más verde al otro lado», y que se llegue a una cierta idealización de la tecnología que puede llegar a resultar frustrante. En otros casos, sin embargo, la racionalización de la imposibilidad de acceder provoca un cierto hastío, un desprecio que permite la autojustificación (tengo conciencia de mi situación de exclusión, pero la sobrellevo más fácilmente porque no me interesa), que entronca con nuestra tercera categoría de actitudes negativas: la indiferencia.

La indiferencia es uno de los grupos habitualmente más numerosos en muchos países. En España, el desinterés supone el porcentaje más elevado de las razones para no conectarse a Internet: personas que son incapaces de ver estímulo alguno en las infinitas posibilidades de la red, o que se refugian y autojustifican en los infinitos y bien alimentados tópicos que la rodean. El examen cuidadoso de esos tópicos revela, en realidad, mucho más que simple ignorancia. El hecho de que resulte casi imposible escuchar la palabra Internet en la práctica totalidad de los informativos de radio y televisión sin que aparezca rodeada de connotaciones negativas se relaciona con el hecho de que dichos medios han detectado, desde hace mucho tiempo, dinámicas de sustitución en el consumo de los usuarios: Internet es, en la práctica, uno de los peores y más temidos competidores de medios convencionales como la televisión. El hecho de que las cifras desmientan tales advertencias de peligro no es óbice para que un amplio porcentaje de la población, especialmente aquella que recibe la mayoría de la información a través de otros medios, siga creyendo en mitos que afirman que Internet es un lugar peligroso donde constantemente roban tarjetas de crédito y donde la delincuencia campa por sus respetos cual si fuera el salvaje Oeste.

¿Qué hay de verdad en la supuesta peligrosidad de Internet? Obviamente, todo en esta vida tiene sus peligros, incluyendo el caminar por la calle o entrar en el banco de la esquina. Nada nos garantiza que no vayamos a ser asaltados cuando extraemos un reintegro de un cajero automático, que no vayan a atracar a mano armada la tienda en la que nos encontramos, o que no nos vayan a timar cuando nos venden algo. Dichos sucesos, además, aunque pueden tener un componente aleatorio, no lo son completamente: es bien sabido que determinadas ciudades o barrios tienen índices de delincuencia superiores a otros, y que ciertos comportamientos, como la ostentación de riqueza o la excesiva confianza en los extraños incrementan la posibilidad de ser robado o timado, Internet es exactamente igual: hay barrios buenos y barrios malos, sitios con garantías y sitios desconocidos, prácticas buenas y malas. Simplemente, en la calle llevamos más años adquiriendo familiaridad con el entorno, mientras que, en Internet, muchas cosas están aún por definir. Pero en general, la mayor parte de los problemas de seguridad se rigen por el puro sentido común: lo que parece demasiado bueno para ser verdad, suele ser porque es demasiado bueno para ser verdad. Si respondes a un mensaje que te promete una herencia millonaria de un desconocido o un pago por ayudar a alguien a sacar dinero de una república africana corrupta y le das tus datos bancarios, te habrán timado. Si escribes el número de tu tarjeta de crédito en un correo electrónico o en una página web sin cifrar, tienes muchas posibilidades de que alguien lo utilice posteriormente para transacciones fraudulentas que aparecerán en tu cuenta como compras hechas por ti, algo que, en virtud de la experiencia acumulada hasta ahora, ocurre aproximadamente con la misma probabilidad en Internet que fuera de Internet. Y que, además, si ocurre, provocará como mucho un pequeño susto y un engorro temporal que terminará, en la práctica totalidad de los casos, con la devolución de todo el importe sustraído en un plazo relativamente corto de tiempo. ¿Te pueden robar en Internet? Por supuesto, pero en la calle también, con parecida probabilidad.

La mayoría de los problemas de seguridad derivan claramente del desconocimiento. Si caminando por la calle se nos acercase un desconocido y nos pidiese las claves de nuestro banco, no se las daríamos a no ser que mediasen amenazas serias a nuestra integridad tísica. Sin embargo, un correo electrónico reclamándonos nuestras claves por un supuesto «problema de seguridad en nuestro banco» hace que un cierto porcentaje de gente, afortunadamente cada vez menor, conteste a dicho mensaje y facilite dicha información sin pensárselo, ignorando que generar un mensaje con apariencia legítima resulta sumamente sencillo en Internet. La conocidísima «estafa nigeriana», «fraude 419» o advance-fee fraud proviene de los años ochenta, del declive de una economía nigeriana basada en el petróleo, y procede, a su vez, de un timo de principios del siglo pasado conocido como «el prisionero español»: en esta estafa, el timador pide ayuda a la víctima para sacar de algún país en situación políticamente complicada una enorme suma de dinero, que será ingresado en la cuenta de la víctima a cambio de un porcentaje de la misma. Cegado por la posibilidad de una ganancia astronómica, la víctima proporciona detalles de su cuenta y realiza pagos al estafador para que pueda llevar a cabo diversos trámites, que invariablemente se complican y requieren más dinero. Semejante burdo esquema, similar a lo que podría ser el timo de la estampita o el del toco-mocho en plena calle, pero organizado con gran profesionalidad en lugares como Nigeria, Benin, Sierra Leona, Costa de Marfil y otros países africanos, se considera uno de las estafas que sigue funcionando mejor en Internet, ostenta el récord en cantidad obtenida por operación, y se beneficia además del hecho de que un elevado número de víctimas, por pura vergüenza, nunca llegan a denunciar lo sucedido.

Todos los años, un cierto número de personas caen víctimas de estafas que, a través de correo electrónico, les piden los detalles de su cuenta bancaria para ingresarles un premio de lotería de algún país extranjero..., en el que por supuesto, nunca jugaron nada. O hacen click en anuncios que les prometen que han obtenido un gran premio por haber sido el visitante número un millón de una página web. El fenómeno del spam, correo basura o no deseado, se alimenta de porcentajes de respuesta de uno de cada doce millones y medio de correos enviados, y se beneficia del uso de millones de ordenadores desprotegidos e infectados con virus que son utilizados para enviar dichos correos sin que sus propietarios lo sepan. En realidad, la mayor parte de la inseguridad atribuida a Internet, que aún a pesar de todo, repetimos, no es mayor que la correspondiente a caminar por la calle, proviene del mal uso que de la red hacen las personas ignorantes. Internet, en cierto sentido, es como una calle por la que circulasen un elevado porcentaje de personas que desconociesen las normas más básicas de circulación y civismo. Afortunadamente, a partir de un cierto nivel de cultura de uso, esos problemas se convierten en insignificantes y en algo que resulta muy sencillo ignorar.

Cuando éramos pequeños, el hecho de que la calle fuera un entorno no completamente seguro no hizo que nuestros padres nos mantuviesen alejados de ella, porque ello habría significado llegar a la edad adulta sin ninguna preparación para movernos por ella con garantías. En su lugar, se dedicaron a decirnos lo que debíamos o no debíamos hacer: «no cruces sin mirar», «no te metas por callejones oscuros», «no hables con desconocidos», etc. Debemos entender Internet como un entorno en el que se va a desarrollar una parte significativa de nuestra actividad queramos o no, y en el que hay que adquirir la soltura suficiente como para encontrarse seguro, sabiendo, como en la calle, que la seguridad total no existe. Y que para los medios de comunicación, el hecho de que todos los días tengan lugar millones de transacciones en Internet sin el más mínimo problema no constituye una noticia, mientras que el hecho de que a alguien le roben el número de su tarjeta de crédito sí que lo puede constituir. Hay asesinatos todos los días, pero el día en que una chica es asesinada y los medios descubren que se relacionaba con su asesino —y con muchas personas más— en una red social, es el momento de decir que las redes sociales son peligrosísimas, un verdadero invento de Satanás. Y, curiosamente, personas a las que se les supone una inteligencia razonable y que reaccionarían con estupor si alguien afirmase que la culpa de un asesinato en la calle la tiene la propia calle, se tragan la noticia de los medios sin pestañear.

La resistencia a la tecnología basada en la ignorancia y la indiferencia resulta enormemente triste y difícil de superar: la detección de la situación de carencia cada vez que el tema es mencionado provoca una reacción de autojustificación, que lleva a los afectados a proclamar su condición de ignorantes como si fuera un honor, casi enorgulleciéndose de ello. La perspectiva les lleva a ver a los usuarios de Internet como a personan «raras» y «extravagantes», a las que acusan de prescindir de la «vida real» para refugiarse en la pantalla del ordenador, en una «vida virtual» supuestamente vacía y ausente. La ignorancia y la indiferencia se alimentan de tópicos, como el que ve al usuario de Internet como a una persona generalmente obesa, asocial, pálida, que únicamente expone su piel a la luz que genera el monitor de su ordenador. El usuario es una persona «con demasiado tiempo libre», sin vida propia, aislado, que solo se relaciona a través de la red con otras personas tan raras como él. En realidad, el afectado se encuentra en una situación carencial: su incapacidad para entender una realidad que cada día le rodea de una manera más clara le provoca una sensación de inseguridad que le lleva a afianzarse en sus problemas, a considerarlos un elemento imposible de superar, y, como reacción defensiva, en un elemento que no quiere superar, un lugar al que no quiere ir. Cada conversación en la que surge la red se convierte en un episodio de negación, de recurso a los tópicos más absurdos para negar su importancia, en un aislacionismo cada vez mayor, en una cada vez más patente reducción al absurdo.

La mejor forma de entender la red para aquellos que se encuentran en este tipo de situaciones es aceptarla como algo completamente natural. Pensar en la red como en un fenómeno social más, como en un «andar por la calle», e intentar interpretar lo que en ella ocurre de una manera completamente natural, por extrapolación de lo conocido. La red es una realidad en construcción, en permanente evolución vertiginosa, y la única manera de aprender a usarla es ni más ni menos que usándola, evitando clichés y lugares comunes, y tratando de experimentar las cosas por uno mismo, en primera persona. Si no lo ha hecho, propóngase hacerlo lo antes posible. La red es un recurso ilimitado en el que encontrará absolutamente de todo, vale la pena probarlo.

CAPÍTULO 8. UNA NUEVA GENERACIÓN

«Chicos, salid ahí fuera y haced lo que sabéis hacer...»

TELÉ SANTANA,

entrenador de fútbol brasileño (1994)

Es, sin duda, uno de los factores que más inquietud producen en todos aquellos que observan la tecnología y sus efectos con cierta aprensión: ver el uso que de ella hace la generación siguiente, sus hijos, sus sobrinos, sus primos más pequeños. Observar a un niño de seis o siete años manejándose con un teclado con total soltura, entrando en páginas para localizar vídeos o juegos que conoce o que le han contado que existen, o comunicándose mediante mensajería instantánea es un proceso fascinante: los adultos estamos, por lo general, acostumbrados a ser quienes sabemos manejar las cosas, quienes enseñan a los niños, no al revés. Sin embargo, la tecnología lleva tiempo provocando una reversión en el flujo del conocimiento: desde el horrible trauma que muchos sufrieron al ser incapaces de programar un simple vídeo doméstico que su hijo manejaba casi con los ojos cerrados, muchos empezaron a darse cuenta de que la gama de habilidades con las que contaban los niños eran, en realidad, bastante diferentes a las que poseían ellos. Los teléfonos móviles han traído a muchos padres la última gran revelación: de repente, personas que creían saber utilizar sus terminales móviles perfectamente pasaban a sentirse como si tuvieran «los dedos mucho más gordos» que sus hijos, provistos de un manejo que ellos eran completamente incapaces siquiera de imaginar alcanzar. A día de hoy, como hemos comentado en un capitulo anterior, si mi padre tiene cualquier duda con su ordenador, me llama a mí para que le ayude. Pero si su duda es con su teléfono móvil, directamente me salta, y se dirige a mi hija de quince años, que además, es perfectamente capaz de ayudarle sin lugar a dudas mucho mejor que yo.

¿Qué factores determinan el manejo que de la tecnología hacen las nuevas generaciones? ¿Y qué tipo de actitudes debemos mostrar ante ello? Las respuestas a este tipo de preguntas resultan un reto sobre todo para aquellas personas a quienes la tecnología les genera miedos e incertidumbres: por un lado, perciben una propuesta de valor clara en el hecho de, por ejemplo, criar a un hijo con un elemento de proximidad a la tecnología que le puede resultar un activo valioso en su desarrollo futuro. Por otro, ven peligros por todas partes, y les preocupa el hecho de que, en conciencia, muchos de esos peligros pueden provocar situaciones que ellos mismos no sabrían manejar. La sensación, en muchos casos, llega a ser bastante agobiante y genera inquietudes, o por el contrario, se relaja excesivamente hasta el punto de utilizar al ordenador como una especie de baby-sitter, como un lugar en el que dejar al niño durante períodos de tiempo «para que no de la lata»: un papel asumido, en muchos casos, por ese fantástico invento denominado «consola» que tantas plácidas estancias en restaurantes brinda cada día a padres de todo el mundo.

La aproximación correcta a la interacción entre tecnología y niños es, sencillamente, cuanto más, mejor. La infancia es una edad ideal para el aprendizaje: la frase «los niños son esponjas» no es simplemente una forma de hablar, sino que indica una manera de internalizar los conocimientos, de asimilarlos de una manera natural en el cerebro, algo que posteriormente acaba representado una enorme ventaja. Y más cuando si hay algo que sabemos seguro es que esos niños acabarán viviendo en un mundo mucho, muchísimo más interconectado que el nuestro. En principio, haciendo lógico uso del sentido común, ponga a sus hijos en contacto con el ordenador lo antes que pueda. No se trata, por supuesto, de abandonar un teclado o un ratón en las manos de un niño de uno o dos años, que podría desde tragarse una tecla hasta asfixiarse con el cable, pero sí, por ejemplo, de empezar a enseñarle contenidos adaptados a su edad desde esa edad, de manera que vea el ordenador como una parte más de su mundo. Sin ánimo de suplir el papel de un pedagogo, los dos años no son una mala edad para que un niño empiece a mover un ratón y a darse cuenta de la correspondencia que se establece entre el movimiento de su mano y el del cursor sobre la pantalla: existen programas para pintar o para registrar sonidos que pueden hacer que un niño de dos o tres años pase ratos entretenidísimo delante de una pantalla, obviamente bajo supervisión. La colección de grabaciones de mi hija entre los dos y los cuatro años, y la primera casita reconocible como tal que pintó a los tres forman una parte entrañable de mi bagaje de recuerdos. Es conveniente, además, introducir en el paquete algún programa de comunicaciones como Skype, Gizmo u otros afines, que permitan a los niños ver aparecer a sus abuelos o tíos en la pantalla y hablar con ellos: les permite tomar conciencia del papel del ordenador como herramienta de comunicación.

A medida que los niños van creciendo, pasamos típicamente por la etapa de los juegos interactivos: proporcionan una experiencia controlada y permiten la adquisición de habilidades más finas, en muchos casos combinadas con el desarrollo de lectura y escritura. Sin embargo, la iniciación a la web surge cada vez más pronto, unida a una gama de propuestas de valor que aumenta constantemente: páginas especializadas para niños, contenidos atractivos, vídeos... Y con la web, surgen los primeros problemas: ¿qué hacer ante una red que vemos repleta de contenidos claramente inadecuados para una mente infantil, o en la que los informativos de la televisión nos dicen que campan terribles delincuentes dispuestos a intentar atacar a nuestros hijos? La interacción entre niños y web plantea, sin duda, un reto importante a padres y educadores.

La respuesta es ni más ni menos que la aplicación del sentido común. En el acceso temprano a la web de los niños no hay más que ventajas: en la sociedad que se está conformando, saber moverse por la web es equivalente a saber caminar por la calle, algo cada día más necesario. La clave en el tema es entender que el acceso de los niños a Internet a día de hoy no es cuestionable: es necesario. Es incluso más que recomendable que invierta la pequeña cantidad de dinero necesaria para adquirir una dirección web con el nombre y apellido de sus hijos, un regalo que en el futuro tendrán, sin duda, oportunidad de agradecerle. Pero, sobre todo, es indispensable entender que Internet no muerde: existen comportamientos que conllevan cierto riesgo, pero no más allá de lo que puede ocurrir cuando caminamos por la calle o cuando los llevamos al colegio. Lo indispensable es mantener en todo momento una actitud adecuada, no interpretar Internet como un sitio en donde «se deja» a los niños, y crear un clima de confianza, en el que el niño pueda preguntar absolutamente cualquier duda que le surja en su manejo.

Una tarea que corresponde a una persona, no a un programa, por sofisticado que sea. De hecho, una de las peores ideas que puede tener es la instalación de un programa filtro o cibernanny: erróneamente impulsados por muchas instituciones, los programas de este tipo contribuyen a crear una falsa impresión de seguridad en los padres, que proceden a su instalación y al inmediato abandono de los niños delante de la pantalla con la conciencia aparentemente tranquila. En realidad, el programa provoca la percepción de una «falsa Internet» en la que los contenidos considerados «nocivos» tales como pornografía están normalmente ausentes, lo que produce que, en el momento en que el filtro falla, es desconectado, o los niños simplemente navegan en un ordenador que no lo tiene instalado, se enfrenten de repente a una serie de contenidos para los que no han sido preparados, y que además llaman muchísimo más su atención debido a la novedad.

En su lugar, a los padres les toca un papel muy diferente: aceptar que sus hijos van a tener delante de sus ojos, cuando busquen determinadas cosas, una serie de imágenes que ellos jamás tuvieron la oportunidad de ver a su edad, pero que en realidad, no provocan nada más que una explicación dada en términos de naturalidad y una aceptación de que se trata de un tipo de contenidos que forman parte de la red hoy en día. Que un niño, buscando inocentes fotografías de perritos, encuentre en un motor de búsqueda fotografías de «parejas en la postura del perrito» no es un problema terrible ni tiene por qué provocar un trauma insondable de ningún tipo, si el adulto está ahí con la adecuada explicación en la mano. Se trata, simplemente, de un ejercicio de naturalidad que puede llegar a prevenir bastantes problemas posteriores. Los niños de hace años crecían de lo más tranquilos escuchando que a los niños los traía una cigüeña o que crecían debajo de una col. Con Internet, vivimos malos tiempos para las cigüeñas transportistas y las coles mutantes, porque son historias que no aguantan ni la primera búsqueda en Google.

El mejor filtro de contenidos es, simplemente, ninguno. Enseñar a los niños una «Internet con filtro» es la garantía de tener problemas cuando, necesariamente, se la encuentren sin él, cuando empiecen, que sin duda empezarán, a «fumar sin filtro». La tarea de racionar a los niños determinadas partes de la red debe formar parte de un proceso de educación equivalente al (pie tiene lugar en muchos otros aspectos: si vas demasiado lento, los niños explorarán por su cuenta, y la oportunidad de marcar u organizar criterios se perderá. Al principio, lo más prudente suele ser proporcionar a los niños experiencias de navegación restringidas a los favoritos del navegador: una colección amplia de páginas previamente probadas que permitan que el niño se familiarice con los aspectos más básicos del funcionamiento del navegador, el proceso de navegación y los rudimentos de la filosofía de Internet. A partir de ahí, es necesario abrir el paso hacia los buscadores: en caso de no hacerlo, estos irrumpirán por su cuenta en conversaciones en el patio del colegio o en la televisión. Por supuesto, el ritmo de esta progresión es algo que corresponde juzgar a cada familia.

Los buscadores son armas poderosas, y su manejo debe ser supervisado al principio. En el caso de Google, por ejemplo, en el que la situación por defecto es la de «filtro moderado», pasar durante una temporada a «filtrado estricto» puede ser recomendable, como situación meramente coyuntural. El siguiente paso, lógicamente, es la web social: para los niños, poseer un lugar en Internet supone una iniciativa enormemente educativa: permite que se rueden en su expresión escrita, que manejen los rudimentos del HTML y vean el copiar y pegar el código de pequeños widgets y aplicaciones como algo completamente natural, que no asusta. Las precauciones en este tipo de temas pueden ser múltiples: al principio, puede ser recomendable excluir las páginas web creadas por nuestros hijos de los índices de los buscadores (crear y editar el fichero robots.txt y añadir el comando disallow excede los propósitos de este libro, pero es un procedimiento ampliamente documentado en Internet), así como hacer que los comentarios que aparezcan en la página sean enviados a la dirección de correo electrónico de los padres. Herramientas como el correo electrónico, que los niños suelen ver generalmente como poco atractivo, o la mensajería instantánea deben formar parte de la caja de herramientas de un niño desde edades relativamente tempranas, aunque por supuesto es preciso explicarles, de manera bastante prolija, sus problemas y peligros. Para un niño, fenómenos como el spam y el phishing, de ubicua presencia en la red, tienen que ser, simplemente, lecciones aprendidas desde el primer día.

Para muchos jóvenes actuales, lo normal ha sido crecer en un entorno en el que de manera completamente habitual había un ordenador encendido y conectado a la red, además de una serie de teléfonos móviles y, en ocasiones, otro tipo de artilugios. Para estas personas, el escenario de acceso a la información y a la comunicación es algo completamente diferente al que la generación anterior tuvo la oportunidad de vivir. La terminología acuñada por Marc Prensky los denomina «nativos digitales», personas nacidas en un entorno digital, en oposición a los «inmigrantes digitales» que nacimos en un entorno completamente analógico y hemos emigrado a uno digital. Los «inmigrantes digitales» podemos intentar «hablar en digital», pero de una manera u otra, no somos nativos, y se nos suele «notar el acento». Muchos autores afirman que, en realidad, la denominación sobrevalora a los más jóvenes y los dota de un aura de «entendidos» bastante irreal, pero no cabe duda de que tener determinados esquemas mentales forjados en la presencia de un nivel tecnológico determinado ayuda a sentirse menos extraño ante determinados cambios y fenómenos.

En cualquier caso, es importante entender que las pautas de uso de la tecnología de niños y jóvenes son muy diferentes de las de los adultos. En los adultos, el paso de considerar Internet como la fuente de información preferente ante cualquier tipo de duda es algo que solo tiene lugar tras un período de adiestramiento importante. Para los jóvenes, en cambio, Internet tiene una presencia constante: cualquier pregunta, por nimia que parezca, es dirigida con preferencia a la red. Una duda ortográfica es solucionada simplemente tecleando las alternativas de la palabra en un buscador y comparando cuál de ellas obtiene un número mayor de resultados. El deseo de saber algo sobre la revolución francesa porque es un tema que se está estudiando en el colegio se cristaliza no en una búsqueda en Google, sino en una en YouTube: para una generación completamente audiovisual, encontrarse un pequeño documental hablando de la revolución francesa o a una persona contándoselo tiene una propuesta de valor sensiblemente mayor que la de enfrentarse a una página de la Wikipedia.

También es importante entender el fortísimo choque de conceptos que Internet plantea con respecto a la educación que reciben nuestros hijos: salvo excepciones, la transmisión de conocimientos que se lleva a cabo en la mayoría de los colegios sigue una aproximación unidireccional: un profesor transmite una serie de conocimientos a los alumnos mediante el recurso a libros de texto o a apuntes y explicaciones en la pizarra. En general, los ejercicios suelen reducirse a un «consígueme esta información y escríbeme un trabajo sobre ella». Los adultos hemos vivido durante toda nuestra vida en un mundo en el que conseguir información era una habilidad, algo valioso: saber a quién preguntar o dónde mirar era algo que se aprendía y se desarrollaba con la experiencia. Cuando adquiríamos un libro, lo hacíamos por el placer de «poseer» la información que contenía: a partir del momento de la compra, ese libro estaba en nuestra estantería (al menos, hasta que se lo prestábamos a ese amigo tan «simpático» que no nos lo devolvía), y podíamos volver a él en cualquier momento, abrirlo por la página 66, y acceder a la información que precisábamos. En cualquier momento, podíamos llegar a la enciclopedia y, haciendo uso del orden alfabético, encontrar lo que buscábamos. Las personas más fáciles de convencer para los vendedores de enciclopedias eran aquellos que tenían hijos: «pero hombre, sin enciclopedia, ¿cómo va a ayudar a sus hijos a hacer los deberes?».

Para la nueva generación, nada funciona así, ni se le parece lo más mínimo. La posesión de información, simplemente, carece de valor. La información está en la red al alcance de cualquiera, y encontrarla es algo que cualquiera, hasta el más tonto, podría hacer (lo cual resulta casi ofensivo, porque nos ven a los adultos como un escalón evolutivo que está claramente por debajo de «lo más tonto»..., ya que «no sabe ni buscar en Google»). Localizar la información es cuestión de minutos, de saber introducir la búsqueda adecuada. Y cuentan con estrategias de búsqueda, en muchos casos, verdaderamente brillantes para encontrar lo que necesitan. La enciclopedia es algo completamente del pasado: no verá a un niño acercarse a ella. De hecho, incluso el venerable orden alfabético resulta del siglo pasado: ¿quién necesita el orden alfabético cuando tiene una mano invisible con un logotipo multicolor capaz de revisar todas las páginas del mundo y traer ante la vista las respuestas adecuadas?

Por supuesto, cuando la información es localizada, el niño se limita a seleccionarla con su ratón, copiarla y pegarla sobre un documento. Documento que, eso sí, pierde tiempo en formatear. Después de todo, ¿qué le ha pedido el profesor? Que encontrase tal o cual información, ¿no? Pues aquí está, y hasta se la he puesto bonita. En realidad, nadie ha explicado al niño que lo que se esperaba era que redactase con sus propias palabras lo mismo que se había encontrado previamente escrito por otra persona, porque entre otras cosas, la tarea resulta completamente absurda ante la mentalidad del niño: si ya está aquí bien escrita, ¿para qué la voy a reescribir yo? En ocasiones, tras la correspondiente sanción «por haber copiado el trabajo» que resulta casi incomprensible para la mentalidad del niño la primera vez, se producen comportamientos casi grotescos de cambio de unas palabras por otras y de mezclas de unos documentos con otros esperando que el profesor no se entere, pero sin terminar de entender del todo lo que le están pidiendo en realidad.

¿Tienen los niños la culpa por copiar trabajos de Internet? En absoluto. Están siguiendo un simple principio de eficiencia. Lo que en este caso hay que hacer es, simplemente, pedir otras cosas. En lugar de pedir la repetición de información, pedir la elaboración de nueva información, el planteamiento de preguntas, la construcción sobre lo que ya existe. Una filosofía que tenga en cuenta que Internet existe y que la información está ahí, al alcance de cualquiera. Que repetirla no tiene sentido de ningún tipo, y que lo que hay que hacer es «manosearla», manejarla, utilizarla para construir sobre ella. Sin embargo, por el momento, a. lo más que llegamos en la amplia mayoría de los colegios es a introducir tecnología en las aulas por el simple hecho de introducirla: pizarras digitales con las que se mantiene la misma metodología que con las de tiza o rotulador, o portátiles con libros electrónicos alojados en su interior para contentar a las poderosas editoriales que los venden. ¿Que puede haber más absurdo que dar a un niño un portátil para que pueda acceder al enorme caudal de información existente en Internet, pero en lugar de enseñarle a filtrarla, calificarla y organizaría, limitarse a encerrar en él un libro de texto? Decididamente, el peor problema de los nativos digitales es el derivado de vivir en un mundo en el que los que toman decisiones sobre ellos no lo son.

Los nativos digitales exhiben, en general, pautas sociales enormemente arraigadas. No es, en realidad, nada nuevo, sino simplemente la traslación de las pautas sociales infantiles o juveniles a un soporte electrónico. Su red social en Internet es enormemente importante para ellos, porque la plantean como complemento, no como sustituto de sus relaciones sociales fuera de la red. Los adolescentes pasan tiempo con sus amigos en el colegio y en la calle, pero cuando llegan a sus casas, mantienen contacto permanente con ellos en una pestaña de su navegador, en la que mantienen abierta su red social. El modo de uso suele ser discontinuo: pueden pasarse horas viendo fotografías de sus contactos o escribiendo en sus páginas, pero si tienen otra tarea que hacer, mantendrán la red social en segundo plano y acudirán a ella con cierta periodicidad para refrescar la pantalla y comprobar si hay algo nuevo.

Lo síncrono tiende a ser fuertemente priorizado sobre lo asíncrono: la mensajería instantánea desplaza a cualquier otra tarea, pero no la usan igual que los adultos. Para un adulto, iniciar una conversación mediante mensajería instantánea es algo sencillo, pero que constituye una tarea prácticamente exclusiva, Si durante el tiempo que dura la primera conversación surge una segunda o una tercera, el adulto se siente incómodo, presionado, molesto. Durante toda su vida le han enseñado que una conversación se inicia con un saludo, prosigue con una dedicación de atención plena (no prestar atención o hacer otra cosa mientras hablamos con alguien es considerado entre adultos una clara falta de educación) y finaliza con una despedida. Sin embargo, en el caso de los niños o adolescentes, resulta perfectamente normal asomarse a su pantalla y encontrarse catorce conversaciones en ventanas diferentes. En realidad, muchas de esas conversaciones están inactivas, pero no se considera necesario saludar, ni mucho menos despedirse. La conversación aparece y desaparece sin ningún tipo de formalismo social, y resulta perfectamente normal dejar una conversación completamente «colgada» durante horas para retomarla después. La impresión es que abren más ventanas y manejan más canales que nosotros, aunque en realidad, lo que hacen es simplemente priorizarlas de manera diferente.

La traslación de lo social a la red en los jóvenes les lleva a. ser mucho más influenciables por sus amigos y por comentarios en los sitios que leen que por la publicidad convencional. La publicidad convencional puede entretenerles, pero, en realidad, solo es considerada positivamente si el grupo la considera positivamente, en una búsqueda incesante del refuerzo social del grupo acrecentada por el efecto amplificador de la red. De la misma manera, se apoyan en el grupo para compartir sus percepciones, lo que genera una especie de conciencia colectiva compartida. La disponibilidad de información y de canales de comunicación eficientes les lleva a un fuerte nivel de pragmatismo: no compran ideales, ni se dejan engañar por promesas, sino que contrastan información y comparten rápidamente la que sus propias experiencias generan.

En su relación con la red, los jóvenes son completamente utilitaristas. La red es la fuente de información, el sitio por el que acceden a su música y a sus películas, y el lugar gracias al cual permanecen en contacto con sus amigos. En su uso de herramientas, optan por aquellas que tienen una relación coste/beneficio inmediata: las redes sociales no exigen una gran inversión de tiempo inicial ni de mantenimiento, por lo que resultan claramente preferidas a los blogs. En la red social se produce una interacción constante que combina lo síncrono con lo asíncrono, y que resulta fundamental para mantenerse en contexto con el grupo. Erróneamente, los adultos tienden a pensar que cuando un adolescente pasa varias horas delante de la pantalla de un ordenador, «se está aislando», cuando en realidad lo que ocurre es precisamente lo contrario: está en permanente comunicación con su entorno. Algunos padres, de hecho, se preocupan por una excesiva dependencia de sus hijos frente a las redes sociales, cuando lo correcto sería, realmente, preocuparse por todo lo contrario: a día de hoy, resulta mucho más preocupante que un joven no forme parte de ninguna red social a que tienda a utilizarlas aparentemente mucho. En algunos casos, se ven comportamientos completamente primarios, como castigar a un hijo por cualquier motivo mediante la privación del uso del ordenador, o más específicamente, de la red social: «castigado sin Tuenti». Un castigo cruel y equivalente al de una especie de «privación sensorial»: al joven se le amputa su conexión con el grupo, se le descontextualiza, lo que provoca un sentimiento de ansiedad, de «están pasando cosas, pero yo no me entero», una sensación de aislamiento al día siguiente en el colegio o en el grupo, «el que no tiene ni idea de lo que pasó ayer en la red».

Los jóvenes suelen hacer un uso mucho más natural de las redes sociales que los adultos. En general, la relación de los adultos con las redes sociales está condicionada por determinadas normas sociales, por la adaptación de normas de fuera de la red a la misma. Para una persona adulta, recibir una petición de amistad en una red social supone, en muchos casos, ponerse a pensar en las variadas circunstancias por las que es posible que conozca a esa persona: examina su fotografía, su red de amigos, comprueba los amigos comunes..., en realidad, la persona está aplicando una norma externa a la red: cuando alguien te saluda, lo educado suele ser devolver el saludo. Darse la vuelta y no decir nada es considerado para muchos y en muchos casos una falta de respeto incluso con un desconocido, de manera que, curiosamente, intentamos casi buscar justificaciones para poder decirle a esa persona que sí, cuando en realidad, el mero hecho de no recordarla debería ser razón suficiente como para no querer tenerla en nuestra red como contacto directo. En ese comportamiento se perciben incluso diferencias de género: los hombres suelen ser más abiertos que las mujeres a la hora, de aceptar esos contactos dudosos.

La interacción con los convencionalismos sociales es tan fuerte que, en mucho casos, en redes como Facebook, nos encontramos con la paradoja de que los usuarios adultos prefieren, antes de rechazar un contacto, simplemente no decir nada, dejarlo sin respuesta, enviar el contacto «a la nevera», una acción que se identifica como «menos brusca». En realidad, la intuición engaña en este sentido: mientras rechazar un contacto ofrece a la persona rechazada la posibilidad de explicarse o de proporcionar alguna pista sobre la relación, ofrecer la callada por respuesta impide a esa persona repetir el contacto. El solicitante, en realidad, percibe una contestación más maleducada y brusca en el caso de la no contestación que en el del rechazo directo. Curiosamente, las relaciones entre adultos en las redes sociales resultan bastante asimétricas en las percepciones: para un adulto es perfectamente normal «explorar» la red social en busca de amigos, evocar relaciones de épocas pasadas, u ofrecer amistad a personas de las que no ha sabido nada en los últimos diez o veinte años.

En el caso de los adolescentes, este tipo de comportamientos no suelen ocurrir. Su manejo de las redes sociales está prácticamente exento de los convencionalismos sociales habituales en adultos: si no conocen a alguien o no mantienen una relación «viva» con él, simplemente dicen no, sin el menor empacho. Si se enfadan con alguien, es incluso habitual que corten inmediatamente su relación en la red, como negando su existencia. Los adolescentes, en realidad, repiten en la red social los patrones de amistad que tienen fuera de ella, sin demasiada exploración adicional, y tienden a utilizar la red para suplementar de alguna manera las mismas mediante notas, recuerdos compartidos, fotografías, vídeos, etc.

Las relaciones entre padres e hijos en la era digital deben tener en cuenta este tipo de herramientas, pero siguen gobernándose prácticamente por las mismas dinámicas tradicionales. A partir de determinada edad, entrando en la adolescencia, la red se convierte en un espacio tan privado como la propia habitación: encontrar a tus padres curioseando en tu red es una ofensa terrible. Sin embargo, resulta perfectamente normal preguntar e interesarse, de la misma manera que habitualmente tendemos a saber con quién salen nuestros hijos, quiénes son sus amigos, etc. El éxito de algunas redes sociales, como es el caso de Tuenti en España, se debe en gran medida al hecho de ser «un espacio libre de padres». Si los padres se hacen un perfil en dicha red, por lo general, el adolescente tenderá a considerarlo una intromisión, y puede incluso llegar a generar situaciones incómodas.

La nueva generación posee pautas de motivación diferentes a la que le precede. Sus escalas de valores son distintas, y eso afecta a todo tipo de circunstancias, incluidas las relaciones laborales. La primera generación que ha crecido completamente rodeada por Internet posee características diferenciales: separarlos de la red, pretender que no accedan a ella en horas de trabajo, etc., solo genera situaciones de frustración. Son enormemente pragmáticos: su conciencia del yo está más desarrollada, y predomina sobre cualquier tipo de identidad corporativa. Entender a la nueva generación es una tarea imposible sin entender su entorno, un entorno en el que la red juega un papel fundamental.

CAPÍTULO 9. LA RED Y EL NEOHUMANISMO

«Money can't buy me love.»

JOHN LENNON, PAUL MCCARTNEY (1964)

El humanismo es, según la definición clásica, una doctrina basada en la exaltación de la persona, de la figura humana. A lo largo de la historia, el humanismo ha emergido en algunos períodos históricos, asociado a momentos como el Siglo de Oro en Grecia, el Renacimiento en Europa, o el idealismo alemán. En la actualidad, el humanismo se consideraba inmerso en una importante crisis: los valores de la Revolución Industrial hacen mucho más énfasis en la consideración del hombre como colectivo, como grupo: los trabajadores, los consumidores, etc., hasta el punto de tratarlos como grupos sin identidad, como segmentos de mercado. El hombre pasa a adquirir una dimensión minúscula, que precisa del apoyo en otros, en un colectivo, en el logotipo de una empresa, en una religión que le aporta una figura de referencia en forma de dios, en un partido político. La individualidad se considera una anomalía, algo digno solamente de personas muy sobresalientes en función de rasgos o habilidades muy concretas: grandes artistas y deportistas, etc., en los que, a pesar de todo, se busca su adscripción a un grupo. Pero no se justifica en las personas normales, sometidas a estímulos comunes: en general, consumimos lo mismo, somos atraídos por similares estándares, nuestros gustos son manipulados por empresas, y todo apunta a una fuerte corriente de pérdida de la identidad individual. Salirse de las normas de una manera demasiado transgresora es considerado una extravagancia que puede llegar a ser en ocasiones socialmente incómoda o incluso inaceptable.

Vivimos en sociedades en las que la identidad individual, la esencia del individuo, ha sido severamente cercenada. Y esa sumisión del individuo al grupo proviene, en gran medida, del éxito de los medios de comunicación masivos. Con su unidireccionalidad, dichos medios ofrecen un mensaje completamente homogéneo a grupos de personas muy numerosos: a las métricas del negocio les cuesta justificar el dirigirse a nichos cuando éstos son pequeños, y mucho menos la dedicación a consumidores individuales. Los modelos de negocio considerados brillantes lo son porque consiguen economías parecidas a las de Henry Ford: «fabricamos automóviles de cualquier color, siempre que este sea negro». O por la industria discográfica, que desprecia todo aquello que no apela al gusto masivo y que no se convierte en lo que denominan «un hit» hasta el punto de restringir su negocio a la comercialización de un escaso 3% de la música producida en la historia de la humanidad.

Estamos acostumbrados a métricas de éxito basadas en las economías de escala, en series descomunalmente grandes que permiten repercutir ahorros importantes. Nuestra comunicación con los clientes es completamente unidireccional: podemos acosarlos, llamarles a su casa a la hora de la cena para que un vendedor casi robotizado insulte su inteligencia leyéndoles mecánicamente un guión, inundar su buzón de correo y su bandeja de entrada... pero no contestamos sus comunicaciones cuando son ellos los que contactan con nosotros. Si llamas a una compañía, te encuentras un sistema que te obliga a escoger entre una serie de opciones mediante tonos, para, al final, llegar a una operadora que es medida en función del poco tiempo que pasa respondiéndote. Si alguna vez ha llamado al 609 de Telefónica y se le ha cortado la comunicación, no se sorprenda: como su intuición indica, no se trata de que las líneas de teléfono de Telefónica sean especialmente malas o frágiles, sino que los operadores de atención al cliente son evaluados de tal manera, que si la llamada se prolonga, les resulta más rentable colgarte el teléfono que solucionar tu problema. Esa es la actitud hacia el cliente de una compañía que, irónicamente, tiene la osadía de definir al cliente como centro de sus prioridades.

El marketing que conocimos durante todo el siglo pasado es ahora precisamente eso, «marketing del siglo pasado». El concepto de product manager, de persona encargada de cumplir su cuota caiga quien caiga y de maximizar ventas por encima de verdaderas preferencias o necesidades del cliente, se muere a pasos agigantados. El siglo pasado fue el de las batallas mes a mes, el de los objetivos a corto plazo justificados en base a lo que sea. Los clientes eran «el enemigo» al que había que convencer, cada venta era una batalla a ganar, hasta el punto que a cada oleada, la llamábamos «campaña», una terminología proveniente del mundo militar. Este siglo, sin embargo, es el de los clientes que nunca olvidan. El del largo plazo, el de las quejas que persisten durante años en Internet, apareciendo en los resultados de búsqueda cuando otros clientes potenciales buscan tu marca y se encuentran con aquellos a los que engañaste antes, contando su historia con pelos y señales. El siglo pasado fue el de tratar al cliente maravillosamente bien hasta que compraba, después, ni maldito caso, gasto reducido a la mínima expresión, y a otra cosa, mariposa: a por el siguiente cliente. En este siglo, si no cumples tus promesas, o no mantienes seducido al que ya te compró, no durarás dos asaltos, porque todos se enterarán.

El ciclo de vida del cliente comienza con el llamado «universo de clientes». Para un ejecutivo de marketing, el universo de clientes es todo aquello que se mueve ante sus ojos, vivo o no, inteligente o no, con dinero o sin él: el universo de clientes no tiene cara ni ojos, no es una lista, en muchos casos no conoce ni su número total. Para un banco, el universo de clientes no es ni siquiera la cifra de población del país o de la región, porque podría haber población no censada a la que eventualmente se podría vender algún producto. Atacar al universo de clientes es posible, pero resulta enormemente caro: hay que irse al medio supuestamente más masivo de todos, y poner anuncios en momentos de máxima audiencia, la serie de más éxito, los intermedios de la Superbowl, los partidos Madrid-Barcelona, las finales de la Copa de Europa, etc.

Un primer filtro convierte al universo de clientes en un conjunto más definido: los llamados prospects o clientes potenciales. Un subconjunto del anterior, todavía indefinido, y creado a partir de las características del producto: si mi producto está pensado para este tipo de gente, y dado que este tipo de gente lee esto, ve esto y pasea por esta zona, puedo ir a por ellos en estas ocasiones. Desde ahí, una segmentación más nos lleva a los llamados responders, definidos como aquellos que han reaccionado a mi llamada, han pedido información adicional o, de alguna manera, han dado pruebas de su existencia. Algunos de esos, lógicamente en una cantidad que se va viendo reducida por la aplicación de los sucesivos filtros, dará un paso más y comprará nuestro producto. Y de esos, algunos se convertirán en consumidores habituales o fieles del mismo, particularmente cuando hablamos de bienes distintos a los llamados «durables», cuya adquisición retira literalmente al comprador del mercado durante un largo tiempo. Finalmente, algunos de estos clientes habituales o fieles, eventualmente, dejará de serlo: habrá encontrado otro producto que le satisface más, habrá habido algo del nuestro que no 1e gustó o, simplemente, y sin ningún tipo de acción por nuestra parte, habrá cambiado sus circunstancias de alguna manera. Puede haber cambiado sus hábitos, su lugar de residencia, su estado civil..., puede incluso que se haya muerto, que nos pongamos como nos pongamos, son cosas que tiene la vida —que de vez en cuando va y se acaba.

En este ciclo de vida del cliente, el grueso del esfuerzo ha tendido a estar acumulado al principio. En convencer al cliente de que somos el producto o servicio que tiene que adquirir. Todas las acciones de marketing tienden a estar enfocadas en convencer al que no está convencido, en convertir al infiel, con un coste determinado y bastante similar para todos los miembros de la industria. Una vez que han comprado, se convierten en tierra conquistada. Ya no hay que esforzarse más, ya están ahí. Es un contrasentido enorme, gastamos importantes sumas en convencer al cliente de que compre, le mandamos folletos a todo color... Pero una vez que compra, las instrucciones de los mismos son de todo menos atractivas y están sobre papel malo en un triste blanco y negro.

Uno de los importantísimos cambios que estamos viviendo es el desarrollo progresivo de una economía de relaciones. Un énfasis cada vez mayor en establecer con el cliente una relación que nos permita monitorizar su progreso, conocer su satisfacción, su implicación con el producto. Frente al énfasis en el coste de adquisición del cliente, en la búsqueda incesante y a toda costa de clientes para los productos que previamente hemos desarrollado, se impone cada vez más una aproximación diferente; desarrollar un cliente, una relación, y buscar productos para él, a menudo con su propia colaboración. Las marcas empiezan a distinguirse en función del número de usuarios comprometidos e implicados que tienen, en función de las personas que las rodean: las comunidades empiezan a dominar a las marcas. En el extremo, aparecen marcas que dependen casi plenamente de sus clientes: hacen poca o ninguna publicidad entendida como tal, y lo dejan prácticamente fiado a los comentarios positivos que esperan obtener de su base de usuarios o de los analistas.

Las marcas se humanizan, adoptan la personalidad de sus líderes o de las personas que trabajan en ellas, a medida que, en paralelo, empiezan a comprobar que su comunicación tradicional, la forma grandilocuente de hablar a través de notas de prensa y en la memoria corporativa resulta cada día más indiferente a quienes les rodean. Es la crisis de la comunicación tradicional, la que hace que los clientes escuchen la publicidad de una manera cada vez más indiferente. Puedes ver un anuncio que te gusta, puedes disfrutar de la buena publicidad, pero cada vez influye menos en tus decisiones de compra, llevadas mucho más por otro tipo de factores: comentarios de conocidos y amigos, análisis en blogs y páginas especializadas, opiniones en la red, experiencias de terceros...

Para la empresas, la situación es desconcertante. Ven cómo sus millonarias estructuras de comunicación se convierten en irrelevantes, en algo que influye a sus clientes cada día menos. Años de mantener departamentos de comunicación, de controlar cuidadosamente la información que salía de la empresa, de tejer agendas de relaciones con los medios, para obtener finalmente como premio la más triste irrelevancia. Para encontrarse una enorme conversación allí fuera, completamente incontrolable, en la que no se puede elegir no estar. Si no se está, malo: es la condena a la irrelevancia, a la no presencia en las mentes de los clientes. Si se está, lo malo tiende a predominar sobre lo bueno, haciendo difícil destacar por aquello que queremos destacar. Para quien lo ve, la solución pasa por hacerse cada día más transparente, por comunicar basándose en la persona, en la conversación, en la integración de las opiniones de terceros, posicionar la marca en función de quienes la usan o trabajan en ella.

El comienzo del énfasis en la individualidad en las empresas coincide, a modo de triángulo perfecto, con otras dos circunstancias: por un lado el fuerte desarrollo de herramientas como blogs y redes sociales que permiten una expresión más sencilla de la individualidad y la personalidad en la red. Por otro, la llegada de una nueva generación con un grado de compromiso infinitamente menor con sus empresas, personas que ven la rotación como algo positivo, como una manera de acumular experiencia y eludir el aburrimiento. El modelo de «persona vinculada a una empresa», de larga carrera profesional en la misma compañía, pasa a resultar de todo menos aspiracional. Una carrera prolongada en una misma empresa se ve como oportunidades perdidas, como aburrimiento y sopor. El joven ya no aspira a entrar en una empresa e ir escalando puestos en ella, no lo valora, y mucho menos lo acepta como supuesto incentivo en virtud del cual está justificado todo sacrificio.

El resultado es un profundo cambio en la percepción del «ser» frente al «estar». Hasta ahora, un empleado de una compañía se presentaba habitualmente a sí mismo como «hola, soy Nombre y Apellido, de la empresa X», exhibiendo una tarjeta de visita. Y además, se sentía así. La preposición «de» indicaba casi un sentimiento de posesión, un «formo parte del inventario de la empresa», unas empresas en las que el mismo nombre del departamento que gestiona a las personas, «Recursos Humanos», indica la consideración que se tiene de las mismas. ¿Cómo que un «recurso»? Mire usted, yo no soy un «recurso», no soy «parte del inventario», soy una persona. Durante muchos años, hemos vivido una relación entre personas y empresas completamente desequilibrada hacia el lado de estas últimas: en lugar de simplemente trabajar en ellas, se diría que nos tatuábamos su logotipo. La sensación que teníamos era la de que lo importante de nosotros mismos era nuestra tarjeta, la empresa y el cargo que ponía en ella, no nosotros mismos. Y de repente, la situación empieza a cambiar: la persona tiene la posibilidad de desarrollar su personalidad en la red, de describirse, de contar quién es, lo que piensa, a qué se dedica, qué le gusta hacer. Empezamos a ver la dicotomía entre el «ser», «yo soy esta persona, con este nombre y este apellido», y el «estar»: «ahora, coyunturalmente, estoy trabajando en 1a. empresa. X con este cargo..., algo que mañana puede seguir siendo así, o no serlo».

Años de relaciones laborales, de jubilaciones anticipadas indeseadas y de expedientes de regulación de empleo nos han enseñado que las relaciones profesionales son eso, relaciones profesionales, y que basar nuestra vida en el dónde estamos en lugar de en el qué somos es algo seguramente poco recomendable. La red, en todo esto, juega un papel de proyección de la persona y de reflejo de su actividad a través de medios como páginas web, foros, blogs, redes sociales, mundos virtuales... formas de presencia social de muy diversos tipos y que posibilitan una amplísima gama de presencia en escenarios diferentes. En su vida profesional, alguien puede ser un profesor de una escuela de negocios, pero en la red puede ser a la vez uno de los mayores expertos en motos antiguas de la marca Montesa, una persona que opina sobre la actualidad de la industria en la que desarrolla parte de su actividad y, en paralelo, y sin ninguna relación con lo anterior, un combatiente nazi o norteamericano que lucha a tiros por liberar una fortaleza en un equipo de seis personas que actúa de manera completamente coordinada. Un yo que no solo se despliega, sino que se regodea en las diferentes posibilidades, se proyecta con diferentes personas en distintas situaciones, que se mezclan o no según lo quiera la persona. El currículo como documento que describe el historial profesional de la persona pierde progresivamente importancia, como lo hace la agenda: lo primero que va a hacer casi cualquiera que tenga tu currículo en sus manos es buscar referencias en la red. No puedes ser un experto en un tema determinado, si no aparece nada tuyo al buscar en Google tu nombre vinculado a palabras clave de esa especialidad. Si al introducir tu nombre y el de tu escuela no aparece nada, es que no estuviste en ella. Si no estás en Linkedin, es que estás desactualizado en eso de la red, es que eres, como se dice jocosamente, un «tecnopléjico». Si quiero tu teléfono o tu dirección de correo electrónico completamente actualizado, me meto en una red social y te busco. En algunos casos, hasta los propios teléfonos móviles buscan los nombres que tienen en la agenda en Facebook y complementan la ficha de nuestros amigos con su fotografía y los datos actualizados, para que no perdamos el contacto si los cambian. La persona ya no es lo que su empresa dice de él, es lo que la red y la persona dicen y demuestran de sí mismo.

¿Qué aparece en la red cuando alguien busca tu nombre? ¿Algo? ¿Nada? ¿Una confusa retahíla de personas que se llaman igual que tú? Si no estás en la red, estás desaprovechando la posibilidad de contarle al mundo quién eres, a qué te dedicas, qué te interesa y qué Le gusta hacer o eres bueno haciendo. Por ahora, tu impresión es que puedes perfectamente prescindir de todo eso. Que prefieres lo que llaman «una existencia discreta», «no hacer ruido». Pero en un mundo neohumanista, eso ya no es así. La exclusión de la red, dentro de poco, será la moderna versión del ostracismo.

CAPÍTULO 10. UN CASO PRÁCTICO: MICROSOFT

«Un ordenador en cada mesa de trabajo y en cada hogar.»

Misión de Microsoft, década de 1990

El caso de Microsoft es una combinación de suerte, sentido de la oportunidad y visión estratégica de esos que habitualmente se usan para ejemplificar lo que se ha dado en llamar «el milagro americano». Sus fundadores, Bill Gates y Paul Allen, crearon la compañía en 1975 para desarrollar intérpretes del lenguaje BASIC para un ordenador en concreto, el Altair 8800, que alcanzó en su tiempo una moderada popularidad: escribieron al fundador de Altair, Ed Roberts, ofreciéndoles su intérprete de BASIC cuando aún no habían ni empezado a programarlo, y constituyeron Microsoft a toda velocidad en Alburquerque, sede de Altair, para vender a la compañía un producto que todavía no existía, En la primera demostración que hicieron del mismo, el programa se colgó intempestivamente en la pantalla de inicio, pero aun así consiguieron cerrar la venta.

La empresa mantuvo un perfil discreto, desarrollando diversos lenguajes de programación, compiladores y sistemas operativos hasta el año 1981, en que tuvo su gran golpe de suerte. En aquel año, la entonces todopoderosa IBM se hallaba en proceso de lanzamiento de un ordenador personal, dedicado a competir en lo que parecía un naciente nicho al que la compañía nunca había dedicado una importancia excesiva. De hecho, el IBM PC se creó casi al margen de la estructura habitual de desarrollo de producto de la empresa: un equipo de apenas doce personas en Boca Ratón, Florida, trabajando con productos no diseñados ni fabricados por la compañía como se había hecho siempre, sino adquiridos en diferentes países a múltiples proveedores. Las razones para una configuración de diseño y lanzamiento de producto tan poco habitual en la compañía obedecían a consideraciones de flexibilidad, velocidad y coste, y parecen indicar que aunque la empresa quería tener un producto en este segmento, no le daba en realidad demasiada importancia en el contexto de sus líneas de productos, dominadas por máquinas enormes por las que los clientes corporativos pagaban sumas muy elevadas. Por otro lado, la compañía se hallaba envuelta desde 1969 en un complejísimo proceso antimonopolístico por tener lo que se estimaba como posiciones dominantes tanto en hardware como en software y servicios, un juicio que no culminó hasta 1982, y que la obligaba a mantener un cierto nivel de precaución en sus movimientos.

Consecuentemente, el sistema operativo para el IBM PC se buscó también fuera de la compañía: Jack Sams telefoneó a aquella pequeña compañía a la que IBM adquiría algunos productos, y pidió cita para el día siguiente. Al llegar, además de confundir al jovencísimo Bill Gates con el chico de los recados, solicitaron inmediatamente la firma de documentos de confidencialidad, y se interesaron por un producto que contuviese el lenguaje BASIC y un sistema operativo, para encontrarse con la respuesta negativa de una Microsoft que carecía de dicho sistema operativo. Ante tal tesitura, Gates envió a Jack Sams a hablar con Gary y Dorothy Kildall, el matrimonio de fundadores de Digital Research, entonces otra pequeña empresa radicada en una casita victoriana de Pacific Grove, que poseían un sistema operativo llamado CP/M. Sin embargo, en un error que seguramente lamentarían el resto de su vida, Gary y Dorothy decidieron negarse a firmar los acuerdos de confidencialidad, ante lo cual IBM abandonó la negociación sin haberla siquiera iniciado formalmente. Tras la negativa de Digital, IBM retornó a Microsoft, y la empresa decidió que no podía dejar pasar una segunda oportunidad: Paul Allen, el otro fundador de Microsoft, localizó a Tim Patterson, propietario de Seattle Computer Products (SCP), otra pequeña compañía que había «adaptado» el CP/M de Digital Research para convertirlo en lo que llamó QDOS, y adquirió todos los derechos del mismo por 50.000 dólares. Microsoft tomó el QDOS, lo rebautizó como PCDOS 1.0, y lo licenció a IBM para su PC: sin duda, el negocio del milenio.

El sistema operativo de Microsoft se reveló como un negocio enorme: el IBM PC resultó ser un éxito como producto en sus sucesivas generaciones, pero dio lugar, además, por el hecho de tener una arquitectura abierta, a una enorme proliferación de máquinas similares de otras marcas y de los denominados «clónicos», aparatos ensamblados con piezas de diversos fabricantes. El hecho de que IBM tuviese que tener cuidado con la legislación antimonopolio y no pudiese lanzar una de sus estrategias de dominación absoluta funcionó como verdadero motor del mercado de los ordenadores personales. En muy poco tiempo, el PC pasó a dominar completamente el mercado con crecimientos desmesurados, y cada uno de los que se vendían llevaba consigo una licencia del sistema operativo, ya conocido como MS-DOS, por la cual Microsoft recibía 50 dólares. Con los fondos generados, la compañía preparó, en 1985, una nueva generación de producto inspirada en el interfaz gráfico desarrollado por Apple a la que denominó Windows, y aprovechó, además, para protagonizar una exitosa salida a bolsa el 14 de marzo de 1986 que convirtió instantáneamente en millonarios a sus fundadores y a su equipo directivo.

En 1989 puso en el mercado uno de sus productos más brillantes: la suite de productividad Microsoft Office. Los productos de Microsoft seguían un posicionamiento de precios extremadamente bajos: en el momento de la salida de Microsoft Office, que incluía un procesador de textos (Word), una hoja de cálculo (Excel), un programa para presentaciones (PowerPoint) y, en su versión Pro, una agenda (Schedule Plus) y una base de datos (Access), el precio era inferior al de cualquiera de sus elementos tomados por separado de la práctica totalidad de sus competidores. Resultaba más caro adquirir WordPerfect, Harvard Graphics, o Lotus 1,2,3 que un Office que además de poseer todas las funciones convenientemente integradas, apuntaba ya los principios de lo que se dio en llamar el WYSIWYG, What You See Is What You Get.{12} El producto resultó enormemente atractivo, y fue un gran éxito de ventas: consiguió llevar la ofimática a un número enorme de personas que empezaron a verla como algo sencillo, manejable, que no requería una gran inversión en tiempo de aprendizaje.

Microsoft lanzaba productos muy rápidamente, y arreglaba sus problemas más adelante. Se llegó a decir que la empresa gastaba más dinero en responder llamadas de usuarios con problemas que en el propio desarrollo del producto. El no tener control sobre todos los eslabones en manos del usuario (las máquinas venían de cualquier fabricante de PC, y las aplicaciones, de cualquier fabricante de software, mientras Microsoft controlaba el sistema operativo y el Office), la impresión era que sus productos no tenían una gran calidad: exasperaban a los usuarios con pantallazos azules y problemas que, en muchos casos, no solo tenían como origen a la propia compañía. La famosa BSOD, Blue Screen Of Death o pantalla azul de la muerte, se convirtió en una especie de icono: cada vez que aparecía, el usuario perdía normalmente gran parte de su trabajo. Sin embargo, la compañía era rápida y sus productos eran baratos, lo que llevaba a muchos usuarios a considerar su falta de fiabilidad como una parte del precio que había que pagar. Además, la compañía utilizó abiertamente el mercado irregular: conseguir una copia de los productos de la compañía era tan sencillo, que incluso muchos fabricantes de ordenadores la ofrecían directamente como posibilidad para rebajar el precio de las configuraciones a sus clientes. Durante muchos años, la compañía mantuvo una posición tibia y tolerante en ese sentido, consciente de que un base mayor de usuarios de sus productos suponía un poder mayor derivado del hecho de controlar los estándares asociados con ellos. Para muchos usuarios, los productos de Microsoft son y han sido siempre gratuitos: se trataba simplemente de traer los discos del trabajo o de pedírselos a un amigo para instalarlos.

La estrategia de Microsoft con Windows ha estado en todo momento guiada por su alianza con los fabricantes de ordenadores personales: cada versión de Windows se mantenía en el mercado mientras sus ventas progresaban al ritmo adecuado. Todo progreso se condicionaba a las cifras de ventas, y se retenía hasta que el mercado empezaba a mostrar signos de agotamiento. En ese momento, se ponía en marcha una siguiente versión, que tenía que ofrecer forzosamente un diferencial de prestaciones suficiente como para que los usuarios quisieran cambiarse, y que tenía indefectiblemente el atributo de no ser capaz de funcionar sobre el parque de ordenadores existente en la inmensa mayoría del mercado. Así, los fabricantes de ordenadores encontraban una razón para generar una renovación del parque instalado: todos los usuarios eran forzados a reemplazar máquinas que, en general, eran perfectamente operativas, por otras nuevas más potentes. Si no lo hacían, pasaban rápidamente a caer en la más patente obsolescencia: veían en otros las nuevas funciones del sistema que ellos no podían utilizar, recibían ficheros de otras personas en formatos que no podían leer, y sentían la presión de todo un mercado que les llevaba a actualizar en un plazo razonable de tiempo. Durante años, las sucesivas generaciones de Windows ha sido la manera en que el mercado del hardware se ha ido dinamitando: los fabricantes de ordenadores eran a la vez los clientes de Microsoft y su canal mayoritario de distribución: la inmensa mayoría de las licencias de Windows se venden asociadas a un nuevo equipo.

Al tiempo, la compañía empezó a aplicar una poderosa estrategia de bundling: muchos de sus productos venían integrados en un lote con el sistema operativo. Haber logrado un éxito importantísimo con Windows le daba la posibilidad de redefinir los estándares y crear los programas más adecuados para funcionar dentro de su propio sistema operativo, mientras al tiempo proporcionaba librerías y herramientas a un enorme ecosistema de desarrolladores para crear todo tipo de productos. Cuando alguno de esos productos creado por otra compañía demostraba su idoneidad, superaba la prueba del mercado y se convertía en un gran éxito, Microsoft podía optar por desarrollar un producto con funciones similares e integrarlo dentro de Windows, lo que obligaba al fabricante a competir con un producto cuyo precio era percibido como cero porque venía incluido en un lote con el sistema operativo. Así, Microsoft, que había inicialmente ignorado el desarrollo de Internet y había visto cómo una empresa, Netscape, dominaba el mercado de los navegadores, fue capaz de derrotarla de manera estrepitosa con un producto técnicamente inferior, Internet Explorer, que además pretendía redefinir los estándares del lenguaje HTML. Con Internet Explorer, Microsoft intentó replicar la estrategia que tan buen resultado le había dado con Windows: manejar los llamados «estándares de facto», arrogándose el derecho a definir como estándar cualquier cosa que pusiese en el mercado, alegando que, por el mero hecho de lanzarla, se convertía en la opción mayoritaria.

Entre los segmentos que Microsoft cuida especialmente está, por supuesto, el mercado académico: las escuelas e instituciones educativas cuentan con un tratamiento preferencial por parte de la compañía, enormemente interesada en que los estudiantes a todos los niveles se acostumbren a trabajar con sus herramientas y a considerarlas la opción por defecto.

En el mercado corporativo, Microsoft sigue una técnica de desarrollo de ecosistema: ofrece herramientas de desarrollo que favorecen la aparición de infinidad de empresas que trabajan en la oferta de soluciones al cliente final, y certifica tanto a los programadores individuales como a las empresas. Esto genera una gran comunidad de socios, que Microsoft ha utilizado en ocasiones para, por ejemplo, acudir masivamente a comités de certificación y sesgar decisiones importantes, como la de estandarizar determinados formatos propuestos por la compañía.

El historial de abusos competitivos de Microsoft es digno de figurar en los libros de historia: además de las demandas antimonopolio y por abuso de posición dominante tanto en los Estados Unidos como en Europa y otros países, y de numerosas class actions o demandas colectivas protagonizadas por grupos de usuarios, la compañía ha mantenido litigios con varias decenas de compañías de todos los tamaños debidas a temas de patentes, copias o estrategias predatorias de todo tipo. Los ejemplos están por todas partes: WordPerfect fue expulsada del mercado debido al retraso en lanzar una versión de su software para Windows, versión que no podía preparar porque carecía de las especificaciones que Microsoft debía proporcionarle para ello. Netscape fue derrotada mediante lo que los tribunales posteriormente denominaron «una estrategia de precios predatorios», mientras que otras empresas como Be, creadora de un sistema operativo, se encontró con que los fabricantes de ordenadores se negaban a distribuirlo por miedo a posibles represalias de Microsoft. Spyglass licenció su navegador a Microsoft a cambio de un porcentaje sobre cada venta, para encontrarse con que Microsoft lo regalaba incluido dentro de Windows y, por tanto, no proporcionaba a Spyglass rendimiento económico alguno. En general, la estrategia de Microsoft con respecto a las demandas ha sido la de un auténtico matonismo judicial: además de defenderse mediante enormes equipos de abogados y tácticas de lobbying, la empresa no ha dudado en pactar fuera de los tribunales, en lanzar demandas con propósitos intimidatorios o en pagar abiertamente las multas con el fin de evitar más consecuencias. Esto, unido a la estrategia empleada para el lanzamiento de sus productos —lanzamiento rápidos y pulido posterior de los errores que iban apareciendo— le ha generado a la empresa un coste importantísimo en términos de imagen: seguramente hablamos de una de las empresas que cuenta con un mayor número de detractores, que tienden, además, a tomar una actitud casi «religiosa» ante una compañía que consideran una especie de «encarnación del mal».

El desarrollo de un mercado dominado en más de un 90% por el sistema operativo de Microsoft ha traído como consecuencia una evidente falta de diversidad que ha generado no pocos problemas para la industria y los usuarios: para un delincuente, crear un virus o un programa malicioso capaz de saltarse la débil seguridad de Windows tenía el incentivo de una difusión rápida entre un porcentaje de mercado enorme, lo que podía generar un rendimiento económico considerable. Como ocurre en la biología, la ausencia de diversidad convierte a los ecosistemas en mucho más vulnerables. A día de hoy, un porcentaje elevado de usuarios con sistemas mayoritariamente basados en Windows no originales y no convenientemente parcheados conforman las llamadas botnets, redes de miles de ordenadores denominados «zombies» que, sin conocimiento de sus usuarios, desarrollan tareas para el gestor de 1a red, como, por ejemplo, reenviar mensajes de correo basura o hacer clic en páginas con anuncios fraudulentos en los que anunciantes legítimos pagan en función de los clics generados.

¿En qué momento entra en crisis el sistema? Lógicamente, en el momento en que una de las versiones del sistema operativo no es capaz de ofrecer un diferencial de prestaciones suficiente como para que los usuarios de la versión anterior quieran actualizarse a la nueva. Para Microsoft, este momento llegó con Windows Vista: un sistema operativo que llevó más de cinco años desarrollar —el tiempo más largo entre versiones jamás transcurrido en la compañía— y que fue «dejando caer» las prestaciones más interesantes de las inicialmente anunciadas en el año 2005. Al final, el sistema fue percibido por el mercado como un Windows XP hinchado y repintado (aunque realmente planteaba toda una nueva ingeniería desde el punto de vista de planteamiento), que requería de nuevo un cambio de ordenador, para el que no resultaba fácil encontrar hardware que funcionase, y que constantemente molestaba al usuario con el llamado User Account Control (UAC), un engendro destinado a ser el enésimo intento de la compañía por hacer un producto seguro, pero que infinidad de usuarios terminaban desactivando, agotada su paciencia por las constantes peticiones de confirmación.

Windows Vista fue, efectivamente, un estrepitoso fracaso para la compañía de Redmond, La compañía lo mantuvo en el mercado menos de tres años, y llegó al punto de ser calificado por algunos analistas como «uno de los peores productos tecnológicos de todos los tiempos». Con Vista en el mercado y en medio de una severa crisis económica, Microsoft pasó por sus peores momentos: recibió una multa de 900 millones de euros de la Unión Europea por prácticas anticompetitivas, el valor de su acción cayó al mínimo de los últimos cinco años, y llevó a cabo su primera reducción de personal poniendo en la calle a 5.000 personas. Al tiempo, el auge de los llamados netbooks, equipos de bajo precio y prestaciones, perjudicó notablemente el margen de la compañía: ante la carencia de un sistema operativo adecuado para ese tipo de máquinas de especificaciones bajas e incapaces de mover esa especie de monstruo inflado llamado Vista, Microsoft optó por resucitar el ya caduco Windows XP como mal menor, reduciendo, además, el precio de su licencia para intentar competir con la gratuidad del software libre. Una estrategia a la desesperada, pero que al menos evitó un crecimiento fuerte de configuraciones basadas en versiones adaptadas de Linux, algo que la empresa consideraba una gran amenaza.

La lucha contra el software libre ha sido una de las cuestiones que han supuesto un mayor desgaste para la compañía. A lo largo de su historia, Microsoft ha llegado a identificar el software libre con todas las plagas del Apocalipsis, y ha utilizado constantemente el recurso a la creación de miedo, incertidumbre y dudas,{13} entre los usuarios para intentar ralentizar su adopción. La lucha en este sentido ha alcanzado el grado de cruzada, con fortísimos esfuerzos de lobby destinados a influenciar las decisiones de adopción de gobiernos, instituciones y empresas de todo tipo, y grandilocuentes anuncios de infracciones de patentes que resultaban estar escasamente sustentados por la realidad.

En la práctica, el software libre supone para Microsoft la mayor de las amenazas: mientras los productos de Microsoft progresan únicamente cuando la generación anterior de los mismos ya no vende lo suficiente, es decir, condiciona el progreso a determinantes puramente comerciales, las comunidades de software libre solucionan los problemas en el momento en que aparecen y mantienen una evolución y adaptación mucho más rápida. Mientras que en el segmento de usuarios finales, la cuota de sistemas operativos basados en Linux se mantiene por debajo del 5%, en algunos segmentos estas cifras son muy superiores: actualmente, las mejores opciones para servidores están basadas en código abierto, la opción adoptada por la práctica totalidad de las compañías relevantes en Internet. El mejor servidor web es Apache, un producto de código abierto. El mejor navegador es para muchos Firefox, igualmente de código abierto. Los mejores lenguajes de programación son PHP, Perl o Python, todos ellos de código abierto. Y el número de segmentos se incrementa con el tiempo. En la lucha contra el software libre, Microsoft se ha hecho mucho más daño a sí misma que el daño que ha podido causar a un supuesto enemigo invisible contra el que nunca supo competir.

Pero más allá de la crisis de ventas, el verdadero problema de Microsoft viene dado por la adaptación a los tiempos: a pesar de seguir siendo obviamente una gran compañía, con un potencial de investigación brutal y una enorme capacidad de influencia sobre la industria, su planteamiento de «un ordenador que funcione con nuestro software en cada escritorio y en cada hogar» ha caído en la más absoluta obsolescencia. A día de hoy, un número cada vez más alto de personas no tienen «un ordenador», sino una amplia gama de dispositivos que incluyen máquinas de sobremesa, portátiles, netbooks, teléfonos móviles, etc., que pretenden compartir datos con aplicaciones que, cada día más, se sitúan ya no tanto en los dispositivos, sino en la web. El paso de un mundo ordenador-céntrico a uno red-céntrico, a pesar de producirse gradualmente a lo largo de muchos años, se ha encontrado con una compañía muy poco preparada para ello. En pleno 2010, la línea de negocio más importante de la compañía sigue siendo la venta de licencias de un software metido en cajas de cartón envueltas en celofán, un modelo de los años ochenta en el que Microsoft resultó ser imbatible, pero la adaptación para competir en el entorno de la red con empresas de otro tipo parece más que dudosa. Microsoft, en Internet, es un competidor completamente mediocre, frente a una Google cada vez más grande y poderosa, que se ha convertido en su principal bestia negra. Ante la evidencia, la compañía intenta trabajar en este sentido: el lanzamiento de Windows 7 es un paso en la dirección adecuada. Hablamos de un sistema operativo que funciona adecuadamente, que recibe buenas críticas, y que funciona en máquinas existentes, sin que ello evite que siga suponiendo un incremento de ventas de ordenadores nuevos con Windows 7 preinstalado. Si en las últimas versiones de Windows eran muy pocos los usuarios que adquirían una licencia para instalarla sobre su máquina, sino que simplemente «venía puesta», en esta ocasión, Microsoft puede encontrarse con un mercado de venta directa interesante de usuarios que abandonan finalmente el vetusto Windows XP, un producto de más de ocho años. Igualmente, Microsoft intenta el paso hacia un Office capaz de competir en la red, aunque evitando con ello perjudicar las ventas de licencias del programa, una combinación sin duda difícil y que se encuentra con un fuerte ascenso de los productos de Google entre particulares y empresas.

El mayor problema de Microsoft hoy en día es la sensación que produce de empresa «lenta y aburrida», que provoca indiferencia, incapaz de protagonizar tendencias, y que se mueve siguiendo los pasos de otras: los usuarios considerados «punteros» o cool están en Linux o en Apple, que aún con cuotas de mercado mucho menores, generan comunidades mucho más entusiastas y comprometidas, dispuestas a hablar de la compañía en todo foro disponible. Cada vez que el fundador de Apple, Steve Jobs, se sube a un escenario, el mundo escucha, y legiones de usuarios con la misma dinámica que los fans de un grupo de rock acuden en tropel a adquirir los nuevos productos que la empresa ofrece. Los desarrollos de Apple marcan la tendencia que, posteriormente, Microsoft intenta copiar con resultados claramente inferiores. En comparación con el entusiasmo generado por Apple, las actualizaciones de Microsoft provocan aburrimiento, pereza y fatiga. Microsoft sigue siendo «la opción segura» en las empresas, la que los directores de tecnología adoptan, sin pensar demasiado, como exenta de riesgo, pero carece de imagen de líder, de vanguardia. El mercado bursátil no permanece ajeno a esa falta de entusiasmo: en los últimos cinco años, las acciones de Microsoft muestran un comportamiento casi completamente plano, que redunda en una caída global del valor de un 5%. Mientras, en el mismo período, Google ha subido más de un 200%, y Apple lo ha hecho casi un 800%.

Sin duda, la empresa se está preparando para competir en un escenario en el que las cosas van a ser muy diferentes a como fueron anteriormente. Nadie duda de la capacidad competitiva de Microsoft como compañía, pero sí de su papel como líder y generador de tendencias, y de que sea capaz de obtener las cuotas de mercado que sostuvo anteriormente. Fundamentalmente, porque la industria sabe que esa situación no es buena para nadie. Ahora, Microsoft ve ascender en cuota de mercado a una Apple que ha superado su mejor porcentaje de todos los tiempos (supera ya el 10%) y a sistemas basados en Linux que cuentan con una fuerte penetración en servidores. En navegadores, el Internet Explorer de Microsoft cuyo logotipo en forma de «e» azul llegó a ser sinónimo de Internet y a alcanzar un 90% de cuota de mercado, ha caído ya por debajo del 60% ante un competidor, Firefox, generado a partir de las cenizas de Netscape y utilizando código abierto, o ante navegadores como Safari, de Apple, o Chrome, de Google. En telefonía móvil, el Windows Mobile de Microsoft es una opción cada vez menos popular en el segmento de los denominados smartphones, un segmento cada vez más repartido entre las BlackBerry de RIM, el iPhone de Apple, y los sistemas basados en el sistema operativo Android, creado por Google.

De Microsoft se puede decir, sin duda, que tuvo una participación fundamental en la popularización de la informática: fue capaz de llevar el ordenador a la vida diaria de millones de personas, y de conseguir que viésemos de manera natural tener un teclado en las manos. La evolución de esa ventaja tan bien conseguida, sin embargo, es mucho más dudosa: pocos dudan de que el progreso tecnológico habría sido mucho más rápido sin el fuerte papel de freno que ha impuesto Microsoft, con sus tácticas de competencia desleal y su obsesión por el control del mercado. Microsoft y el ecosistema generado por sus productos es, en gran medida, responsable de una gran parte de la inseguridad existente en la red: mientras que sus usuarios deben instalar necesariamente antivirus y cortafuegos, los de ordenadores con Linux y Apple viven completamente al margen do este tipo de problemas.

Para muchos, la compleja adaptación de Microsoft a los nuevos tiempos es un caso que requiere un auténtico cambio generacional. La salida de Bill Gates de la compañía en el año 2000 dejó un vacío importantísimo: con todas sus carencias y su incapacidad para entender el fenómeno del software libre en un contexto amplio, Bill Gates era un tecnólogo, una persona capaz de entender e interpretar la tecnología, mientras que la persona que le sucedió en la dirección ejecutiva, Steve Ballmer, respondía decididamente al perfil de un hombre de negocios, de un financiero. Bajo la dirección de Ballmer, en la primera década del siglo, la compañía ha logrado llevar la facturación desde los 23.000 millones de dólares hasta los 58.000, pero lo ha hecho a cambio de hipotecar su futuro, de convertirse en una empresa burocrática y lenta, incapaz de mantener el ritmo trepidante de la industria. Sin duda, la compañía mantiene todavía una posición dominante en algunos aspectos, y apalanca muy bien las ventajas obtenidas durante muchos años: es el líder absoluto en sistemas operativos y en aplicaciones de escritorio, lo que podría llevarnos a calificar a Microsoft como «el líder de los mercados vetustos». Pero de ahí a que pueda volver a ser la compañía que marque tendencias o la más relevante en la industria, la que señale la dirección a seguir en los nuevos mercados, va un largo trecho. Como caso de estudio, Microsoft, a pesar de su dominio del mercado y de su volumen de facturación, no puede, paradójicamente, ser calificada de éxito. Es, mucho más, la historia de una falta de adaptación.

CAPÍTULO 11. LA EVOLUCIÓN DE LA WEB

«Estamos contemplando nada, más y nada menos que el nacimiento de una nueva economía, una economía digital, y un nuevo medio global que va a ser el motor de cambio social, económico y de negocios más importante de todo el siglo que viene.»

LOU GERSTNER, CEO de IBM,

14 de junio de 1999

Internet, en muchos sentidos, funcionó como el descubrimiento de un nuevo continente: una abundancia de suelo fértil sobre el que desarrollar y lanzar innovaciones. Durante la segunda mitad de la década de los noventa y la siguiente, surgieron en Internet creaciones interesantísimas, que han ido añadiendo características y prestaciones para dar forma a la red que hoy conocemos. Repasarlas con una cierta estructura es un ejercicio que puede proporcionarnos una buena visión acerca de la evolución y las características que han llevado a la web a ser lo que es hoy en día.

En 1994, Jeff Bezos creó Amazon, en su momento una librería online; hoy, sin duda, el mayor exponente de comercio electrónico y una de las tiendas líderes en volumen de negocio en todas las grandes citas comerciales norteamericanas. En realidad, Bezos escogió comenzar por los libros, por ser un producto de fácil descripción con el que poder «aprender el negocio», pero su metodología comenzó pronto a extenderse a un número cada vez mayor de categorías, desde joyas hasta muebles de jardín. Pero a pesar de convertirse en la mayor tienda en la red, la importancia de Amazon proviene de otro tema posiblemente menos conocido: la capacidad de desarrollar un sistema de recomendaciones capaz de sugerir al visitante los artículos en los que puede estar interesado. El sistema de recomendaciones de Amazon origina más de la mitad de las ventas de la página, y sería el equivalente a tener una tienda cuyo escaparate cambiase en función de los gustos expresados anteriormente por cada cliente que dirigiese su mirada hacia él. Las recomendaciones comienzan a trabajar cuando alguien entra, por primera vez en Amazon: en esa primera pantalla de bienvenida, el sistema le recomienda los artículos más vendidos porque es la forma de maximizar las probabilidades de acertar, pero pasa a mejorar sus recomendaciones a partir del primer clic. Cada vez que el usuario toma una opción, hace clic en un artículo o introduce palabras para una búsqueda, la enorme base de datos de Amazon extrae una consulta en la que agrupa a las personas que en algún momento han tomado una opción similar, y recomienda los elementos más comunes a ese subconjunto. Al cabo de unos cuantos clics y visitas, la exactitud del sistema puede ser mejor que la que tendría nuestro amigo el librero de la esquina que nos conoce desde que llevamos pantalón corto, y ni siquiera ha necesitado saber nuestro nombre o que adquiriésemos producto alguno. Por supuesto, en el momento en que lo hacemos, Amazon anota nuestro nombre, dirección de envío y datos de tarjeta de crédito, y los asocia con todo el historial de navegación del ordenador, poniendo así «cara y ojos» a ese registro de su base de datos. Ese marketing de base de datos, esa capacidad de dinamizar la oferta en función de las características del usuario, ha dado origen a los denominados «sistemas de recomendación», que sin duda jugarán un papel importantísimo en la red del futuro. En una red con cada vez más opciones disponibles, todos necesitaremos agentes capaces de reducir la complejidad por nosotros en función de criterios sociales, personales o de otros tipos. En 1995, Pierre Omidyar desarrolla eBay, la empresa reina de las subastas en Internet, y pone de manifiesto otra propiedad fundamental de la red: la capacidad de agrupar demanda, por peregrina o residual que esta sea, y dar origen a mercados donde antes no existían. Antes de eBay, un coleccionista de, por ejemplo, dispensadores de caramelos Pez (la propia mujer del fundador, sin ir más lejos) debía pasar por un calvario importante para poder tener la oportunidad de conocer a otras personas con una afición como la suya y aspirar a intercambiar con ellos elementos de su colección. Después de eBay, ese mercado existía, era dinámico y estaba al alcance de un clic, sirviendo además todas las categorías de productos imaginables. El ideal de «la basura de una persona es el oro para otra» dio origen al mercado más grande del mundo, al que millones de personas acudían constantemente para comprar y vender cualquier tipo de artículo. Gracias a eBay pudimos comprobar el concepto del precio dinámico: progresivamente, eBay dejó de ser una galería de excentricidades que no encontraban otro lugar donde venderse, un rastrillo para vender lo que nos sobraba en el trastero o un regalo que no nos gustaba, y se convirtió en un lugar al que acudían un porcentaje cada vez mayor de comerciantes profesionales, en un mercado capaz de saturar con mucha mayor eficiencia el espacio delimitado por la curva de la oferta y la demanda: cada producto podía tener no únicamente un precio, sino varios, en función de diferentes criterios.

En 1998, dos jóvenes estudiantes doctorales de la Universidad de Stanford crean Google, y redefinen el concepto de relevancia: Google es una de esas empresas que delimitan un antes y un después, que hacen realidad el sueño de tener toda la información al alcance de la mano mediante un algoritmo de búsqueda no completamente conocido, pero que todos podemos entender partiendo de una base lógica. La empresa comienza sin una idea clara sobre su modelo de negocio, pero con la conciencia de que estaba sentada sobre algo grande: en muy poco tiempo, la práctica totalidad de los motores de búsqueda anteriores ven reducida su participación ante el crecimiento de Google. Cuando un cierto tiempo después incorpora el modelo de publicidad contextual que habían visto desarrollado en Overture, Google se convierte en una empresa de grandes ingresos y crecimiento (a pesar de tener que pagar posteriormente a Overture 2,7 millones de acciones en concepto de licencia perpetua por la tecnología en la que se habían inspirado), y empieza a funcionar como un poderoso actor en la definición de la web que conocemos hoy en día. Realmente, sería muy difícil imaginarse la web sin Google y sin el historial de desarrollos y adquisiciones que ha ido protagonizando a lo largo de los años.

Una de las grandes características de la red es su fortísima vocación democratizadora. Creaciones como la ya mencionada de Shawn Fanning en 1999; Napster, o el BitTorrent desarrollado por Bram Cohen en 2001 hicieron realidad lo que para algunos pasó a ser de manera inmediata una auténtica pesadilla: la posibilidad de que cualquier contenido pudiese obtenerse fácil y rápidamente mediante la compartición de recursos de las máquinas conectadas a la red. La posibilidad de poner un archivo cualquiera en una carpeta de tu disco duro y, de manera automática, poder compartirlo con todos los usuarios de un programa determinado alimentó el sistema de distribución más eficiente que el hombre ha diseñado en toda su historia: el P2P. Antes del P2P, era necesario tener un gran servidor y la disponibilidad de un importante caudal de ancho de banda para poder ofrecer un contenido o cualquier otro archivo en la red» algo que hoy absolutamente cualquier persona puede hacer con simplemente instalarse un programa de sencillo manejo, y que supone un importantísimo incremento de la eficiencia de la red como medio transmisor. La oposición de la industria de los contenidos al P2P supone, precisamente, la demostración palmaria de lo comentado en el capítulo anterior: una industria oponiéndose a una forma de hacer lo mismo que ellos hacían, pero con una mayor eficiencia. Lo que viene a demostrar que estas empresas vivían, precisamente, de la existencia de una gran ineficiencia. Toda la lucha por la vía legal y política de los lobbies del copyright ante el P2P debe ser interpretada como la resistencia de una industria ineficiente ante el progreso de la tecnología, que permite hacer las cosas con menos fricción y resistencia. Otro de los grandes cambios definitorios del entorno actual comenzó a fraguarse en febrero de 2003. Pyra Labs, una pequeña start-up californiana fundada por Meg Hourihan y Evan Williams en 1999 dedicada al desarrollo de herramientas de publicación personales al alcance de cualquiera, recibe una oferta de compra procedente de Google, que acepta tras unas muy breves negociaciones. El producto principal de Pyra Labs era una herramienta de tipo Content Management System, un sistema gestor de contenidos llamado Blogger, que permitía generar un blog con enorme facilidad. Si bien los blogs, abreviatura de web log o «diario web», no se originaron con Blogger, sí sería adecuado atribuir a la adquisición de Pyra Labs por Blogger la popularización de los mismos: en muy pocos años, el término blog pasó a ser de uso común, a aparecer en los diccionarios de muchas lenguas, y a contar con millones de practicantes que, con mayor o menos periodicidad, utilizaban plataformas como Blogger, TypePad o WordPress para reflejar sus pensamientos en la web. El uso de una herramienta de este tipo hacía realidad el concepto de push-button publishing, publicación a golpe de botón: todo el mundo, independientemente de sus conocimientos de tecnología, podía publicar una página en Internet.

Una disminución tan brusca de las barreras de entrada para la publicación en Internet produjo precisamente el resultado que se podía esperar: un crecimiento brutal de la web como plataforma, y una multiplicación exponencial del número de autores en la misma. La llegada de las herramientas de publicación sencillas al alcance de cualquiera cambió la naturaleza de la red, y se convirtió en uno de los grandes detonantes de los cambios que detallaremos en los próximos capítulos. De repente, pasábamos de una web hecha por aquellos que podían, a una que hacían todos aquellos que querían, y esa diferencia entre capacidad y voluntad marcaba una enorme diferencia. Al principio, pocos supieron reaccionar: la sensación era la de que la web se poblaba de pensamientos sin sentido, que recogían desde el día a día de personajes completamente anónimos hasta todo tipo de falacias y barbaridades que pasaban a tener, por el hecho de presentarse a través de una pantalla, un aspecto «razonable», una pretensión de autoridad. Para quienes estaban acostumbrados a recibir a través de una pantalla únicamente aquellos mensajes que habían sido previamente autorizados por una autoridad determinada, la idea de ser ellos mismos —o peor, cualquier otro— capaces de utilizar un canal similar provocaba vértigo. Igualmente, abundaban los razonamientos espaciales: la web se va a quedar pequeña, se va a saturar, no habrá espacio para tantas páginas, o peor, será imposible encontrar lo que necesitamos entre tanta basura. Una lógica proveniente del mundo físico, pero sin ningún sentido en el mundo de la red: en realidad, la progresiva disminución del coste de almacenamiento y el incremento de la eficiencia de los soportes lleva a que el espacio disponible en la red sea, por pura ley de oferta y demanda, cada vez mayor, no menor, mientras que los motores de búsqueda se encargan de conseguir que podamos encontrar lo que buscamos entre la inmensidad de datos existentes. Casi diez años después, la idea de una web que se queda sin espacio y donde ya no se puede encontrar nada resulta completamente ridícula ante la evidencia de una web más potente que nunca, llena de información útil, donde puede encontrarse prácticamente todo, y con un valor generado absolutamente ilimitado en una amplísima multiplicidad de facetas, que van desde el ocio al negocio: todo está en la web, disponible a pocos clics de distancia, y toda una nueva generación de profesionales se plantea cómo podría hacer su trabajo si esa web no estuviese ahí.

En 2002, Jonathan Abrams lanza Friendster, la primera red social considerada específicamente como tal. La idea era reunir, en una base de datos accesible en Internet, a todos los contactos de una persona, de manera que pudiera recurrir a ella para mantenerlos actualizados, interactuar o enviarse mensajes de todo tipo. Durante el año siguiente, 2003, vieron la luz proyectos como Hi5, Linkedin o MySpace. Facebook lo hizo en 2004, y la web se pobló de millones de personas que volcaban en ella sus datos, sus contactos, sus fotografías y una parte cada vez mayor de su vida.

La dinámica, además, es saludada de manera entusiasta por la comunidad de negocios: en 2005, el gigante de la comunicación News Corp., dirigido por Rupert Murdoch, adquiere MySpace, una de las redes sociales que había manifestado mayor actividad, por un total de 770 millones de dólares en metálico, transacción a la que siguen la compra de Bebo por AOL por 850 millones de dólares, y una inversión de Microsoft en Facebook, que adquiere un 1,6% de la empresa a cambio de 240 millones de dólares, lo que suponía por grosera extrapolación una valoración por encima de los quince mil millones. Vistas esas cifras, muchas voces gritan «¡burbuja!», y se plantean dónde está el valor generado por esas redes sociales en las que tantas personas se intercambian contenidos fundamentalmente frívolos, y que en casi todos los casos tienen cuentas de ingresos minúsculas derivadas de una publicidad en la que pocos visitantes llegan incluso a reparar. Además, las redes sociales estaban sujetas a patrones de evolución completamente caprichosos e impredecibles: bastaba ver el ejemplo de la red que dio origen al fenómeno, Friendster, que a pesar de haber sido creada por un norteamericano, se había convertido en una red verdaderamente importante en países como Filipinas, Indonesia, Malasia y sudeste asiático en general, hasta que terminó, en diciembre de 2009, siendo vendida a una empresa malaya.

El caso de Orkut, red social creada por Google en 2004, resulta todavía más paradójico si cabe: el proyecto provenía del trabajo en su 20% del tiempo asignado a temas de interés personal del ingeniero turco Orkut Büyükkókten, trabajador de Google, que tomó en principio la decisión de abandonar la compañía para lanzar su red social de manera independiente. La respuesta de Google, sin embargo, fue la de ofrecerle un reajuste en su dedicación que le permitiese lanzar la compañía como subsidiaria de Google con el apoyo decidido de esta, oferta a la que Büyükkókten respondió afirmativamente. Pero tras el lanzamiento, que tenía una lógica visión universal, las cosas empezaron a evolucionar de manera extraña: durante el primer año, predominaron los usuarios estadounidenses, como de hecho ocurría en la mayor parte de las redes sociales de la época originadas en ese país. Pero a lo largo de 2005, Orkut comenzó a sufrir una auténtica avalancha de ﻿mecanismos virales, boca-oreja y por la influencia de la blogosfera, lo que determinó que el idioma habitual de la red pasase lentamente a ser el portugués, y la salida de muchos angloparlantes que se sentían desplazados por un predominio fortísimo de contenidos escritos en un idioma que no podían entender. En la actualidad, la mitad del tráfico de Orkut proviene de Brasil, país al que Google decidió desplazar tanto la dirección como los servidores de la empresa en agosto de 2008, en buena prueba de que las dinámicas sociales suelen ser profundamente difíciles de contrarrestar. El otro país mayoritario en Orkut, la India, aporta en torno a un 15% de los usuarios, que comenzaron a entrar a partir de 2007. Los Estados Unidos actualmente suponen menos de un 9% del total.

Obviamente, en la serie de operaciones de adquisición y toma de participación a las que mucha gente se refirió como «la fiebre del oro de las redes sociales», había mucho más que explicar. Mientras las adquisiciones de MySpace y de Bebo eran compras completas que valoraban a cada usuario, respectivamente, en 30 y en 22 dólares, una cantidad que podríamos llegar a pensar que un usuario sería susceptible de generar con su atención si hiciese clic o visualizase la publicidad apropiada o llevase a cabo determinadas transacciones, la entrada de Microsoft en el capital de Facebook tenía muy poco que ver con estas. Analizada con los mismos parámetros, valoraba a cada usuario de Facebook por encima de los trescientos dólares, y además, no adquiría el control de la compañía, sino únicamente un 1,6%. En realidad, lo único que pretendía Microsoft era tener una posición más adecuada para negociar el contrato de publicidad que le daba la exclusividad para poner anuncios en las páginas de Facebook, un negocio que la empresa anticipaba interesante en función del número de páginas vistas que generaba, y por el que pugnaba también el ya considerado entonces mayor competidor de Microsoft: Google. En aquel momento, en términos de lo que se denomina «inventario publicitario» (número de páginas en las que poder poner anuncios), Google disponía de todas las páginas vistas que generaba la actividad de su buscador, unidas, además, no solo a las generadas por sus múltiples actividades adicionales, sino también a la red de páginas propiedad de terceros que gestionaba mediante AdSense (véase más información en el capítulo dedicado a Google). Mientras, Microsoft tenía un portal y un buscador en franca decadencia, y veía cómo lo que estimaba que podía ser uno de los negocios más importantes de la web se le escapaba: en la práctica, Microsoft habría pagado cualquier cantidad para evitar la entrada de Google, lo que explica en gran medida la astronómica y a todas luces irreal valoración. El caso de Facebook es también especialmente interesante: hasta mediados de 2007, la red social creada por Mark Zuckerberg evolucionaba prácticamente igual que el resto, con crecimientos razonablemente elevados, pero no destacables, en su contexto, y no amenazaba el liderazgo mundial de MySpace. Sin embargo, las cosas cambian el 14 de mayo de 2007 con la publicación de su plataforma de aplicaciones: ideada para que los desarrolladores puedan crear aplicaciones y ponerlas a disposición de los usuarios de Facebook, su popularidad crece a gran velocidad y convoca a programadores de todo el mundo. A pesar de que obligaba a los programadores a utilizar un lenguaje propio, el FBML (Facebook Markup Language) en lugar del habitual HTML de la web, la plataforma provocó que, de la noche a la mañana, Facebook se poblase de todo tipo de aplicaciones dedicadas a los fines más variados y, por qué no decirlo, más absurdos. Había aplicaciones que servían para desafiar a tus amigos a un quiz, para enviarles mordiscos de vampiro, lanzarles bolas de nieve, darles abrazos de oso o compartir con ellos todo tipo de cuestiones sin equivalente real en el mundo virtual, como cervezas. Sin embargo, semejante proliferación de aplicaciones aparentemente sin sentido tuvieron, como resultado, un enorme crecimiento, tanto en el número de usuarios, que pasó de unos quince millones en mayo de 2007 a más de sesenta millones en diciembre de ese mismo año, como en la permanencia de los mismos en la red. En septiembre de 2009, Facebook alcanzó los 300 millones de usuarios y entró en cash-flow positivo, destrozando infinidad de mitos sobre la insoportable levedad de los fenómenos tecnológicos.

Facebook había descubierto un axioma fundamental en las redes sociales: para provocar y favorecer el uso de una red social, había que provocar y favorecer las circunstancias que creaban contexto en esa red social. Si aislamos las relaciones sociales habituales entre las personas y las sometemos a una observación fría y desapasionada, comprobaremos que una gran mayoría de nuestros gestos sociales se basan en convenciones y acciones que, en sí mismos, poseen un valor práctico más bien escaso: una reunión social entre amigos suele consistir en actos sociales vinculados en muchos casos a la bebida y la comida, a una sucesión de chistes más o menos imaginativos, risas, bromas, comentarios de situaciones acaecidas a unos y otros miembros del grupo y, en general, a cuestiones perfectamente prescindibles, salvo por el importante detalle de que no queremos prescindir de ellas porque hacen nuestra vida mucho más agradable. Las relaciones sociales están en gran medida construidas sobre este tipo de materiales, de manera que, para que una red social cumpla su función, es importante que sirva para proporcionar contexto a estas relaciones. No es lo mismo entrar en la red social para felicitar los cumpleaños y enviar un par de mensajes, que hacerlo y que te proporcione la oportunidad de enviar mil y una cuestiones entre simpáticas y entretenidas que permiten mantener viva la relación social en ausencia de contacto tísico. Después de todo, el hombre es la única especie en el planeta capaz de sublimar las relaciones sociales para que estas se mantengan sin un contacto directo, algo que no resulta trivial y en cuya base se encuentra, precisamente, la tecnología.

El año 2004 nos trae a la web otra interesantísima dimensión todavía no completamente explorada: la aparición de la información geográfica. A finales de ese año, una Google reforzada económicamente por su muy exitosa salida a bolsa de agosto, adquiere Keyhole, una empresa de tan solo tres años de antigüedad dedicada a la visualización de datos geoespaciales, e incorpora a su cartera de productos Google Maps y Google Earth, capaces de permitir una navegación en mapas con un interfaz de manejo espectacular. La incorporación de la información geográfica a la web coincide en el tiempo con la popularización de la tecnología de geoposicionamiento por satélite o GPS: en un año, el GPS pasa de ser un costoso instrumento cuasiprofesional a convertirse en la estrella de las compras navideñas. La combinación de dispositivos GPS cada vez más baratos e integrados en teléfonos móviles y las herramientas de la web geográfica están posibilitando un desarrollo enorme de combinaciones, y permitiendo una dimensión aun escasamente explorada: la de un Internet sensible al espacio, en el que podamos buscar cosas, por ejemplo, en función de que estén más o menos cerca de nosotros o de nuestro barrio, otorgando unas importantísimas posibilidades de desarrollo futuro a la llamada «Internet local».

En 2006, otra adquisición de una Google convertida ya en un líder sólido introduce, con una enorme fuerza, uno de los contenidos que en la actualidad aparecen dotados de un mayor dinamismo: el vídeo. La empresa adquirida, YouTube, era una pequeña startup que unos amigos habían creado tan solo un año antes, tras una fiesta en casa de uno de ellos, en la que varios de los asistentes llevaban cámaras de vídeo. Al día siguiente, algunos de los asistentes quisieron compartirlos materiales grabados la noche anterior, y se encontraron con una patente limitación: el correo electrónico era demasiado limitado para el manejo de ficheros de vídeo. Muchas cuentas de correo limitan el tamaño de los ficheros adjuntos, y el vídeo se caracteriza precisamente por la generación de archivos voluminosos. La opción de situar los archivos en un servidor para su descarga era también compleja, o excedía al menos las habilidades y disponibilidades de una amplia mayoría de usuarios de la web. La opción ideal, además, era la de ver directamente los vídeos desde el lugar en que estuviesen almacenados, sin tener que solicitar su descarga y esperar a que se completase para empezar a verlos. Con tales premisas, crearon una página simple a la que los usuarios podían subir los vídeos, y en la que estos eran convertidos al ya entonces prácticamente ubicuo formato Flash, que podía ser visualizado en la gran mayoría de los ordenadores en cualquiera de las plataformas habituales. La plataforma tenía, además, funciones sociales: un usuario podía definir amigos, y compartir con ellos los vídeos que fuese subiendo a la plataforma, poner nota a cada vídeo con un sistema de estrellas, y la interesante posibilidad de integrar («embeber») los vídeos en cualquier otra página. El éxito del sitio fue impresionante. En muy poco tiempo, millones de usuarios habían poblado YouTube con todo tipo de vídeos, que aparecían además por doquier en miles de páginas en la red, y el sitio sostenía unos ritmos espectaculares en cuanto a número de usuarios y materiales disponibles. Los usuarios, además, hacían un uso prácticamente mínimo: casi todos ellos compartían sus vídeos con toda la red, sin restricciones de ningún tipo. Los intentos de otros competidores, incluida la propia Google, de crear sitios similares no fueron capaces de rivalizar en ningún momento con las dimensiones de YouTube, que aparecía como un gigante tanto en el volumen de material que manejaba como en el ascenso de los costes de ancho de banda que debía afrontar para distribuir un número de vídeos tan elevado. Finalmente, en noviembre de 2006, Google hizo una oferta por la compañía por valor de 1.650 millones de dólares en forma de cotizadísimas acciones de Google, que los fundadores del sitio aceptaron sin pestañear. En el período que iba desde febrero de 2005 hasta noviembre de 2006, algo menos de dos años. Steve Chen, Chad Ilurlev y Jawed Karirn habían pasado de empleados de PayPal a multimillonarios. El mismo día del anuncio de la operación, los dos primeros publicaron un vídeo en el que, en pleno ataque de risa histérica, agradecían a los usuarios su contribución: lo que realmente valía dinero en YouTube no era la plataforma, relativamente sencilla en su desarrollo, sino el enorme inventario de videos de todo tipo con el que los usuarios la habían llenado (y continúan haciéndolo a un ritmo que supera las veinte horas de vídeo cada minuto que pasa), y que la había convertido en el mayor repositorio de vídeo jamás puesto a disposición de la Humanidad.

El éxito de YouTube resulta sumamente significativo: el desmesurado coste que representaba almacenar y servir a los visitantes la inmensa cantidad de vídeos que solicitaban la situaba muy lejos de la rentabilidad, pero alguien como Google, incapaz de competir con ella con su propia iniciativa, Google Video, llegaba, ponía 1.650 millones de dólares encima de la mesa, y convertía en ricos a sus fundadores, evocando de nuevo la archi-famosa burbuja de finales de los noventa que todos los escépticos sacaban a relucir en estos casos. ¿Que llevaba a una empresa cotizada en bolsa y con accionistas y directivos supuestamente en sus cabales a desembolsar una gran cantidad de dinero para hacerse con una página web creada hacía menos de dos años, y que perdía una barbaridad de dinero cada minuto que pasaba, cada vez que sus millones de visitantes hacían clic en uno de sus infinitos vínculos? La respuesta, contrariamente a lo que algunos pretendían, no estaba relacionada con ningún tipo de enajenación mental transitoria, sino con algo denominado «economía de la atención»: YouTube representaba la primera televisión a escala planetaria, un canal que podía ser visto por personas de todo el mundo, y que a su vez contenía un infinito número de canales, administrados de todas las maneras imaginables: podía regular qué vídeos servía en que lugares, a qué usuarios, y qué contenido los rodeaba o era recomendado en ellos.

Finalmente, en octubre de 2009, se desveló el misterio: en realidad. Google no pagaba prácticamente costes de ancho de banda por el vastísimo flujo de información generado por YouTube, sino que lo convertía en tráfico intercambiado con otros proveedores de acceso a la red a través de la enorme red de fibra que Google había ido adquiriendo a lo largo del tiempo. YouTube había llegado a un punto en el que el crecimiento, lejos de convertirse en un problema, le resultaba beneficioso. Y obviamente, había redefinido los hábitos de toda una generación, que cada vez que deseaba acceder a un contenido audiovisual, se iba directamente a esa página. Actualmente, YouTube protagoniza una auténtica redefinición de hábitos de consumo: resulta cada vez más frecuente verlo como punto central de ocio de personas de distinta composición demográfica, desde niños que buscan contenidos de entretenimiento hasta adolescentes buscando música, pasando por adultos que comparten vídeos graciosos o momentos nostálgicos de cuando eran niños. YouTube es el mayor repositorio de contenidos en vídeo del mundo, la auténtica «Biblioteca de Alejandría del vídeo», y sus posibilidades solo están empezando a ser exploradas.

Para una empresa como Google, que se preciaba de querer «organizar la información mundial para que resulte universalmente accesible y útil», YouTube representaba el dominio de un tipo de contenido en tan franco crecimiento como el vídeo, como adquirir la mayor cadena de televisión del mundo, libre, además, de restricciones geográficas. El hecho de que dicha cadena de televisión no fuese todavía capaz de ganar dinero se reducía, en la mentalidad de Google, a la proyección de ese mismo adverbio de tiempo, «todavía». Un adverbio cuya importancia en la evolución tecnológica y en nuestra asimilación de la misma deberíamos seguramente tener en buena consideración: la diferencia entre los escépticos más recalcitrantes y los auténticos visionarios tecnológicos está en la interpretación de ese adverbio, en la estimación no de lo que la tecnología va a permitir, sino de cuándo va a hacerlo.

CAPÍTULO 12. UN CASO PRÁCTICO: GOOGLE

«La misión de Google es organizar la información mundial para que resulte universalmente accesible y útil.»

LARRY PAGE y SERGEY BRIN (1999)

Google es una historia reciente, de desarrollo vertiginoso, que casi todo el mundo conoce. Todo en la empresa es como una película en cámara rápida: en once años, Google ha pasado de ser un proyecto en la cabeza de dos estudiantes doctorales de Stanford, a ser el motor de búsqueda utilizado por la gran mayoría de la población en casi todos los países del mundo, una de las mejores empresas para trabajar según la revista Fortune, una de las mejor cotizadas en los mercados financieros, y una de las más fuertemente vigiladas por las autoridades antimonopolio. Jocosamente, hay quien habla de un AG y un DG, un Antes de Google y un Después de Google, algo que, al menos en lo concerniente a la historia de internet, es decididamente así.

Ante tan vertiginosa evolución, no resulta sorprendente darse cuenta de que la gran mayoría de las personas que utilizan Google todos los días o escuchan hablar de ella no sepan, en realidad, cómo funciona. Para el usuario medio, el funcionamiento de Google es un misterio: no entienden cómo es capaz de brindarlos mejores resultados en su buscador ni cómo puede valer tanto una empresa que no cobra prácticamente nada a sus usuarios. Pero tampoco se preocupan demasiado por entenderlo. Es como si el rutilante éxito de Google prevaleciese sobre la necesidad de explicación alguna para sus usuarios: funciona y ya está. Para las empresas, sin embargo, no es así: en muchos países del mundo, tener una buena posición en Google cuando tus clientes potenciales buscan determinadas palabras significa ya la diferencia entre el éxito y el fracaso. Cada día más, lo que no aparece en Google, no existe.

Situémonos en la cabeza de sus fundadores en 1999, hacer un doctorado en una de las universidades más prestigiosas de los Estados Unidos es una experiencia sumamente intensa. Los estudiantes doctorales llegan al punto de obsesionarse con sus temáticas, de vivirlas muy intensamente y, por qué no, de odiarlas también en determinadas ocasiones. Y uno de los momentos más importantes en esa denominada grad-student life, o «vida del estudiante doctoral», es la decisión del tema de investigación. Así, cuando Larry Page y Sergey Brin empezaron a barajar la hipótesis de que analizar las relaciones en forma de enlaces entre páginas web era una manera mejor de calcular la relevancia que simplemente contar el número de apariciones de una palabra en ellas, cayeron en seguida en la cuenta de que tenían entre manos algo muy importante.

Para entender la importancia de este hecho, demos un paso atrás e intentemos entender el funcionamiento de algo tan fundamental en la web como un motor de búsqueda: para desarrollar su trabajo, un motor de búsqueda no puede salir a buscar lo que un usuario le pide cada vez que este introduce un término en la cajita correspondiente. Con la dimensión de la web, encontrar en tiempo real cuál es la página más relevante para ese término sería algo que llevaría no horas, sino seguramente días: los usuarios tendrían que enviar sus consultas y quedarse esperando a recibir el resultado, algo impensable. En su lugar, lo que hace un buscador es construir un índice, una base de datos propia, una especie de copia referenciada de la web. Para ello, desarrolla unos programas denominados «arañas», bots, spiders o crawlers, que llegan a una página y realizan un análisis de esta, que suele denominarse parsing o, en versión castellano imposible, «parseado», esto consiste en la obtención de métricas como las frecuencias de las palabras existentes en la página, la eliminación de artículos y otras palabras irrelevantes, las distancias relativas entre términos, la importancia o peso de los mismos en función de que estén en un título o en el cuerpo del texto, etc., que son inmediatamente almacenadas en una base de datos junto con la dirección de la página. Cuando la araña termina el análisis de una página, simplemente toma un vínculo en ella, y se desplaza a otra página. Al llegar a la nueva página, debe primero comprobar si se encuentra ya en la base de datos. Si efectivamente es así, comprobará la versión existente en la base de datos con respecto a la que ha encontrado en la web, verificará la fecha de la última actualización, y si no ha habido cambios, continuará a la siguiente página. Si, por el contrario, los ha habido, actualizará la base de datos con los cambios correspondientes. Así, paso a paso, a una velocidad vertiginosa, la araña acaba construyendo una base de datos en la que se encuentran los análisis de una gran cantidad de páginas de la web. Una base de datos sobre la que, con la suficiente memoria y capacidad de proceso, sí es posible lanzar consultas con respecto a un término específico con posibilidades de devolver resultados en un tiempo razonable. Esta base de datos, o índice, es objeto de una gran competencia entre los diferentes motores de búsqueda: lógicamente, a mayor índice, mayor cobertura y mejores resultados.

Sin embargo, con este procedimiento solo hemos solucionado la primera mitad del problema, que es además conceptualmente la más sencilla. Una vez que la base de datos devuelve como resultado de la búsqueda de nuestro término un listado de todas las páginas que lo contienen, encontramos el segundo problema: ¿cómo ordenar ese listado para que ubique primero las páginas más relevantes para ese término específico? En algunos buscadores, la respuesta a la pregunta era clara: los primeros resultados, los más destacados, eran simplemente vendidos al mejor postor, aquellos que pagaban más. En ocasiones, de hecho, las páginas que el buscador devolvía eran sencillamente irrelevantes para el término buscado, pero habían pagado por él: se trataba simplemente de ver qué términos eran más buscados en la red, y adquirirlos para así garantizarse una visibilidad elevada: comprar el término «Pamela Anderson» era como comprar las mejores vallas de la ciudad, y poco importaba que tras hacer clic el usuario apareciesen, en lugar de las obras literarias seleccionadas de Pamela Anderson, la web de una empresa de hipotecas.

El problema de este método era que los usuarios no percibían relevancia: los resultados no correspondían a lo que ellos realmente buscaban, sino al criterio de quién pagaba más. En realidad, era un modelo de negocio perfectamente válido, y es de hecho el que se emplea en la publicidad convencional: aquel que tiene recursos para adquirir los mejores espacios o las mejores franjas de tiempo, pone sus productos o servicios delante de los ojos de los clientes, sin más complicación, sin prácticamente adaptar el producto anunciado al contexto en el que aparece el anuncio. Lo que ocurre es que, como modelo de búsqueda, simplemente, no era sostenible, porque los resultados, que eran la razón de la búsqueda, no eran adecuados. Además, el problema volvía a aparecer superadas las posiciones objeto de la venta: ¿cómo debíamos ordenar las siguientes? En ese sentido, los buscadores empezaron a aplicar la estadística: si una página repetía muchas veces una palabra, debía ser que dicha palabra era importante en la página, y por tanto la ponían por encima de otra página que los repitiese en menos ocasiones. Un razonamiento simplista y obvio, que, precisamente por serlo, no funcionaba: dados los incentivos para querer aparecer entre los primeros resultados, los webmasters de las páginas tomaban los términos más buscados en Internet, y los repetían un montón de veces en sus páginas, para así engañar al motor ele búsqueda. Como una cosa así no tenía sentido ninguno de cara al visitante, lo que hacían era aplicar trucos tan burdos como poner dichos términos destinados a engañar a la araña de manera que no molestasen al visitante, por ejemplo, en color de letra blanco sobre fondo blanco. Rudimentario, pero eficiente durante un cierto tiempo. Otras metodologías incluían el análisis de las metaetiquetas de la página, es decir, de los descriptores del contenido que el propio webmaster aplicaba, y que, por tanto, eran objeto de igual tipo de fallos. La evidencia era clara: la relevancia era algo demasiado importante como para dejarlo en manos de los propios sujetos que debían ser ordenados por ella.

La idea de Larry y Sergey era originalmente tan simple como buscar un criterio que en lugar de estar en la propia página, estuviese en otro lugar, bajo el control de otros actores, y con las menores posibilidades de manipulación. Y esa métrica «mágica» la encontraron en los vínculos o enlaces: por lógica, si muchas páginas vinculaban a una concreta cuando mencionaban una palabra determinada,{14} debía de ser porque esa página era relevante para esa palabra. Si muchas páginas, al referirse a Pamela Anderson, vinculaban a www.pamelaanderson.com, seguramente era porque esa página era la mejor para localizar sus obras literarias más prominentes. Después de todo, el procedimiento no hacía más que reflejar una verdad de la sabiduría popular: «¿dónde va Vicente? Donde va toda la gente». En el fondo, cada enlace era como un dedo señalando un contenido, y aquel contenido que era señalado por más dedos, tenía grandes posibilidades de ser el más relevante.

Para mejorar el procedimiento o «algoritmo» de búsqueda, Larry y Sergey resolvieron que lo mejor era dar a cada enlace un peso determinado, construido con una medida similar: un enlace tendría tanto más peso, cuantos más enlaces entrantes tuviese la página que lo originaba. Es decir: si una página que recibe muchos enlaces entrantes vincula a otra para un término determinado, su «voto» valdrá más que el de una página recién creada o con pocos enlaces entrantes. Para ello, crearon el denominado PageRank, una escala de cero a diez en función, principalmente, del número de enlaces entrantes a una página para un término determinado.{15} Con el tiempo, el algoritmo de Google se ha ido complicando, introduciendo cada vez un mayor número de criterios, medidas, correcciones y excepciones. A día de hoy, el famoso algoritmo es como el mito de la fórmula de la Coca Cola: muy poca gente lo conoce, entre otras cosas para evitar que sea sencillo jugar a trucar su funcionamiento, pero sobre todo, para proporcionar un resultado lo más relevante posible. Algunos factores, tales como el hecho de que una página esté bien construida y sea fácilmente analizable por las arañas de búsqueda, mejoran la puntuación de la misma. También lo hace el que el término buscado esté incluido en la dirección o URL de la página, o destacado en forma de titular en la misma. Existen una infinidad de criterios, como la frecuencia de actualización o la densidad de enlaces, que tienen igualmente su peso determinado. Pero la esencia no cambia: cuando una página aparece en los primeros lugares en los llamados «resultados naturales» de la búsqueda de Google para un término concreto, es porque otras páginas vinculan a ella con ese término.

Por lo general, la mayor parte de los usuarios tienden a creer que lo que hace que una página aparezca en los primeros resultados de Google es el hecho de que tenga un gran número de visitas. No es así. El número de visitas no es causa, sino consecuencia. Por el hecho de aparecer en los primeros resultados de Google para un término muy buscado, una página puede llegar a obtener un gran cantidad de tráfico, pero no al revés. La mayor parte de los usuarios no expertos revierten ese vínculo causal, lo que les lleva a pensar cosas como que si llegan muchos visitantes a su página, Google pasará a tenerlos en mejor consideración. Tampoco es extraña la confusión con el modelo económico: muchos usuarios preguntan a menudo cuánto tienen que pagar a Google para que les ponga de primeros en el resultado de una búsqueda determinada, y se manifiestan dubitativos y desconfiados cuando les aseguran que los resultados de las búsquedas son, como dice el pasodoble español, «como el cariño verdadero, que ni se compra ni se vende». La confusión, en este caso, proviene de dos cuestiones: por un lado, el hecho de que muchos de los motores de búsqueda anteriores a Google funcionasen así, vendiendo los resultados más destacados de cada palabra. Por otro, el que la propia Google sí venda publicidad en determinadas posiciones de sus páginas de resultados, aunque no mezcle esta con los resultados naturales de la búsqueda.

Actualmente, Google obtiene la inmensa mayoría de sus ingresos de una sola fuente: la publicidad. Sin embargo, la publicidad que Google hace no formaba parte de la idea de negocio original que llevó a Larry y a Sergey a abandonar su doctorado y lanzarse a crear Google: se les ocurrió más adelante. Cuando acuñaron la idea, tuvieron en cuenta que tenían entre manos un servicio que podría alcanzar una gran popularidad, pero no tenían claro en su cabeza cómo rentabilizarlo, cómo convertirlo en un verdadero negocio. De hecho, su primera idea fue vender el motor de búsquedas a aquellos que lo pudieran necesitar: empresas que quisieran ofrecer resultados de búsquedas en sus páginas, periódicos, catálogos, etc. Entre sus primeros clientes, podemos encontrar publicaciones como El Mundo o portales como Yahoo!, que ofrecían búsquedas jerarquizadas, pero no tenían un buen sistema para hacer búsquedas por palabra clave todo a lo largo de sus páginas. En Yahoo!, que contrariamente a lo que muchos piensan no era un buscador, sino un índice, los usuarios no tecleaban palabras, sino que iban señalando lo que buscaban mediante secuencias de clics: si quería un hotel en Roma, me iba a Viajes, después a Italia, seguidamente a Roma, y en las opciones de la página correspondiente a Roma, escogía Hoteles, lo que generaba una lista. En un buscador, en cambio, tecleo «hoteles Roma», y la lista se genera ante mí. Para Yahoo!, poder ofrecer un sistema de búsqueda era un servicio más para sus usuarios, y estaba dispuesta a pagar a Google en función del uso de dicho motor. Sin duda, un negocio, pero muchísimo más pequeño que el que Google fue capaz de encontrar con la publicidad.

La idea de la publicidad contextual (segmentada en función de lo que el usuario ha introducido y de la página de búsqueda que ha generado) no se le ocurrió a Google. En realidad, proviene de otra empresa, llamada Goto.com, posteriormente rebautizada como Overture, y que finalmente fue adquirida por Yahoo! y vuelta a bautizar como Yahoo! Search Marketing. En su momento, Overture llevó a los tribunales a Google por haber copiado la idea de subastar anuncios en los resultados de las búsquedas. El caso, tras una dura batalla legal, acabó en un pacto extrajudicial en agosto de 2004 coincidiendo con la salida a bolsa de Google, que pagó a Yahoo! 2,7 millones de acciones (entre 260 y 290 millones de dólares) a cambio de una licencia perpetua de las tecnologías en cuestión.

¿En qué consistía la idea en cuestión? En primer lugar, en un respeto cuasirreligioso a los resultados de la búsqueda: estos debían presentarse intocables, procedentes del algoritmo, sin alteración ni intervención alguna. Sin embargo, la columna de la derecha de la página, encabezada como «Resultados patrocinados», provenía de otro proceso: la subasta de la palabra o palabras utilizadas en la búsqueda entre los posibles anunciantes interesados en adquirirla. La idea de «adquirir una palabra» resulta cuando menos, chocante: las palabras no son de Google, son de todos. El lenguaje es un patrimonio común. ¿Cómo puede venderse una palabra? Sin embargo, la cuestión tiene mucho sentido: para un anunciante, ofrecer sus productos o servicios precisamente a quienes se ha interesado por localizarlos a partir de la búsqueda de una palabra determinada puede ser una proposición netamente ganadora. Si unimos a esto el hecho de que dicho anunciante puja ofreciendo una cantidad por cada clic que el usuario hace en su anuncio, pero que no tiene que pagar nada si no se genera clic alguno, la idea resulta más provocativa todavía: lo que en realidad se ofrece es pagar únicamente cuando la atención generada se convierte en una acción, en alguien que, a partir de la acción de buscar, aterriza en nuestra página.

La mejor analogía puede ser la de un autobús: imaginemos un conductor de autobús tan intuitivo, que es capaz de leer en la mirada de la gente que ve por la calle lo que están deseando comprar. Cada mañana, nuestro hábil conductor sale con su autobús, lo llena de personas interesadas en comprar una cámara digital, y cuando lo tiene lleno, detiene el autobús en la puerta de una tienda especializada en la venta de cámaras digitales. Abre la puerta, y cobra a la tienda una cantidad en función del número de personas que salen de su autobús y entran en ella. Que compren o no, que compren mucho o que compren poco, es problema de la tienda, no del conductor del autobús. El se limita a traer las visitas y asegurar que están interesadas en ese artículo. Si tiene usted algún tipo de tienda, piense si no pagaría por algo así.

El sistema, por supuesto, necesita algunos ajustes: es preciso evitar, por ejemplo, que alguien adquiera las palabras más habituales en las búsquedas y se limite a aparecer a la derecha, pero sin obtener ningún clic porque sus anuncios son en realidad irrelevantes para los usuarios. Una conducta así mantendría ocupadas las posiciones publicitarias, pero no sería buena ni para el usuario, que vería como esa publicidad no le proporciona lo que busca, ni para Google, que no cobraría nada porque no se generarían clics. Para evitarla, la compañía introduce una puntuación en función de la relevancia de los anuncios: si un anuncio recibe un porcentaje elevado de clics, es considerado relevante, y obtiene un plus que lo eleva en su posicionamiento a la derecha (las posiciones más altas son las mejores). Por el contrario, si un anuncio no recibe clics, irá descendiendo hasta desaparecer, aunque esté dispuesto a pagar hipotéticamente mucho por unos clics que, en realidad, nunca se producen. De hecho, Google considera los resultados publicitarios como un servicio para el usuario: han de ser relevantes, y por ello no está en la primera posición necesariamente el que más paga. La introducción del concepto de quality scores o «puntuación por calidad», responde precisamente a que aquellos enlaces patrocinados que más relevancia ofrecen al usuario puedan estar en una mejor posición.

La puja por palabras no deja de tener sus aspectos polémicos. En principio, un usuario puede comprar cualquier palabra, a no ser que se trate de una marca registrada cuyo propietario haya excluido específicamente. Se puede comprar el genérico de los productos o servicios que uno vende, el nombre de empresas de la competencia, o cualquier palabra relacionada dentro de un amplísimo abanico conceptual. Típicamente, las palabras más obvias y más genéricas suelen ser caras, mientras que muchas palabras o combinaciones de palabras no tan intuitivas pueden ser baratas y brindar buenos resultados.

El resultado es un sistema denominado Adwords, que permite que prácticamente cualquiera pueda acceder a hacer publicidad. Antes, la publicidad era generalmente un juego para empresas grandes, que podían superar la barrera de entrada solicitada por los soportes. Ahora, cualquier empresa puede probar a anunciarse con las palabras que considere oportunas, y pagar solo cuando sus anuncios generan una visita a su web. En mi trabajo como profesor he visto alumnos que utilizaban los vales promocionales de 50 euros regalados por Google como parte de una clase práctica para anunciarse a sí mismos comprando como palabras clave los nombres de las empresas en las que les gustaría trabajar: un anuncio para que lo vea uno, que anuncia un producto único con nombre y apellidos. Si eso no es especialización y disparo con mira telescópica, pocas cosas lo son.

Con Adwords como forma de generar ingresos, la estrategia debería ser incrementar la cantidad de páginas en las que poner anuncios, razón por la que dichas páginas se denominan en este negocio «inventario». Para incrementar su inventario, Google fue creando cada vez más propiedades: empezamos a ver un número cada vez mayor de servicios. Blogger, iGoogle, Gmail, Google Finalice, Google Books, Google Maps, YouTube y toda una creciente pléyade de páginas con nuevas prestaciones destinadas atraer a una audiencia cada vez mayor. En muchas de estas páginas, Google podía mostrar publicidad de una manera similar a como lo hacía en el buscador,{16} lo que tenía el efecto de ir incrementando el inventario disponible. Es importante tener en cuenta que todo el inventario de Google es completamente dinámico: mientras que una empresa de televisión o de vallas publicitarias tiene un inventario determinado, y una vez que lo ocupa, todos los que tienen oportunidad de verlo ven exactamente lo mismo, en Google la realidad es justamente la contraria. Si dos personas entran en el buscador o en su correo de Gmail, verán anuncios diferentes en función de toda una serie de parámetros obtenidos de los términos de búsqueda, del contenido de su correo, de su dirección IP, etc. Con respecto a la televisión, reina tradicional de los medios publicitarios, la comparación es durísima: mientras Google es capaz de reconocer a sus usuarios y servirles publicidad en función de sus características y deseos, la televisión es una vieja dama sorda: no escucha nada, y simplemente pone anuncios sin importarle demasiado quién está al otro lado de la pantalla.

Sin embargo, el avance más drástico se obtuvo con la prolongación natural de Adwords; el programa AdSense, destinado a que cualquier página pudiese destinar un espacio a publicidad administrada por Google en función del contenido de la página. La cuestión resultaba perfectamente lógica: dado que Google ya había indexado la página, y conocía por tanto su contenido, hacer una correspondencia de dichos contenidos con las pujas de los anunciantes por las palabras relacionadas era algo relativamente sencillo, y que pasaba a incrementar el inventario en el que Google podía poner anuncios hasta hacerlo prácticamente ilimitado, con un crecimiento casi paralelo al de la web. Cada vez que un usuario generaba un clic en un anuncio en una página de un tercero, los ingresos derivados de dicho clic se repartían entre Google y el propietario de la página en cuestión, lo que permitía que personas que jamás habían tenido ingresos en concepto de publicidad, pasasen a ingresar pequeñas cantidades en función de su popularidad. Si algo puede decirse del desarrollo de negocio de Google, es que ha sido capaz de democratizar en gran medida la publicidad: cualquier empresa, por pequeña que sea, puede hacer publicidad, y cualquier página puede convertirse en un soporte para la misma.

Por otro lado, resulta sumamente interesante plantearse cuál es realmente nuestra relación con Google: ¿qué somos con respecto a la empresa? La mayor parte de los usuarios de Google sentiría la tentación de definirse a sí mismos como «clientes», dado que ellos son quienes libremente toman la decisión de buscar en Google o hacerlo en otra página. Sin embargo, la verdad es que, por lo general, los usuarios no pagamos nada a Google por sus servicios, mientras que existe otra parte, los anunciantes, que sí lo hacen. Si consideramos a los anunciantes como clientes, ¿qué papel queda reservado para nosotros, los usuarios? La respuesta no es sencilla ni inmediata: los usuarios, o mejor, la atención de los usuarios, es la materia prima que Google utiliza para nutrirse. En realidad, Google es un experto en generar la atención de los usuarios hacia unos contenidos determinados (que en raras ocasiones produce realmente y que provienen, en realidad, del procesamiento de información de terceros) y, sobre todo, en segmentar dicha atención, en ordenarla para poder ofrecerla a los anunciantes de una manera que resulte manejable para estos. Cada vez que situamos un vínculo en nuestras páginas, cada vez que hacemos clic, cada vez que buscamos o utilizamos sus herramientas, estamos trabajando para Google, generando un flujo de atención que la empresa es capaz de convertir en un flujo económico. La atención es la moneda con la que pagamos a Google los servicios que nos ofrece aparentemente gratis. En la práctica, si los servicios no fuesen buenos, no nos interesaría. Pero mientras buscar en Google ofrezca mejores resultados que buscar en otro buscador, el sistema seguirá funcionando, y lo hará además con soberbios resultados.

La salida a bolsa de Google, el 19 de agosto de 2004, fue una de las más populares y de éxito más rotundo de la época. Las acciones salieron al mercado en 85 dólares, y cerraron por encima de 108 dólares, convirtiendo en millonarios sobre el papel a muchos empleados de la compañía y dotando a esta de un fenomenal músculo financiero. Con los fondos obtenidos, Google intensificó mucho sus adquisiciones: además de adquisiciones anteriores que se convirtieron en un fuerte impulso para el desarrollo de la llamada «web social», como Blogger, compró también empresas como Keyhole, dedicada a llevar a la red mapas de todo el mundo, o unos dos años después, YouTube, una pequeña startup que ofrecía una herramienta para que los usuarios pudiesen subir vídeos a la red. Algunas de estas adquisiciones han contribuido en gran medida a definir la web que vivimos hoy; la inyección de fondos en Pyra Labs, creadora original de Blogger, conllevó un desarrollo fortísimo del fenómeno blog tal y como lo conocemos. Google Earth y Google Maps adquirieron una enorme popularidad y han convertido la web en algo geosensible, que muchísimas personas utilizan para ver el mundo desde arriba o desde la calle (Street View), o llevan en dispositivos móviles como iPhone, Android o BlackBerry para moverse por el mundo. El reciente anuncio de la inclusión de instrucciones de navegación en los teléfonos móviles con Android fue suficiente como para provocar descensos en la cotización de más del 20% en algunas de las compañías líderes en la industria del GPS, como Garmin o TomTom: Google se ha convertido en una empresa tan poderosa, que cualquier movimiento o anuncio es susceptible de tener una influencia desmesurada.

YouTube ha cambiado la forma en la que accedemos y utilizamos los contenidos en vídeo: toda una generación, acostumbrada a acceder a cualquier contenido audiovisual a golpe de clic, completamente a demanda del usuario. En el momento de escribirse este libro, YouTube servía más de mil millones de visualizaciones de vídeo cada día, y cada minuto, los usuarios subían a la plataforma más de veinte horas de contenidos. YouTube se ha convertido en el repositorio más grande de contenidos jamás construido por el hombre. Haga la prueba: entre en YouTube y busque una canción determinada., una serie que veía en la televisión cuando era pequeño, un lugar del mundo que haya visitado. La sensación de abundancia resulta casi mareante.

Pero la publicidad, pese a representar, sin duda, la fuente de ingresos más significativa de Google, no es la única. En realidad, los desarrollos de la compañía apuntan a un futuro mucho más ambicioso: extrapolar su modelo de servicios a prácticamente todo lo imaginable. El desarrollo de Google Apps, una suite de herramientas en la nube que incluye un proceso de textos, una hoja de cálculo y un programa de presentaciones sencillo y la integra con otras herramientas de la compañía, como el correo de Gmail o la agenda de Google Calendar, pretende cambiar la forma de trabajar de los usuarios para adaptarla a la nueva realidad de un universo multidispositivo y con conectividad cada vez más ubicua: en lugar de utilizar herramientas complejas, pesadas y con muchísimas prestaciones de las que los usuarios llegan a utilizar como mucho un 5%, propone un esquema de herramientas básicas, pero que pueden utilizarse desde cualquier ordenador. Y sobre todo, que mejoran sobremanera el trabajo colaborativo. En el esquema del siglo pasado, ordenador-céntrico, un usuario creaba un documento en una máquina, y si quería colaborar con otro, debía generar una copia de dicho documento, con todo lo que ello conlleva: a partir del momento en que tenemos dos copias de un mismo documento, cada una pasa a tener una existencia independiente, a evolucionar por su cuenta en manos de dos personas diferentes, que después tienen, lógicamente, que ponerlo en común mediante herramientas tan absurdas y de uso tan incómodo como el control de versiones. Cuando un usuario quería continuar un documento en otro ordenador, debía meterlo como fichero adjunto en un correo electrónico, y enviárselo a sí mismo: algunas personas tenían tanto correo de sí mismas a sí mismas, que parecían auténticos esquizofrénicos digitales.

En su lugar, Google propone un trabajo red-céntrico: el documento tiene una única existencia en un servidor, el autor envía invitaciones con diferentes niveles de privilegios a aquellas personas que deben recibirlas, y todos trabajan sobre el mismo archivo, que almacena los cambios, permite ver el trabajo de los otros colaboradores en tiempo real —puede verse incluso el texto apareciendo en una zona determinada— y revertir a una versión anterior si es preciso. La propuesta, que ha seducido ya a más de dos millones de empresas de todo el mundo que pagan unos cincuenta dólares por usuario y año, se adapta mucho mejor al esquema de trabajo actual: una persona puede empezar un documento en el ordenador fijo de su despacho, seguirlo en su portátil, en el de su casa o en el de casa de su suegra, y dar entrada a colaboradores sin ningún tipo de problemas. Este libro que tiene entre manos está escrito íntegramente sobre Google Docs: durante los meses que duró su redacción, trabajé en muy diversos sitios y ordenadores, y mi editor pudo en cada momento ver cuál era la situación de cada capítulo o introducir correcciones en los mismos con total control por mi parte. Hay partes de este libro escritas en Majadahonda, en mi despacho de María de Molina en Madrid, en un hotel en San Francisco durante una tormenta que desaconsejaba salir a la calle, en un avión cruzando el Atlántico, en mi casa de A Coruña y en el Balneario de Mondariz, a cuyo relajante entorno acudí a terminarlo. Pero a pesar de los cambios de lugar y de máquina, cada archivo siempre fue único, sin generar ningún problema de consistencia. Y para mi editor, supuso la tranquilidad de poder ver el progreso del libro en cada momento.

En paralelo, Google ha seguido desarrollando piezas en segmentos muy importantes: de cara al mundo de la movilidad, ha desarrollado Android, un Linux adaptado a teléfonos móviles, y lo ha hecho utilizando una licencia Apache, que cualquier fabricante de terminales puede tomar, modificar a su gusto, pero sin estar obligado a devolver las modificaciones a la comunidad, como ocurre con muchas otras licencias de software libre. Esto permite que Android se convierta en una plataforma, en un ecosistema sobre el que muchos fabricantes pueden competir con sus diferentes terminales y versiones. En el momento de escribir este libro, Android había seducido ya a algunos de los fabricantes más importantes del mundo, tales como HTC, Huawei, Lenovo, LG, Motorola, Samsung o Sony Ericsson, convirtiéndose en una de las plataformas más pujantes a la hora de competir contra el iPhone de Apple o el BlackBerry de RIM en el segmento de los llamados smartphones. Para máquinas algo más grandes, como los llamados netbooks, Google propone el desarrollo de un sistema operativo partiendo de su navegador, Chrome, denominado Chrome OS, que viene a ser esa mínima capa necesaria para que la máquina se conecte y gestione la conectividad con la oferta de la empresa en la nube.

En otros ámbitos, las líneas en las que Google trabaja representan auténticos cambios conceptuales en cosas tan básicas como la manera en que nos comunicamos o trabajamos: es el caso de Google Wave, una herramienta de colaboración y comunicación capaz de combinar correo electrónico con mensajería instantánea, herramientas multimedia e integración con cualquier tipo de dato dentro de un navegador. Dos personas trabajando dentro de Wave tienen la sensación de estar prácticamente en la misma habitación: cada uno ve escribir al otro, puede corregir sus textos, puntualizarlos como en una conversación, y añadir cualquier elemento imaginable: frente a los métodos tradicionales de trabajo mediante correo electrónico y archivos adjuntos, Wave plantea un nivel de productividad y eficiencia del que resulta difícil pensar que las empresas no sean capaces de extraer ventajas competitivas (y si comparamos con esas empresas en las que todavía se trabaja con papeles metidos en sobres, es como imaginarse a un Cro-Magnon golpeando una piedra para tallar una flecha).

La estrategia de Google es, sin duda, una de las mejor adaptadas a la evolución definida por la tecnología, pero esto no resulta especialmente extraño: ha conseguido ser una parte fundamental en su definición. El éxito de la oferta de Google en muchos aspectos es arrollador: en España, por ejemplo, tiene más de un 96% de cuota de mercado en búsquedas en la web. Es líder en todos los países del mundo, salvo Corea del Sur, Rusia y Japón, en los que hay competidores locales muy implantados, y algunas de sus adquisiciones, como la de la agencia de publicidad digital Doubleclick o el intento de alianza con Yahoo! han sido cuidadosamente inspeccionados por la legislación antimonopolio. A la compañía fundada por Graham-Bell, AT&T, le llevó 94 años toparse con unas leyes antimonopolio que la obligaron a escindirse en las Baby Bells. Ese mismo proceso tuvo lugar en IBM en 1999, 45 años después de su fundación (con distinto resultado: la compañía pudo mantener su actividad), mientras que en el caso de Microsoft, sucedió en 2000, 25 años después de la constitución de la compañía. En el caso de Google, la primera inspección de las autoridades antimonopolio se debió a la adquisición de Doubleclick en 2007, tan solo ocho años después de haber sido fundada.

Parece evidente que la tecnología es un factor capaz de generar ventajas competitivas de una forma cada vez más rápida y consistente, y que cuando una empresa logra un dominio tan importante en algunos aspectos, debe ser objeto de un cierto control que evite la restricción de la competencia. En este sentido, Google no es una excepción: la visibilidad que Google tiene a día de hoy sobre los patrones de navegación en la red es comparable a la del «ojo de Horus» de los antiguos egipcios, el «ojo que todo lo ve». Mediante herramientas como Analytics, AdSense o DoubleClick es capaz de estar presente en casi un 90% de los sitios web de la red, muchos de los cuales comparten sus datos con la empresa con el fin de tener acceso a análisis más potentes. La compra de DoubleClick en abril de 2007 colocó a la empresa en una posición de dominio del mercado de la publicidad en la red tan desmesurada, que para que una compañía en, por ejemplo, el mundo financiero, igualase la concentración producida por la adquisición de DoubleClick por Google, debería poseer nada menos que los quince mayores bancos y gestores de patrimonios de Wall Street, alrededor del 60% de todos los hedgefounds y private equities, las bolsas de Londres y Nueva York, los dos proveedores más importantes de información financiera (Bloomberg y FactSet), dos de los tres proveedores norteamericanos de perfiles de crédito (Equifax y Experian), y en torno al 60% de los datos completos de consumidores de la Reserva Federal y del Censo de los Estados Unidos. Algo, por supuesto, completamente impensable desde un punto de vista de la economía tradicional, y que sin embargo fue aprobado por las autoridades reguladoras norteamericanas y europeas dentro de los plazos razonables para este tipo de operaciones. Su siguiente intento de expansión, un acuerdo con Yahoo!, fue, sin embargo, mal recibido por el regulador norteamericano, y la compañía se retiró de la operación para evitar un revés legislativo.

Uno de los temas más frecuentemente reprochados a Google es la acusación de ser «un monstruo de la privacidad», una amenaza enorme que sabe más de las personas que las personas mismas. La acusación proviene del mismo modelo de negocio de la compañía, un intermediario que, como hemos comentado anteriormente, permite a los usuarios pagar por los servicios que utilizan con la moneda de su atención. Esta atención, convenientemente procesada y analizada, es el producto que Google revende a sus anunciantes a cambio de cuantiosos beneficios. Como consecuencia, la compañía acaba acumulando una gran cantidad de información de sus usuarios, información que, lógicamente custodia con sumo cuidado. Una de las consecuencias de tal acumulo de información debería ser, si todo funcionase como es debido, que los usuarios recibiesen una publicidad cada vez mejor segmentada, mejor adaptada a sus intereses. ¿Quién no estaría contento de recibir menos publicidad irrelevante y, en su lugar, más de la que es posible que le interese? Por supuesto, el hecho de que la compañía sepa mucho de cada usuario no impide que dicho usuario pueda «desprenderse de dicho pasado» en un momento dado: basta entrar en los servicios de la compañía desde un navegador con las opciones de privacidad activadas (tanto Firefox como Safari, Chrome, Opera o Explorer cuentan con dicha opción de navegación anónima) para pasar a ser un usuario completamente anónimo. Lo importante es entender el manejo de la privacidad por parte de Google como una ecuación de sostenibilidad: si un determinado porcentaje de usuarios de Google se sintiesen intrínsecamente incómodos debido al manejo de la privacidad que la compañía lleva a cabo, no pasaría mucho tiempo hasta que dichos usuarios saliesen por la puerta con destino a otros servicios proporcionados por otras compañías que entendiesen menos intrusivas. La respuesta de Google ante tales preocupaciones ha sido siempre la transparencia: en todo momento, un usuario puede entrar en su perfil e historial, ver lo que la compañía sabe de su persona, y eliminar o revocar los permisos y la información conveniente. La diferencia entre Google y muchas otras compañías que también manejan información de sus usuarios es que, en el caso de Google, podemos saber de qué información se trata y manejarla con relativa facilidad.

Sobre la condición de monopolio de Google habría mucho que escribir: dominar un mercado de forma clara no implica necesariamente abusar de dicho dominio, y, por el momento, el único mercado en el que Google verdaderamente ejerce un dominio es el de la publicidad relacionada con las búsquedas, en el que ostenta una fuerte supremacía apoyada en la cuota de mercado de su buscador. Sin embargo, hablamos de un mercado dinámico y, sobre todo, caprichoso: un nuevo buscador capaz de proponer un esquema nuevo de relevancia podría hacer que abandonásemos a Google por otro, con la misma velocidad con la que en su momento decidimos abandonar a Altavista por Google. Sin duda, Google es a día de hoy uno de los actores más destacados en el panorama tecnológico, y sus movimientos son seguidos por muchos: muchas empresas aspiran a convertirse en objetivos de adquisición, al tiempo que otras la critican por la invasión de sus terrenos tradicionales. Como gigante que es, Google tiene necesariamente que moverse con cuidado si no quiere ver sus actividades y su crecimiento dificultados. Pero, sin duda, nos hallamos ante una de las empresas más prometedoras y estratégicamente bien situadas de la historia de la tecnología, de las que más claramente marcan tendencias y con una gran importancia de cara al futuro.

CAPÍTULO 13. LA EVOLUCIÓN DE LA TECNOLOGÍA: DEL ORDENADOR A LA NUBE

«La idea que subyace detrás de ARPA es que la promesa ofrecida por el ordenador como medio de comunicación convierte en insignificante su origen histórico como herramienta de cálculo.»

Documentos fundacionales de ARPA (1957)

La evolución tecnológica en los últimos años ha sido completamente vertiginosa, hasta el punto de sacudir los conceptos más básicos. En un contexto que se ha movido a tal velocidad, resulta razonable que incluso personas que se consideran expertos en tecnología o que trabajan con ella en su día a día se encuentren confusos, incapaces de apreciar la magnitud de los cambios.

Hace tan solo 20 o 25 años, un ordenador era una máquina con un procesador muy inferior en prestaciones al que hoy tiene un teléfono móvil sencillo, y dotado de muchísima menos memoria. Si nos remontamos un poco más en el tiempo, los ordenadores que manejaban las personas no tenían siquiera procesador o memoria: eran los llamados «terminales tontos», que constaban únicamente de una interfaz de uso con pantalla y teclado para acceder a un ordenador central con capacidad de proceso y memoria. La comparación entre un IBM PC de 1981 y un iPhone resultaría casi grotesca: desde el procesador Intel 8088 capaz de correr a 4,77 MHz hasta el de un iPhone 3GS actual, preparado para correr a 833 MHz, aunque se utiliza a «tan solo» 600 MHz, con el fin de reducir el consumo y la generación de calor, hablamos de una potencia superior en más de cien veces, y estamos comparando un ordenador con un simple teléfono de bolsillo: en el caso de un ordenador de sobremesa comparable, las magnitudes de la comparación son sumamente difíciles de apreciar, por la presencia no solo de incrementos brutales en velocidad, sino también de múltiples núcleos, memorias internas y unidades de procesamiento paralelo. En términos de memoria, el iPhone es capaz de almacenar hasta 32 GB en memoria Flash, frente a un IBM PC que carecía de disco duro y que únicamente permitía trabajar con dos disquetes de cinco pulgadas y cuarto, de 360 kB cada uno. Si comparásemos uno de los primeros ordenadores, el construido en el Massachusetts Institute of Technology (MIT) en 1965, con un teléfono móvil actual, nos encontraríamos con un dispositivo que, puesto en números redondos, es alrededor de mil veces más potente, cien mil veces más pequeño, y un millón de veces más barato. Una comparación de magnitudes sencillamente mareante, que nos da idea del brutal avance de la tecnología en el tiempo.

Pero, más allá de la evolución de las capacidades tecnológicas, resulta interesante pensar en otro cambio de concepto, si cabe más importante: ¿para qué tipo de tareas utilizábamos un ordenador entonces? Por regla general, el usuario de un IBM PC podía, en su vertiente más seria, utilizar un procesador de textos como WordStar, una hoja de cálculo como Lotus 1,2,3 y un programa de gráficos como Harvard Graphics, por no mencionar bases de datos como DBASE y otros programas como compiladores, etc., de un uso generalmente más técnico. Puestos a analizar el tipo de tareas que se llevaban a cabo, podríamos decir que un procesador, de textos viene a ser como una máquina de escribir en edición corregida y mejorada: su metodología permite disociar la fase de introducción de texto de la representación del mismo sobre un papel, lo que permite una productividad muy superior. En lugar de escribir una carta cada vez que aparece directamente sobre el papel y nos obliga a recurrir al popular Tipp-ex para enmendar posibles errores, el procesador de textos permite componer un texto en pantalla, e imprimirlo todas las veces que queramos, con posibles cambios y correcciones parciales. Sin duda, el procesador de textos representaba un enorme avance en productividad. El análisis de una hoja de cálculo nos llevaría prácticamente a las mismas conclusiones: en realidad, se trata básicamente de una «calculadora con esteroides» capaz de almacenar en su memoria una cantidad casi ilimitada de operaciones encadenadas, algo que indudablemente nos permite multiplicar nuestra eficiencia cuando trabajamos con números. Pero, yéndonos a la esencia de la cuestión: ¿qué es lo que realmente cambia entre máquina de escribir y procesador de textos, entre calculadora de bolsillo y hoja de cálculo? A todos los efectos, un ordenador se consideraba una máquina destinada a hacer lo mismo que haríamos sin ella, pero mucho más rápido, con una mayor eficiencia.

El verdadero cambio se llamaba productividad. Entendida simplemente como producción por unidad de tiempo, la productividad era vista como el argumento fundamental para la adopción de tecnología, y lo fue por supuesto en el caso de los ordenadores y sus primeros clientes, las empresas. Si pensamos en las primeras áreas funcionales de las empresas que en su momento decidieron incorporar ordenadores, nos encontramos en una amplia mayoría de casos con Contabilidad y Finanzas, áreas en las que no solo se precisaba un intenso tratamiento y análisis de datos que resultaba muy repetitivo, sino que, además, existía un requisito legal de almacenamiento de los mismos. Otras áreas tales como Recursos Humanos contaban también con procesos, como el cálculo de nóminas, altamente cíclicos y repetitivos, que podían beneficiarse en gran medida del poder de cálculo de los ordenadores. Su incorporación posibilitaba la programación de las secuencias de trabajos, que tras la introducción de los datos, pasaban a un tratamiento automatizado que se desarrollaba a mucha mayor velocidad. Como en el caso de la productividad personal, un ordenador era para la empresa una máquina para llevar a cabo esencialmente los mismos procesos, pero a mayor velocidad. Para muchas, muchísimas personas y empresas, un ordenador sigue siendo exactamente lo mismo: una máquina para desarrollar las mismas tareas, pero más rápidamente. En realidad, la productividad de los ordenadores se manifestaba en muchos más aspectos: poder calcular más deprisa en una hoja de cálculo no permitía simplemente terminar la tarea antes, sino que proporcionaba además una capacidad analítica derivada del hecho de poder visualizar más escenarios, hacer más hipótesis o llevar a cabo más pruebas, lo que acababa redundando en una comprensión del problema muy superior.

Pero más allá de estas ganancias incrementales, consideremos ahora los tiempos en que vivimos: ¿cuál diríamos que es, en este momento, la pieza más importante de su ordenador de sobremesa? ¿Qué parte del ordenador de su despacho le hace pensar en volverse a su casa cuando no funciona? La respuesta es clara: la parte más importante de su ordenador de sobremesa está por detrás: es su cable de red. Si la red no funciona, usted mirará lastimeramente a la máquina, y pensará eso de «me voy a tomar un café, el ordenador no funciona». En realidad, la máquina funciona perfectamente, es la conexión a la red la que no lo hace. ¿Qué nos lleva a tener esa sensación de que la máquina no sirve para nada cada vez que la red se cae? ¿Qué programas tenemos más tiempo hoy en día delante de nuestros ojos en la pantalla? Salvo casos muy específicos, la respuesta para la mayoría de usuarios será que los programas que acaparan la pantalla durante más tiempo son el gestor de correo electrónico, la mensajería instantánea y, sobre todo y cada vez de manera más patente, la gran estrella: el navegador. En muy pocos años, el ordenador ha pasado de ser una máquina destinada a la productividad, una herramienta para hacer lo mismo pero más rápido, a ser una ventana abierta al mundo, el punto a través del cual nos comunicamos, nos relacionamos, recibimos y enviamos información.

El cambio tiene una magnitud tan brutal, tan importante, como lo que supondría que, en pocos años, los automóviles dejasen de ser máquinas que nos llevasen de un lado a otro y pasasen a ser utilizados fundamentalmente para calentar café: algo completamente impensable. Pero está sucediendo, y a una velocidad enorme: cada vez más, el navegador toma una preponderancia mayor, y va acaparando progresivamente una mayor cantidad de nuestras tareas habituales. Los navegadores se sofistican: se dotan de pestañas múltiples que permiten desempeñar varias tareas a la vez, adquieren nuevas funciones gracias al desarrollo de extensiones, y comienza una carrera competitiva por la cuota de mercado de los mismos que incluye a jugadores como el Internet Explorer de Microsoft, que poseía la mayor cuota de mercado, y otros más rápidos y pujantes en su desarrollo, como el Firefox de la Mozilla Foundation, heredero del antiguo Netscape, o jugadores interesantes como Apple (Safari), Opera, o Google (Chrome). Cada vez más, el navegador, unido a unos servicios en la red cada vez más avanzados, va haciendo que el ordenador sea menos relevante: para ejecutar un navegador y utilizar servicios online, no es necesaria una gran potencia en la máquina, lo que altera el panorama competitivo en la venta de hardware: por primera vez, el avance de la tecnología no implica necesariamente adquirir un ordenador nuevo, sino que puede llevarse a cabo sobre máquinas que incrementan su vida útil notablemente, acercándose a lo que debería ser su ciclo de funcionamiento razonable. Un ordenador, encendido casi a todas horas de manera continua, puede llegar a durar, en función de la calidad de sus componentes, unos seis o siete años, y podría seguir funcionando tras la sustitución selectiva de algunos componentes. Sin embargo, los usuarios estaban acostumbrados a un panorama en el que cada tres años como media, era preciso cambiar de ordenador si se quería instalar la última versión de sistema operativo, y no hacerlo significaba quedarse desactualizado y empezar a recibir archivos en formatos que no se podían abrir.

Además, los usuarios van pasando de escenarios tecnológicos en los que se utilizaba un solo ordenador, a otros en los que pasan a utilizar varios terminales de diversos tipos: los casos en los que una persona utiliza un ordenador en su casa, otro en su despacho, un ordenador portátil cuando viaja y un teléfono móvil con capacidad de conexión empiezan a ser más norma que excepción, y definen casuísticas en las que el mejor escenario consiste en tener los datos en un repositorio central en la red, al que se puede acceder desde cada una de las máquinas sin necesidad de duplicar innecesariamente los archivos.

En estas condiciones, en 2007, la empresa taiwanesa Asus decide lanzar su serie Eee: la denominación correspondía a Easy to learn, Easy to work, Easy to play, y era pequeña, ligera, con sistema operativo Linux, y costaba únicamente unos doscientos dólares. La pequeña Asus Eee fue un enorme éxito de ventas, y se convirtió rápidamente en la máquina a la que corresponde el honor de inaugurar toda una nueva categoría, la de los denominados ultraportátiles, o netbooks, a la que rápidamente se apuntaron muchas otras marcas como Acer, HP o Dell. Durante el año 2008, se vendieron en el mundo 16,5 millones de unidades de ultraportátiles, que pasaron a ser más de 35 millones en 2009, un crecimiento del 9.9%, frente a un mercado de portátiles convencionales de 130 millones de unidades completamente estabilizado en crecimiento cero.

¿Cuál es la razón para el brusco incremento de popularidad de una gama de ordenadores menos potente que la existente, con teclados en los que es preciso teclear con las puntas de los dedos y pantallas menores de diez pulgadas? Los ultraportátiles resultan notablemente incómodos para trabajar un tiempo prolongado con respecto a un portátil o sobremesa convencional, pero cuestan entre cuatro y ocho veces menos, y tienen un planteamiento muy diferente al de estos: de hecho, quien adquiere un ultraportátil pensando que se trata simplemente de «un ordenador pequeñito» suele acabar frustrado, porque el concepto es completamente diferente. La idea de un ultraportátil, todavía no completamente llevada a la práctica, es la de ofrecer una máquina que arranca en segundos al abrir su tapa, que tiene un sistema operativo completamente minimalista, seguro y a prueba de bomba en su estabilidad, y que es capaz de detectar el estado de conexión del momento: si está en una zona con conectividad inalámbrica en abierto o en una red de la que tiene la clave, se conecta automáticamente. De no ser así, lanza la conexión telefónica con el operador correspondiente a la tarjeta que lleve puesta (muchas de estas máquinas se venden subvencionadas en conjunción con un contrato de telefonía ligado a un compromiso de permanencia determinado). Y en caso de interrupción de la conexión, trabajan sobre el almacenamiento de la máquina (habitualmente, una tarjeta de memoria Flash) de manera temporal utilizando como referencia la última copia sincronizada hasta que recuperan la conexión, momento en el cual vuelven a sincronizar los cambios. Se trata de máquinas que no tienen sentido sin la red, sin la llamada «nube»: trabajan sobre servicios provistos por empresas que, a cambio de un pago periódico o de la provisión de publicidad, proporcionan al usuario tanto una aplicación determinada como el espacio de almacenamiento necesario para guardar los archivos del usuario.

Las primeras aplicaciones de este tipo que empezaron a ganar popularidad lo hicieron a nivel de usuarios particulares, no de empresa. El caso de Gmail, el correo electrónico de Google, fue especialmente interesante: en un mercado dominado por proveedores como Hotmail o Yahoo! Mail, que ofrecían en sus versiones gratuitas unos pocos megas de espacio de almacenamiento y no permitían ficheros adjuntos grandes, apareció de repente un competidor, Gmail, que ofrecía nada menos que un giga de espacio de almacenamiento y ficheros adjuntos de un tamaño muy elevado. El servicio, que se ofrecía únicamente en beta cerrada y por invitación, comenzó a crecer muy rápidamente en popularidad: los que lo usaban se daban cuenta de que era diferente. Al tener una disponibilidad de espacio tan elevada, los usuarios dejaban simplemente de borrar correos, y se dedicaban a almacenarlo todo salvo lo puramente prescindible, apoyándose en el hecho de que la búsqueda permitía encontrar cualquier correo en cuestión de segundos recordando únicamente algún detalle del mismo. El crecimiento en popularidad de Gmail fue muy rápido, y forzó de manera casi inmediata cambios en las ofertas de los otros competidores para intentar que el ridículo al que les había sometido con su llegada al mercado durase un poco menos. Desde el momento de su lanzamiento, que Google hizo coincidir, en un guiño humorístico, con el día 1 de abril, April Fools de 2004 (el equivalente en el mundo anglosajón al día de los Santos Inocentes), Gmail tardó hasta febrero de 2007 en abrir el servicio al público en general, pero en julio de 2009 alcanzaba ya los 146 millones de usuarios en todo el mundo. La popularidad de Gmail hizo que muchos usuarios empezasen a plantearse que cuando utilizaban el correo gratuito de Google, se encontraban muchas mejores prestaciones que al recurrir a sus cuentas de correo corporativas. En general, Gmail presentaba estadísticas de disponibilidad mucho mejores que cualquier correo gestionado por una empresa, mejor seguridad, mejor filtro antispam (los filtros antispam de Gmail son colaborativos, de manera que cada usuario «enseña» al sistema y este acaba detectando la gran mayoría del spam de manera automática) y, sobre todo, mejor accesibilidad, particularmente desde dispositivos móviles. La experiencia de uso llevó a un número cada vez mayor de directivos a introducir reglas de redirección permanente en sus correos corporativos, reenviándolos a sus cuentas de Gmail personales. Los responsables de tecnología de las empresas se rasgaban las vestiduras: ¿cómo puedes manejar la información sensible de la empresa en una cuenta externa y gratuita? Y sin embargo, el resultado era contundente: no solo no planteaba problemas de seguridad, sino que ofrecía mejores prestaciones.

Tras Gmail, Google fue ofreciendo progresivamente otras prestaciones, tales como agenda y, finalmente, documentos. La funcionalidad de Google Docs era sumamente simple en comparación con un programa de ofimática convencional (el 90% del mercado de las suites ofimáticas estaba dominado por el Office de Microsoft), pero... ¿qué porcentaje de las funcionalidades de una suite ofimática utilizaban realmente los usuarios? Las estadísticas afirmaban que dicho porcentaje se encontraba por debajo del 5%: la mayor parte de los usuarios tecleaban textos, los imprimían o enviaban, modificaban algún formato como tipo de letra, negrita, cursiva o subrayado, y poco más. Pero la diferencia a la hora de trabajar en grupo resultaba contundente: mientras que los usuarios de Office se tenían que enfrentar a un «control de cambios» completamente enervante e incómodo en su uso, los de Google Docs simplemente invitaban a sus colaboradores mediante correo electrónico, y trabajaban todos ellos sobre el mismo documento, contando incluso con una ventana de mensajería instantánea para coordinarse. Y, sobre todo, surgía una gran ventaja: en la metodología convencional, cuando dos personas trabajaban juntas, se generaba una dinámica de envío de versiones que acababa resultando sumamente engorrosa: cada usuario enviaba no el documento, sino una copia del mismo, copia innecesaria y que introducía una nueva versión en estado diferente de actualización a la que cada uno de los miembros del equipo tenía consigo. Esas diferencias incrementales en las versiones provocaban problemas de coherencia y generaban complejidad. Utilizando un documento online, todos los conflictos de versiones se eliminaban automáticamente: todos los participantes trabajaban sobre un único documento. De hecho, era posible quedarse mirando el documento en la pantalla, y verlo cambiar a medida que otros miembros del equipo iban introduciendo modificaciones en cualquiera de sus partes. A pesar de la sencillez del programa en términos de funcionalidades, la diferencia a la hora de trabajar en grupo era como comparar la edad de piedra y el futuro.

A medida que la oferta de este tipo de programas «en la nube» crecía, empezó a ponerse de manifiesto que sus necesidades de máquina eran absolutamente espartanas: cualquier ordenador, o incluso un teléfono, capaz de arrancar un navegador servía para acceder a este tipo de funcionalidades. Los ultraportátiles, por tanto, diseñados no para funcionar como un ordenador convencional, sino para trabajar sobre la red, parecían un soporte ideal. Pero ¿es esto una limitación?

En realidad, trabajar sobre la red tiene ventajas muy interesantes: en primer lugar, los archivos del usuario pasan de estar en un disco duro, expuestos a averías, problemas, pérdidas o intrusiones, a estar en un servicio gestionado profesionalmente por especialistas, que pueden ofrecer protocolos de seguridad dotados de una eficiencia muy superior. Intuitivamente, el concepto funciona justo al revés: cualquier persona acostumbrada a pensar en términos analógicos, creerá erróneamente que sus posesiones están más seguras cuanto más cerca las tenga de sí misma, al alcance de sus ojos. Sin embargo, el concepto es erróneo, como demuestra el hecho de que subcontratemos la custodia de muchos objetos valiosos depositándolos en la caja de seguridad de un banco. Hay muchísimos más robos de propiedades particulares en hogares que en las cajas de seguridad de bancos, que exigen un tipo de preparación muy superior por parte del delincuente. Y la razón por la que no llevamos al banco todas nuestras posesiones es que la caja de seguridad del banco puede ser efectivamente muy segura, pero no es especialmente conveniente; ¿se imagina teniendo que planificar qué ropa, qué reloj o que joyas se pone un día o unos días antes, coincidiendo con los horarios de apertura del banco? En algunos casos, como el de joyas, relojes muy valiosos o documentos de gran importancia, es posible que se haga. Pero en otros, resultaría claramente ineficiente. De la misma manera, mientras no hemos tenido una infraestructura de comunicaciones suficientemente desarrollada, lo razonable era tener el almacenamiento de datos dentro de las empresas, donde podía ser consultado de manera operativa. Pero en el momento en que dicha infraestructura de comunicaciones pasa a ser rápida y a tener una disponibilidad elevada, resulta claro que el mejor lugar para nuestros datos no está en nuestra empresa, que no es especialista en su almacenamiento ni se dedica profesionalmente a ello, sino en un proveedor especializado. De la misma manera, mientras no existió una infraestructura desarrollada para el transporte de agua o electricidad, muchas fábricas cavaban sus propios pozos o construían minicentrales eléctricas, algo que hoy en día nos resultaría completamente aberrante o que sería incluso ilegal. En el momento en que la madurez de la infraestructura tecnológica lo ha permitido, ha ocurrido lo que era lógico que ocurriese: que la tecnología se ha convertido en algo que, en el momento en que lo necesito, prefiero «abrir un grifo» y que venga desde un proveedor.

Depender de un tercero especializado tiene una serie de implicaciones interesantes: al desarrollarse el mercado de proveedores de servicios de este tipo, pasaremos a tener, como en todo mercado, proveedores buenos, regulares y malos, posiblemente con diferentes precios. La calidad se pacta en un acuerdo de nivel de servicio (Service Level Agreement o SLA), y su incumplimiento conlleva penalizaciones pactadas de acuerdo con el perjuicio producido al cliente. Pero no lo olvide: contrariamente a lo que su intuición analógica le dice, en el mundo digital es imposible que usted, en su empresa dedicada a algún tema no relacionado, tenga mejores protocolos de actualización o seguridad que un proveedor especializado en ello y que aspire a ganar dinero prestando dichos servicios.

Sin embargo, a pesar de las grandes ventajas de funcionalidad, seguridad y prestaciones que este tipo de sistemas genera, el desarrollo de los mismos en el entorno corporativo está siendo relativamente lento: ¿a qué se debe esta lentitud? Recuerde el capítulo dedicado a la resistencia tecnológica: a la presencia de personas en las empresas, particularmente los encargados de gestionar los centros de datos y las infraestructuras tecnológicas corporativas, que se negarán a aceptar las evidencias digan lo que digan los datos. Para el responsable de la informática corporativa, salvo honrosas excepciones, un centro de datos remoto siempre será un peligro para la seguridad, nunca estará tan bien atendido como en la propia empresa y tendrá un nivel de servicio infinitamente inferior. No solo lo afirmará con vehemencia, sino que tendrá varios casos que citar, varias historias para no dormir recogidas de las habladurías y exageraciones de sus compañeros de profesión, que demuestran exactamente lo contrario. La evidencia, simplemente, contradice su experiencia, su aprendizaje, la esencia de su trabajo, el cómo se hacían las cosas antes de que la tecnología cambiase. Y pone decididamente en peligro su puesto de trabajo: ante la externalización de muchos servicios, es indudable que el dimensionamiento de los departamentos de tecnología dentro de las empresas se verá en cierta medida reducido. Algo que no es fácil de aceptar para nadie.

La transición a la nube, por tanto, está determinando la evolución desde sistemas operativos grandes, pesados y cargados de funcionalidades de todo tipo hacia plataformas cada vez más minimalistas que simplemente se utilizan para la interacción básica con los componentes de la máquina, arrancan en cuestión de segundos, y dan paso rápidamente a un navegador en el que el usuario lleva a cabo la mayor parte de su trabajo. A todos los efectos, el cambio de tendencia, el momento culmen de los sistemas operativos de la generación anterior, se produjo con el lanzamiento de Windows Vista. Tras el fracaso de Vista, los sistemas operativos han comenzado una evolución hacia la ligereza, una evolución progresiva hacia el reduccionismo: algunos ultraportátiles llevan versiones de Linux tan aligeradas, que su sistema operativo recuerda al de los teléfonos móviles. De hecho, algunos fabricantes han incorporado en su línea modelos de ultraportátiles que utilizan el sistema operativo Android, desarrollado por Google en conjunción con una plataforma de 48 fabricantes de hardware y software más, precisamente para teléfonos móviles. La propia Google, siendo coherente con la filosofía reduccionista, ha anunciado el lanzamiento de un sistema operativo construido alrededor de su navegador, Chrome: el sistema operativo ya es tan sencillo, que hereda el nombre del navegador que el cliente tiene delante de sus ojos. En no mucho tiempo, el sistema operativo será simplemente algo embebido en los chips de la propia máquina, en la BIOS: la mínima expresión de lo que en su momento se llegó a definir como el componente más estratégico en la relación con el usuario, sobre el que se edificó el dominio de una empresa como Microsoft. Desde el «domina el sistema operativo y dominarás el mundo», a ser un mínimo componente embebido en la BIOS de la máquina. En menos de diez años.

El desplazamiento de muchas actividades a la nube conlleva necesariamente una concentración de los centros de datos. Las empresas que aspiran a ofrecer servicios de este tipo precisan enormes granjas de ordenadores, que son, además, la parte más crítica e importante de su negocio, el constituyente fundamental del mismo. Centros de datos que deben funcionar con total habilidad, con redundancia ante posibles fallos de componentes individuales y, sobre todo, con una eficiencia en costes que jamás se había exigido a instalaciones de este tipo. Para una empresa convencional de cualquier industria, el coste del centro de datos es un número más en una cuenta de gastos generales, y un número que además, por principio, no se toca: a «los informáticos» se les mide por las estadísticas de funcionamiento, por la estabilidad, porque el sistema no se caiga, pero no por el gasto de corriente eléctrica en el que incurran. Para una empresa proveedora de servicios, el gasto energético es el ser o no ser, la posibilidad de ser más competitivo que otros, la esencia de su ventaja competitiva. Una dinámica que nos va a llevar, en muy pocos años, a la desaparición de la inmensa mayoría de los centros de datos corporativos, sustituidos por inmensos centros de datos situados en lugares donde el ahorro energético resulta más propicio: a medida que vayan cayendo los mitos de la inseguridad, el control y la privacidad presas de análisis medianamente serios, solo continuarán manteniendo centros de datos propios aquellas empresas con la escala suficiente como para ser razonablemente competitivas en costes.

La estructura de esos centros de datos también empieza a cambiar como consecuencia de esa búsqueda de la eficiencia: en una sorprendente vuelta de tuerca bastante poco habitual en la historia de la industrialización, el progreso tecnológico se alinea con el respeto al medio ambiente. Y lo que es todavía mejor, no lo hace pretendiendo legar un planeta mejor a nuestros hijos ni haciendo gala de ideas ecologistas o conservacionistas, sino por una razón mucho más fácil de entender: el dinero. La presión por incrementar la eficiencia de los centros de datos y el progresivo aumento de potencia de los ordenadores lleva a la virtualización: en una máquina física que incrementa progresivamente su potencia se establecen un número cada vez mayor de máquinas virtuales que funcionan sobre la misma infraestructura.

Virtualizar servidores conlleva multiplicar la capacidad del centro de datos, pero también reducir enormemente las necesidades de recursos físicos, y particularmente de uno de ellos: la refrigeración. En un centro de datos convencional, en torno a la mitad de la energía eléctrica suministrada se dedica a refrigerar las instalaciones mediante aire acondicionado. Virtualizar servidores supone sustituir máquinas físicas por máquinas virtuales, con ratios que comienzan en 4:1 pero van progresando a gran velocidad merced a la implacable ley de Moore.{17} Tener varios ordenadores virtuales sobre uno físico implica no solo utilizar una sola fuente de alimentación, sino tener además una mayor facilidad a la hora, por ejemplo, de duplicar esa máquina, copiarla o trasladarla a otra localización para incrementar la flexibilidad o responder a una emergencia.

Además de la virtualización, un amplio abanico de tecnologías que van desde el uso de aire exterior en climas fríos hasta el uso de ordenadores con formas que permitan una mejor evacuación del calor (los denominados blades en lugar de los típicos pizza boxes, entre muchas otras innovaciones) o la centralización de componentes como las fuentes de alimentación ininterrumpida han dado lugar a una carrera por la eficiencia energética, que, unida a iniciativas como el incremento de la reciclabilidad de los componentes y el uso de materiales más ecológicos en la fabricación, definen la llamada Green IT, tecnología verde. El ratio más habitual utilizado para medir la eficiencia energética es el conocido como PUE (Power Use Efficiency, o eficiencia en el uso de electricidad), que representa la potencia que hay que suministrar a un centro de datos para llegar a suministrar un vatio al ordenador, y suele oscilar, en centros de datos de construcción antigua, en valores superiores a dos: por cada vatio que llega al ordenador y se convierte en potencia de computación, ha sido necesario suministrar al centro de datos dos vatios, uno de los cuales se ha perdido en refrigeración, elementos de conexión, sistemas de alimentación ininterrumpida, etc. Frente a las intenciones de la Agencia de Protección Ambiental de los Estados Unidos de imponer un FUE inferior a 1,9 para el año 2013, algunas empresas como Google han llegado, en esta carrera por la eficiencia a obtener en sus enormes centros de datos, valores de PUE de entre 1,13 y 1,29, con una media ponderada de 1,21. El valor en ahorro energético de una magnitud así se ha convertido en un factor clave a la hora de competir en el mercado, y permite por ejemplo a Google ser mucho más agresivo a la hora de ofrecer servicios gratuitos que otros competidores que tienen que afrontar costes más elevados.

La evolución de la tecnología, por tanto, nos acerca cada vez más a un mundo en el que la tecnología, tanto a efectos de almacenamiento como de procesamiento» se convierte en una commodity, en algo que proviene de un centro de datos construido seguramente en alguna latitud fría o con disponibilidad de agua utilizable para refrigeración, donde la lucha contra el calentamiento de las máquinas puede llevarse a cabo de una manera más económica. Un ejemplo claro lo constituye de nuevo Google: su red de centros de datos es un complejo entramado de 36 localizaciones (en el año 2008), al que se une otra red secundaria de localizaciones compartidas en régimen de alquiler, que conforman una red completamente distribuida. Gracias a la tecnologías de virtualización empleadas por la compañía, la carga de cualquiera de esas localizaciones puede ser desplazada de manera sencilla e inmediata a cualquier otra, dando lugar a una red prácticamente líquida, a una capa que trasciende las localizaciones individuales.

¿Dónde está la verdadera ventaja? Dada la estructura de Google, la ventaja no tiene nada que ver con ofrecer versiones locales, dado que estas pueden, en un momento dado, estar corriendo sobre cualquier localización física, sino que se produce en términos de costes operativos: existen centros de datos como el de Saint-Ghislain (Bélgica), que no solamente tratan de reducir el número de aparatos de aire acondicionado, sino que directamente los eliminan por completo. El clima de Saint-Ghislain (entre 18.8 y 21,6 °C) permite refrigerar mediante la simple circulación del aire exterior durante una media de 358 días al año, unido a un sistema de refrigeración por agua que utiliza, en lugar de agua de la red municipal, la que circula por un canal industrial cercano, que es purificada a la entrada del centro de datos (la legislación medioambiental suele prevenir que las industrias devuelvan el agua empleada en procesos de refrigeración a una temperatura mayor a la que tenía cuando fue recogida). Si el tiempo cambia, y llegamos a uno de esos escasos días de más calor en Saint-Ghislain, en los que sería preciso utilizar uno de esos inexistentes aparatos de aire acondicionado para refrigerar el centro de datos, la solución consiste en ir apagando ordenadores hasta que el calor generado se reduce lo suficiente, ordenadores cuyas tareas son inmediatamente asumidas por máquinas en otras localizaciones geográficas. Lógicamente, esto conlleva que las localizaciones más interesantes de cara al futuro para la compañía son aquellas en las que el rango de temperaturas resulta más adecuado y en las que el hecho de tener muchos centros de datos distribuidos a lo largo de diversas franjas climáticas con diferente estacionalidad permite compensar estos balances.

Pero no es este el único tipo de balanceo que puede llevarse a cabo: existe también la posibilidad de desplazar carga hacia lugares que pueden aprovechar tarifas más bajas de electricidad, como, por ejemplo, durante horas nocturnas de bajo consumo, lo que origina un trasvase constante de recursos de computación entre diferentes zonas horarias siguiendo una estrategia conocida como follow the moon. Posibilidades como estas indican que en un futuro, la mayor parte de los recursos de computación estarán desarrollados por proveedores globales de este tipo capaces de generar arquitecturas globales que aprovechen esta clase de estrategias. Tener un centro de datos corporativo en una empresa normal supondrá un diferencial de coste absurdo, de muy difícil justificación racional. Una forma de entender la transición progresiva a la nube como una simple cuestión de costes, de última línea de la cuenta de resultados, que además en este caso se alinea con una mayor sostenibilidad: a fuerza de evolucionar y perfeccionar la tecnología utilizada en los centros de datos, estos se han convertido en un negocio suficientemente especializado como para ser desarrollado, lógicamente, por especialistas.

CAPÍTULO 14. NUEVAS HERRAMIENTAS PARA NUEVOS ESCENARIOS

«No pretendamos que las cosas cambien si hacemos siempre lo mismo.»

ALBERT EINSTEIN

Los nuevos escenarios comunicativos y de interacción se basan en nuevas herramientas, en nuevos desarrollos. En muchos casos, esos desarrollos no esperaban ser protagonistas del enorme cambio que hemos experimentado, sino que lo fueron más bien «por accidente». Pero el resultado final es un complejo crisol de tecnologías, hábitos, usos y costumbres que han dado forma a la red actual y, con ella, a una parte cada vez más importante de la sociedad humana.

En la base de todos estos cambios, tenemos una cuestión bastante imperceptible salvo para miradas suficientemente entrenadas, que encontraremos si comparamos una página antigua con una actual: la sintaxis del código utilizado. Sin ánimo de entrar de detalles y complejidades técnicas más de lo que sería recomendable en un libro de este tipo, resulta muy interesante ver la evolución: el HTML utilizado en las páginas de hace algunos años estaba diseñado para definir la estructura de un texto, y entremezclaba de manera constante los formatos, en forma de etiquetas encerradas entre signos de «menor qué» y «mayor qué», con el contenido propiamente dicho. Los elementos estructurales y estilísticos, tales como las tablas, las listas numeradas o de puntos, los tipos de letra, las negritas o las cursivas se intercalaban con el texto que finalmente se mostraba. La forma y el contenido resultaban inseparables a todos los efectos, creando páginas en las que estos elementos se utilizaban de maneras completamente arbitrarias en función de las necesidades de representación del contenido.

El HTML no era, en sí mismo, un lenguaje excesivamente complejo. Su estructura era bastante inteligible para el profano, y muchos de sus códigos se podían tomar de otras páginas y simplemente cambiar los textos para adaptarlos a otras. En la década de los noventa, miles de aficionados se dedicaron a crear una amplísima variedad de páginas en servicios como Tripod o Geocities, que adquirieron una enorme popularidad para la escala de la red de entonces y convirtieron en millonarios a sus creadores tras sus respectivas ventas a Lycos y Yahoo!. Las páginas de este tipo de servicios, caracterizadas por formatos claramente no profesionales, trufados de gráficos intermitentes, galerías caóticas de estilos y de tipos de letras y efectos estéticamente bastante discutibles, fueron deviniendo con el tiempo en un auténtico cementerio de la nostalgia: Geocities, que había sido adquirido por Yahoo! en enero de 1999 por 3.570 millones de dólares en acciones, fue finalmente cerrado en octubre de 2009, en medio de un coro de semblanzas plañideras de todos aquellos que en su momento se iniciaron en sus páginas. Tripod, tras recurrentes rumores de cierre, permanece todavía abierto, aunque su popularidad es a día de hoy bastante escasa.

La evolución de las páginas web fue llevando al desarrollo del XML: un lenguaje en el que los elementos ya no definen el formato, sino directamente el contenido. Todos los elementos de formato se definen al principio en el documento: las cabeceras serán así, los títulos serán de esta manera, las entradas llevarán este o aquel formato, los vínculos se verán así... Al separar una cosa de la otra, el contenido puede mostrarse sin formatos, completamente aislado de los mismos, Las implicaciones de este hecho van mucho más allá de una consideración meramente purista o de algún tipo de exquisitez: al separar la forma del contenido, este puede ser tratado de manera independiente a todos los efectos, y eso permite una amplísima gama de posibilidades. Un blog, por ejemplo, no es más que una página en la que el usuario define primero el formato, la forma en la que se va a representar el contenido, mediante una serie de opciones y menús de manejo sencillísimo, para posteriormente empezar a introducir el contenido, lo que elimina todo atisbo de complejidad. Si el autor de un blog tuviese que construir su página siguiendo el modo tradicional, la tarea sería únicamente posible para aquellos que tuvieran un extensivo conocimiento de HTML, una habilidad que, a pesar de que no hablamos de un lenguaje especialmente complejo, no suele encontrarse entre las habituales del común de los mortales.

La separación de formato y contenido supone también una posibilidad interesantísima; las páginas web pueden producir un archivo exclusivamente con sus contenidos libres de formatos, y brindarlos para que estos puedan ser utilizados en otros sitios, dejando atrás sus formatos originales. Los llamados feeds RSS son precisamente eso: una secuencia de los contenidos producidos por una página, al que un usuario puede suscribirse, y leerlos en un formato completamente diferente al de la página original, o reutilizarlos como parte de otra página. Los feeds RSS son producidos de manera automática por las propias herramientas que los usuarios utilizan para escribir sus contenidos: muchos usuarios ni siquiera saben que lo están haciendo. Pero al hacerlo, posibilitan un nivel de seguimiento y de difusión de sus contenidos desconocido hasta entonces. Todos los días, millones de usuarios leen el contenido de sus publicaciones favoritas en los llamados «lectores de feeds», herramientas como Google Reader, Bloglines, MyYahoo! y otros, o componen escritorios virtuales personalizados sobre la marcha con Netvibes o iGoogle escogiendo aquellas partes de páginas que quieren seguir, como quien se compone un periódico con los deportes de Marca, las columnas de opinión de La Vanguardia, el internacional de El País y las guías de espectáculos de Vocento. Ante la facilidad de poder publicar en la red cualquier contenido despreocupándose completamente de su estructura, los usuarios empiezan a subir de todo; las fotos a sitios como Flickr, Photobucket o Fotolog, los vídeos a YouTube, sus pensamientos a Blogger o a WordPress, mientras los interesados en seguirlos se suscriben a sus feeds RSS y reciben la secuencia de lo publicado en sus lectores de feeds en tiempo prácticamente real, cada vez que se ponen delante de la pantalla de su ordenador.

En pocos años, la red pasa de ser el lugar donde unas pocas empresas y medios desarrollaban su presencia a modo de escaparate, a ser el sitio de intercambio para todos, el ágora virtual donde comentar las noticias, escribir reflexiones, intercambiar fotos o mantener un diario, todo sin necesidad de conocer ningún lenguaje especial, utilizando herramientas simples y gratuitas al alcance de cualquiera. Lo que se sube a la red, se queda en la red: con los blogs, se desarrolla el concepto de permalink, el vínculo permanente que sirve para acceder a una información aunque esta se haya archivado posteriormente: aquellos periódicos que no utilizan permalink, cuyas noticias desaparecen de la red a los pocos días, se ven progresivamente ignorados por los internautas, que no quieren vincular a páginas que después dejan de existir y generan un error 404.{18} En un espacio virtual ilimitado, ¿cuál es el sentido de eliminar las cosas? ¿Por qué no mantenerlas en la red, para que puedan tener una vida propia, ser vinculadas por terceros, comentadas o encontradas de nuevo?

La secuencia de acontecimientos deja sumamente patente la progresión: Blogger fue creada por Evan Williams y Meg Hourihan en 1999 y adquirida por Google en 2003. La inyección económica que supuso tal adquisición proporcionó a los blogs su verdadera carta de naturaleza: el número de blogs se disparó, y llegó a los 170 millones en menos de cinco años. Crear un blog estaba al alcance de cualquiera: se hacía en cinco minutos, era gratuito, no se necesitaba saber HTML ni ningún código complicado, y permitía aparecer relativamente bien situado en los motores de búsqueda. Los blogs personalizaban el tipo de contenido que Google adoraba: actualizado frecuentemente, con muchos vínculos y con código limpio merced al uso de la herramienta correspondiente, lo que permitía que el motor de indexación pudiese trabajar con facilidad. Sin embargo, si bien crear un blog resultaba sencillo y suponía una fortísima disminución de las barreras de entrada para la creación de contenidos, mantenerlo no lo era tanto. Mantener un blog suponía una dificultad que poco tenía que ver con la tecnología: suponía tener algo interesante que decir. Así, muchos de los blogs que empezaron con gran impulso, fueron ralentizando su marcha o dejando completamente de actualizarse, quedando relegados o bien al papel de diarios personales con poco interés más allá del autor y sus contactos más cercanos, o bien a ir disminuyendo su ritmo de actualización hasta desaparecen En muchos casos, la creación de contenido original se convierte en mínima, haciendo que los lectores acaben recurriendo a las fuentes primigenias o, simplemente, la presencia en la red pasa a ser demasiado costosa para el autor, que va abandonando progresivamente las tareas de actualización. Lentamente, los blogs van evolucionando o bien para convertirse en herramientas de publicación cada vez más profesionalizadas que se transforman en auténticos medios, o bien en páginas puramente personales que un autor usa para reflejar un conjunto de intereses o desarrollar una presencia en la red.

El desarrollo de la llamada blogosfera rompió los esquemas de la comunicación tradicional y tomó a una gran cantidad de actores completamente por sorpresa. Los primeros blogs eran simplemente páginas personales que trataban temas que interesaban a su autor o autores, pero que lo hacían además desde una óptica informal, próxima, con una cercanía comunicativa que podía resultar sumamente atractiva para los lectores, que, a su vez, gustaban también del tema tratado. La aproximación era lo que hasta aquel momento se denominaba amateur, personas que por lo general sin ser profesionales de un tema determinado, lo trataban porque les resultaba muy atractivo, lo suficiente como para leer abundantemente sobre ello, documentarse, encontrar referencias y, además, disfrutar mientras lo hacían. La primera generación de blogs estaba fuertemente dominada por temáticas tecnológicas: a pesar de no ser necesario un conocimiento sólido de tecnología para publicar un blog, el fenómeno tenía sus orígenes donde los tenía, y resultaba imposible retrotraerse al mismo. Y fue precisamente vinculado a esta información tecnológica donde empezó a percibirse un fenómeno interesante: aquellos blogs recién creados, carentes de tradición, de profesionalidad y de medios, superaban en muchos casos en profundidad de análisis y en conocimientos sobre el tema a publicaciones «de toda la vida». La razón era evidente: mientras las publicaciones tecnológicas convencionales contaban con redactores generalistas, los blogs cubrían áreas mucho más estrechas, y contaban con auténticos especialistas que, además, escribían por genuino interés personal. La comparación, en realidad, no era entre una revista y un blog, sino entre una revista y todos los blogs que cubrían temáticas relacionadas. Y en tal comparación, la revista palidecía: le resultaba imposible contar en su redacción con tantos especialistas como contaba la blogosfera en su conjunto. Además, los blogs contaban no solo con sus autores, sino también con un fenómeno cada vez más pujante: su comunidad. Lo que en una revista clásica eran lectores, que como su nombre indica se limitaban a eso, a leer, en un blog se transformaban mágicamente en comentaristas activos, que aportaban en muchos casos información adicional, puntualizaciones, datos o discusiones que enriquecían en gran medida la noticia o el tema tratado. Además, el desarrollo de la comunidad determinaba una sensación de pertenencia, que acababa redundando en una mayor fidelidad: lectores más implicados.

El éxito de los blogs, a pesar de ser un fenómeno de pequeña escala, provocó no pocos quebraderos de cabeza a los medios tradicionales: de repente, muchas de las reglas conocidas de los mismos parecían hacer aguas. Tradicionalmente, las ediciones online de las publicaciones convencionales no incluían vínculos externos: se trataba según su razonamiento de una manera de enviar tráfico al exterior, de perder lectores a manos del sitio al que apuntaba el vínculo. Sin embargo, los blogs hacían justo lo contrario: estaban literalmente sembrados de vínculos, desde el cuerpo de las noticias hasta los laterales, donde ubicaban el denominado blogroll, la lista de las páginas que el autor leía habitualmente. La sensación para los medios tradicionales era que los blogs se dedicaban a «echar» a sus lectores, a enviarlos a otros lugares: si te está gustando lo que lees, haz clic aquí, profundiza, vete a este sitio, vete a este otro, vete a las fuentes que yo utilicé... y sin embargo, los lectores volvían, y lo hacían con interés renovado: aquel sitio que les había enviado a otro lugar anteriormente, se convertía en referencia, adquiría un valor mayor. Progresivamente, los medios convencionales fueron adquiriendo cada vez más características de los blogs: primero fueron los comentarios, después los vínculos, después los permalinks, etc. Estaba claro: el negocio estaba cambiando, pero los medios convencionales, a pesar de sus muchos años de historia, ya no manejaban esa evolución. La pauta la marcaban unos recién llegados.

Los blogs también empezaron rápidamente a representar un problema de interacción: en muchos abundaba la crítica hacia productos o empresas, algo que en los medios convencionales solía tener un nivel de representación relativamente bajo. Lo habitual en los medios era algo similar al «si al hablar no has de agradar, es mucho mejor callar»: el hecho de depender en su modelo de ingresos de la publicidad llevaba al desarrollo de un clima que tendía a evitar la confrontación. Lo habitual en una revista de videojuegos era que los juegos tuviesen siempre puntuaciones elevadas, por encima del siete o el ocho: hacer lo contrario era arriesgarse a ser excluidos, a que la empresa dejase de contar con la publicación para hacer su publicidad o para enviar juegos de prueba. El poder de negociación entre marcas y publicaciones se encontraba fuertemente desplazado hacia las primeras. Y lo mismo ocurría en muchos otros sectores, desde el automóvil hasta la hostelería.

Pero los blogs eran, en general, bastante más insensibles a este tipo de protocolo. En los blogs, los análisis eran habitualmente sinceros, y se basaban en las experiencias del autor como consumidor, no mediatizadas por otros factores. Y en muchas ocasiones, con un marcado predominio de lo negativo: mientras el consumo satisfactorio de un producto o servicio no necesariamente conllevaba una reseña en ese sentido, las experiencias negativas sí se veían en muchos casos seguidas de la correspondiente crítica, a menudo jalonada de expresiones que no eran habituales en el lenguaje periodístico del momento. Recordemos, además, que los buscadores tendían a tener una cierta predilección por los blogs, a los que indexaban con premura y eficiencia, y tenemos todos los elementos para una relación turbulenta entre marcas y blogs: en la nueva sociedad conversacional, las empresas y sus responsables de comunicación corporativa miraban horrorizados los resultados de sus ego-searches (la búsqueda de uno mismo, su empresa, sus marcas, sus ejecutivos, etc.) sin saber cómo reaccionar, algo en lo que entraremos con una mayor profusión en el próximo capítulo. Una mala experiencia de un cliente ya no significaba que el círculo habitualmente restringido de contactos directos dejase de consumir la marca, sino que podía convertirse en un escándalo en Internet y en un punto de encuentro para todos aquellos que habían tenido una experiencia similar. Las empresas habían perdido el control de la ecuación comunicativa.

En el lado positivo de la ecuación, algunas empresas empiezan a encontrarse con el fenómeno contrario: en algunos casos, el blogger está dentro de la compañía, y se convierte, queriéndolo o no, en una voz que la representa, que humaniza su comunicación. En ocasiones, el blogger llega a adquirir una visibilidad notable, lo que tampoco está exento de problemas: los departamentos de comunicación se encuentran con un fenómeno que no saben controlar, con el que no se saben reaccionar. En el muro que habían construido para separarse del exterior, para poder facilitar a las entidades externas únicamente la información que deseaban y cuando lo deseaban, habían aparecido fisuras. La comunicación corporativa veía la imperiosa necesidad de convertirse en más transparente, pero no veía la posibilidad de hacerlo: imaginarse a una empresa adoptando una actitud verdaderamente conversacional, admitiendo no tener razón o criticando un producto de la competencia de manera directa era algo complejo. Las empresas, simplemente, no sabían hablar, y se limitaban a lanzar mensajes acartonados, vacíos, con elogios que sonaban completamente falsos, incluso en los casos en que las marcas desarrollan sus propios blogs, estos parecen tener sentido únicamente cuando sus autores tienen nombre y apellidos, respetando el principio de ser medios de información personal, una situación que genera no pocas tensiones.

Decididamente, la relación entre las empresas y la web social nunca fue sencilla. A lo largo del tiempo hemos visto sucederse demandas, cartas de cease and desist escritas por gabinetes jurídicos solicitando de manera amenazadora la retirada de determinados contenidos, intentos de ejercer presiones, etc. La práctica totalidad de estos intentos se topó con el denominado «efecto Streisand»: la reacción de las compañías acababa generando muchísima más atención hacia la crítica o el contenido que supuestamente pretendían retiran Definido originalmente por la reacción de Barbra Streisand en el año 2003 solicitando mediante una demanda de 50 millones de dólares la retirada de unas fotografías en las que aparecía su casa de la costa de California, el efecto Streisand es una de esas cuestiones que todo experto en comunicación tiene que saber manejar: en la red, la densidad de interconexiones es tal, que este tipo de efectos se acentúa en un altísimo grado. Los ejemplos se sucedieron a gran velocidad, demostrando de nuevo a las empresas que los parámetros de la ecuación comunicativa que habían manejado durante décadas habían cambiado por completo. El movimiento de información se había convertido en un fenómeno viral: un simple cliente descontento podía desencadenar un verdadero escándalo comunicativo, y hacerlo además en cuestión de pocas horas.

A este fenómeno contribuyó también, en gran medida, la aparición de los llamados «filtros sociales». El primero de ellos, Digg, fue desarrollado por Kevin Rose tras meditar sobre el funcionamiento de su blog favorito, Slashdot, en el que una serie de editores recibían noticias de los lectores y actuaban como filtro para determinar su aparición en el sitio y ser sometidas a los comentarios de la comunidad. Slashdot se dedicaba a analizar noticias relativas a la tecnología y sus implicaciones desde la óptica generalmente del software libre y la ciberdisidencia, y era un sitio de muchísimo éxito, tanto que aparecer en él podía llegar a significar que el sitio referenciado en sus vínculos no pudiese soportar la presión del número de visitas y cayese víctima de la sobrecarga de tráfico, fenómeno conocido como ser slashdotteado o Slashdot effect. El citado Kevin Rose comenzó a dar vueltas a la idea de un Slashdot sin editores, con un componente que calificaba como de «verdadera democracia»: un sitio donde los propios lectores votasen cuáles de las noticias enviadas por ellos mismos debían alcanzar la portada. El resultado, al que llamó Digg, fue un aplastante éxito de público, una comunidad con infinidad de ojos y oídos a la que muchos se acercaban con la idea de no perderse detalle de la actualidad, pero que, además, era capaz de generar fenómenos de atención súbita. El éxito de Digg conllevó, además, el desarrollo de clones locales, algunos de notable éxito: el caso de Menéame en España, desarrollado íntegramente en código abierto, es hoy uno de los puntos más importantes de generación de atención y tráfico en la red española, y ha generado ya más de ciento treinta clones en todo el mundo que utilizan su código.

Lejos de reducirse, la presencia de los particulares, de los consumidores, en la red fue creciendo de manera exponencial. En paralelo al desarrollo de los blogs, aparecieron, con enorme pujanza, las redes sociales. El auge de las redes sociales proporciona a las personas una manera mucho más natural y sobre todo, más sencilla, de mantener su presencia en la red: sus usuarios trasladan a estas redes sus bases de datos mentales de amigos, conocidos y contactos, y las utilizan para llevar a la red todo un modelo de interacción complementario al de fuera de la red. Cada vez más, nos acordamos de los cumpleaños de nuestros amigos gracias al aviso que la red social nos comienza a dar unos días antes, y compartimos las fotografías en la red en lugar de hacerlo físicamente, sin que esto suponga necesariamente vernos menos: la red actúa como un complemento de la relación que tiene lugar fuera de la red, no como un sustituto de la misma En realidad, las redes sociales representan una derivación de un fenómeno global a nivel de Internet: todo en Internet se vuelve social. No se trata de subir tus fotos a una página, sino de hacerlo con una red social superpuesta a ello que te permita elegir quién ve cada una de ellas. En el caso de Flickr, por ejemplo, el resultado es sencillo y brillante, dividido en cuatro niveles: familia, amigos, contactos y resto del mundo. Una división simple e intuitiva, que permite tomar decisiones en cada momento cuando se sube una fotografía tomada durante un evento público o una de la playa con la familia en bikini. La web va adquiriendo un marcado componente social en prácticamente todas sus actividades, que le da su verdadero sentido. Leer las noticias en Google Reader puede ser cómodo y conveniente en lugar de tener que pasearse por todas las páginas de infinidad de medios, pero también proporciona la posibilidad de ver las noticias que más han leído o que han marcado como favoritas en mi red de amigos, dotando a la lectura de noticias de ese mismo componente social.

Incidentalmente, uno de los protagonistas de la revolución que supusieron los blogs, Evan Williams, aparece implicado en la concepción y el desarrollo de otra herramienta: los llamados microblogs, representados especialmente por Twitter. Concebido originalmente como una herramienta para que los usuarios puedan, simplemente, actualizar en la red lo que están haciendo en cada momento, Twitter comienza una escalada de popularidad vinculada al efecto que provoca en sus usuarios: el de sentirse automáticamente más cerca de aquellos a los que siguen. El fenómeno Twitter solo puede ser entendido por aquellos que lo han probado: de entrada, la idea de escribir lo que uno está haciendo en un máximo de 140 caracteres suena banal, incluso absurdo para mucha gente, rayano en el exhibicionismo. Sin embargo, adquiere pleno sentido cuando se produce en compañía de otros: de repente, las relaciones entre personas pasan a adquirir una sensación de proximidad absoluta, incluso cuando se desarrollan a miles de kilómetros de distancia.

Patrimonio, al principio, de usuarios relativamente avanzados, y desarrollado a caballo del fenómeno de los blogs, Twitter no tarda en demostrar que ninguna de las iniciativas desarrolladas con la misma idea puede hacerle sombra —a pesar de que algunas de ella provenían de empresas como Google, Facebook o los creadores de Digg— y a empezar a colonizar el segmentó del gran público. En 2008, Twitter da un paso más y se convierte en herramienta habitual de los famosos a la hora de manejar sus relaciones naturalmente asimétricas en la web, lo que conlleva una tasa de crecimiento muchísimo mayor. En 2009, una nueva ronda de financiación sitúa el valor de Twitter, una empresa que en ningún momento de su historia había llegado a facturar nada, en mil millones de dólares, valoración que muy poco tiempo después empieza a parecer más factible en cuanto la compañía anuncia acuerdos con Microsoft y Google para que éstos hagan uso de la información generada por los millones de actualizaciones de Twitter, en lo que empieza a denominarse real-time web, la «web en tiempo real», que muestra lo que está ocurriendo en este mismo instante. Actualmente, Twitter factura a estas empresas en función de la indexación de su tráfico, y ultima el desarrollo de herramientas de publicidad no convencionales (según las palabras de uno de los fundadores de la empresa) y de servicios por los que los usuarios puedan estar interesados en pagar.

En paralelo al desarrollo de todas las herramientas de la web social, encontramos otro fenómeno que interconecta con el mismo: el crecimiento de la movilidad, uno de los fenómenos de difusión tecnológica sujeto a una evolución más rápida. Los teléfonos móviles dejan de ser simplemente teléfonos, y empiezan a ser auténticos microordenadores de bolsillo, con capacidad para conectarse a la red, actualizar información, consultar lo que hacen los contactos de una persona, etc. En este momento, la difusión de la telefonía móvil en casi todas las economías desarrolladas supera el 100%, y el crecimiento en economías en vías de desarrollo es asimismo enormemente pujante. Como comenta acertadamente Howard Rheingold, autor de Smart Mobs, uno de los libros más recomendables en este sentido, «no todo el mundo tiene una cuenta bancaria, pero casi todo el mundo tiene un teléfono móvil». En diciembre de 2008, el total de usuarios de telefonía móvil en el mundo era de 4.100 millones, sobre una población mundial de 6.700 millones de personas.

El segmento de mayor crecimiento es el de los denominados smartphones, caracterizados por su capacidad de acceso a la web, y que encuentran en las aplicaciones sociales un verdadero factor fundamental para su desarrollo. Los usuarios de redes sociales son verdaderos demandantes de movilidad, un hecho que ha sido aprovechado por varios fabricantes de terminales para lanzar cada vez más aparatos destinados al consumo constante de este tipo de contenidos, una tendencia que continúa creciendo de manera imparable y que está determinando una pléyade de cambios sociales en la forma de utilizar los dispositivos y de integrarlos en nuestra vida cotidiana. El teléfono móvil es el aparato sin el que muchas personas jamás salen de casa: sin él se encuentran perdidos, indefensos..., básicamente, desconectados. El terminal es la conexión permanente con sus amigos, su red, su actualidad y su contexto.

La función primigenia de la comunicación mediante voz queda, en algunos rangos de edades, completamente minimizada por el uso que hacen de conexión permanente con sus redes sociales, compartición de fotografías y vídeos, mensajería de todo tipo, navegación en la red o incluso actualización de su posición geográfica: el 40% de los móviles que se fabrican hoy llevan incorporado un sistema GPS, un porcentaje que en tan solo dos años será del 100%. Por otro lado, las implicaciones de que una amplísima mayoría de la población lleve en todo momento encima una cámara de fotos o vídeo, un disco duro capaz de transportar infinidad de datos de todo tipo o un dispositivo capaz de mantener una transmisión de banda ancha con un servidor están todavía comenzando a desvelarse. Existen ya algunos provectos, como el Total Recall de Microsoft Research o los de varias start-ups que esperan lanzar productos comerciales en el año 2010, que especulan en torno al concepto de life-recorders, la posibilidad de una vida completamente grabada: cámaras de botón en miniatura que graban de manera permanente la vida de una persona, y suben imágenes y sonido a un servidor. Las implicaciones de una vida con todas sus imágenes y sonidos completamente recogidos sobre un soporte indexable tienen connotaciones de muy largo recorrido, mucho más de lo que podemos imaginar fácilmente: pensemos en compartir recuerdos con amigos, en evocar momentos anteriores de nuestra vida, o en decidir qué recuerdos conservar y cuáles eliminar. ¿Perderemos conciencia de los recuerdos desagradables, como hoy nos ocurre con los números de teléfono que antes de la llegada del teléfono móvil solíamos recordar sin ningún tipo de dificultad?

El futuro es ilimitado, prácticamente exponencial. Recientemente, en los laboratorios de Alcatel, se ha obtenido una velocidad de transmisión de 100 petabytes por segundo y kilómetro a través de fibra óptica mediante el uso de láser, unas diez veces más rápida que la de un cable transoceánico, una base fundamental para soportar la escalada de ancho de banda en la que vivimos, y un golpe más para aquellos agoreros que afirmaban que Internet «se llenaría», «se saturaría» o «sería imposible de manejar».

CAPÍTULO 15. LA SOCIEDAD HIPERCONECTADA

«La especie que sobrevive no es la más fuerte ni la más inteligente, sino la más adaptable al cambio.»

CHARLES DARWIN

¿Cuáles son las implicaciones de que una parte rápidamente creciente de nuestra sociedad, de que un porcentaje cada vez mayor de nuestros ciudadanos, estén interconectados entre sí y con la información de una manera que ven como algo completamente natural? Muchas de estas implicaciones están simplemente empezando a insinuarse, pero nos permiten inferir que van a afectar poderosamente a muchísimas áreas, desde cómo vivimos, a cómo consumimos, nos comunicamos o hacemos política.

Cuando a finales del año 2008, un político norteamericano, Barack H. Obama, resultó elegido como 44º presidente de su país, muchos empezaron a frotarse los ojos de manera extrañamente persistente. Sin duda, la elección de Barack H. Obama había roto una gran cantidad de moldes. Además de ser el primer presidente negro, había confiado una parte enormemente significativa de su campaña electoral a la red con una eficiencia verdaderamente notable, y anunciaba que, tras haber usado la tecnología para acceder a lo más alto de la política, se disponía a seguir utilizando la tecnología para cambiar la política.

Obama había sido capaz de, utilizando una combinación de aprendizaje desarrollado durante campañas de candidatos demócratas anteriores como la de Howard Dean en el año 2004 y el estado de madurez de la web social, obtener enormes réditos de cara a su candidatura: una financiación récord aportada por pequeñas donaciones individuales de simpatizantes le liberaban de compromisos con los lobbies económicos habituales en todas las campañas anteriores, mientras que su participación en redes sociales y páginas web le proporcionaba un apoyo sin precedentes. Había conseguido apalancar la participación en la web para convertirla en la traslación moderna del significado de una militancia política que se hallaba en franco desuso hasta el momento, pero que de repente pasaba a ser una opción sencilla, atractiva y al alcance de cualquiera. Personas de todos los rangos de edad reenviaban a sus amigos vídeos de YouTube con intervenciones del candidato, utilizaban las fotos que publicaba en Flickr con licencia Creative Commons para ilustrar sus entradas en blogs, o defendían sus posturas en discusiones interminables en redes sociales como MySpace o Facebook, o en filtros sociales como Digg.

A mediados de 2008, Twitter se consideraba todavía un servicio para usuarios relativamente avanzados: los «famosos» no habían prácticamente hecho su aparición, y el usuario con más seguidores, 56.000, era Kevin Rose, fundador de Digg. En agosto de 2008, Barack Obama superó a Kevin Rose con 58.000 seguidores, mientras su más directo oponente, John McCain, tenía pocos más de mil en una cuenta en la que resultaba incluso difícil averiguar si pertenecía al propio candidato. No se deje engañar por las magnitudes: Twitter, como hemos comentado, era en aquel momento un servicio relativamente poco conocido entre el gran público. En el momento de escribir este capítulo, la cuenta @barackobama tenía ya más de 2.650.000 seguidores.

El importante papel jugado por la red en la elección de Barack Obama no podía de ninguna manera calificarse como de espejismo. De hecho, ni siquiera había sido especialmente original en el uso de herramientas: simplemente había dado con el momento adecuado de madurez y de penetración para hacer un uso eficiente de ellas. Si interpretásemos la campaña de Obama como un movimiento de tipo empresarial, cuyo objetivo es vender el candidato a un mercado de electores, la compañía resultante habría estado notablemente retrasada con respecto a dicho mercado: los hipotéticos «compradores» eran, en un porcentaje importante, más avanzados en el uso de herramientas que la compañía a la que «compraban». Pero en comparación con Obama, el resto de los candidatos presidenciales no solo palidecerían, sino que directamente vestirían pieles y habitarían en cavernas: eran, en su gran mayoría, una panda de absolutos ignorantes que o bien no eran capaces ni de consultar su correo electrónico (verídico, en el caso del septuagenario John McCain), o que se aproximaban a Internet como quien se acerca a repartir folletos a la puerta de una estación de metro.

El ejemplo de Barack Obama y su campaña presidencial de 2008 nos sirve como perfecto argumento para introducir el tema: esa es exactamente la situación actual. Mientras el conocimiento de herramientas de la llamada Web 2.0 o web social ya es de uso común en muchos estratos de la sociedad, las empresas permanecen en su gran mayoría al margen de la misma, y los pocos ejemplos que son capaces de mostrar una proactividad levemente mayor obtienen de ello sabrosas plusvalías comparativas, acentuadas además por las mieles del llamado «efecto pionero».

Pero entremos en detalles: ¿qué significa exactamente el término «sociedad hiperconectada», y qué implicaciones presenta para personas y empresas? Una sociedad hiperconectada es, como hemos mencionado en capítulos anteriores, una en la que sus elementos constituyentes, sean personas físicas, personas jurídicas o instituciones, se encuentran unidas por líneas virtuales de comunicación bidireccional de diversa naturaleza. En nuestra sociedad actual, todos nos hallamos unidos por una serie de líneas de comunicación más o menos visibles, que se expresan de una manera u otra en la red. Una cantidad creciente de personas desarrolla una presencia en Internet que puede ser de muchos tipos: voluntaria o involuntaria, deseada o no deseada, gestionada por ellas o por terceras personas. Esa presencia puede ir desde la aparición de su nombre en un Boletín Oficial a causa de una multa de tráfico inadvertidamente impagada, hasta una página personal que recoge todos los detalles de su vida incluyendo un listado exhaustivo de sus actividades durante las últimas horas.

Estar o no en Internet, o de qué manera estar depende de muchísimos factores, del nivel de actividad y conocimientos de las personas, del componente público o privado de su perfil, de su nivel de visibilidad, de las personas que tienen en su entorno... Un cargo público en el gobierno de una nación no puede elegir entre estar presente o no en la red: como mucho, podrá optar por un nivel de presencia más o menos activo, pero la gran mayoría de los factores implicados no solo no dependen de sí mismo, sino que resultan completamente incontrolables: un personaje no especialmente polémico puede aparecer de la noche a la mañana en 1.500 páginas en la red por el simple hecho de salir casualmente en una fotografía con los ojos cerrados y aspecto adormilado, aunque de hecho no tuviese el más mínimo sueño y se trate de un mero capricho de la instantánea.

Todos, desde nuestra abuela de 80 años que jamás ha puesto una mano en un ordenador hasta los hijos de nuestros vecinos de cuatro y seis años, estamos sujetos a un nivel de exposición inédito en las sociedades humanas. La privacidad como tal se ha convertido, de hecho, en una anomalía histórica: mientras los seres humanos vivíamos en pequeños núcleos de población, la privacidad prácticamente no existía. Nos sabíamos con cierto detalle las vidas de nuestros vecinos, los hijos que tenían, sus enfermedades y los detalles de su trabajo. Solo en el período histórico en el que las ciudades se fueron haciendo más grandes y la tecnología no se había desarrollado lo suficiente fuimos capaces de tener cierto nivel de privacidad, debida a que mentalmente éramos incapaces de abarcar el número de personas que teníamos cerca. Con el desarrollo de Internet, un número creciente de nuestras actividades pasan a dejar automáticamente trazas en la red: organismos públicos, redes sociales, actividades de nuestros amigos y conocidos... Si el resultado de una búsqueda de un nombre y un apellido no devuelve ningún resultado conocido de una persona determinada, empezamos a pensar que se trata de un caso muy raro, y casi nos empieza a sonar mal hasta el punto de preguntamos si será primo lejano de Ted Kaezynski, también conocido como Unabomber.{19} Cada día más, la red nos rodea, como un constituyente más de nuestras vidas, como un líquido en el que estamos inmersos queramos o no, y resulta no solo imposible, sino incluso poco recomendable permanecer al margen de ella.

Si la lectura de los dos últimos párrafos le resulta desagradable, incluso claustrofobia, recuerde algo que comentamos al inicio de este libro: no estoy afirmando que la situación descrita sea buena o mala, cómoda o incómoda, agradable o preocupante. No es mi papel venderle nada ni convencerle de que algo es bueno o malo en sí mismo. Estoy afirmando que la situación es así, sin más. Dejemos a un lado los siempre cuestionables juicios de valor y centrémonos en la descriptiva.

Una sociedad hiperconectada parte de la base de un acceso relativamente ubicuo a los medios de producción. Indudablemente, la situación no es todavía así. En nuestra sociedad actual sigue existiendo una relativa brecha digital, una serie de segmentos poblacionales que no pueden acceder a un ordenador o a una conexión de banda ancha por razones económicas, culturales o de otro tipo. Sin embargo, la velocidad de difusión y la evidencia de la progresiva universalización de Internet debería llevarnos a pensar en un escenario futuro en el que el ostracismo de la red sea una situación muy poco habitual. En este momento, si tiene usted en torno a los 40 años y una cierta vida en Internet, y emprende la tarea de buscar a sus amigos del colegio, es más que posible que la sensación sea la de aislamiento: en un gran número de casos, el buscador le devolverá resultados no relevantes de personas con nombres parecidos o iguales, pero diferentes a las que usted buscaba. La red social Facebook alcanzó en diciembre de 2009 los 350 millones de usuarios, lo cual representaría en torno a un escaso 5% de la población mundial tomada sin ningún tipo de restricciones. Si filtrásemos ese porcentaje por países o por otras variables (los Estados Unidos se encuentran enormemente sobrerrepresentados, como lo están determinados grupos de edad frente a otros, etc.), el resultado podría ser seguramente más bajo en el caso de un español de edad media. Sin embargo, la velocidad de extensión del fenómeno de las redes sociales es muy elevada, lo que lleva a que sus usuarios experimenten la sensación de una creciente presencia en un número cada vez mayor de segmentos sociodemográficos. Si eres usuario de una red social generalista como Facebook, empezarás a encontrarte con que cada día incorporas personas con perfiles más discordantes: tus compañeros de trabajo, una cara conocida de la televisión, tu suegra, etc., lo que empieza a reclamar una gestión cada vez más cuidada de los contactos en grupos en función de diversas variables.

La experiencia de vivir en una sociedad hiperconectada no resulta todavía habitual para la mayor parte de los usuarios de Internet. El perfil de usuario medio de la red suele ser el de una persona que la utiliza, por lo general, para tareas como acceder a su correo electrónico, leer la prensa, ver el extracto del banco, descargar algunas canciones y películas, comprar entradas de espectáculos y buscar algunas cosas en un buscador. Si tomamos a ese usuario medio y lo acercamos, de repente, a la evidencia de que vive en una sociedad hiperconectada, lo normal es que su gesto sea de sorpresa, tal vez incluso de susto.

Y es que el nivel de conexión empieza a alcanzar niveles verdaderamente sorprendentes: resulta perfectamente normal que una persona se levante por la mañana, y se dirija a su ordenador antes de siquiera acercarse al desayuno. En muchos casos, desde la misma cama, navegando a bordo de dispositivos portátiles como iPhones o BlackBerries, convirtiendo la navegación en deporte compatible con el remoloneo de primera hora de la mañana. En ese primer acercamiento a la red, lo normal es hacer una revisión de la bandeja de entrada, pero también del lector de feeds. El fenómeno de los lectores de feeds RSS, cuyo exponente más destacado lo constituye a día de hoy Google Reader, es digno de mención: sus usuarios se suscriben a aquellas publicaciones o secciones de publicaciones que les interesan, y combinan los blogs de sus amigos y conocidos con las columnas de The Economista los reportajes del Business Week o las alertas del Wall Street Journal. El lector de feeds multiplica exponencialmente su capacidad de hojear, de leer titulares, por lo que resulta perfectamente normal que mientras que un usuario tradicional reduce su lectura a un periódico o dos, uno que utilice un lector de feeds revise todos los días 40 o 50 fuentes diferentes. En muchos casos, esos feeds no son siquiera publicaciones, sino búsquedas de palabras clave o términos como el nombre de su empresa, de competidores o de productos que le brindan un panorama inmediato de lo que se ha mencionado al respecto en cualquier lugar de la red. El lector de feeds ofrece una manera rápida y cómoda de consumir noticias, y facilita, además, todas las funciones adicionales necesarias: la de almacenar algo para referencia posterior, la de reenviarlo a un amigo o conocido, y la de compartirlo con sus amigos, así como su correspondiente función simétrica, la de basarse en lo que sus amigos leyeron o recomendaron para construir su lectura diaria.

A la lectura de feeds se superpone, en muchos casos, el uso de herramientas de microblogging, con Twitter como el fenómeno más destacado. ¿Qué lleva a muchos millones de personas en todo el mundo a compartir con otros lo que están haciendo, en un máximo de 140 caracteres? La experiencia de Twitter resulta indispensable para entender su uso: por mucho que nos lo expliquen, no lo entenderemos en base teórica. Sin embargo, su uso es enormemente adictivo, porque permite, por un lado, sentirse mucho más cerca de aquellos a los que se sigue y que le siguen a uno, mientras posibilita también la construcción de canales de comunicación inmediatos con un efecto brutal sobre la viralidad, sobre la difusión de información. Un usuario con un Twitter muy seguido puede difundir información entre miles de personas en cuestión de segundos: un comentario sobre el nombramiento de un ministro puede desencadenar cientos de comentarios y reenvíos (conocidos como retweets o RT), que a su vez llegan a las redes de todos los usuarios que lo hacen, y terminar conformando, por ejemplo, una reacción de rechazo que se articula en cuestión de escasas horas y se convierte en un auténtico y espontáneo tour de forcè con el gobierno que lo nombró. Y ese mismo canal utilizado como arma de acción política, puede servir en cambio durante muchos otros momentos como herramienta para comentar que una persona va a salir de copas, para contar un chiste, para difundir un enlace o, simplemente, para dar los buenos días o las buenas noches. En agosto de 2009, un usuario de Twitter recolectó una media de 11.000 tweets, o actualizaciones de Twitter, al día con la frase «buenos días» y sus equivalentes en diferentes idiomas, y las representó sobre un globo terráqueo, mostrando una secuencia de actividad a nivel mundial retransmitida por este canal, como si fuera un patio de vecinos a escala planetaria. Durante la primera mitad de ese mismo año 2009, Twitter perdió su identificación con el perfil de usuario avanzado, y conquistó los mercados masivos: personajes habituales en el papel cuché, presentadores de televisión, actores, deportistas y famosos de todo tipo irrumpieron en Twitter y acumularon de golpe millones de seguidores, proporcionando una nueva oleada de popularidad a la herramienta y orientándola al consumo masivo.

La sociedad hiperconectada significa que miles de personas permanecen en un estado de constante conexión: no prestan atención consciente a ello ni le dedican tiempo de manera exclusiva, pero reciben una retroalimentación constante de su entorno. En realidad, los usuarios de este tipo de herramientas utilizan, por ejemplo, una pestaña o una serie de pestañas en un navegador, que suelen mantener abiertas en todo momento, pero pocas veces en primer plano. Mientras están leyendo noticias, trabajan con esa pestaña, pero en cuanto terminan, la mantienen abierta en segundo plano o minimizada, y simplemente vuelven a ella en las pausas de su trabajo principal o cada cierto tiempo. En un terminal móvil, el modelo de interacción es idéntico: el terminal permanece en el bolsillo o es utilizado para hablar por teléfono, escuchar música o ver una película, pero, con cierta periodicidad, no necesariamente regular, el usuario conmuta a la tarea correspondiente y echa un vistazo a lo que están haciendo sus amigos en Twitter, dónde se encuentran físicamente en Latitude, los nuevos mensajes de correo, etc. Algunas tareas gozan de estados especiales, como es el caso de los SMS o la mensajería instantánea, a los que se reconoce un carácter de sincronicidad que hace que el usuario interrumpa casi cualquier otra tarea para pasar a atenderlas (de ahí lo molesto y ofensivo que resulta su mal uso, como es el caso, por ejemplo, de los SMS publicitarios enviados por las compañías telefónicas).

Algunos usuarios mantienen, además, presencia en la web mediante un blog. Los bloggers forman un subconjunto especialmente interesante dentro de la web social, porque son uno de sus principales productores. Crear un blog resulta extremadamente sencillo, algo que verdaderamente cualquiera puede hacer. Pero mantenerlo no lo es tanto, y obliga a un derroche creativo importante, a una complejidad no vinculada a la herramienta, sino a la creatividad, a la inspiración. El aporte de muchos blogs es completamente original, procedente de las neuronas de su autor o autores, a menudo tras interaccionar con otros contenidos que, además, suelen siempre enlazar. El de muchos otros es resultado de tomar contenidos que han visto en otros lugares y llevar a cabo un proceso de redifusión con un grado de transformación mayor o menor: los llamados clipblogs, o blogs de recortes sin más añadido de valor que la mera recopilación, tienen generalmente muy poca popularidad, porque los lectores prefieren dirigirse a las fuentes en las que estos beben. Cuando la tarea de recopilación es manual, llevada a cabo por un autor en función de su criterio, el aporte de valor existe: el lector está liando su selección al criterio del recolector, y, por tanto, obtiene de ello un ahorro de trabajo. Cuando la selección se limita a escoger unas pocas fuentes que posteriormente son recolectadas de manera automática, el valor aportado es prácticamente nulo.

Algunos blogs, fundamentalmente a base de trabajo y demostración repetitiva de aporte de valor a sus usuarios, consiguen convertirse en referentes para algunos segmentos de usuarios. Habitualmente, los usuarios acaban «conociendo» al autor o a los autores del blog, asumen sus sesgos, filias y fobias, y escogen entre identificarse con ellos o descontarlos adecuadamente. Junto con el autor o los autores del blog, los usuarios del mismo conforman su comunidad, y contribuyen a la generación de valor gracias a sus comentarios y reacciones al contenido. No todos los blogs tienen comentarios, pero los comentarios son una parte importante de un blog: sin ellos, un blog puede seguir siendo un medio participativo porque otros bloggers pueden sostener la conversación mediante referencias en sus respectivas páginas, pero el volumen de aportaciones disminuye notablemente como resultado de la discriminación («si tienes página propia, puedes participar porque, de hecho, no podría impedírtelo, pero si no la tienes, limítate a leer»).

Los bloggers suelen, además, llevar un sistemático control de sus estadísticas de acceso. Tener un blog y no tener cuenta en servicios como StatCounter o Google Analytics es como navegar a ciegas. Se trata de servicios que se instalan sin complejidad de ningún tipo copiando y pegando un código sobre la página del blog: en realidad, estos servicios, conocidos como «de seguimiento de huella», generan una llamada a un servidor cada vez que la página es descargada por un usuario, servidor en el que se lleva a cabo un control estandarizado. Generalmente, el blogger no se limita a comprobar el número de visitas, sino que interactúa con el contenido y la comunidad mediante una observación cuidadosa de, por ejemplo, las páginas de las que proceden sus visitas, los contenidos que reciben una mayor popularidad, o el nivel de seguimiento de los enlaces que aporta en las entradas. Los enlaces, no lo olvidemos, son la verdadera esencia de un blog: un blogger se diferencia de un medio convencional porque aporta a su lector todo lo necesario para que este pueda volver a las fuentes que se utilizaron, las que puedan servir para ampliar la información facilitada, o las que son consultadas de manera habitual. Contrariamente a lo que ocurre en los periódicos online tradicionales, que se negaban a incluir enlaces por miedo a que los lectores «se escapasen», un blog es un ejercicio constante de envío de lectores a las fuentes utilizadas: «si te ha interesado lo que has leído, vete aquí, vete allí, léete esto o aquello, etc.». Y contradiciendo el razonamiento de los editores de diarios clásicos, esta actividad de referencia se convierte en uno de los principales valores de la página: el lector, cuando los enlaces suministrados están bien escogidos y aportan valor, pasa a considerar la página como una referencia válida, y vuelve a ella con asiduidad, se fideliza cada vez más en su uso.

Los comentarios son otro fenómeno interesantísimo y en permanente evolución: para la mayoría de los blogs, obtener comentarios es una tarea ardua, porque precisan de una implicación del usuario mucho mayor que la simple lectura. En un blog que reciba, por ejemplo, muchísimas visitas procedentes de motores de búsqueda, será relativamente normal encontrar un número de comentarios bajo: los visitantes que proceden de motores de búsqueda, conocidos habitualmente como «paracaidistas», carecen por lo general de la implicación necesaria para escribir comentarios de calidad. Sin embargo, no debemos considerar a esos visitantes como de menor importancia o valor: si el blog tiene publicidad, esos mismos «paracaidistas» pueden ser una interesante fuente de ingresos, dado que acuden al blog en el llamado «modo búsqueda», y son, por tanto, más susceptibles de hacer clic en la publicidad del blog que aquellos que acuden rutinariamente en busca de nuevos contenidos. Esto genera una interesante dualidad entre bloggers comerciales y los que no lo son, y nos permite hacernos una idea del valor de muchas variables en esta especie de «economía de la atención» que se genera en torno a esta sociedad conversacional.

En el fenómeno blog, autentico detonante en su momento de la génesis de la sociedad conversacional, cabe absolutamente cualquier cosa. Un amplio porcentaje de blogs creados en servicios gratuitos como Blogger o Squidoo corresponde a los denominados sblogs o spam blogs, páginas de enlaces destinadas a la venta ficticia de productos fraudulentos o al envío de enlaces a otras páginas para mejorar su visibilidad en los buscadores. Pero también hay blogs personales pertenecientes a individuos que extraen múltiples posibles beneficios de los mismos, desde mantener un catálogo de enlaces interesantes comentados hasta dar rienda suelta a su imaginación, pasando por constituirse de un modo u otro como referente en un tema o conjunto de temas determinado, o generar ingresos gracias a la publicidad. Hay blogs personales que acaban teniendo, cuando su escala lo permite, un componente importante de blog comercial, de ingresos razonables derivados de la publicidad, pero también hay blogs comerciales diseñados como tales desde el primer momento, como es el caso de redes conocidas como Weblogs, Inc. o Gawker Media en los Estados Unidos, o Weblogs S.L. en España. En octubre de 2005, AOL adquirió Weblogs, Inc. por 25 millones de dólares, y levantó la alarma acerca del impacto de los blogs en el panorama publicitario: debido a la mayor implicación de sus usuarios y a una concepción de la publicidad mucho más adaptada al medio y a las reglas tácitas del juego, muchos blogs estaban obteniendo resultados notables en cuanto a eficiencia publicitaria, resultados que iban en muchas variables bastante más allá que lo que se obtenía habitualmente en los medios online convencionales como diarios o portales. Los blogs, por norma general, se negaban a mezclar publicidad y contenidos, rechazaban los formatos intrusivos tan típicos del momento como insoportables y odiados por los internautas, y heredaban en muchas ocasiones ratios de clickthrough proporcionales a los de los enlaces de otros tipos: en cierto sentido, un blog que proporcionaba a sus visitantes enlaces de calidad, trasladaba ese efecto incluso a su publicidad.

Paradójicamente, el resultado de este descubrimiento fue la ruptura casi inmediata de la profecía que mostraba a los blogs como adalides de la pureza: la operación AOL-Weblogs, Inc. llevó a un desarrollo rápido de otras redes de blogs comerciales que buscaban crecer y ser adquiridas, redes que en muchos casos no eran tan escrupulosas con las prácticas que habían llevado a sus predecesoras a jugar un importante papel en sus respectivos mercados. Comenzaron a aparecer redes que descarada y descarnadamente se aprovechaban del trabajo de sus bloggers, a los que no pagaban nada o prácticamente nada, a la vez que aceptaban todo tipo de prácticas de las agencias y anunciantes: vender el contenido de sus entradas al mejor postor, elogiar productos a cambio de pagos, o permitir formatos molestos. La necesidad y la codicia llevaba a la mayoría de estas redes secundarias a aceptar la llamada «publicidad de relleno», anuncios de bajísima calidad de actividades como casinos online, pornografía, contactos, punteros de ratón, etc., que, por supuesto, nada tenían que ver con la temática de la página, y que solo aportaban molestias a sus visitantes. El problema, además, era autorreferente: cuando una agencia lograba convencer a una red de blogs de entrar en una práctica como la venta de entradas sin advertencia alguna, acudía a la siguiente solicitando lo mismo y lo comentaba a sus anunciantes, generando un mal ambiente al que resultaba muy complicado retrotraerse. A lo largo del tiempo, las redes que lograron mantener estándares coherentes han sido capaces de alcanzar lugares destacados en los panoramas mediáticos de sus respectivos mercados, mientras que las otras han ido desapareciendo o perdiendo relevancia de forma progresiva.

En medio del fenómeno blog, empezó a irrumpir una nueva especie: los blogs corporativos. Las primeras empresas en hacerlo fueron empresas tecnológicas, para las que el medio resultaba de alguna manera algo natural. En otros casos, se trata de bloggers que trabajan en empresas tradicionales, pero que empiezan a hablar de ellas en sus blogs personales y permiten así a algunas de ellas convertir en tangibles las posibilidades del medio. De un modo u otro, algunas empresas van dándose cuenta de que un blog corporativo puede suponer un buen canal de comunicación, y empiezan a explotarlo como tal. El camino de los blogs corporativos, no obstante, no ha estado exento de dificultades: durante décadas, la comunicación empresarial se ha basado en la construcción de una muralla de información alrededor de la compañía, muralla solo agujereada a través de una pequeña ventana rígidamente controlada por el Departamento de Comunicación Corporativa. Tal era el poder de dichos departamentos, que en muchos casos, ni siquiera los más altos directivos de la compañía podían hacer declaraciones a los medios sin pasar por su filtro. Para los departamentos de Comunicación Corporativa, los blogs representaban la peor de las pesadillas: un medio fuera de su control, anárquico y sujeto a las veleidades personales de sus autores, En muchos casos, la creación de blogs corporativos ha respondido a un criterio de «moda tecnológica», pero se ha dejado guiar por los criterios convencionales de la comunicación empresarial, lo que ha dado como resultado blogs insulsos, a modo de folletos en formato blog, asépticos, aburridos, exentos de rasgos personales y cargados de medias verdades. En diciembre de 2008, un informe de Forrester Research ubicaba a los blogs corporativos como el medio que generaba un nivel más bajo de confianza y credibilidad: sobre la base de contaminar la esencia del blogging con la forma de hacer las cosas de la aséptica y despersonalizada comunicación convencional, habían conseguido matarlos. Y es que la credibilidad o la confianza no provienen del uso de una herramienta determinada, sino más bien de lo que se dice en ella.

Entre tanto, algunos blogs corporativos daban ejemplo de cómo hacer las cosas, y conseguían convertirse en canales de comunicación rentables, que humanizaban la imagen de la empresa alrededor de una persona o grupo de personas. Ejemplos como el de Bob Lutz en General Motors, Jonathan Schwartz en Sun Microsystems, o la etapa de Robert Scoble en Microsoft dejaban claro que los blogs corporativos debían, para ser eficientes, responder a lo que los usuarios esperaban de un blog: una comunicación directa, personal y con las restricciones claramente establecidas: ¿que no podías hablar de ganancias futuras porque siendo el CEO de la compañía eso era considerado por la Federal Trade Commission como un forward-looking statement?. Dínoslo claramente, y lo entenderemos. Engáñanos, y no te leeremos más. Con los blogs corporativos empezaba a hacerse realidad aquella doble portada de Wired de abril de 2007 con Jenna Fischer, la protagonista de The Office: «Desnúdate, y domina el mundo». Los blogs, tímidamente, anunciaban la llegada de todo un nuevo esquema comunicativo, mucho más adecuado al contexto de la nueva sociedad hiperconectada.

Las decisiones sobre los comentarios, por ejemplo, eran claramente uno de los temas que traía más de cabeza a los responsables de comunicación. Y no era, de hecho, algo privativo de los blogs de empresa: una cosa era permitir la participación, y otra muy diferente que esta ocurriese sin estar sometida a reglas de ningún tipo. En los blogs de empresa, el tema revestía características de crisis profunda: en el modelo de comunicación habitual, la crítica estaba, sencillamente, prohibida. Las empresas habían, durante décadas y décadas, acostumbrado al mundo a una comunicación «en modo Disneylandia»: todo era maravilloso, el producto no fallaba jamás, nunca había una sola queja ni descontento, y los problemas pertenecían al lejano Reino de Nunca Jamás. Pero la blogosfera, por supuesto, no estaba exenta de la máxima fundamental del periodismo: que un perro muerda a hombre no es noticia, que un hombre muerda a un perro sí que lo es. Lo negativo siempre destacaba más que lo positivo. Una persona podía usar durante años un producto y estar plenamente satisfecha con él, pero las probabilidades de que en algún momento se dedicase a glosar las excelencias del mismo en un blog eran escasas. Eso sí: en cuanto el producto fallaba, la reacción inmediata era saltar a la web a expresar las quejas, de manera, en muchos casos, profundamente desagradable. Si la empresa tenía blog, no se daba a la cuestión ni medio segundo de reflexión: el blog se convertía en el escenario perfecto para hacer una escena de cliente profundamente defraudado.

Para las empresas, aceptar que las cosas, en ocasiones, no salían del todo bien fue algo complejo, tanto, que muchas compañías no han llegado a aceptarlo aún. Los comentarios negativos eran sistemáticamente «moderados», un eufemismo que quería decir que pasaban a dormir el sueño de los justos, aunque estuviesen expuestos con la mayor corrección. Además, aparecen comportamientos inmaduros: empresas que dedican personas a introducir comentarios en páginas de la competencia simulando ser clientes insatisfechos, conformando un escenario de «patio de colegio» que tiene una duración e impacto limitados y que ha significado no pocos disgustos. Obviamente, el calado de una actitud como esta es escaso, como lo es su verdadera importancia: la gestión debe limitarse a intentar identificar y ridiculizar aquellos comentarios fraudulentos (por dirección IP en muchos casos, ya que la empresa que manifiesta estas actitudes no suele tener ni siquiera las habilidades técnicas para enmascarar la IP o para hacerlo desde un ordenador diferente), contestar adecuadamente con la paciencia y la templanza adecuadas, e intentar preservar el espíritu de la conversación. Tan malo como tener comentarios fraudulentos o malintencionados es no tenerlos, o mantener una versión «dulcificada» de la página, en la que se eliminan todos los comentarios negativos, legítimos o no: un comentario negativo sobre un aspecto de nuestro negocio, incluso aunque sea malintencionado y sospechemos que proviene de una fuente de neutralidad comprometida, puede ser una oportunidad para contestar resaltando con corrección lo absurdo del planteamiento, o incluso para identificar y corregir problemas, mostrando así una actitud abierta.

Los comentarios no son el único caso en el que la sociedad hiperconectada supone un problema para las empresas. A partir del momento en que todos los participantes en la sociedad tienen acceso a una herramienta de difusión potencialmente masiva, la ecuación de poder cambia, y aparecen los fenómenos extraños, los Poltergeist comunicativos. A mediados de 2005, un profesor, periodista y blogger llamado Jeff Jarvis adquiere un portátil Dell, y decide contratar la garantía de más amplia cobertura, que incluye la reparación a domicilio en un plazo de pocas horas. Sin embargo, su recién estrenado portátil se avería a las pocas horas de uso, y tras llamar al técnico, observa no sin desesperación que el arreglo no solo no puede ser realizado en su domicilio, sino que tarda diez días en ser llevado a cabo. Tras recibir de nuevo el portátil, una nueva avería le lleva a quedarse sin ordenador de nuevo durante varios días. A la tercera vez, la indignación de Jeff ha crecido ya lo suficiente como para empezar a escribir sobre ello en un tono verdaderamente agrio, que va escalando ante la evidencia de que la máquina que ha adquirido es lo que los norteamericanos llaman «un limón», el resultado evidente de algún problema en la cadena de montaje que hace obvia la necesidad de un cambio integral. Al solicitarlo, la respuesta de la marca es clara: están dispuestos a cambiar todas y cada una de las piezas del ordenador según vayan fallando, pero no lo reemplazarán por una máquina nueva porque «no es su política». Las quejas de Jarvis, jalonadas por cifras de comentarios cada vez más elevadas en sus entradas, acaban deviniendo en el llamado Dell Hell, el «infierno Dell», una fuerte oleada de publicidad espantosa y menciones negativas en blogs de todo el mundo, que termina, como no podía ser de otra manera, llegando a la prensa y los medios convencionales. En septiembre de ese mismo año, un reportaje en profundidad en el Business Week pone de manifiesto las dificultades de Dell, mientras la marca inicia una caída en su cotización que la lleva a alcanzar sus mínimos históricos. Además, pierde su liderato en la fabricación de ordenadores a manos de HP, liderazgo que no ha vuelto a recuperar desde entonces.

¿Qué había ocurrido? El protagonista de la historia, Jeff Jarvis, no era ningún personaje VIP, ni mucho menos un enviado de la competencia que intentaba dañar la imagen de marca de la compañía. Era simplemente un cliente. Un cliente más, de esos que antiguamente comunicaban su satisfacción con nuestro producto a tres personas, pero sus quejas, en caso de haberlas, a unas diez. Pero Jeff Jarvis no era un cliente normal: era un blogger, un «cliente con megáfono». Y con un megáfono muy grande: en la clasificación de aquel momento en BlogPulse, una herramienta de Nielsen para medir la popularidad de los medios sociales, el blog de Jeff, llamado BuzzMachine, ocupaba el puesto 78 en popularidad (entre los más de setenta millones de blogs registrados entonces), escribía unas veinticuatro veces a la semana, y recibía en torno a doscientos cincuenta enlaces entrantes cada mes. Claramente, «no un blog más» o «un blog cualquiera»: Buzzmachine era, en aquel momento, una piedra angular de la comunidad conversacional de la blogosfera. Las entradas del blog de Jeff referentes a Dell alcanzaban medias de quinientos comentarios de personas que no estaban en contra de la marca, ni la odiaban...; eran, simplemente, clientes que habían tenido una mala experiencia con los productos fabricados por la compañía, personas a las que el blog de Jeff pasaba a servirles como foro para vocear sus críticas.

La marca, mientras tanto, decide hacer lo que prácticamente todos los departamentos de comunicación corporativa tenían como práctica habitual en aquella época: look, don't touch, «mirar y no tocar». O, como se solía expresar en un malintencionado refrán, «nunca te pelees con un cerdo en el barro, porque los dos acabaréis de barro hasta las orejas, pero él, además, disfrutará». La actitud de «la marca no puede bajar a pelearse con un simple cliente» era parte del libro de oro de la comunicación corporativa, de la forma en que las compañías se expresaban, de lo que se enseñaba en las facultades de publicidad y relaciones públicas. Pero en el momento en que la sociedad cambia y se convierte en hiperconectada, esto es un enorme error. Si mira a su alrededor, es posible que todavía encuentre compañías como la Dell de 2005: empresas que se sienten con derecho a martirizarle con su propaganda incluso de productos que usted ya les adquirió anteriormente, pero que se niegan a contestar a sus llamadas o correos electrónicos. Pero sobre todo, si encuentra en su compañía la más mínima referencia a esa actitud definida por el look, don't touch, por el «mirar y no tocar», reaccione inmediatamente: es una receta para el desastre.

Al caso Dell Hell siguieron toda una infinidad de circunstancias similares en todos los países. En España, tuvieron una cierta relevancia los que afectaron a empresas como IKEA, Axxe o Air Europa, afectadas por críticas de clientes o ex-empleados que pasaban a un escalón de relevancia superior por la condición de blogger del afectado. En algunos casos, la cuestión quedaba en mera anécdota (lo que no la exime de haber sido objeto de horas y horas de reuniones preocupadas en el Departamento de Comunicación y de oleadas de publicidad negativa), mientras en otros, llegaban hasta los tribunales. Pero la evidencia era clara: en la sociedad hiperconectada, la comunicación empresarial había cambiado, y a las empresas les estaba costando hacerse a ello.

Uno de los motivos más evidentes para el cambio era la actuación coordinada, en muchos casos de manera espontánea, en otras ocasiones orquestada o provocada. El llamado «efecto Streisand» recibe su nombre de un caso sucedido en 2003, en el que la conocida artista Barbra Streisand demandó a una página web, Pictopia.com, y a un fotógrafo, Kenneth Adelman, por un total de 50 millones de dólares, solicitando la retirada de una fotografía de su mansión al borde del mar, incluida en una colección de 12.000 imágenes aéreas de la costa de California hechas para documentar procesos de erosión. Tras la denuncia, la fotografía recibió casi medio millón de visitas en el sitio, además de pasar a aparecer en infinidad de sitios más, lo que venía a demostrar que, en muchos casos, es mejor no llamar la atención sobre un tema (de hecho, muy pocas personas habrían llegado a ver la fotografía en medio de una colección de 12.000, y muchísimas menos habrían llegado a averiguar que se trataba de la mansión de la cantante) que recurrir a los mecanismos que supuestamente funcionaban en este sentido en una sociedad no hiperconectada. Para muchas empresas, acostumbradas a utilizar mecanismos legales, como las denuncias por difamación o por uso de propiedad intelectual, el efecto Streisand supone un verdadero reto inesperado: el hecho de que una publicación aparezca pasa a carecer absolutamente de control, y el que lo publicado sea correcto o incorrecto, verdadero o falso, o constituya o no una difamación se convierte en algo irrelevante. Lo único que importa es la credibilidad de la fuente y la capacidad de la marca para entrar en una conversación, si lo estima oportuno, con la fuente en cuestión, algo que prácticamente ninguna sabe cómo hacer.

Las empresas asisten, con cara de idiotas, al desarrollo de un nuevo estilo comunicativo dotado de una eficiencia explosiva, brutal. En julio de 2009, un grupo musical, Sons of Maxwell, decide volar con United Airlines a Nebraska para un concierto. Mientras están en el avión, ven horrorizados cómo los empleados de la aerolínea lanzan su equipaje por los aires, y cómo alguna de sus queridas guitarras llega a caer al suelo. Al llegar al destino, comprueban que la guitarra de uno de los miembros del grupo, la adorada Taylor de Dave Carroll, está completamente destrozada. Ante la indiferencia de los empleados de la aerolínea, el grupo abandona el aeropuerto con la guitarra rota, sin saber que con ello pierden el derecho a reclamaciones posteriores. Al episodio sigue una cadena de reclamaciones a United, que va pasando el asunto de persona a persona hasta, finalmente, denegar el pago de la reparación de 3.500 dólares de la guitarra. Vista la actitud negativa de la empresa, los miembros del grupo deciden grabar una serie de tres canciones sobre el episodio y subirlas a YouTube: la primera de ellas, titulada, United breakguitars, que recomienda no volar con la aerolínea, con un estilo country pegadizo y divertido, alcanza más de un cuarto de millón de visualizaciones en pocos días. Como resultado, la popularidad del grupo se incrementa y la cotización de las acciones de United en los mercados financieros cae un 10%, mientras su nombre queda, en la mente de muchos usuarios, vinculado al pegadizo estribillo como una compañía con la que no es recomendable volar.

A las empresas, decididamente, no les gustan las críticas, y estaban acostumbradas a hacer lo que fuera necesario para ocultarlas. En octubre de 2008, un pequeño blog llamado «Photoshop Disasters» alojado en Blogger, el servicio de blogs de Google, publicó una foto de una modelo de la firma Ralph Lauren con una cintura absurdamente delgada y desproporcionada, imposiblemente estrecha. Boing Boing, uno de los blogs más conocidos y leídos del mundo, publicó la fotografía, lo que provocó la alarma de la marca, que inmediatamente solicitó la retirada de la misma a los proveedores de acceso de ambos blogs. Mientras Google procedía a la retirada inmediata a la espera de explicaciones (la publicación de material sujeto a derechos de autor con el propósito de crítica está dentro de lo que se considera un uso justificado, o fair use, en la legislación norteamericana), el proveedor de Boing Boing simplemente informó al blog de la circunstancia pero no retiró el material, lo que llevó a que este publicase la historia del intento de censura y la foto alcanzase todavía más repercusión. Tras el escándalo, la marca se excusó por el exagerado retoque de la foto, pero no por haber intentado censurar a ambos blogs. En el mundo que Ralph Lauren conocía, ningún blog o persona discutía una orden de retirada: asustados por el poderío legal de las compañías, los usuarios solían limitarse a obedecer órdenes para evitar una posible demanda. En la sociedad hiperconectada, las cosas ya no funcionan así: la información viaja a enorme velocidad, y en cuestión de horas, el daño ya está hecho. Tras muchísimos años aprendiendo a blindar su comunicación, las empresas se encuentran con que no saben hablar, no saben comunicarse con las personas: en una sociedad hiperconectada y conversacional, no saber hablar y pretender utilizar la fuerza —en este caso, la fuerza de la ley— supone un problema muy difícil de solventar. En los blogs personales, los comentarios han seguido en muchos casos una evolución parecida, lo que nos lleva a plantearnos algunos aspectos llamativos de la interacción habitual en la sociedad hiperconectada. ¿Qué lleva a las discusiones en la red a radicalizarse con tanta facilidad, o a la existencia de personajes, denominados habitualmente trolls, que parecen disfrutar haciendo comentarios completamente fuera de tono, maleducados, hirientes o directamente estúpidos? La evolución en los blogs personales, y anteriormente en los foros, no deja lugar a dudas: mientras una página tiene un nivel de visitas bajo, la comunidad que se forma alrededor de ella suele ser muy escasa. En un blog de creación reciente, por ejemplo, costará mucho conseguir comentarios a las entradas, la gran mayoría carecerán de ellos, en parte debido a la presencia de pocos lectores, pero también debido al ratio habitual de participación en Internet, una regla 1:9:90 que suele repetirse cual número cabalístico en la mayor parte de los sitios conocidos: por cada persona que produce contenido, suele haber otras 9 que comentan sobre ese contenido, lo valoran, lo votan o llevan a cabo alguna acción de menor implicación; y 90 más que simplemente observan, sin crear nada, y que suelen conocerse como lurkers.

La primera fase de un blog, por tanto, suele caracterizarse por una participación muy escasa hasta que se supera un cierto volumen de visitas. Tras ese paso, la comunidad empieza a construirse, aparecen usuarios fieles que comentan de manera habitual y expresan sus opiniones en la página, como quien encuentra un bar agradable y decide pasarse por él a diario para leer el periódico y hacer un poco de tertulia. A medida que la parroquia crece y la página adquiere más presencia en buscadores, empiezan a aparecer participantes menos habituales, y también los trolls, que parecen no aspirar más que a causar incomodidad y daño, y que en la mayor parte de los casos acaban, a medida que la página crece, obligando al desarrollo de algún tipo de medidas de control. Lo típico en páginas personales es, en este sentido, pasar de no moderación a posmoderación (eliminar los comentarios molestos o maleducados tras su aparición) y, finalmente, a premoderación (aprobación de los comentarios antes de que estos aparezcan en el sitio), aunque por supuesto existen en este sentido todo tipo de casuísticas. La violencia dialéctica en Internet obedece, en realidad, a una causa puramente fisiológica, encuadrada dentro de lo que la psicopatología clínica denomina online disinhibition effect, la desinhibición aparente que surge en la participación en la red, y que lleva a que personas aparentemente normales se conviertan en auténticos imbéciles socialmente inaceptables en la red. El fenómeno de la escalada dialéctica, o flaming, que supone un problema importante de cara al desarrollo creciente de relaciones a través de medios electrónicos, está de hecho siendo estudiado desde un punto de vista de neurofisiología: la mecánica neural detrás de un flaming responde, en realidad, a un fallo de diseño en la interfaz entre los circuitos cerebrales que gobiernan el comportamiento social y las características del mundo online. Así, en la interacción cara a cara, el cerebro hace uso de un flujo continuo de signos emocionales y pautas sociales, que vincula con el desarrollo del siguiente movimiento en la interacción para asegurar que esta se desarrolle adecuadamente. Este proceso de autocontrol social ocurre en el córtex orbitofrontal, que actúa como un centro para la empatía, que define, entre otras cosas, las habilidades de una persona de cara al trato social. Este córtex utiliza el escaneo de las señales emocionales de la interacción cara a cara para asegurar que la interacción continúe adecuadamente y la persona que tenemos delante no se sienta incomodada. Las lesiones en este córtex, de hecho, conllevan comportamientos socialmente inaceptables, salidas de tono y actos de aparente mala educación. El córtex se alimenta, por tanto, de un flujo de señales tales como cambios en el tono de voz, expresiones faciales, etc., para poder así moderar y canalizar adecuadamente nuestros impulsos. Un flujo de señales que en la comunicación electrónica simplemente no existe, de no ser por el uso de esos aparentemente inocentes signos denominados emoticonos, que algunos utilizamos con notable profusión. En un entorno online, la ausencia de esas señales nos lleva a no moderar esos impulsos, a escribir un mensaje que sería, en un entorno electrónico, completamente inaceptable, y a apretar el botón de «enviar» antes de que un segundo pensamiento más moderado nos lleve a utilizar el botón «descartar». Consecuentemente, pasamos a convertirnos en un troll. Otro de los grandes fenómenos provocados por la aparición de una sociedad hiperconectada es el desarrollo de la llamada «viralidad» o transmisión rápida de mensajes a través de las redes. El fenómeno de la viralidad aparece al principio como manifestación de los llamados «memes», ideas sencillas que pueden ser contadas o descritas con suma facilidad, y que se transmiten con enorme velocidad entre las personas.

La memética intenta estudiar las características de este tipo de mensajes y los factores que dan lugar a su transmisión, mientras que el marketing viral intenta apalancarse en ese tipo de fenómenos para provocar una comunicación más eficiente y dotada de una mayor credibilidad: ante una publicidad que nadie se cree y que, de hecho, va ni siquiera intenta incidir en la comunicación de factores objetivos, sino en el mero impacto recordatorio, los creativos intentan pasar del canal marca-persona a un canal persona-persona, para obtener una transmisión directa dotada de mayor credibilidad. Para ello, se recurre generalmente a soportes que requieren poco procesamiento por parte del receptor, como el vídeo, pero que presentan mensajes sorprendentes, graciosos, divertidos, realizados con apariencia artesanal o que, por la razón que sea, el receptor «quiere creer»: el resultado es un receptor que se afana en redistribuir el mensaje entre sus amigos y conocidos, de una manera que, además, le lleva a reforzarlo con su propia persona, con un mensaje a sus amigos del tipo «míralo, te va a gustar». El mensaje deja de ser simple publicidad, un mensaje que el receptor descuenta de manera automática, y pasa a ser algo que hereda de alguna manera la relación y la credibilidad de la persona que lo envía (que, como todo, también puede agotarse: todos tenemos el típico amigo que, a fuerza de reenviar cosas, se convierte en un pesado). Las marcas y los creativos publicitarios, además, empiezan en seguida a matar a la gallina de los huevos de oro: los clientes hacen cola en la puerta de las agencias al grito de «hazme un viral», y los mensajes van abandonando su inicial carácter espontáneo para convertirse en sofisticados engaños que pasan a resultar insultantes para el espectador, o que directamente socavan su confianza futura en la marca.

CAPÍTULO 16. REVISANDO LOS PAPELES: PARTICIPACIÓN Y COMUNIDADES

«Lo que no puede ser medido, no puede ser mejorado.»

JOSEPH M. JURAN, «padre de la calidad»

A estas alturas de libro, muchos de los temas que hemos ido tratando deberían estar cristalizando en forma de lista de temas de aplicación a su propio entorno personal o profesional. ¿Debe, por ejemplo, empezar un blog? La respuesta es clara y concisa: si no lo ha hecho hasta ahora, sí. Diríjase a cualquiera de las aplicaciones habituales para ello (Blogger, WordPress, etc.) y empiece uno inmediatamente. No sea tímido: puede empezar un blog personal sobre sí mismo, sus temas de interés y sus aficiones, o puede ponerle un seudónimo y que no le reconozca nadie. No se sienta presionado: si lo abandona a los tres días, nadie va a decirle nada, ni a reprocharle el espacio que (no) ocupa en el mundo virtual, precisamente porque el concepto de espacio, en el mundo virtual, es completamente superfluo. Abrir un blog, aunque sea simplemente para almacenar enlaces a noticias que le han resultado interesantes y comentarlas mínimamente, le permitirá familiarizarse con el medio. Podrá desarrollar experiencia en el manejo de las herramientas, en el uso de enlaces, en las estadísticas... las mecánicas web no se entienden realmente hasta que se han visto sobre una página propia. Generalmente, empezar un blog le demostrará lo que ocurre con la gran mayoría de los blogs y con la mayor parte del contenido en internet: solo lo lee su autor, y en algunos casos sus amigos y familiares más cercanos. Sin embargo, a fuerza de vincular a otros contenidos que estime relevantes y de intentar aportar valor a sus, al principio, hipotéticos lectores, algunos blogs encuentran su sitio, y pueden convertirse en ocasiones en fenómenos muy interesantes por volumen o por comunión de intereses alrededor de un tema determinado.

El blog tiene el potencial de convertirse en una herramienta con las características de un megáfono. En ocasiones, ese megáfono sirve para hacerse oír en el contexto de una comunidad restringida de especialistas o aficionados en un tema determinado, mientras que otras veces puede llegar a foros más generales. Las mecánicas que llevan a un extremo u otro, o a cualquiera de las situaciones intermedias, son completamente imprevisibles: afecta a la calidad y el tono de la escritura, la temática, la personalidad del autor o los autores, el aporte de material original, la posición en buscadores o las citas desde otras páginas conocidas. En general, el género se ve favorecido por actitudes alejadas del maximalismo: no se puede escribir pretendiendo ser el que más sabe de un tema, por mucho que se sepa de él. Hay que aceptar el hecho de que siempre hay alguien ahí fuera que sabe de un tema más que nosotros, y que posiblemente nos encontrará y pasará a discutirnos, además de unos cuantos más que, sin saber del tema más que nosotros, están completamente convencidos de que es así. También es importante la generosidad: el tráfico es la moneda universal de la atención en la web, y hay que manejarla bien: dar crédito a los lugares donde hemos visto las cosas, que las han visto antes que nosotros, que las han tratado de una manera que nos ha proporcionado ideas, aunque provengan de la mismísima competencia, y no tener miedo a enviar visitantes a otros sitios: si les gustan esos lugares, volverán a por más. De hecho, una de las mejores métricas que puede manejar no es el número de visitantes, sino la frecuencia con la que estos hacen clic en los enlaces que usted les proporciona.

En el ámbito personal, un blog puede ser casi cualquier cosa, desde un mero pasatiempo o un ensayo banal hasta un escaparate para un tema determinado o una manera de ganarse la vida. Cuando introducimos en la ecuación el ámbito profesional, encontramos todo tipo de aplicaciones: muchas empresas, de hecho, han llegado a los blogs tras observar historias de éxito interesantes en personas de la propia organización, historias que no en todos los casos han terminado bien. Hay organizaciones más preparadas que otras para aceptar la importancia de las personas, y personas que manejan dicha dualidad de papeles mejor y peor. Pero lo importante es entender que para la empresa, los blogs y la web social, en general, pueden suponer una oportunidad tan importante que en ocasiones deja de ser una opción: no tomarla implica directamente una destrucción de valor.

Para valorar lo que la web social significa potencialmente para la empresa haga un pequeño y sencillo experimento: elabore una lista con los términos que pueden ser importantes para sus clientes en relación con su empresa. Tome, por supuesto, el nombre de su empresa, pero también sus productos principales, los nombres genéricos, las marcas de su competencia, y todos aquellos términos que cree que un cliente puede tener en la cabeza cuando piensa en sus productos. Tome esa lista de términos y búsquela en la red. Pruebe con motores de búsqueda generalista como Google o Bing, pero también con otros especialistas en medios sociales: Google Blogsearch, Technorati, BlogPulse u otros similares, y opte por la ordenación cronológica. ¿Qué encuentra? ¿Existe una conversación alrededor de esos temas? ¿Es una conversación fluida, con múltiples participantes, con numerosas entradas? ¿O por el contrario, no encuentra más que un desierto? Si encuentra una conversación, no se preocupe si los términos de la misma son desagradables, negativos o incluso rayanos en la difamación: como ya hemos comentado anteriormente, es perfectamente normal que en la red predominen las opiniones negativas sobre las positivas. Un usuario puede ser fiel a una marca con plena satisfacción durante años y no mencionarla para nada en su página, dando a esa trayectoria la consideración de «lo normal». De repente, un fallo, un problema, algo que considere una mala atención por parte de la empresa, etc., puede significar una entrada en su blog en unos términos en los que claramente olvida toda la trayectoria positiva anterior. No preste atención a eso todavía. Por ahora, concéntrese en el volumen.

Si efectivamente existe una conversación sobre el tipo de productos o servicios que su compañía comercializa, intente hacerse una composición de lugar: ¿están su marca o su empresa en esa conversación, o esta se concentra alrededor de otras marcas o empresas? Si existe una conversación y su empresa no está en ella, preocúpese: otros competidores están posicionándose mejor que usted para los términos que muchos clientes podrían buscar cuando se interesan por un producto o servicio como el suyo. Si, por el contrario, su empresa está en la conversación, o incluso la acapara, la situación es mejor. Aunque pueda no parecerlo si en el tono de la conversación predomina la negatividad: como decía Oscar Wilde, lo único peor que el que la gente hable de ti, es que la gente no hable de ti. Toda situación negativa puede ser revertida con mayor o menor trabajo y dedicación. Finalmente, cabe la posibilidad de que no exista una conversación sobre sus productos o servicios, que simplemente sean temas sobre los que no se habla en la red. No se sienta mal por ello: ni la red es representativa del total de la población, ni todos los temas se hablan o tienen cabida de forma natural en ella. Simplemente, piense en la posibilidad de ser usted quien desarrolle dicha conversación o si, por el contrario, le parece imposible hacerlo. En ese caso, no se dé cabezazos contra la pared. Pero no deje de vigilar los términos: constrúyase una lista de búsquedas con ellos y suscríbase a esas búsquedas, para poder reaccionar rápidamente si detecta actividad en ese sentido. En un mundo hiperconectado, la franquicia sobre los términos de búsqueda resulta fundamental: el que no aparece, no sale en la foto, no recibe visitas, y aunque su producto o servicio no se adquiera a través de Internet, puede dejar de estar en los conjuntos de consideración de los potenciales clientes.

Si ve posibilidades de crear un blog para su empresa, hágalo. Pero no olvide que si bien crear un blog es una tarea sencillísima, al alcance de cualquiera, mantenerlo no lo es tanto. El blog requiere personas que estén a gusto con lo que hacen, convencidos, apasionados: el primer día que se acerque al blog con mirada de resignación, con expresión de «a ver que narices cuento yo hoy», habrá perdido la batalla. En un blog hay que comentar novedades, hablar del sector, de tendencias, de experiencias, de artículos que se han leído en otros sitios... Si va a hacer un folleto para cantar sus alabanzas, no haga un blog, porque nadie lo leerá. Pero si lo hace, tenga en cuenta que, muy posiblemente, se posicionará en los buscadores por encima de la propia página que su empresa lleve años manteniendo. El misterio es fácil de entender: los blogs son páginas tematizadas en las que se repiten habitualmente las palabras clave a posicionar, que cuentan con bastantes vínculos, que se actualizan con bastante periodicidad, y que tienen una estructura limpia (al estar hechas con el esquema de la herramienta de blogging en lugar de con el criterio del autor a la hora de programar). Todos esos criterios son altamente valorados por los robots indexadores de los motores de búsqueda, con todo lo que ello conlleva. Pero, además, los blogs generan enlaces: a poco que comience a ser conocido, aquellos que hablen de su marca enlazarán a su blog, inyectándole relevancia. Si es el primero de su industria en dar ese paso, pronto empezará a ver los efectos.

Pasemos al hecho de la reputación en la red. Como comentábamos, en la red, en muchas ocasiones, predomina la negatividad. Partiendo de esa base, pensemos qué ocurre cuando la percepción de las marcas se convierte en algo más próximo, más cercano, más humanizado: utilice su blog como forma de atraer los diálogos sobre su marca. Al menos, de esa manera, la conversación se producirá en su propia casa, y no tanto en lugares sobre los que no tiene ningún control. Por supuesto, eso incluye respetar las normas: si en las normas de su casa pretende imponer un «prohibido criticar a la empresa», las críticas seguirán produciéndose en otros sitios. Pero si simplemente pide colaboración para mantener su página libre de insultos y palabras gruesas, y se mantiene fiel a sus reglas sin convertirse en un censor, puede obtener réditos interesantes. Por otro lado, piense que las críticas le deberían servir para señalar las percepciones de sus clientes y usuarios: utilícelas, recoléctelas, úselas para identificar los aspectos que necesita mejorar. Y responda, sin entrar por supuesto en la violencia dialéctica, pero responda. Han sido demasiados años de mensajes unidireccionales: es el momento de hablar.

Para la conversación y, sobre todo, para cuestiones como la respuesta rápida a críticas, hay una herramienta que brilla con luz propia: Twitter. El microblogging permite reacciones rápidas y es ideal para manejar relaciones: aquellos que estén interesados en su marca, en conversar sobre ella, o que tengan problemas de cualquier tipo con ella se suscribirán a su Twitter si en él consigue generarles algún tipo de valor. Algunas marcas, como es el caso de Comcast (pocas empresas concilian odios tan unánimes entre su base de clientes como las del sector de las telecomunicaciones) han sido capaces de utilizar Twitter para manejar sus relaciones con clientes, y han obtenido mejoras interesantísimas en las percepciones de estos, por supuesto partiendo de la base de que es preciso un equipo de personas que manejen dichas relaciones como si se tratase de un call-center. Otras, como Dell, han logrado construir canales de comunicación a través de los cuales venden determinados artículos (un millón de dólares en ventas el primer año, y seis millones durante el segundo, 2009). Twitter será lo que usted quiera que sea, pero no lo olvide: es un canal de comunicación ideal para muchas cosas, y su uso está creciendo de tal manera, que no sería extraño pensar que dentro de poco sea capaz de sustituir a Google como la mayor fuente de tráfico hacia la práctica totalidad de los sitios de la red. En mi página personal, en determinados momentos, ya me ocurre.

Gran parte de las cuestiones que ocurren en la red caen dentro de lo que se ha venido en llamar recientemente community management, o gestión de comunidades, una función que adquiere cada día más importancia para las compañías. En un mundo hiperconectado en el que invariablemente se crean comunidades, saber gestionarlas es una habilidad fundamental. Y gestionarlas quiere decir ser capaz de proveer a dichas comunidades de un entorno adecuado para que conversen, para que vinculen, para que creen contenidos asociados a la marca. La conversación, no lo olvide, no se puede «controlar», únicamente se puede aspirar a ser una parte relevante en ella. Los mejores gestores de comunidad son aquellos que han desarrollado experiencia en ello: es una habilidad para la que indudablemente se necesitan determinadas características, como madurez, templanza y educación, pero que se desarrolla utilizándola más y más. En realidad, este tipo de medios y herramientas van a ir tomando una parte cada vez más significativa de la comunicación de su empresa: acostúmbrese a un mundo en el que, como directivo, tendrá que desenvolverse en un mundo de relaciones caracterizadas por una total inmediatez, un nivel de personalización desconocido en las décadas más recientes, y elementos comunicativos que en lugar de estar creados por la empresa, lo están por los propios usuarios. Los llamados User Generated Contents, «Contenidos Generados por los Usuarios», o simplemente UGC, se convierten en primordiales para el posicionamiento de una marca, y las mejores marcas se miden en función de la cantidad de contenido de terceros que son capaces de generar. En el caso de Apple, por ejemplo, los llamados fanboys de la marca son capaces de imaginarse futuros diseños de productos aún sin anunciar, y de proporcionar ideas interesantes y un nivel de ruido muy elevado alrededor de los mismos. Los clientes toman cualquier cosa relacionada con la marca, desde el logotipo hasta las fotos de los productos, y juegan con ella, para horror de los responsables de imagen corporativa. Es inevitable, y es preciso no solo aceptarlo, sino incluso ponérselo fácil, proporcionando en nuestra web ficheros de alta calidad con todo material de nuestra empresa que se nos ocurra: fotos, logos, etc.

La comunicación pasa a ser una responsabilidad diferente, en la que hay que trabajar con la óptica graduada para dar respuestas infinitamente más sutiles que las que manejábamos anteriormente. Si antes enviábamos notas de prensa a un selecto grupo de periodistas a los que además tratábamos habitualmente muy bien y en cuyos medios éramos en muchos casos anunciantes, ahora comunicamos con miles o millones de personas, cada una de las cuales lee nuestros mensajes proactivamente y genera reacciones en muchos casos en sus nanomedios de comunicación. La ecuación comunicativa cambia, y el control se des-localiza, la marca debe relajarlo, permitir que los clientes tomen parte del mismo. Es preciso sinceridad, apertura, transparencia: muchos clientes escrutarán nuestra comunicación constantemente con lupa en busca de inconsistencias, de fallos, de mentiras, y si las encuentran, se convertirán en auténticos canarios de los que se usaban en las minas para detectar gases, en lo que los norteamericanos denominan whistleblowers, los que soplan el silbato para alertar a los demás. Pero lo que sí tiene que estar claro, en todas y cada una de las personas que trabajan en la empresa, es la voluntad de generar una respuesta: donde antes enviábamos notas de prensa —unidireccionalidad—, ahora publicamos directamente y esperamos que los usuarios reaccionen. Publicamos con el ánimo de que los usuarios reaccionen, y evaluamos el éxito de nuestra comunicación en función de cuánto han reaccionado: de métricas como el número de comentarios, twitts y retwitts (ya casi castellanizados por la vía del uso como «tuiteos» y «retuiteos»), acciones en Facebook (comparticiones, comentarios, «me gusta», etc.), entradas en blogs y foros, vídeos en YouTube... Repetimos: al principio, costará mucho acostumbrarse a esta nueva manera de hacer las cosas. Dará la impresión de que hemos perdido el control del proceso comunicativo, o que todo está saliendo mal porque aparecen comentarios negativos o parodias en vídeo en YouTube: da lo mismo. Usemos las quejas para mejorar, participemos en todo aquello que merezca una respuesta (las estupideces y vulgaridades en tono soez no la merecen; recuerde que la regla de oro es «no alimentar al troll» ni mucho menos ponerse a su nivel) y demostremos al mercado nuestra voluntad de participar en la conversación. Una conversación que, por supuesto, dista mucho de ser perfecta y libre de ruido, pero que con el tiempo va mejorando en calidad y llegando a asimilar protocolos razonables de uso.

Las métricas son importantes, sin duda. Plantearse participar en la web social sin ellas es como intentar conducir a ciegas, con una venda en los ojos: por mucho que conozcamos el camino, nos estamparemos en alguna curva. Pero también es fundamental no obsesionarse con ellas, ni compararlas con las de terceros sin cierto nivel de precaución: cada temática, cada empresa, cada conversación acaba encontrando su audiencia, y con suerte, será además cualquier cosa menos una «audiencia», en el sentido en que liará mucho más que simplemente «oír». Comunique, genere contenidos, e intente que se conviertan en «objetos sociales», sin machacar a nadie ni hacer spam, con paciencia, con la actitud del niño que llega a un colegio nuevo: lo que eres fuera de la red puede tener alguna importancia a la hora de determinar tu alcance, pero ser enorme fuera de la red no te garantiza serlo dentro de ella, y si lo crees, serás calificado de pretencioso e insoportable. Las métricas, además, deben cubrir dos segmentos: lo externo, lo que se produce fuera de nuestra empresa (conversaciones en sitios sociales, blogs, foros, clipping de medios, etc.) y lo interno, lo que tiene lugar en nuestro servidor (visitas, páginas vistas, usuarios únicos, términos de búsqueda con los que llegan al sitio, etc.). En todo momento, tenemos que ser capaces de explicarnos cualquier pico o caída en estas gráficas, sean debidos a un período vacacional, al lanzamiento de un producto o a una mención en otro sitio. Conocer el «efecto menéame» como quien antes se sabía las audiencias del prime time en televisión, porque frente a esa televisión unidireccional y de mensaje homogéneo, ahora tenemos un medio bidireccional, personalizado y con muchas más posibilidades. ¿Sus métricas son bajas? No se preocupe, hágalo bien, sea genuino en sus ganas de compartir y generar información relevante, y ya verá como van creciendo.

La reputación en la red es un trabajo a largo plazo. Obsesionarse por menciones o críticas negativas es de histéricos. Conteste a lo que crea que debe ser contestado con argumentos y razones, trabaje en aquellos comentarios negativos que le indiquen que tiene áreas de mejora, y plantéese que la red es como el mar: si echa basura, recibirá basura. Si echa cosas buenas, la marea acabará trayéndole cosas buenas. La responsabilidad de gestionar comunidades no es sencilla, y requiere muchos atributos, que, sin duda, irán teniendo cada vez más un soporte educativo en forma de estudios, prácticas o especializaciones. No se deje manipular por clichés típicos de quien intenta reducirla complejidad de las cosas: gestionar su comunidad es fundamental en una época caracterizada por la pujanza de la web social, hablemos del tipo de comunidad que hablemos. Si se trata de su blog personal, su gestor de comunidad será usted mismo, y cualquier esbozo en su carácter de características contrarias a las habilidades de gestión de comunidad serán un lastre potencial en los fines de la misma. Si tiene usted tendencia a exteriorizar un genio vivo y es habitual de las reacciones rápidas, las comunidades que tenga a su alrededor sufrirán como consecuencia de ello: el comportamiento en comunidad es como la educación, se aprende lentamente a medida que las reglas van asentándose en nuestro carácter, a medida que vivamos situaciones de las que vamos progresivamente derivando experiencia. A día de hoy, la educación y las buenas maneras en la red están todavía en proceso de consenso social, en fase de generación y asentamiento: muchas personas se comportan en la red con maneras que jamás utilizarían en sus relaciones frente a frente, por razones que van desde la falta de habilidad personal hasta la carencia de autocontrol social derivada de la falta de estimulación del córtex orbitofrontal, que habitualmente son generados por cambios en el tono y la inflexión de la voz, los gestos y expresiones faciales y corporales, etc. En la red, como en el resto de situaciones de nuestra vida cotidiana, trataremos con personas de todo tipo, a veces perfectas maleducadas. Antes, podíamos escoger dentro de un orden con quién nos relacionábamos, ahora, no tanto, porque medio mundo está a un clic de distancia. Una consecuencia más de la reducción de los costes de transacción.

CAPÍTULO 17. EL FUTURO

«La mejor forma de predecir el futuro es inventarlo.»

ALAN KAY, programador

No sería correcto llegar al último capítulo de este libro sin especular un poco sobre lo que nos depara la evolución de todos los factores que hemos ido tocando en cada una de sus partes. Recordemos el plan inicial, el «contrato» que firmamos cuando empezó usted la lectura de este libro: se trataba de aportar ideas e interpretaciones para estructurar el sentido común sobre un campo de actuación nuevo. Y hasta el momento, muchas de esas ideas e interpretaciones han venido derivadas de hechos del pasado, algunos tan recientes como unos pocos días antes de la fecha en que terminó la redacción del libro, pero pertenecientes a ese pasado, situados a la izquierda de esa línea temporal que en los últimos tiempos parece avanzar a tanta velocidad. Ahora intentemos especular sobre el futuro.

La primera conclusión clara es que la velocidad no va a disminuir. La sensación que tiene toda persona que se considere inexperta en tecnología es que la inversión en tiempo y esfuerzo necesaria para comprenderla no vale la pena: cuando se ha hecho, la tecnología ha cambiado, han aparecido nuevas posibilidades, cosas nuevas que hay que aprender, y nunca resulta posible alcanzarla. Todos conocemos el caso de esa persona que nunca llega a comprarse un ordenador, porque siempre está pensando que va a salir otro mucho más potente y más barato la semana siguiente: en tecnología hoy es muy difícil que la espera llegue a compensar el lucro cesante de no tenerla disponible para su uso un día antes.

Cada día, un número mayor de personas vuelcan una cantidad mayor de sus vidas en Internet. Y contrariamente a la impresión negativa que esto puede provocar en los escépticos, no se convierten en seres aislados, taciturnos y cetrinos, sino que, por lo general, obtienen beneficios que complementan su vida normal: se encuentran más vinculados y próximos a sus amigos y su entorno, disfrutan de una cantidad mayor de información que asimilan de maneras más ventajosas, y hasta consumen productos mejores y más baratos. En el nivel personal, este tipo de ventajas se transforman en pequeños detalles cotidianos, en manejo y soltura para utilizar herramientas que definen a quienes saben adaptarse al escenario en que viven, A nivel empresarial, estas ventajas definen que empresas se enfrentan al futuro con mayores ventajas competitivas, con más facilidades a la hora de desarrollar su actividad. Y en la interfaz entre lo personal y lo profesional, estar al día convierte a las personas en más atractivas para el mercado de trabajo, y separa cada día más a los contratados de los que se quedan en perpetuos aspirantes, o define a los emprendedores capaces de encontrar nichos de actividad marcados por nuevos escenarios tecnológicos.

A medida que la red va ofreciendo más y mejores propuestas de valor, un número mayor de personas van encontrando estas atractivas. Mi padre, tras una vida profesional bastante alejada del uso habitud de ordenadores, se encontró empezando a utilizar uno para comunicarse conmigo durante mis cuatro años de estancia en los Estados Unidos, y de ahí pasó a utilizarlos con total normalidad: de no haber comprado ni vendido acciones en su vida, a manejar una cartera de valores, leer y analizar noticias con total soltura, y participar en foros de todo tipo. Y no por el hecho de tener contacto conmigo, sino por un motivo mucho más directo: haber encontrado su propuesta de valor. Para mi padre, el ordenador es una manera de dinamizar su cerebro, al tiempo que le permite comunicarse con su familia y amigos. Mi impresión, de hecho, es que no tiene especial interés en ganar dinero comprando y vendiendo acciones, pero sí tiene interés en mantener su cerebro activo llevando a cabo complejos análisis y monitorizaciones de valores financieros: las hojas en las que recopila datos son dignas de analistas profesionales. Sin embargo, ese mismo padre atribuye a las redes sociales un papel de «herramienta para hacer amigos», forma su cliché mental en base a ese estereotipo, y dado que no se encuentra motivado por esa teórica propuesta de valor (considera que ya tiene el número adecuado de amigos y que los amigos, por lo general, se encuentran en la vida y no en la red), decide simplemente no entrar en ellas.

Todas las personas, independientemente de su edad, nivel cultural o de su experiencia, poseen arquetipos, clichés y lugares comunes de los que resulta extraordinariamente difícil aislarse: intentar manejar la complejidad reduciéndola a una explicación simplista es un fenómeno muy común en el ser humano a la hora de enfrentarse a lo nuevo, un reflejo muy difícil de obviar incluso para personas cultas y con mentalidad abierta. Sin embargo, existe un conjunto creciente de usuarios que, tras probar algunos de los aspectos de la red y encontrarlos positivos, van avanzando en su uso, y convirtiéndose en perfiles diferentes, en personas que manejan información de otra manera y se alejan de la pasividad que ha dominado una gran parte de los aspectos de nuestras vidas desde la Revolución Industrial. La masificación de la fabricación, la comunicación y el consumo convirtió al hombre en un ser alienado, acostumbrado a un tratamiento general, le llevó a una fortísima pérdida de la individualidad: todos consumíamos lo mismo, la masa nos reforzaba en nuestras creencias, en las consideraciones de calidad, en nuestros gustos, en nuestra forma de ver la vida. Las opciones políticas se simplificaron hasta el bipartidismo, los gustos se convirtieron en rankings en los que triunfaba aquel que, sin ser necesariamente mejor que otros, tenía la fuerza para comunicarlo de esa manera. La comunicación se redujo a la elección de canal sin moverse de un sofá, con un nivel de implicación y participación próximo a cero, pero que etiquetaba como antisociales a los que decidían no seguir la corriente.

Con esos pronunciamientos, resulta perfectamente inteligible que la expansión progresiva del uso de la red genere resistencias en las empresas que triunfaron en la época anterior. La red da paso a un consumidor diferente, muchísimo más informado, dispuesto a establecer vínculos comunicativos multi-direccionales, en sentido inverso con las marcas, y en sentido horizontal con otros consumidores: una situación que la inmensa mayoría de las empresas no saben cómo manejar. La Revolución Industrial tuvo lugar entre finales del siglo XVIII y principios del XIX: más de cien años en los que, básicamente, hemos estado haciendo lo mismo. Durante la próxima década, vamos a vivir una escalada progresiva de cambios que vienen a demostrar precisamente eso: que se pueden hacer cosas diferentes, y que en eso consiste el progreso.

El progreso es imposible de detener, y por progreso se entienden cosas que, si bien lo son para algunos, para otros no tienen necesariamente por qué serlo. La libre difusión de materiales sujetos a derechos de autor a través de la red es completamente imparable se interprete como se quiera interpretar, y es vista por los usuarios como un claro índice de progreso: ahora pueden acceder a todo tipo de obras, a precios muy inferiores o gratuitos, y mediante un solo clic. Obviamente, esto no es etiquetado precisamente como progreso por las empresas que anteriormente se encargaban de fabricar y distribuir estas obras sobre soportes físicos, como bien pudimos ver en el caso de la industria de la música: en lugar de llamarlo «progreso», lo califican con lindezas como «piratería» o «robo», y cometen errores que, a pocos años de los mismos, pueden ser calificados como prácticamente de infantiles. ¿Qué habría ocurrido si en lugar de cerrar Napster y convertirla en la muestra para la infinidad de clones que vinieron después, la industria musical hubiese colaborado con ella, la hubiese favorecido y la hubiese convertido en un canal preferencial a través del cual obtener música por una fracción de su precio, por un importe disuasorio? A estas alturas. Napster sería el equivalente de iTunes, la industria habría aprendido cómo adaptarse a ese nuevo canal, se habría ido reconvirtiendo sin grandes traumas, y el éxito de Napster habría disuadido a muchas otras iniciativas similares.

Sin embargo, la máxima que dice que «el hombre es el único animal que tropieza dos veces con la misma piedra» no es una frase hecha por casualidad. En pleno año 2010, nos disponemos a ver cómo otras industrias repiten algunos de los errores cometidos por la industria de la música, en lugar de intentar aprender de ellos. Este libro que tiene usted en sus manos estará disponible a través de redes P2P seguramente a los pocos días de su puesta en el mercado (de hecho, me defraudaría bastante si no fuera así), sin que el autor o la empresa editora puedan hacer nada por evitarlo. ¿Significará eso menos ganancias para alguno de los dos? Ya lo veremos, porque no necesariamente tiene por qué ser así. En principio, el reconocimiento de la inestabilidad de oponerse a ello ya está hecho, de ahí el que este libro se venda con una licencia Creative Commons, de la que hablaremos en un momento, que no impide hacer copias de su contenido cuando se hagan sin ánimo de lucro. ¿Para qué intentar impedir lo que no puede ser impedido por medio razonable alguno? Simplemente este hecho supone ya de por sí un avance mayor que el que la industria de la música ha conseguido recorrer en sus últimos —y convulsos— diez años. Pero el siguiente paso es el que cuesta: reconocer la lucha contra el progreso como algo imposible e inútil y cambiar la estructura para adaptarse a un mundo en el que muchas de las bases tradicionales del negocio ya no son necesarias y la estructura de márgenes se altera por órdenes de magnitud es algo que en la mayoría de ocasiones está fuera del alcance de los actores de toda la vida, y que por tanto necesita de nuevos entrantes que dinamicen el panorama.

El cambio que estamos viviendo reviste características de extrema globalidad. Ahora que está llegando al final de este libro, es un momento adecuado para volver al principio: su título, «Todo va a cambiar». La historia que hay detrás es como mínimo curiosa: cuando la editorial me contactó para encomendarme la escritura del libro, una de las condiciones que me puso fue que no lo escribiese para los lectores de mi página. Esos, aparte de representar un pequeño «descremado» exclusivo de la sociedad desde el punto de vista de conocimientos tecnológicos, tenían, en general, bastante claro el proceso por el que estábamos pasando como individuos, como empresas y como sociedad. Para ellos, un título como «Todo va a cambiar» habría sido factualmente incorrecto: en realidad, dirían ellos, «todo ha cambiado» ya. Hablamos de personas que permanecen más tiempo a lo largo del día conectados a la red que desconectados de ella, y que cuando ven una noticia en el telediario relacionada con tecnología, miran la televisión con displicencia, porque ya la han visto en la red varias horas antes. El lector objetivo del libro debía ser esa persona que ve cambios, que escucha las noticias y se entera de avances tecnológicos que le resultan sorprendentes, que se preocupa y se pregunta alguna versión del «dónde vamos a ir a parar». El público objetivo del libro debían ser personas que en caso de ver un libro titulado «Todo ha cambiado» mirasen a izquierda y derecha, pensasen en cómo se desarrolla su vida con la perspectiva de los últimos años, en cómo trabajan, viven y se relacionan con otras personas, y dijesen «no, no ha cambiado».

Pero ¿ha cambiado o no? Tangibilicemos las cosas: A mi casa en Majadahonda (Madrid) llegan habitualmente múltiples cajas, transportadas por operadores logísticos de todo tipo: puestos a enumerar algunos de los habituales, nos encontraríamos con las naranjas de Naranjas Inza, los tomates de SoloRaf, las verduras de Recapte, el pescado y marisco de Lonxanet, las flores de Flores-Frescas, los calsots de Calsotsxom, el vodka de chocolate de San Pancracio, el café de Nespresso, los quesos asturianos de La-Barata.com, y algunas veces, Telepizza... pero eso no es todo, porque lo normal también es que el resto de la compra del supermercado sea también encargada a través de la red. Unamos eso a productos como los billetes de Iberia o Renfe (a pesar de su ominosa web, es lo que tiene ser un monopolio), las reservas de hoteles, las entradas de cine, los libros de Amazon o hasta los ordenadores y accesorios de Apple. En todos esos casos, y seguramente en muchos más que no recuerdo, esos bienes y servicios llegaron a mi casa a golpe de clic. Aislando diferentes casuísticas, podríamos citar desde productos de relativo «lujo», no tanto por su precio como por lo que supone poder acceder a ellos desde donde yo vivo, hasta servicios en los que el lujo proviene de no tener que ir a por ellos: quien haya comprado entradas de cine por Internet, frente al hecho de llegar y hacer cola, sabe a lo que me refiero. Algunos de esos productos me salen más baratos del precio que tendrían en tiendas de Madrid a las que podría ir en coche o andando. Otros me salen a precios parecidos o más caros, pero me resultaría sencillamente imposible acceder a productos similares de una calidad equivalente: en esos casos, me comporto como gourmet. Algunos productos simplemente no los había probado nunca, y el hecho de disponer de ellos en la red me ha convertido en consumidor habitual de temporada, como los deliciosos calsots. En mis compras online tengo motivaciones de todo tipo: desde el deseo de mantener una alimentación equilibrada y sana, hasta el capricho de disfrutar de una calidad determinada, pasando por la comodidad de no acarrear bolsas del coche a casa. O por la pura y dura nostalgia. El café de Nespresso es insultantemente más caro que un café normal comprado en paquete de cuarto kilo, pero después de probar el primero, volver al segundo supone una especie de tortura.

Por otro lado, pensemos en cómo funciona esto: ¿me ha vuelto el hábito de comprar en la red más taciturno, soy alguna clase de eremita que no ve la luz del sol, o me lleva a pasar mi tiempo en pijama» sin afeitar y con un aspecto patibulario? Ni mucho menos. No solo me permite disfrutar más de determinados productos, sino que me libera tiempo para otras cosas. Problemas de seguridad, solo los he tenido una vez: una transacción realizada con la tarjeta de crédito de mi mujer fue interceptada, y entre un sábado y un domingo aparecieron cargos en ella por valor aproximado de unos tres mil euros. Tras la correspondiente denuncia al banco y a la Guardia Civil, el dinero fue íntegra y satisfactoriamente reintegrado en un plazo relativamente corto. Teniendo en cuenta el uso que se hace en esta casa del dinero de plástico a través de la red, la incidencia me parece tan aislada, que resulta perfectamente comparable al riesgo de andar por la calle: de hecho, a mi mujer le dieron un tirón en el bolso el año pasado, y el trastorno ocasionado fue muy superior. Pero no caiga en el error de considerar que la web es un sitio que sirve únicamente para el consumo, para el comercio electrónico. Si medimos la importancia de la web en un país en virtud del comercio electrónico que se desarrolla en el mismo, estaremos cometiendo dos errores: primero, restringir la actividad a una frontera administrativa que en Internet ya no tiene sentido o no debería tenerlo, y segundo, suponer que la actividad principal del hombre es comprar. El primer error es responsable precisamente de que el comercio electrónico no funcione: los norteamericanos de Sand Hill Road, la calle de Silicon Valley en la que se sitúan la mayoría de los capitalistas de riesgo e inversores en nuevas empresas, suelen rechazar, salvo algunas excepciones, todo plan de negocio que no tenga desde el primer momento un ámbito claramente internacional. En Europa, muchas de las iniciativas de comercio electrónico son completamente mediocres porque únicamente un 4% de las tiendas online europeas sirven pedidos a más de dos países, y la responsabilidad, por supuesto, no recae en estas tiendas, que estarían encantadas de vender más, sino en el complicado camino administrativo y de armonización legislativa necesario para ello: es precisamente en la red donde la unión de Europa deja más que desear.

Donde verdaderamente noto que todo ha cambiado no es en cómo consumo, sino en cómo vivo y me comunico. Aparte de trabajar prácticamente desde donde quiero y me conviene, algo solo al alcance de aquellos que podemos hacerlo merced a la naturaleza muchas veces intangible de nuestro trabajo, la red funciona ahora para mí como un dispositivo de percepción sensorial expandido: cada cosa que pasa, cada relación, cada contacto, cada evento deportivo, cada película... todo alcanza una expresión social a través de la red. Desde levantarme en medio de un partido para comentar un gol en mi red social, hasta ver un programa entero con el portátil en las rodillas. Entrar para felicitar a los amigos que están de cumpleaños, para ver qué han hecho desde que se han levantado, para ver qué noticias que han leído les han parecido interesantes y han decidido compartir: si no ha probado Google Reader, no ha entendido lo que es leer la prensa colaborativamente, saber cuáles de tus amigos van convirtiéndose en referencia para ti, quienes escriben mejor, cuáles escriben comentarios más ocurrentes...; la auténtica máquina de café virtual, a la que acudes a comentar lo que ha pasado. Si además vives en un hogar, como es mi caso, en el que tanto tu mujer como tu hija disfrutan de exactamente lo mismo, cada uno con sus temas y sus diferentes círculos de amigos, con sus lógicas superposiciones, la sensación que tienes es la de una tangibilización tan grande de tu vida social, una cercanía tan grande a tus amigos, que el planteamiento es hasta qué punto este es el mejor invento del mundo desde que decidieron hacer el pan en rebanadas.

Si piensa que la vida no ha cambiado, prepárese para enfrentarse, en el curso de los próximos años, a una serie de enormes desafíos. Prepárese para ver cómo muchos de sus compañeros de trabajo empiezan a desempeñar un número progresivamente más alto de tareas desde su casa, a ver cómo cada vez menos gente compra sin pasar antes por la red para tomar decisiones sobre lo que va a adquirir, a empezar a sentir rancia la información del día anterior que lee sobre restos de árboles muertos comprados en un quiosco, a darse cuenta de que un político es algo más que una marioneta a la que votar cada cuatro años. En los próximos años, va a notar cambios en todas las áreas importantes y menos importantes de su vida, desde el ocio y la cultura hasta la política y la empresa, pasando por cómo se relaciona con sus amigos, cómo aprenden sus hijos en el colegio o cómo aparca su coche en la vía pública. Muchas, muchísimas cosas que algunos hemos conocido en el mundo de la red se trasladarán de manera casi exacta a la llamada «vida real» (como si la vida en la red no lo hiera). Imagínese, por ejemplo, un motor de búsqueda: ¿cómo sería un motor de búsqueda aplicado al mundo real? El resultado ya existe:{20} ponga la cámara de un teléfono Android mirando hacia una tienda, y el teléfono le dará datos sobre ella. Hágalo sobre una tarjeta de visita, y procesará los datos de la misma y le ofrecerá introducirlos en su agenda. Diríjala hacia una botella de vino y le proporcionará las notas de cata y algunos datos de la bodega. Cada día más, lo que no está en la web, no existe: ese pequeño restaurante de aspecto poco elegante en el fondo de una ría gallega pero que cocina el marisco a la parrilla como ninguno podrá tener un aspecto muy discreto y ningún cartel que lo anuncie, pero su teléfono podrá decirle exactamente dónde está, qué pedir y qué opinaron otros comensales anteriores. Y, además, si el resultado de todo esto le resulta agobiante, si le parece que excede su capacidad de procesamiento o si le provoca inquietud, no se preocupe: nadie le va a preguntar lo que opina de todos esos cambios, si le gustan o le dejan de gustar, si le afectan positiva o negativamente en su trabajo o en su vida. Simplemente, aparecerán, como consecuencia de la ley de Pringles: «cuando haces pop, ya no hay stop».

El futuro inmediato está marcado por los intentos de control de una red que se ha ido erigiendo progresivamente en principal protagonista de la escena político-económica. Las empresas e instituciones que dominaron la política y la economía durante el último siglo, incapaces de aplicar la misma lógica a la red que al resto de su entorno, seguirán durante un cierto tiempo intentando modificar la naturaleza y el funcionamiento de la red para tratar de impedir lo inevitable, hasta que la presión de los usuarios llegue a resultar insostenible. Los esfuerzos para desnaturalizar la red vendrán por tres frentes: empresas intermediarias de bienes culturales, poder político y empresas de telecomunicaciones.

Las empresas intermediarias de bienes culturales, representadas por las discográficas y los estudios cinematográficos agrupados en asociaciones patronales de diversos tipos{21} que seguirán intentando a la desesperada preservar sus márgenes comerciales en una actividad cada día más innecesaria, mientras ven como los artistas que saben adaptarse al nuevo entorno ganan más dinero al margen de ellas. Dichas sociedades continuarán acusando a la gran mayoría de los usuarios de Internet de cosas tan ridículas como «piratería» o «robo», esgrimiendo cifras de supuestas pérdidas millonarias que no reflejan más que su propia inadaptación.

Usando esas cifras como ariete, las empresas intermediarias de bienes culturales seguirán intentando presionar al legislativo para cambiar las leyes en contra de la lógica y de la evolución, y sobre todo, en contra de los deseos de sus electores. ¿Qué factores fundamentales hay detrás de la afinidad de muchos políticos con dichas empresas y asociaciones? Por un lado, el supuesto poder de convicción e influencia de muchos intérpretes, autores, directores de cine, etc. sobre la opinión pública, que seguirá siendo instrumentalizado en épocas de campaña electoral «sacando a los artistas a la calle» para intentar atraer el voto. Por otro, la idea de un mayor control sobre los medios de publicación en la red y sobre las comunicaciones de los ciudadanos, con la falsa excusa de que un mayor control conlleva un nivel de seguridad mayor. En realidad, el mayor control acaba significando una violación sistemática de los derechos de los ciudadanos, sin traducirse en un mayor nivel de seguridad: lo que los políticos en realidad pretenden es obtener mecanismos que les permitan controlar de alguna manera la opinión de los ciudadanos expresada a través de la red. Para los partidos políticos, acostumbrados a relacionarse con unos pocos medios de comunicación y a controlarlos mediante la zanahoria de los presupuestos de publicidad institucional, una red en la que cualquiera puede expresar libremente y sin control críticas a su gestión resulta la mayor de las pesadillas, razón por la que alinean sus intereses con las citadas empresas. Diciembre de 2009 fue el mes en el que el gobierno chino decidió suprimir el derecho de sus ciudadanos a tener páginas web personales, a expresarse en la web. ¿Cuántos gobiernos occidentales envidian secretamente al gobierno chino?

Los terceros actores, las empresas de telecomunicaciones, resultan algo más paradójicos, por ser precisamente los propietarios de las infraestructuras por las que discurre la sociedad de la información. Sin duda, el desarrollo de la sociedad de la información ha supuesto un papel de enorme centralidad para estas empresas: hoy en día, las telecomunicaciones son, sin duda, la columna vertebral de nuestra sociedad. Sin embargo, estas empresas plantean una disyuntiva: para mantener el progreso de una sociedad de la información que demanda cada vez más un ancho de banda mayor, las empresas de telecomunicaciones deben invertir cantidades importantes de dinero en despliegue de infraestructuras, inversiones a las que exigen una rentabilidad determinada, condicionada por la evolución histórica de la misma.

En la situación actual, las empresas de telecomunicaciones perciben ingresos en función del uso, pero esos ingresos evolucionan cada día más hacia tarifas planas, con el fin de incentivar un consumo que puede aceptar facturas más elevadas o más bajas, pero nunca impredecibles. En estas condiciones, las empresas de telecomunicaciones observan cómo los contenidos que discurren por sus redes se convierten en generadores de grandes e interesantes negocios, mientras que ellas se quedan únicamente con el negocio del acceso, considerado cada día más una commodity, un servicio sin diferenciación alguna, similar al que pueden tener otras utilidades como el suministro de electricidad o de agua. La disyuntiva de las empresas de telecomunicaciones es clara: para ser una commodity, les sobra dimensión. Pero para intentar extraer una mayor rentabilidad, necesitan introducirse más en la cadena de valor, algo que han intentado de muchas maneras invirtiendo en empresas de contenidos, etc., pero con un nivel de éxito muy bajo: hablamos de otro tipo de negocios, con necesidades y condicionantes completamente diferentes.

¿A dónde nos lleva esta situación? A que las empresas de telecomunicaciones sigan intentando convertirse en algo más: en responsables no solo de los cables, sino también de lo que fluye por ellos. La lucha por la neutralidad de la red, en la que las empresas de telecomunicaciones alegarán su imposibilidad de mantener los márgenes exigidos por sus accionistas y presionarán a los gobiernos comprometiendo las inversiones futuras en infraestructuras, se mantendrá durante varios años. Detrás de esta lucha, en la que se asociarán con las empresas de contenidos para justificar el control de lo que fluye por los cables, y con el poder político para garantizar un mayor nivel de control de actividades delictivas y terroristas, es donde está la mayor amenaza a la naturaleza de la red: si estas empresas obtienen en alguna medida su propósito, Internet perderá muchas de las características que lo han llevado a ser el fantástico vehículo de expresión colectiva que es hoy, y representará el mayor paso atrás en la historia del desarrollo de la humanidad en su conjunto. Afortunadamente, el nivel de consenso internacional necesario para que algo así llegase a ocurrir lo convierte en algo muy altamente improbable.

La segunda década del nuevo siglo tendrá un inicio marcado por la fuerte lucha entre la adaptación de los derechos de los ciudadanos al nuevo entorno digital y los intentos de control por parte de esos tres ejes: políticos, empresas de telecomunicaciones y lobbies de la propiedad intelectual. La tensión será importante en muchos momentos, y sufrirá evoluciones diferentes en diversos países en función del poder de los diferentes actores y de diferencias culturales, pero determinará que algunos países vayan adquiriendo, al hacer uso de la tecnología con menores restricciones, un papel cada vez más importante en el concierto económico y político mundial. Sin duda, la principal amenaza provendrá de quienes pretenden extender a la red conceptos de propiedad intelectual completamente absurdos en una era digital: tratados que quieren homologar torpemente la circulación libre de los bits con delitos ajenos a lo digital, como la falsificación (ACTA, el conocido como Anti-Counterfaking Trade Agreement o «Tratado de comercio contra la falsificación»), con el potencial de traer auténticas «épocas oscuras» similares a las vividas en tiempos de la sagrada Inquisición. Merced a este tipo de siniestros tratados podremos ver intervenciones de ordenadores portátiles al paso por las aduanas, violaciones de derechos fundamentales, y todo tipo de atrocidades pretendidamente justificadas con los intentos de preservar modelos de negocio imposibles.

Este tipo de escenas que ya estamos viviendo, sin duda profundamente desagradables, representan ni más ni menos que batallas en una revolución. Algunos, de manera optimista y seguramente debido a la falta de perspectiva que provoca intentar analizar la historia desde dentro de la misma, piensan que la fase de «revolución» marcada por Internet correspondió a la primera década del siglo XXI, mientras que la segunda década traerá la consolidación de los derechos de los ciudadanos en ese nuevo entorno. Desgraciadamente, no va a ser así. En realidad, la primera década ha estado marcada por una gran ignorancia e indiferencia, mientras que será la segunda década la que traiga la verdadera revolución de los usuarios, al empezar a comprobar estos de manera fehaciente las posibilidades de la red y de qué manera les serían estas hurtadas en caso de permitir a las viejas estructuras seguir ejerciendo su poder. La segunda década del siglo XXI será la que verdaderamente marque el enfrentamiento entre los ciudadanos y las estructuras del poder establecido, en escenas que podrían interpretarse desde un punto de vista abiertamente marxista: los ciudadanos se dan cuenta de que pueden controlar los medios de producción, de que pueden producir y manejar su propia información, y pasan a protagonizar una lucha de clases, en la que se enfrentan a los antiguos «terratenientes de la información». El resultado es una catarsis de los viejos sistemas de producción, manejo y control de la información, que cambiará completamente los esquemas que habíamos vivido desde los tiempos de la. Revolución Industrial.

Por el momento, los viejos poderes viven en la fase de negación de la evidencia: se niegan a aceptar que el mundo ha cambiado, y su proximidad a los poderes políticos y económicos les permite crearse la ilusión de intentar retrasar o evitar esos cambios. Las persecuciones, los «sistemas de tres avisos», el endurecimiento legislativo o el matonismo judicial, aplicado a ciudadanos indefensos, son reflejos de esa lucha desigual, en la que se enfrentan, por un lado, enormes imperios económicos con recursos prácticamente ilimitados, y por otro, simples ciudadanos que dedican a ello su tiempo libre, los ciclos ociosos de su tiempo. El enfrentamiento es completamente desequilibrado: hablamos de lobbies poderosísimos, capaces de introducir cláusulas en las leyes al margen de su tramitación parlamentaria regular,{22} alterando las normas fundamentales en cualquier democracia, que pretenderán seguir ejerciendo dicho poder durante mucho tiempo.

Pero la resistencia de los viejos poderes tiene un alcance meramente generacional: la segunda década del siglo XXI verá la llegada a las estructuras de poder de esos nativos digitales educados en el activismo de la primera década, personas para las cuales los intentos de control no tienen ningún tipo de sentido. ¿Quién querría, tras haber crecido en un entorno en el que los bits son libres, colaborar para intentar que dejen de serlo? En muchos partidos políticos y en muchas empresas, el recambio generacional en los puestos de responsabilidad está determinando no solo un cambio de actitud en este sentido, sino también la identificación de factores competitivos a explotar radicados en adelantarse a otros en la aplicación de estos principios.

En realidad, todo concepto de evolución futuro pasa necesariamente por una fuerte redefinición del concepto de propiedad intelectual, que se ha convertido en el moderno equivalente de la Santa Inquisición. Dicha redefinición, en realidad, ha comenzado ya. Ese copyright que desde el Estatuto de la reina Ana en 1710 se convirtió en la base del negocio asociado a la creación cultural durante más de doscientos años se ha quedado completamente obsoleto, y está evolucionando, mutando hacia fórmulas menos restrictivas, menos basadas en blancos y negros, más enfocadas a proveer tonos de gris. En una economía basada en la atención en la que el derecho a hacer copias no puede ser restringido por ningún medio, el copyright, la señal de «prohibido el paso» no resulta operativa ni práctica. Es preciso introducir fórmulas como las señaladas por Creative Commons, una organización sin ánimo de lucro que define y libera alternativas al copyright en las que se pueden definir todo tipo de situaciones intermedias: en el caso de este libro, por ejemplo, el tipo de licencia obliga a indicar la autoría, pero impide la explotación comercial. Si alguien decidiese atribuirse la autoría del libro o hacer una copia con expreso ánimo de lucro (con el fin de venderla), estaría vulnerando el contrato, y podría ser objeto de persecución legal a todos los electos oportunos. Sin embargo, la licencia permite la copia sin ánimo de lucro, fundamentalmente porque entiendo que se trata de algo para lo que, en este caso, no puedo dar o quitar permiso: sería simplemente imposible de controlar, y yo no voy a perder el tiempo persiguiendo aquello que no puede ser perseguido. Igualmente, puedo tomar decisiones sobre si permito o no obras derivadas de esta, o sobre si, por ejemplo, estoy dispuesto a aceptar el uso comercial del libro en países en vías de desarrollo, aunque lo excluya expresamente en países no pertenecientes a ese segmento de la clasificación de la OCDE. Una licencia Creative Commons permite definir todas las gamas de grises, y está, además, adaptada a la legislación de cada país para que pueda ser legalmente reconocida, protegida de manera efectiva o compatibilizada, por ejemplo, con un interés comercial vinculado a la difusión de la obra.

El nuevo concepto de propiedad intelectual emergerá de la evidente necesidad de reforma del mismo. No es viable ni práctico tener a una gran mayoría de la ciudadanía en situación de delincuencia. Y no se deje engañar: ni siquiera en los Estados Unidos, uno de los países que de manera más dura ha intentado perseguir a quienes se descargan música y películas mediante un auténtico acoso y matonismo judicial, las cifras han descendido lo más mínimo. Pero lo importante ahora es entender el tipo de mundo que emergerá bajo estas nuevas condiciones: ¿qué ocurre cuando la propiedad intelectual deja de intentar poner puertas al campo y adopta premisas realistas? ¿Qué tipo de mundo tenemos cuando se reconoce el hecho de que, en realidad, toda producción cultural se apoya en infinidad de creaciones anteriores de manera más o menos expresa, y que, por tanto, carece de autoridad moral y práctica para conceptualizar la protección en términos de impedir el acceso? Como ya hemos comentado anteriormente: si tu modelo de negocio consiste en impedir a otros el acceso a tus bits, vete pensando en cambiar de modelo de negocio. El tuyo, en la sociedad de la información, se ha vuelto inviable, por duro que te parezca.

Si algo distinguirá al mundo post-Internet es el reconocimiento de que, en él, todos somos creadores. El progresivo abaratamiento y la facilidad de manejo de las herramientas de producción llevarán a que todos podamos llevar a cabo la tarea de producción cultural en prácticamente todas las vertientes. A día de hoy, podemos cantar sobre un karaoke electrónico en Internet o hacerlo en nuestras casas con una consola, podemos tocar en el Guitar Hero y hacernos la ilusión de que suene relativamente bien, podemos filmar vídeo con una cámara sencilla y barata de alta resolución, podemos hacer fotografías digitales y acabar obteniendo un buen resultado, aunque sea por mera persistencia o ensayo y error, podemos mezclar y editar música o vídeo con programas de edición que hasta hace muy poco estaban tan solo en manos de profesionales... y podemos publicarlo todo en Internet, tenga el formato que tenga. Por supuesto, no todo lo que se produzca será bueno, pero la situación llevará a una hiperinflación de contenidos culturales, que redundará seguramente en un nuevo Renacimiento: las buenas ideas tomarán cuerpo en una meritocracia en la que todos somos autores, y donde los rendimientos económicos provienen de aquello que no resulta repetible o de la extracción de beneficios de terceros. Ganará dinero con sus creaciones todo aquel que vea cómo son utilizadas para generar a su vez un beneficio económico: resulta perfectamente razonable pagar por utilizar música en un bar, en una discoteca, en una radio, o en cualquier sitio en el que la música forme una parte integrante del negocio. Será negocio ceder música para publicidad, como lo será para películas o para otros usos lucrativos. Existirá un importante negocio en torno al directo, devolviendo la lógica a un mercado en el que quien quiere dinero, debe trabajar para conseguirlo. En la red, la descarga seguirá funcionando, pero su uso ira disminuyendo a medida que nuevas opciones más cómodas y sencillas adopten esquemas de precios disuasorios, un estudio de elasticidad de precio que las empresas actuales han despreciado hacer. Y por supuesto, no, no se acabará la música, ni se dejarán de hacer películas. Nunca, en toda la historia de la humanidad, hubo nada capaz de parar el desarrollo cultural. Nada. Hemos vivido cambios tecnológicos brutales, censuras, inquisiciones, entornos de enorme carestía de recursos... y la creación cultural continuó.

Retomando una de las preguntas importantes de los primeros capítulos: ¿a dónde va todo ese valor económico desintermediado, aparentemente desaparecido con las ganancias de eficiencia que la tecnología trae consigo? ¿Dónde está el dinero que la empresa productora agrícola ya no paga a intermediarios, a mercados centrales, a asentadores y a distribuidores o puntos de venta finales, cuando yo adquiero mis frutas y verduras directamente a través de la red? ¿Qué será de esos «intermediarios desintermediados»? La respuesta, por supuesto, es que ese valor no habrá desaparecido, sino que habrá migrado. Parte del margen se lo quedarán ahora los agricultores, mientras que otra parte la recibirán operadores logísticos redefinidos que tomen un papel activo en la cadena, que añadan un mayor valor y que operen con una gran cercanía al cliente. Otra parte, simplemente, se invertirá en mayor eficiencia a lo largo de la cadena, lo que podrá redundar en precios inferiores o en otros modelos de negocio. En muchos casos, las cadenas se complicarán mediante el uso de mercados de dos caras, en los que lo que aparentemente es un esquema producto-cliente se convierte en realidad en un esquema más complejo en el que el cliente no siempre resulta obvio: el caso de Google, analizado en el capítulo 14, es especialmente interesante en este sentido. Una empresa que no cobra nada a la inmensa mayoría de sus usuarios, que les proporciona productos de alta calidad completamente gratuitos, y que, en realidad, cobra sus servicios mediante un nuevo tipo de moneda, la atención, que es captada, cuidadosamente segmentada y cualificada en función de intereses y hábitos, y vendida posteriormente a aquellos que desean exponer su publicidad ante ella.

El contexto económico vinculado con el progreso tecnológico es, en realidad, una auténtica redefinición del capitalismo postindustrial a gran escala. Por un lado, posee tintes que evocarían al más ferviente marxismo: las herramientas de producción pasan a estar en manos del pueblo. Por otro, se crea todo un nuevo sistema económico basado en una nueva moneda, la atención, dotada de nuevas reglas y esquemas en los que caben cosas prácticamente incomprensibles para las empresas «de toda la vida», como ofrecer productos gratuitos. Numerosos esquemas de gratuidad van abriendo nuevos caminos en la forma de hacer negocios: según la tipología establecida por Chris Anderson en su magnífico libro Gratis, podemos hablar de modelos como el freemium (prestaciones básicas gratuitas, unidas a ofertas completas a cambio de un pago), publicidad (el modelo más clásico, el del contenido o servicios gratuitos financiado mediante anuncios), coste marginal cero (entregar gratis productos, como la música, que son producidos para su uso, por ejemplo, en conciertos en directo), subsidios cruzados (regalar productos y cobrar por demanda asociada a los mismos), intercambio de mano de obra (usuarios que desarrollan un trabajo por el que un tercero está dispuesto a pagar), o simplemente, economía del regalo (donaciones de tiempo o trabajo a determinados servicios, como Wikipedia).

La economía de la atención es seguramente el cambio más interesante sufrido por el sistema capitalista en toda su historia, y su auge está basado en la disponibilidad de una plataforma con la universalidad de Internet que se convierte en el medio perfecto para la difusión de la atención. Pero seamos pragmáticos: de la atención no se come. La economía de la atención genera esquemas interesantísimos y empresas capaces de aprovecharlos de maneras muy brillantes, pero al final, en prácticamente todos los casos se necesita una conexión con la «economía real», con el mundo del dinero. Por supuesto, la economía de toda la vida no va a desaparecer, pero sí va a ir abriendo vías de interacción con esa economía de la atención, hasta encontrarse en un punto en el que la mayoría de la comunicación con sus clientes se desarrolle a través de esta.

Pensemos, por ejemplo, en el caso de Twitter: una empresa que intentaba desarrollar una aplicación para que todo el mundo pudiese subir a la red archivos de audio (Odeo, ahora desaparecida), pero se da cuenta de que, dado el perfil ultratecnológico y geek de sus trabajadores, les resulta muy difícil reunirse: cada uno trabaja desde un sitio, entran y salen de la empresa a horas intempestivas, programan por la noche y duermen durante el día, etc. Para arreglar ese problema de comunicación, crean una aplicación sencilla que les permite actualizar lo que están haciendo en mensajitos de 140 caracteres, y se encuentran con que el uso de dicha aplicación les resulta extrañamente adictivo, les proporciona una especie de «conexión permanente» con sus amigos. Al mostrarlo en una conferencia tecnológica, el SXSW, muchos asistentes comienzan a usarlo, y de repente, llega una explosión: millones de personas en todo el mundo se lanzan a twittear como posesas. Todo el mundo habla de Twitter. Pero cuando me piden que entreviste a uno de sus fundadores, Biz Stone, en una conferencia en Sevilla, compruebo una curiosa disfunción: mientras todo el mundo le pregunta cómo será su modelo de negocio, Biz permanece impasible, dice no saberlo ni preocuparle, y afirma estar completamente concentrado en hacer de Twitter una aplicación sólida y útil para sus usuarios. Finalmente, tras más de dos años de uso financiado por capitalistas de riesgo, la empresa empieza a facturar: al convertirse en una aplicación que tantos usuarios adoran y utilizan para comentar lo que les ocurre y lo que les llama la atención, Twitter pasa a ser uno de los pocos sitios donde es fácil seguir la actualidad, donde se proyecta algo tan novedoso e interesante como la llamada real-time web. Algo que no pasa desapercibido para los motores de búsqueda: tanto Bing como Google, al darse cuenta de que Twitter tiene la clave para hacer búsquedas sobre lo que ocurre en el mundo en cada momento, deciden pagar a Twitter por el uso de su información.

¿Ha creado valor Twitter? ¿Ha producido algo tangible? ¿Cómo ha contribuido a la generación de riqueza? Twitter es un metamediario, un intermediario que agrupa y convierte en accesible la información creada por millones de usuarios. Algo que antes de Twitter no existía, y que genera un valor enorme: gracias a Twitter, miles de personas saben lo que están haciendo sus amigos y se sienten más cerca de ellos, o siguen la actualidad, o siguen a sus personajes favoritos, o lo utilizan como forma de generar transparencia. El Twitter de Barack Obama, alimentado por su oficina de campaña, logra que muchísimos electores se sientan integrados en el proceso electoral, conecten con los mensajes lanzados por el candidato, y se precipiten a votar con inequívoco signo el día de las elecciones. En España, el Twitter de varios internautas asistentes a una reunión con una ministra de Cultura se convierte en una conexión permanente con el exterior, y desactiva los mensajes de «armonía y normalidad» que la ministra pretende lanzar a la prensa cuando sale de la reunión. ¿Puede la economía tradicional calcular el valor de algo así? La respuesta es no. Pero gracias a Twitter, Dell vende más de seis millones de dólares en ordenadores al año, y muchas empresas se comunican directamente con sus clientes y generan en ellos sentimientos de fidelidad. Twitter, en realidad, genera valor sirviendo de plataforma para las actividades de terceros.

El caso de Facebook es todavía más curioso: sobre la base de convertirse en una plataforma para las actividades sociales de sus miembros, la empresa consigue acumular tantas horas de atención, que puede cobrar a otras empresas por redirigir tráfico de usuarios desde unos sitios a otros ¡dentro de su propia red! Si una empresa quiere atención, puede poner anuncios en Facebook en páginas que reúnan a usuarios con los demográficos e intereses que estimen oportunos, y tratar de redirigir ese tráfico hacia la página de la compañía, que igualmente se encuentra dentro de Facebook.

A la pregunta de si existe un nuevo modelo económico, la respuesta indudable es que sí. Un modelo infinitamente más tercerizado, lleno de mercados con varias caras, en los que una atención cada día más dispersa encuentra maneras de transformarse en actividades generadoras de valor. Las empresas de la economía tradicional no desaparecen, pero necesitan entender el nuevo ecosistema en el que se mueven sus clientes, la manera en que estos se relacionan, se enteran de la actualidad, consumen contenidos, etc. Si no se enteran, corren el riesgo de desperdiciar valiosísimos recursos en anuncios en televisión que nadie ve, en cortes de radio que nadie escucha o que provocan que cambiemos de emisora, o en páginas de periódicos que solo se usan al día siguiente para envolver productos en el mercado. La nueva economía no está integrada por un montón de empresas «raras» que hacen cosas que pocos alcanzan a comprender: está integrada por empresas de toda la vida que intentan aprender a utilizar las plataformas, productos y servicios proporcionados por esas empresas para seguir teniendo acceso a sus clientes, a sus mercados, a la información que precisan para desarrollar su actividad. Como cliente, si no sabe usar estos productos y servicios, verá como poco a poco se va sintiendo más torpe, más anticuado, cómo otros clientes obtienen mejores productos que usted y en mejores condiciones. Como empresario o trabajador, si no entiende las reglas de la nueva economía, verá como su actividad va cayendo cada día más en el olvido: primero verá cómo se incrementa la edad media de sus clientes, después cómo los más jóvenes de estos pasan a preferir marcas con las que se sienten más identificados, con las que se pueden comunicar. Si es usted un político, verá cómo su forma de intentar comunicar con los votantes, pegando ridículos carteles por las calles y hablando en mítines con tono de telepredicador norteamericano, pierde completamente su efectividad.

Pero no lo olvide: nadie, ni este libro ni ningún otro método, puede enseñarle cómo todo está cambiando, cómo tantas cosas han cambiado ya. La única manera es verlo por usted mismo. Las nuevas herramientas, los nuevos usos y costumbres solo se comprenden cuando se utilizan, cuando se manosean, cuando se tienen en la cabeza. No es un camino sencillo, tienen sus propias reglas, y si aplica las de antes, las que le funcionaban en la era pre-Internet, tiene muchas posibilidades de hacerlo mal. Acérquese a su ordenador (y si no lo tiene, cómprese uno a la voz de ya), Lea noticias en un lector de feeds, suscríbase a blogs que encuentre interesantes, a búsquedas de términos que tengan importancia para usted. Pruebe a abrirse un blog, aunque sea para escribir sobre un hobby o afición, aunque sea pensando en abandonarlo a las pocas semanas. Siga sus estadísticas, sienta lo que se siente cuando alguien encuentra algo que usted ha escrito y se detiene a leerlo, o lo enriquece con un comentario (o lo encuentra de repente lleno de comentarios de spam ofreciéndole alargar de manera inverosímil determinadas partes de su anatomía, que también puede ocurrir). Entre en una red social, busque a sus amigos, comuníquese con ellos, conviértala en un suplemento de su vida social normal. Solo probando este tipo de herramientas logrará entender su verdadero alcance, hacerse una idea de cómo afectan a la forma de vivir de las personas, a la manera de comunicarse, de consumir, de competir...

No se quede en el «todo va a cambiar». En realidad, todo ha cambiado ya.

Este libro está sujeto a una licencia

Creative Commons BY NC SA

Usted es libre de:

- copiar, distribuir y comunicar públicamente la obra

- hacer obras derivadas

Bajo las condiciones siguientes:

Reconocimiento — Debe reconocer los créditos de la obra de la manera especificada por el autor o el licenciador (pero no de una manera que sugiera que tiene su apoyo o apoyan el uso que hace de su obra).

No comercial — No puede utilizar esta obra para fines comerciales.

Compartir bajo la misma licencia — Si altera o transforma esta obra, o genera una obra derivada, sólo puede distribuir la obra generada bajo una licencia idéntica a ésta.

{1} Una red P2P, o peer-to-peer, es aquella en la que los diferentes nodos interactúan entre sí sin necesidad de un ordenador o nodo central. Muchas redes, como eMule, KaZaA, BitTorrent, etc., actúan así. En el caso de Napster, la primera de ellas, existía un nodo central que no almacenaba materiales sujetos a copyright, sino simplemente la identificación y la lista de recursos de cada uno de los participantes.

{2} La payóla es un mecanismo de pago a las emisoras de radio a cambio de que estas difundan con mayor intensidad una canción determinada. En los Estados Unidos, país donde se inventó el concepto, la payóla es ilegal desde los años cincuenta: las emisoras pueden aceptar dinero por poner más una canción determinada, pero deben revelarlo y, además, contar las reproducciones de esa canción como parte del tiempo dedicado a publicidad.

{3} Una suscripción premium a Spotify, por ejemplo, cuesta 9,90 euros, un precio determinado por la rentabilidad que las discográficas exigen a la propia Spotify. En realidad, ese precio está calculado para que dichas discográficas puedan mantener los mismos ingresos por canción que cuando tenían que grabarla sobre un CD, imprimir un folleto, ponerla en una caja jewel de plástico transparente, meterla en una caja de cartón, distribuirla en camiones, llevarla a tiendas por todo el mundo y pagar un margen importante al distribuidor. ¿Cuántos usuarios descargarían en el P2P si la suscripción premium de Spotify en lugar de casi diez euros, costase uno o dos?

{4} En gallego, «esto es cosa de brujas».

{5} El juez Gómez, francés, impuso a Yahoo! Auctions en noviembre de 2000 la obligación de implantar filtros para intentar evitar que los usuarios franceses de la compañía accediesen a artículos de memorabilia nazi, cuya comercialización resulta ilegal en Francia, que eran ofrecidos en la web de la compañía, la decisión provocó que eBay optase por dejar de ofrecer ese tipo de productos en todo el mundo.

{6} Los errores en los programas se denominan bugs («bichos») debido a lo habitual que resultaba, en los primeros ordenadores, que los errores se debieran a la acción de polillas que entraban atraídas por la luz y el calor de las válvulas y tubos de vacío.

{7} Enormemente recomendable en ese sentido es el libro Valué migration, de Adrián J. Slywotzky, un «clásico» publicado en 1995.

{8} Literalmente: If you talked to people the way advertising talked to people, they'd punch you in the face. Hugh MacLeod en su blog, Gapingvoid. 9 de mayo de 2006.

{9} Derivado del inglés «freak», utilizado inicialmente para referirse a aberraciones de la naturaleza, la palabra fue evolucionando para referirse a personas que persignen una afición o pasión de modo obsesivo, a seguidores devotos de una conducta determinada, o incluso a tener una relación directa con el uso de niveles avanzados de determinadas tecnologías.

{10} Infografía de Tom Walker, publicada en Creative Cloud, disponible en http://www.cartridgesave.co.uk/news/if-you-printed-the-internet/ en el momento de imprimir este libro.

{11} Abreviatura de What You See Is What You Get («lo que ves es lo que obtienes»), programas que mostraban en pantalla las cosas tal y como se suponía que saldrían después al ser interpretadas. Programas típicos de esa generación fueron, por ejemplo, el desaparecido FrontPage de Microsoft, o el DreamWeaver de Macromedia (hoy Adobe).

{12} Literalmente «lo que ves es lo que obtienes», supuestamente referido al resultado obtenido al imprimir sobre un papel.

{13} Estrategia conocida en inglés como FUD (Fear, Uncertainty and Doubts).

{14} Un enlace es un comando HTML que contiene dos partes: un texto y un destino. Google interpreta un enlace como un voto hacia la página destino para el término especificado como texto.

{15} El PageRank de una página puede ser consultado, por ejemplo, mediante la barra de Google, o mediante diversas herramientas que se añaden a navegadores como Firefox, como SearchStatus u otras.

{16} Google no mostraba publicidad en todas sus páginas (en Google News, por ejemplo, no lo hacía), o bien rediseñaba el tipo de publicidad adecuándola al tipo de página, como en el caso de sus productos geográficos, Google Maps y Google Earth.

{17} La ley de Moore predice el incremento de las prestaciones de los ordenadores, y afirma que cada dieciocho meses se duplica el número de transistores en un circuito integrado. Es una ley basada en la observación, y sus resultados se han cumplido razonablemente bien hasta el momento.

{18} El error 404 es el que devuelve un servidor cuando no encuentra el recurso solicitado por una petición.

{19} Theodore Kaezynski, actualmente en cadena perpetua, es un matemático norteamericano que, entre los años 1978 y 1995, llevó a cabo el envío de 16 bombas por correo a objetivos no relacionados entre sí, lo que provocó la muerte de 3 personas y heridas a otras 23. Tras su captura, se descubrió que vivía en una cabaña en las montañas, sin luz ni agua corriente, y que sus reivindicaciones estaban relacionadas con la protesta por el avance de la tecnología, lo que relaciona a Kaezynski con corrientes como el neoludismo y el anarcoprimitivismo.

{20} Google Goggles, lanzado por Google en diciembre de 2009, ofrece precisamente esa posibilidad. Y si este pie de página le parece muy actualizado o incluso futurista, a pesar de hablar de un producto que ya apareció, no se preocupe: en futuras ediciones de este libro tendremos que pensar si vale la pena mantener este pie de página aquí, o si ya contiene una información demasiado obvia.

{21} En España, hablamos de asociaciones como la SGAE, definida por las encuestas de popularidad como la entidad más odiada del país, EGEDA (la patronal de las empresas discográficas) o la Coalición de Creadores (que agrupa a todas ellas intentando hacer ver que concilia los intereses de los autores, cuando en realidad predominan claramente los de los editores). En los Estados Unidos, sus equivalentes son asociaciones igualmente de enorme impopularidad, como la RIAA (responsable de miles de procedimientos de marcado matonismo judicial que han obligado a infinidad de usuarios a pagar para evitar ir a juicio), ASCAP o la MPAA.

{22} En España se ha llegado a ver, en diciembre de 2009, cómo a un anteproyecto de ley de economía sostenible «le aparecía», de repente, un quinto punto dedicado expresamente a la propiedad intelectual, en el que se vulneraban los derechos fundamentales de los ciudadanos, y que se pretendía que pasase como si lo hubiesen redactado originalmente y pasado por el Consejo de Ministros, una burla a la democracia tan alucinante que supera toda concepción de personas supuestamente decentes.
