¿Hay Alguien Ahi?

ISAAC ASIMOV

¿HAY ALGUIEN AHÍ?

PLAZA JANES EDITORES. S.A.

IS ANYONE THERE?

Corrección de Dom

©1956, 1957, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, Isaac Asimov

Traducción: © Miguel Giménez Sales, 1973

De la presente edición: © 1988, PLAZA JANES EDITORES, S. A.

Virgen de Guadalupe, 21-33 Esplugues de Llobregat (Barcelona)

____________________

Printed in Spain – Impreso en España ISBN: 84-01-45080-2 – Depósito Legal: B. 2.115-1988

INTRODUCCIÓN

Es bien sabido que soy un escritor de ciencia-ficción. También se sabe que soy miembro de la Facultad de Medicina de la Universidad de Boston. En consecuencia, es natural que a menudo se me pregunte qué opinan mis colegas del hecho de escribir obras de ciencia-ficción.

Tal vez quien tal pregunte tenga la impresión de que tropiezo con ceños fruncidos y miradas desaprobadoras; que mi camino se halla erizado de espinos sobre los que yo ando descalzo, y que mi carrera profesional se ve obstaculizada y desviada.

Para mí resulta un poco desalentador tener que negar el drama, pero lo cierto es que mi vida profesional no es dura. Algunos colegas míos ignoran que escribo ciencia-ficción, y no creo que les importase el saberlo. Otros están enterados de ello, y lo consideran simplemente como otra idiosincrasia académica. Algunos son apasionados lectores de tales novelas y leen a menudo las mías…, espero que complacidos. Y unos cuantos, por el cielo, también son escritores de ciencia-ficción.

Esto no quiere decir que no hubiese una época en que yo mismo me pregunté si resultarían compatibles una carrera académica y una reputación como autor de ciencia-ficción.

La posibilidad me asaltó con toda su fuerza en junio de 1949, cuando tuvieron lugar dos acontecimientos. Primero, estaba a punto de ingresar en la Facultad. Segundo, acababa de vender mi primer relato de ciencia-ficción a «Doubleday y Compañía», e iba a aparecer como un «libro regular».

Llevaba once años escribiendo narraciones de ciencia-ficción para las publicaciones y revistas del género, pero siempre había pensado que se trataba de un oscuro ejercicio del que se derivaba un secreto entre los exóticos aficionados a esa literatura y yo. Sin embargo, un libro era diferente, porque no podía mantenerlo en secreto.

Afortunadamente, no me hallaba inmerso en ningún dilema, ni me veía acosado por ninguna incertidumbre. Desde muy temprana edad supe que me gustaba escribir y también que, si algún día me veía obligado a escoger entre la literatura y otra profesión elegiría la primera. (Conocer por anticipado el curso de acción personal a emprender, procura una gran paz mental, y a esto atribuyo yo estar libre de úlceras a pesar de un estilo de vida compuesto casi exclusivamente de titulares.)

Por tanto, no veía la necesidad de actuar con vacilación. Si alguna vez tenía que enfrentarme con una elección, era ahora. Y así, le pedí una entrevista al decano.

–Señor -le comuniqué cortés, pero firmemente-, como ya sabe, soy el nuevo instructor de Bioquímica. Sin embargo, creo justo manifestarle que dentro de unos meses verá la luz mi primera novela de ciencia-ficción en un volumen y la Facultad de Medicina se hallará identificada indirectamente con él.

–¿Es una buena obra? – me preguntó el decano a su vez.

–En la editorial «Doubleday» así lo creen -respondí cautelosamente.

–Entonces -decidió el decano-, me encantará identificarme con él.

Y así fue. En los años transcurridos desde entonces, nadie de la Facultad se ha opuesto a mis escritos de ciencia-ficción, al menos delante de mí, y, que yo sepa, tampoco a espaldas mías.

En mi cerebro tuvo lugar otra crisis cuando empecé a publicar libros científicos. En 1952, fui coautor de un libro de texto de bioquímica para estudiantes de Medicina, y desde aquella época he publicado muchos libros científicos sobre una amplia variedad de temas.

Al principio, pensé que tal vez resultaría mejor usar un seudónimo.

–Vamos, Asimov -murmuró a mi oído, un editor fantasma-, no podemos arruinar la venta de un libro serio, haciendo que sus probables lectores digan: «Esta obra no puede ser buena, ya que la ha escrito ese autorzuelo de ciencia-ficción.»

Me dispuse a librar batallas homéricas, pues decidí firmar con mi nombre todos mis libros. (En primer lugar, me gusta mi nombre; en segundo, soy una persona centrada en sí misma; en tercero, me siento orgulloso de la ciencia-ficción y de mi lugar en la misma, por lo que no deseo que se la insulte.)

¡Ay! Las batallas homéricas no se libraron jamás. Ningún editor, ni uno solo, se opuso nunca al halo que la ciencia-ficción ha puesto alrededor de mi cabeza. Incluso observé que, en muchos casos, la minúscula biografía inserta en las solapas de mis libros científicos mencionaban mi cualidad de autor de ciencia-ficción como prueba de que yo era un escritor de calidad.

Lo cual me condujo al bastión final de la posible falta de apreciación de la gente en general. La buena ciencia-ficción, al fin y al cabo, atrae a las minorías, de eso no hay duda. Las válvulas de escape que, necesariamente, debían de atraer a un auditorio más vasto y variado, tenían que descartarse. Esta razonada conclusión quedó destruida con la llegada de la era espacial en 1957. De repente, el público en general y hasta la parte menos culta de ese público, se sintió profundamente interesado por los temas más extraños. Y empezaron a desear leer artículos relativos a los asuntos situados en las fronteras de la Ciencia, y a sentirse cada vez más atraídos por los relatos de ciencia-ficción.

De nuevo encontré que mis antecedentes como autor de ciencia-ficción no obstaculizaban en nada mi carrera; al contrario, la ayudaban. Me pidieron que escribiera diversos artículos que unos años atrás no me habría atrevido a escribir siquiera. Fingiendo indolencia, los escribí y no tardé en descubrir que, a pesar de conservar mi puesto en la Facultad, tenía que abandonar la enseñanza. Y ahora me he convertido en un escritor profesional.

¡Qué distinta es la situación de ahora a la de 1949, por ejemplo! Entonces, yo estaba convencido de trabajar en completa oscuridad, y que si se me formulaba la pregunta ¿Hay alguien ahí? con respecto a mis lectores, la respuesta procedería de un inmenso vacío:

–Solamente nosotros, los partidarios de la ciencia-ficción, Asimov.

Mas ahora, cuando considero la larga lista de escritos diversos de los que soy responsable (todos basados en mi reputación como escritor de este género literario), sé que la respuesta sería muy diferente y halagüeña para mí.

Y para completar el círculo, de nuevo estoy en «Doubleday», donde se publicó mi primera novela. Estos caballeros están totalmente dispuestos a publicar una colección de mis artículos dispersos en multitud de revistas, revisados y puestos al día. Varios de dichos artículos tratan de temas de ciencia, algunos de cálculos y especulaciones, y otros de ciencia-ficción…, las tres patas de mi trípode.

Primera parte

RELATIVA A LO MÁS O MENOSCONOCIDO

1. LA VIDA

1. La materia sobre la mente

¿Qué es la mente? ¡No importa!

¿Qué es la materia? ¡No importa! [1]

Este antiguo rasgo de ingenio afirma la convicción del hombre a través de las edades, relativa a que la mente humana supera a la materia, y que no se halla limitada por las reglas ordinarias que la rigen groseramente.

La estructura física del organismo vivo se acepta como un conjunto de átomos y moléculas, gobernado por las mismas leyes que gobiernan a las rocas que pisamos y a las estrellas del firmamento. Esto es tan cierto para el Orgulloso Hombre como para el Minúsculo Gusano. Pero… ¿y la mente del hombre? ¿Es posible analizar el genio creador que da lugar a la obra de arte? ¿Es posible pesar, contar y medir las emociones y la imaginación, el amor y el odio, la pasión, el pensamiento y el sentido del bien y del mal?

Siempre ha existido el fuerte impulso de colocar la mente por encima de la materia y de aplicar reglas diferentes y más sutiles a la primera. Por tanto, parece natural que las medicinas de la ciencia médica no actúen con eficacia sobre la mente. Shakespeare ya hizo que Macbeth le preguntara cínicamente a un médico, con respecto a la curación de las pesadillas sufridas por su esposa, Lady Macbeth:

Canst thou not minister to a mind diseased,

Raze out the written troubles of the brain.

Raze out the written troubles of the brain,

Ana with some sweet oblivious antidote

Cleanse the stuffed bosom of that perilous stuff

Which weighs upon the heart?

A lo que el médico responde con humildad:

Therein the patient

Must minister to himself [2].

Tres siglos después de Shakespeare, cuando los médicos empezaron a «curar la mente enferma», lo hicieron sin el «suave antídoto, como el olvido», sin ninguna poción, mejunje o artilugio material. Para llegar hasta la mente, las leyes de la materia no eran suficientes; por tanto, la propia mente tenía que ser el instrumento. Los médicos empezaron a conversar con sus pacientes y, más importante aún, a escuchar lo que decían aquellos. En lugar del estetoscopio del médico y el tubo de ensayo del clínico, tuvimos el diván del psiquiatra.

Los científicos clínicos se han visto fuertemente tentados a no explorar más en este asunto y no efectuar ningún otro movimiento en favor de las personas trastornadas mentalmente. Abordar las vastas complejidades de la mente con los fríos instrumentos materiales de la ciencia, requería una buena dosis de heroísmo. Existía la falaz promesa del fracaso inevitable respecto al dragón que despide llamas de la química mental, que tendía a intimidar al presunto san Jorge del microscopio y la regla de cálculo.

Y sin embargo, el cerebro está compuesto de átomos y moléculas…, igual que el resto del cuerpo. Las moléculas de las células corporales, y las del cerebro en particular, son tantas, tan variadas y versátiles, que se interaccionan y cambian según unas normas asombrosas que todavía no hemos comprendido por completo. Pero el mismo enigma de este complejo químico infunde ciertas esperanzas, puesto que es, concebiblemente, lo bastante complejo como para ser responsable de todas las infinitas sutilezas de lo que llamamos mente.

Esta complejidad se ve ahora atacada por nuevas técnicas que dan por resultado singulares adelantos en la química cerebral y en la psicología. Se emplean ordenadores para analizar los datos de las ondas cerebrales, con una exactitud jamás lograda hasta el presente. La mejor comprensión de los ácidos nucleicos en relación con el mecanismo de la herencia está produciendo sugerencias excitantes respecto a la mecánica de la memoria (lo que estudiaré con más detalle en el capítulo segundo).

Además, se están utilizando nuevas drogas que afectan al trabajo cerebral, a veces de forma drástica, ofreciéndonos diversos atisbos de dicho trabajo. Esta última técnica creada es la más excitante, puesto que entraña, entre otras cosas, el compuesto llamado LSD, que ofrece a la Humanidad una nueva dimensión en el uso de las drogas y las consecuencias de las mismas.

Los nuevos adelantos, sorprendentes respecto a las manifestaciones más sutiles del cerebro (memoria, percepción, razón), no han surgido de la nada. Existe un siglo de adelanto respecto a los aspectos menos complicados de la acción cerebral. Aunque el sistema nervioso es un conjunto intrincadamente entrelazado, en casi todos los planos de su actividad, muestra, en ciertos aspectos, una especie de complejidad gradualmente creciente de las funciones, de abajo arriba. Esto ha ayudado a los científicos a avanzar mediante fases sencillas, hasta haber logrado hoy día tratar de enfrentarse razonablemente con la maquinaria mental que entremezcla todos los planos del sistema nervioso.

Más abajo del cerebro se halla la médula espinal, una casa de tejido nervioso, estrecha, de una longitud de unos 45 centímetros, que desciende por el centro de las vértebras, formando el espinazo. La médula espinal es un centro receptor de muchos de nuestros reflejos más corrientes. El individuo toca algo caliente y esta sensación se difunde por la médula, convirtiéndose en un impulso nervioso exterior que ordena la inmediata retirada de la mano. Ésta se aparta antes de que la mente consciente haya tenido ocasión de decir: «Está caliente.»

(Naturalmente, no es esto todo lo que hace la médula espinal. Ésta se halla unida, mediante distintos nervios, a los diversos centros del cerebro y forma parte de un todo unificado. Sin embargo, esta acción refleja fue la primera en ser comprendida, y yo la simplifico deliberadamente para obtener una visión más clara de la perspectiva histórica.)

En su extremo superior, la médula espinal se ensancha para formar la médula oblonga u oblongada, sobre la cual se asienta el cerebro como un pedazo abultado de una fruta arrugada. La médula oblonga, también llamada «bulbo raquídeo», se cuida de asuntos más complicados que los simples reflejos. Es un centro importante que controla la forma en que nos sostenemos, por ejemplo.

De pie, usamos activamente varios músculos para mantener erguida nuestra espalda y nuestras piernas contra la atracción de la gravedad. Para conseguirlo con eficiencia, ha de existir una interacción constante y delicadamente ajustada. No se le permite a ningún grupo de músculos que nos desequilibre a un lado o a otro sin que otro grupo entre rápidamente en acción para reajustar el equilibrio. Ordinariamente, no nos damos cuenta de esta actividad, pero si llevamos algún tiempo de pie, el cansancio resulta desagradablemente evidente, y si perdemos el conocimiento estando de pie, los músculos se relajan y caemos al suelo al instante.

Si fuese nuestra mente consciente la que estuviese constantemente preocupada por los músculos que nos mantienen de pie, apenas tendríamos tiempo para ocuparnos de otras cosas. Sin embargo, de esto se encarga el bulbo raquídeo, casi sin interferencias de la conciencia. Estamos de pie, y nos equilibramos ajustadamente, por muy distraídos que nos hallemos, por muy meditabundos que estemos, siempre que no durmamos o perdamos el sentido.

Encima del bulbo raquídeo hay dos grandes lóbulos con superficies arrugadas, cada uno dividido casi por la mitad. El mayor es el cerebro (del latín, cerebrum), y el menor el cerebelo.

El cerebelo se halla un poco detrás del bulbo raquídeo. Y hace algo más que conservarnos en equilibrio estando inmóviles, ya que conserva nuestro equilibrio cuando nos movemos. Mientras andamos, levantamos una pierna, perdemos temporalmente el equilibrio, y adelantamos la pierna para apoyarla sobre el suelo de una manera calculada para recuperar el equilibrio. Si movemos una mano hacia el bolígrafo, por ejemplo, la mano debe proceder con lentitud antes de cogerlo, y detenerse al llegar a él.

Tiene que haber un «cálculo». Tenemos que ver (o sentir) el movimiento de una parte de nuestro cuerpo, calcular su distancia desde su objetivo y ajustar su velocidad y dirección constantemente sobre la base de la cambiante situación. El cerebelo se ocupa de esto. Lo hace de manera automática, de modo que si deseamos coger un bolígrafo lo hacemos de una manera perfecta, sin damos cuenta de la dificultad de la tarea. Pero fijaos en alguien que padezca parálisis cerebral y no pueda realizar tales cálculos. Es incapaz de efectuar ni la menor tarea sin continuos desaciertos y fallos.

Junto con esto, las sensaciones externas deben producir cambios químicos en las células cerebrales que, a su vez, dan lugar a impulsos nerviosos que ocasionan específicas respuestas musculares. Claro que aún ignoramos los detalles de dichos cambios químicos.

Al llegar al cerebro, encontramos que está más directamente relacionado con la química. Al fondo del cerebro, por ejemplo, existe una zona llamada hipotálamo, una de cuyas funciones es actuar como un termostato. El calor corporal se produce mediante una vibración constante y suave de los músculos, a un promedio de siete a trece veces por segundo, hecho comprobado en 1962. El hipotálamo «siente» la temperatura de la sangre que pasa por él. Si la misma es demasiado baja, provoca un aumento de calor. Si la temperatura es demasiado elevada, el hipotálamo rebaja el promedio de vibraciones. De esta manera se mantiene de un modo constante el calor corporal, a pesar de los cambios exteriores.

El hipotálamo también detecta la concentración acuosa de la sangre y actúa por medio de una glándula próxima, la pituitaria, para ajustar el trabajo de los riñones; éstos eliminan más agua si la sangre se aclara; menos, si se espesa. El hipotálamo también mide constantemente la concentración de azúcar de la sangre. Cuando la misma es demasiado baja, el hipotálamo actúa para producir sensación de hambre (ver capítulo tercero).

Aquí tenemos unos ejemplos más claros de la labor química. Los pequeños (e inofensivos) cambios químicos de la sangre, producen alteraciones en los mecanismos corporales para impedir otros (y crecientemente perjudiciales) cambios en tal dirección. De esta forma se conserva ajustada y equilibrada la química corporal.

Sin embargo, los detalles han de ser extraordinariamente complicados. Los mecanismos corporales están estrechamente relacionados entre sí, y el hipotálamo ha de producir cambios necesarios en una sola parte de esta supercomplicada red, sin provocar otros cambios innecesarios en otras zonas. Esta dificultad se ve clara con la forma en que casi todas las drogas aplicadas al hombre, a pesar del empleo más cuidadoso, producen siempre, por desgracia, «efectos secundarios». El hipotálamo ha de actuar con una increíble seguridad para evitar tales efectos.

Mas, ¿qué ocurre en las partes superiores del cerebro, las que se hallan ocupadas particularmente con los movimientos y las sensaciones conscientes, con el pensamiento y la razón, la memoria y la imaginación? Si nos vemos asombrados por la química de cosas tales como los reflejos y el equilibrio acuoso, seguramente nos encontraríamos mudos de estupor ante la química de la memoria, pongo por caso.

En realidad, como veremos en el próximo capítulo, no es así. Actualmente, realizamos grandes progresos, al menos en apariencia, sobre la comprensión de la memoria, y en el horizonte se perfilan ya las más excitantes perspectivas.

No es sólo la mente razonablemente sana la que nos ocupa, sino lo que llamamos trastornos mentales que pueden realizar simplemente cambios o desviaciones en la obra química del cerebro. Si las enfermedades mentales no son más que un mal funcionamiento material, mediante el estudio de la química cerebral podemos descubrir las curas que hasta ahora han eludido constantemente los psiquiatras.

Por ejemplo, consideremos la esquizofrenia, la más común y grave de las enfermedades mentales. Este nombre lo inventó en 1911 un psiquiatra suizo. Paul E. Bleuler, derivándolo de la expresión griega que significa «mente dividida», porque se observaba frecuentemente que los individuos que sufrían esta dolencia parecían dominados por una serie de ideas (o «complejos»), con exclusión de las demás, como si la labor armoniosa de la mente se hubiese interrumpido y dividido, controlando una sola parte de dicha mente el resto de la misma. Otro nombre anterior para esta enfermedad fue el de «demencia precoz», término que intentaba diferenciarla de la demencia senil, enfermedad mental que afecta a los ancianos a causa del deterioro cerebral, debido a la edad. La esquizofrenia suele presentarse a una edad relativamente temprana, por lo general, entre los 18 y los 28 años.

Existen diversas variedades de esquizofrenia, según el complejo predominante. Puede ser «hebefrénica» (mente infantil), en la que el síntoma predominante es la conducta infantil o atontada. Puede ser «catatónica», en que la conducta presenta un tono bajo y el paciente parece retraerse de su participación en el mundo objetivo, enmudeciendo e inmovilizándose. Puede ser también paranoica («locura»), caracterizada por una extrema hostilidad y suspicacia, y con manías de persecución.

Al menos, la mitad de los pacientes recluidos en sanatorios mentales son esquizofrénicos de alguno de estos tipos, calculándose que esta enfermedad afecta globalmente al uno por ciento de la Humanidad. Esto significa que en el mundo hay, como mínimo, unos treinta millones de esquizofrénicos, cifra igual a la población total de una nación como España.

¿Puede tratarse esta variedad tan común de las enfermedades mentales con un «antídoto suave, como el olvido»?

Existen precedentes que infunden ciertas esperanzas. Algunas enfermedades mentales ya son curadas, y la mente se ha mostrado dócil al tratamiento físico…, por lo menos en algunos casos.

Un ejemplo es la pelagra, enfermedad antaño muy corriente en las riberas mediterráneas y en el sur de América. Se caracteriza por las llamadas tres D: diarrea, dermatitis y demencia. En realidad, la pelagra era producida por la falta de una vitamina, la falta de niacina en los alimentos. Una vez administrada la niacina a los pacientes, en dosis suficientes, la enfermedad desapareció. No sólo cesaba la diarrea, sino que la piel enrojecida, rugosa e inflamada, recobraba su aspecto normal, y también concluían los trastornos mentales. La misma técnica que curaba el cuerpo sanaba la mente. En este caso, al menos, la materia dominaba a la mente.

La pelagra es una enfermedad provocada por la falta de suministros del exterior. Pero, ¿y el mal funcionamiento causado por los desajustes en la maquinaria química del cuerpo? Cada reacción química del cuerpo está controlada por unas sustancias muy complejas llamadas enzimas, y cada reacción tiene su enzima particular. ¿Qué ocurre, entonces, si una persona nace sin la habilidad necesaria para fabricar algún enzima especial?

Ésta es la situación en el caso de una enfermedad denominada oligofrenia fenilpirúvica, caracterizada por una grave deficiencia mental. Esta enfermedad, poco común, por fortuna, está presente ya en el nacimiento. Un niño nace sin la facultad de fabricar cierto enzima que provoca la transformación de una sustancia llamada fenilalanina en otra denominada tiroxina. La primera, no pudiendo seguir su curso normal, se cambia en otras sustancias anormales. Y éstas se acumulan e interfieren en la química cerebral.

Por desgracia, en tal caso la situación no es tan fácil de corregir como en el caso de la pelagra. Aunque sea sencillo administrar una vitamina, es todavía casi imposible proporcionar un enzima del que carece el cuerpo. No obstante esto, algunos pacientes han acusado cierta mejora en esta condición mental, manteniéndolos en una dieta baja en fenilalanina.

¿Es posible, pues, que la esquizofrenia sea también el resultado de un fallo químico, bien externo o interno? El doctor A. Hoffer, de la Universidad de Saskatoon, Canadá, lleva muchos años tratando la esquizofrenia mediante la administración de grandes dosis de niacina, con considerable éxito. Aparentemente al menos, algunas formas de esquizofrenia se deben a deficiencias vitamínicas, como una forma de pelagra más grave.

Para tratar la esquizofrenia se necesita más niacina que para la pelagra, y Hoffer aporta para esto una razón. La niacina se convierte dentro del cuerpo en una sustancia más compleja llamada NAD, que es la que realmente actúa. El organismo normal puede fabricar el NAD a partir de la niacina con suma facilidad y rapidez, si ésta está presente en la dieta alimenticia. (Por esto, se cura la pelagra tan pronto como se añaden dosis de niacina a la dieta suficiente en otro sentido.) Pero el equizofrénico puede padecer un trastorno químico, caracterizado en parte por la incapacidad de formar fácilmente el NAD. Por tanto, hay que suministrar grandes dosis de niacina a fin de que la ineficaz maquinaria química produzca al menos un poco de NAD.

Hoffer informa que en la primera mitad de 1966 ensayó la administración de NAD, con resultados muy esperanzadores. Las dosis más pequeñas produjeron mejorías más rápidas. (Como es costumbre en el caso de tratamientos experimentales en las fronteras de los conocimientos humanos, también hay informes de otros laboratorios donde los resultados fueron desalentadores.)

El fallo químico en el caso del esquizofrénico (ya sea la incapacidad para producir el NAD a partir de la niacina, o por cualquier otra causa), tiene que ver aparentemente algo con la herencia, y ciertamente, se hereda la tendencia a desarrollar esta enfermedad. Las probabilidades individuales de desarrollar la esquizofrenia, son, como se ha dicho, del uno por ciento. Sin embargo, si una persona tiene un hermano o hermana esquizofrénico, las probabilidades de que ella misma presente esa dolencia son del uno por siete. Si se trata de un mellizo esquizofrénico, las probabilidades son del uno por tres o cuatro.

La gente, con toda seguridad, no suele nacer con síntomas de esquizofrenia; esta dolencia no es innata, como lo es la oligofrenia fenilpirúvica. Podemos decirlo de esta manera: el esquizofrénico no nace con el fallo de una parte de su maquinaria química, sino con una parte frágil, que se deteriora a edad muy temprana de su vida. Y lo que se hereda es esta fragilidad.

¿Pero por qué el NAD (si se trata efectivamente del NAD) conserva al cuerpo humano en estado normal? ¿Qué le ocurre al organismo cuando falta ese mismo NAD?

Se han formulado suposiciones con respecto a una parte del plan químico que empieza con una sustancia denominada adrenalina. En cantidades pequeñísimas, la adrenalina estimula ciertos nervios que controlan los latidos cardíacos, la tensión arterial, la respiración y otras funciones corporales. La glándula adrenal (un pequeño bultito de tejido encima de cada riñon) tiene, entre otras funciones, la secreción de adrenalina al líquido sanguíneo en momentos de tensión, fatiga o molestia. Cuando nos enfadamos o asustamos, al momento se segrega la adrenalina a fin de elevar la tensión sanguínea, nuestro corazón late más de prisa, nuestros pulmones aspiran con más rapidez el aire. Nos hallamos situados en un estado de emergencia que nos capacita para luchar o correr.

Naturalmente, es importante que, una vez concluida la emergencia, el cuerpo retorne a su estado normal. Por esta razón, el cuerpo posee ingenios químicos para la destrucción rápida de la adrenalina. Esta destrucción está supervisada por un enzima llamado aminooxidasa, que se combina con la adrenalina y la mantiene quieta, hablando vulgarmente, en tanto la transforma en sustancia inofensiva.

Mas ¿y si dicha enzima se halla ocupado en otro sentido? Ordinariamente, las enzimas son muy específicas, y solamente se ocupan de ciertas moléculas que poseen una forma particular, sin trabajar con otras. Ésta es la forma de trabajo de las enzimas llamadas de «llave y cerradura» (ver Capítulo 7). Una llave especial abre una cerradura particular y sólo ésta.

Sin embargo, la especificación de las enzimas no es perfecta. Una enzima puede combinarse con una molécula que casi tenga la forma de la debida. Entonces, la molécula equivocada compite con la buena para unirse con la enzima, y si ésta está ocupada con la primera no puede trabajar con aquélla, de forma que su acción se ve inhibida. A este fenómeno se le llama «inhibición competitiva», y puede ser grave.

Cuando la enzima se une a la molécula debida, efectúa una tarea en la misma, y la abandona; pero cuando se une con una molécula equivocada puede quedar más o menos permanentemente unido a ella, como una llave errónea queda encajada en una cerradura, y hay que romperla.

Cuando tal ocurre, incluso una diminuta cantidad de molécula equivocada puede provocar un trastorno químico continuo, que perjudique al organismo, llegando en ocasiones a provocar la muerte. Generalmente, así es como actúan los venenos.

Tal vez, pues, algunan enzima, aminooxidasa o de otra clase, se halle sujeta a la inhibición competitiva por algo que se forma en ausencia del NAD, y no en su presencia.

La posibilidad de que esta inhibición competitiva se halle presente en este proceso queda subrayada dramáticamente por el caso de un cacto, natural de América del Sudoeste, que contiene el compuesto denominado mescalina. La molécula de mescalina posee cierta semejanza general con la adrenalina, lo bastante semejante en realidad para permitir que la mescalina se interfiera en el amino-oxidasa. Esta clase de interferencia, incluso con un simple enzima, puede ejercer un efecto muy amplio sobre la función cerebral. Las funciones químicas del cerebro pueden compararse con un vasto encaje tridimensional, intrincadamente interconectado. Un tirón dado en una parte mueve todas las demás, hasta cierto punto. En consecuencia, cuando las partes del cacto que contienen la mescalina se mastican, el enzima que destruye la adrenalina se ocupa con aquélla, y la adrenalina se acumula, produciendo toda clase de efectos raros. Una persona experimenta entonces percepciones sensoriales sin existencia objetiva. Ordinariamente, los objetos cambian de forma, color o valor. En resumen, la mescalina produce alucinaciones, por lo que es un «alucinógeno».

Además, las reacciones del masticador de mescalina son inapropiadas al universo real. Dependen de sus distorsionadas percepciones sensoriales, y a veces, ni siquiera se hermanan con éstas. Su comportamiento es peculiar e imprevisible. Los indios del Sudoeste, que experimentan todo esto cuando mastican dicho cacto, supusieron que ello les abría la puerta del mundo situado fuera del de los sentidos ordinarios. Por tanto, emplearon la mescalina en los ritos religiosos.

La conducta inducida por la mescalina se asemeja a la de los esquizofrénicos, y es natural llegar a pensar si es posible que se forme dentro del organismo un producto químico que cause efectos similares a los de la mescalina.

Tal vez ese producto químico se forme con más facilidad cuando existe deficiencia de NAD, de modo que las personas que nacen con una tendencia a desarrollar ineficiencias en las reacciones que fabrican el NAD se hallen sujetas al efecto de dichos productos químicos.

Es posible alterar fácilmente la adrenalina en el tubo de ensayo, formando un compuesto ligeramente cambiado llamado adrenocromo. Éste, inyectado en el fluido sanguíneo, produce ataques temporales de conducta similar a la esquizofrénica. En realidad, el adrenocromo no se forma en el organismo normal, aunque sí podría formarse tal vez en el esquizofrénico.

Resulta de interés, por tanto, estudiar y analizar con detalle estas partes del organismo esquizofrénico que pueden obtenerse fácilmente y analizarse, como por ejemplo, la sangre o la orina. Cualquier sustancia que se halle en todos, o casi todos los esquizofrénicos, y no se encuentre en todos, o casi todos los seres normales, resulta instantáneamente sospechosa.

Una forma de analizar los fluidos corporales es usar una técnica llamada del «papel cromatográfico». Se extienden diferentes clases de moléculas de los fluidos orgánicos, haciéndoles ocupar puntos separados, sobre fragmentos de papel poroso. Estos lugares pueden hacerse visibles dejando que las moléculas que los ocupan sufran una reacción química que produce un material colorante.

En 1962, Arnold J. Friedhoff, de la Universidad de Nueva York, descubrió que con un cierto curso de tratamiento podía obtenerse un punto rosado de la orina de quince de cada diecinueve esquizofrénicos, y ni uno de cada catorce normales.

Desde entonces, se han realizado análisis similares en mayores cantidades de personas. En una serie de experimentos, llevados a cabo por C. A. Clarke, de la Universidad de Liverpool, no se encontró ni un solo punto rosado en doscientas sesenta y cinco personas sanas, ni en ciento veintiséis enfermas de otras dolencias distintas a la esquizofrenia. Sin embargo, se hallaron puntos rosados en cuarenta y seis de ochenta y cuatro esquizofrénicos. La mayoría de estos que no presentaron el punto rosado pertenecían a la variedad paranoica. Entre los no paranoicos, había el punto rosado en cuatro de cada cinco.

¿Qué era el punto rosado? Resultó ser un producto químico llamado dimetiloxifeniletilamina (DMPE), y su estructura se halla situada entre la adrenalina y la mescalina.

Dicho de otro modo, ciertos esquizofrénicos (bien por carencia de NAD o por otras causas), forman sus propios alucinógenos, y se hallan, en realidad, bajo una embriaguez permanente de mescalina.

Esto no es más que un mero principio en el ataque fisicoquímico de la esquizofrenia, pero es un principio esperanzador. El punto rosado (y los demás productos químicos que puedan formarse) ayudarán a los médicos a descubrir las causas de la esquizofrenia mucho antes que de otras formas, en un momento en que la terapia sea más fácil. Estudiando los procesos químicos que dan lugar a la presencia del punto rosado, puede detectarse la sección anormal del mecanismo químico del ser esquizofrénico, otorgándosele entonces el tratamiento adecuado.

Pero la adrenalina no es el único producto químico que se halla íntimamente relacionado con las funciones cerebrales. También existe una sustancia denominada serotonina.

La importancia de la serotonina se puso de manifiesto en relación con la dietilamida del ácido lisérgico, hoy día conocida mundialmente como LSD. El LSD posee una estructura más complicada que la serotonina, pero los químicos pueden hallar fácilmente una «cadena» de serotonina en la molécula de LSD. No es sorprendente, por tanto, que el LSD pueda competir con la serotonina por un enzima particular, como el DMPE compite con la adrenalina, y con idénticos resultados. En otras palabras, la ingestión de LSD puede provocar la acumulación de serotonina en el cerebro, con la aparición de síntomas esquizofrénicos.

Esto se descubrió por casualidad en 1943, cuando el químico Albert Hoffman trabajaba con LSD, con propósitos puramente químicos. Sin querer, se llevó a los labios algunos cristales de LSD incrustados en las yemas de sus dedos, y cayó en un estado de postración, semejante al ensueño, que le impidió seguir trabajando. Regresó a su casa, experimentando una especie de fantasía alucinatoria. Supuso que era debida al LSD y al día siguiente (con notable valor) tomó una cienmilésima de onza de dicho producto, arriesgándose sólo con lo que creía era una dosis pequeñísima. En realidad, era muy grande, ya que con una décima parte habría tenido suficiente. Volvió a sufrir las fantasías y alucinaciones del día anterior…, y ya lo demás es historia.

Hoffman volvió a sentirse completamente normal al cabo de veinticuatro horas, sin sufrir perjuicio alguno, ni causárselo a los demás estando bajo la influencia del LSD.

Por desgracia, no es ésta la regla general. Cada ser humano posee una maquinaria química propia, de modo que los efectos inducidos por el LSD varían de un individuo a otro. Uno experimentará un caso débil de fantasía, y otro uno grave; algunos se recuperarán rápidamente, y otros con mucha más lentitud.

La maquinaria química es, en algunos individuos, más frágil en puntos claves que en otros, en el sentido de que pueden ser más propensos a romper dichos puntos. Si el punto en cuestión es de los que producen la esquizofrenia al romperse, no es aconsejable tomar LSD.

Ordinariamente, el punto frágil del esquema químico puede resistir toda una existencia de tensiones ordinarias, de forma que un ser humano puede ser propenso a la esquizofrenia sin llegar a desarrollarla jamás. Sin embargo, bajo el poderoso influjo del LSD, el punto cede, y lo que para algunos sería sólo un experimento temporal y desusado, para otros se convierte en un cambio permanente y grave.

Como nadie sabe cuál es la firmeza de los puntos cruciales del esquema químico personal, el uso del LSD sin el mayor cuidado profesional es una especie de ruleta rusa mental. Es una invitación a la locura temporal para todo el mundo… y a la locura permanente para algunos.

El LSD es un instrumento importante para la investigación de las enfermedades mentales. Estudiando las causas de dichas dolencias hallaremos la curación adecuada.

Esto lo vemos en el ejemplo de los investigadores médicos que, hace un siglo, estudiaban las más peligrosas bacterias con el fin de hallar una cura para las enfermedades infecciosas. Es de suponer, y desear, que en esta mitad del siglo XX, las enfermedades mentales sean, en este sentido, lo mismo que las infecciones lo fueron en la segunda mitad del XIX.

Pero existe una importante diferencia. Los estudiantes universitarios de finales del siglo pasado no pensaron jamás que fuese una diversión excitante inyectarse los bacilos del cólera.

2. Yo recuerdo, yo recuerdo

Es corriente asociar la buena memoria con una gran inteligencia. Los programas de acertijos, muy populares hace algunos años, se consideraban ampliamente como el resultado de un genio, cuando en realidad se apoyaban en trucos de memoria que a veces (no siempre, claro) se hallaban totalmente en desavenencia con los criterios actuales de una mentalidad poderosa.

Un ejemplo que salió a la luz a este respecto fue el caso de los mellizos que saltaron a la actualidad periodística por su habilidad de dar el día de la semana para cualquier fecha que se les nombrara, aunque perteneciese a miles de años del pasado: y hacerlo rápida y correctamente.

Se ignora cómo lo lograban. ¿Habían logrado aprenderse de memoria el calendario? ¿O un resumen del mismo, por semanas? ¿Conocían el día de la semana mediante fechas clave y lo calculaban rápidamente gracias a éstas? Bien, es imposible afirmarlo. Ni siquiera pueden explicarlo los propios mellizos. Ya que son mentalmente retrasados.

Más aún, esta habilidad no era compartida con ninguna otra clase de cálculo. Incluso las sumas y las sustracciones más sencillas se hallaban fuera de su alcance.

Tales prodigios han sido bastante corrientes a lo largo de la Historia. Un inglés del siglo XVIII, Jedediah Buxton, multiplicaba las cantidades 23.145.789 x 5.642.732 x 54.965, mentalmente, con suma rapidez, y daba el total adecuado, y sin embargo tenía una mentalidad embotada y jamás pasó de ser un triste obrero toda su vida. Zerah Coiburn, nacido en Vermont en 1804, podía dar el resultado de 816 (o sea la multiplicación de 8 dieciséis veces por sí mismo) en unos segundos, y extraía la raíz cúbica de 268.336.125 casi al instante. Sin embargo, no fue una persona notablemente inteligente.

Existen diversos casos similares. ¿Cómo actúan? Probablemente, se trata de una casi indeleble memoria para los números. Los cálculos que llevan a cabo en su cerebro puede efectuarlos cualquier persona normal sobre el papel, mediante los cálculos parciales y otros intermedios.

El prodigio calculador escribe tales resultados parciales en su cerebro y los suma interiormente. Hay casos de personas prodigio que pueden hallar la mitad de la clave de un problema, ocuparse de otras cosas, y tras un largo período de tiempo, volver al problema abandonado, recordar lo descubierto y solucionarlo totalmente sin vacilación. Si hacen esto, ello se debe a una práctica constante e intensa de una mente dedicada sólo a la misma.

No es necesario poseer una inteligencia más o menos normal para ser un calculador prodigioso. Grandes matemáticos como André M. Ampére, John Wallis, Leonhard Euler y, el mayor de todos. Carl Friedrich Gauss, poseían una memoria prodigiosa. Sin embargo, la misma, si bien les ayudaba en su labor como matemáticos, no era la causa de su genio.

Dejando aparte los casos de personas prodigio, supernormales o subnormales de inteligencia, hallamos que, en general, la memoria y la inteligencia se armonizan siempre. Cuanto más inteligente sea una persona, mayor será su memoria. La extensión del vocabulario que se comprende y emplea es, por ejemplo, un buen indicio de la eficacia de la memoria personal y la extensión de la inteligencia en el mismo individuo.

Entonces, si se nos preguntara por qué una persona posee más memoria que otra, sólo podríamos responder que ello se debe a la misma causa que hace que un ser humano sea más inteligente que otro.

Las teorías sobre la memoria, antiguas o modernas, parecen apoyarse en una de estas dos posibilidades: memoria por asociación de ideas, o memoria por imágenes.

Casi todo el mundo acepta estas dos teorías como auténticas. Nos atamos un hilo al dedo para recordar que hemos de comprar pan, y cada vez que vemos el hilo, exclamamos:

–¡Oh, sí, tengo que comprar pan!

Al cabo de unas cuantas veces, este asunto queda firmemente grabado en nuestro cerebro. La asociación de ideas se ha convertido en una imagen.

El psicólogo ruso Iván P. Pávlov consiguió establecer los «reflejos condicionados» de los animales mediante asociaciones continuas. En algunos experimentos llevados a cabo a principios de este siglo, hacía sonar un timbre, luego le mostraba comida a un perro y éste respondía con la insalivación. Eventualmente, tras varias repeticiones, el perro asociaba el timbre con la comida de forma tan intensa que insalivaba sólo con oír el timbre sin ver la comida. El mecanismo de insalivación del animal «recordaba» ya que el timbre significaba comida.

Esto condujo a una escuela de psicología que, en su forma más extrema, afirmaba que todas las enseñanzas y todas las respuestas eran el resultado de los reflejos condicionados. Era como si uno recordase un poema de memoria, por asociar cada frase con la anterior; o porque cada frase estimulase la siguiente como una respuesta condicionada.

Sin embargo, no hay duda de que la memoria no es meramente una secuencia de causa y respuesta, de una cosa que recuerde otra que, a su vez, recuerda otra y así sucesivamente. Es posible recordar en imágenes.

Si se me permite ponerme como ejemplo (yo conozco muy bien mi propia memoria), yo poseo una memoria indiferente para los números. No puedo multiplicar de memoria dos números de tres dígitos sin un gran esfuerzo.

Sin embargo, poseo un mapa muy claro de los Estados Unidos grabado en mi mente y puedo mirarlo y copiar los nombres de todos sus Estados con la misma rapidez con que escribo. (De joven, solía ganar apuestas escribiendo los nombres de todos los Estados en menos de cinco minutos.)

La memoria también se cuenta por variaciones de duración. Hay memorias de corto plazo y otras de largo plazo. Si uno busca un número telefónico, no es difícil recordarlo hasta que se ha marcado; después suele olvidarse automáticamente. Sin embargo, un número telefónico usado frecuentemente entra ya en la categoría de la memoria a largo plazo. Y es posible recordarlo incluso al cabo de varios meses.

Es fácil suponer que una memoria comienza con un plazo corto y se convierte en otra de plazo largo con el uso. Para comprender esto, consideremos la estructura del sistema nervioso.

El sistema nervioso está compuesto de numerosas células microscópicas llamadas neuronas. Éstas son de forma irregular, con unas proyecciones muy finas que surgen en diversas direcciones. Estas proyecciones se denominan dendritas, término griego que significa árbol, porque en realidad se asemejan a las ramas de un árbol. Cada neurona posee una prolongación particularmente larga llamada axón. Las dendritas de una neurona pueden aproximarse mucho al axón de otra, pero sin tocarse jamás. Y la diminuta brecha existente es una sinapsis.

Una neurona, al ser estimulada, es capaz de transmitir una ligera corriente eléctrica a lo largo de su superficie, que descienda por sus proyecciones. Ordinariamente, la corriente se detiene en una sinapsis, pero bajo ciertas condiciones, el ambiente químico de la misma cambia de tal forma que permite que la corriente salte la brecha y pase a través de otra célula. Saltando de una sinapsis a otra, una corriente eléctrica puede seguir un sendero específico desde un punto a otro del sistema nervioso.

Supongamos, pues, que con cada sensación que se recibe, un grupo particular de sinapsis queda afectado de forma que deje pasar con más facilidad la corriente nerviosa. Este grupo de sinapsis está tan bien escogido que la corriente fluye de una célula a otra, y a otra, hasta regresar a la célula de origen, formando un circuito cerrado.

Naturalmente, este ciclo persiste durante un período de tiempo, como los corredores de carreras en tomo a la pista. La sensación original y un ciclo de corriente particular pueden considerarse asociados. En tanto el organismo puede experimentar un ciclo de corriente particular y seleccionarlo de entre otros (aunque se ignora cómo tiene lugar esta selección), puede recordar la sensación que originó ese ciclo de corriente especial.

Con el tiempo, no obstante, se desvanece el efecto en las sinapsis, el ciclo de corriente desaparece, y la memoria se esfuma. Se trata de una memoria a corto plazo.

Pero cada vez que se experimenta el ciclo de la corriente y regresa la memoria, es posible que se intensifique el cambio en las sinapsis, de modo que la corriente sea más potente. Eventualmente, puede cambiar hasta la estructura física de las células, y pueden formarse más dendritas entre las células que componen el ciclo, facilitando el paso de la corriente. Casualmente, la corriente puede quedar tan firmemente establecida que continúe indefinidamente sin reactivación adicional. La memoria se ha convertido en una de largo plazo.

Naturalmente, cuanto más tiempo ha tenido existencia un ciclo de corriente, más firmemente puede asentarse, siendo para muchos de nosotros, por lo tanto, considerablemente más fácil recordar cosas aprendidas en la juventud, que las cosas del año anterior.

Tal vez en casos excepcionales, los cerebros se hallen construidos de modo que algunos tipos de memoria a largo plazo, como los relativos a los números, se formen con una especial facilidad, dando lugar a prodigios mentales, aunque el cerebro no se halle construido asimismo para ser especialmente inteligente. Tal vez algunos tipos de ciclo de corriente se formen a través del uso, con más facilidad y se establezcan antes que otros, de modo que tengamos una persona que recuerde los nombres pero no los rostros, o bien el profesor distraído que posee una memoria excelente para todo lo relacionado con su ocupación, pero encuentra difícil incluso acordarse de la dirección donde vive.

Pero, ¿existe suficiente espacio en el cerebro para todos los distintos ciclos de corriente? Se calcula que el cerebro, durante una existencia entera, absorbe hasta mil billones (1.000.000.000.000.000) de fragmentos separados de información.

En el cerebro hay aproximadamente diez mil millones (10.000.000.000 de células grises o neuronas, y unas nueve veces más de células auxiliares o gliacitos. (Algunos sugieren que los gliacitos, o células auxiliares, se ocupan de la memoria a corto plazo, mientras que las neuronas mayores están relacionadas con la de largo plazo.) Si cada ciclo de corriente ocupa sólo a dos células, hay espacio para más de un trillón de ciclos…, espacio para diez millones de veces como memorias podrían acumularse en toda la vida. Naturalmente, existen grandes cantidades de células que no están contiguas, pero por otra parte, los ciclos de corriente pueden ocupar a más de dos células, incluso a varias docenas si es necesario. En este último caso, hay espacio suficiente para todos los ciclos de corriente necesarios.

Es posible que el cerebro no sólo posea amplio espacio para todos los ciclos necesarios, sino para establecer cada ciclo con muchas copias, puesto que es posible efectuar operaciones quirúrgicas en el cerebro sin perjudicar gravemente a la memoria. Si se eliminan algunas copias de ciclos individuales, hablando vulgarmente, mediante la cirugía, en otras partes del cerebro permanecen otras copias intactas.

Por tanto, es posible asegurar algo tan obvio como que la memoria a corto plazo puede transformarse en otra de plazo largo. A veces, cuando se estimulan eléctricamente ciertas partes del cerebro (por razones legítimas durante las intervenciones quirúrgicas), el resultado es un flujo de recuerdos. Este flujo se llena con detalles de tal verosimilitud que el paciente revive virtualmente una parte de su vida pasada, aun cuando continúa plenamente consciente del presente. Wilder G. Penfield, de la Universidad McGill, podía de esta forma lograr que un paciente, a voluntad, oyese fragmentos musicales evocando escenas de su niñez.

Esta clase de descubrimientos le tientan al científico a suponer que el cerebro contiene una impresión perfecta e indeleble de todas las sensaciones que recibe. Todas las memorias son de largo plazo, pero se ven obstruidas a menos que se evite con la continuada repetición. (En cuyo caso, los prodigios poseerían un mecanismo de obstrucción imperfecto.)

Para Sigmund Freud y sus seguidores, esta obstrucción de la memoria no es automática ni mecánica, sino que entraña un proceso activo, aunque sea inconsciente. Los recuerdos individuales se olvidan por algún motivo; por ser tristes, penosos, embarazosos, humillantes, porque atraen el castigo, porque no encajan con un plan de vida elegido. Se trata del proceso de «represión».

Esta represión no es perfecta, y algunos analistas sugieren que la neurosis es el resultado de la imperfección en el acto de olvidar. Lo que la mente querría olvidar se recuerda inconvenientemente, y ha de ser enmascarado, a menudo de forma irracional (como en la neurosis). La cura de la neurosis depende, según Freud, en sacar a luz la memoria a través de la libre asociación, del análisis de los sueños, o mediante otras técnicas. Una vez se pone al descubierto la memoria, puede ser tratada racional y no neuróticamente.

No todos los psiquiatras, no obstante, pertenecen a la escuela freudiana, y es posible argüir que el olvido no es siempre una represión penosa. Si el cerebro es un instrumento perfecto de la memoria, para la supervivencia es necesario un olvido selectivo. Si un individuo recordase todos los números de teléfono vistos u oídos, qué difícil resultaría acertar con el que desea, entre los otros más triviales que ya nunca se necesitan.

En efecto, ¿cuál es el mecanismo del recuerdo? Muchos recuerdos permanecen, aun después del olvido selectivo. ¿Cómo elige el individuo en su mente un recuerdo entre todos los demás similares?

Volviendo a personalizar, poseo una memoria bastante buena en relación con los nombres y las fechas históricos. Si me preguntan cuándo falleció la reina Isabel II contestaré que en 1603 sin pausa perceptible, y diré que en 336 antes de Cristo, si la pregunta es la fecha en que fue asesinado Filipo de Macedonia. Ignoro cómo recuerdo estas fechas. Las cito sin esfuerzo visible, y no conozco ningún sistema especial en mi cerebro.

La dificultad de determinar en qué lugar del cerebro se concentran los ciclos de corriente de la memoria, de intentar seguirlos una vez localizados y, además, descubrir los que acaso existan, son problemas de primera magnitud. ¿Es posible, entonces, pasar el ataque a otra zona, de la fisiología y las células a la química y las moléculas? En 1874, el biólogo inglés T. H. Huxiey sugirió que existía una molécula clave por separado, en el cerebro, por cada recuerdo en particular. El paso de las células, que al menos pueden ser visibles, a moléculas, invisibles, no es, en realidad, una dificultad insuperable. Se parece al cuento del médico que le recetó al paciente de un resfriado que se duchase con agua fría y se sentase en una corriente de aire.

–¡Pero, doctor -exclamó el enfermo-, esto hará que mi constipado se convierta en una pulmonía!

–Exactamente -asintió el médico-, y nosotros sí podemos curar las pulmonías.

Por el año 1950, los bioquímicos se mostraron altamente confiados en que cierto compuesto muy intrincado llamado ácido ribonucleico (abreviado usualmente como ARN) se hallaba mezclado en la fabricación de las proteínas. Esto casaba muy bien con anteriores descubrimientos, según los cuales el ARN estaba presente, en alta concentración, en las células que fabricaban cantidades inusitadas de proteínas. Entre las mismas, se hallaban células que crecían y se multiplicaban, y otras que producían grandes cantidades de secreciones ricas en proteínas.

Sin embargo, y de forma extraña, la célula más rica en ARN era la cerebral, y las células cerebrales jamás crecen, se multiplican ni producen secreciones. ¿Por qué, entonces, todas tenían ARN?

Un neurólogo sueco, Holger Hyden, atacó este problema en la Universidad de Gothemburgo. Desarrolló técnicas que podían separar las células cerebrales y analizar su contenido de ARN. Experimentó con ratas colocadas en condiciones en las que se veían obligadas a aprender nuevas habilidades, por ejemplo, haciendo equilibrios largo tiempo sobre un alambre. En 1959, descubrió que las células cerebrales de las ratas se veían obligadas a aumentar su contenido de ARN hasta un 12 por ciento más que el de las ratas existentes en condiciones normales.

Esto implicaba que el ARN se halla relacionado con el aprendizaje, y por tanto, con la memoria (sin la cual, es imposible aprender). Pero, ¿es esto concebible? Concediendo que una serie de cien mil millones de células pudiesen incluir ciclos de corriente en cantidad suficiente para retener los recuerdos de toda una vida, ¿cómo sería posible apretujarlos dentro de la estructura de una sola molécula?

La molécula de ARN se compone de una larga cadena de cuatro unidades estrechamente relacionadas, aunque sensiblemente diferentes. Cada punto de esta cadena puede ser una de las cuatro unidades A, B, C o D. Dos unidades contiguas pueden ser cualquiera de las 4 x 4, o dieciséis, combinaciones diferentes de dos unidades: AA, AB, AC, AD, BA, BB, BC, BD, CA, CB, CC, CD, DA, DB, DC o DD. Estas unidades contiguas pueden ser cualquiera de las 4 x 4 x 4, o sesenta y cuatro combinaciones diferentes, y así sucesivamente.

El posible número de combinaciones se eleva a una cantidad enorme. Una molécula de ARN compuesta solamente de veinticinco unidades puede poseer cualquiera de mil millones de combinaciones diferentes, si cada unidad molecular puede ser cualquiera de las cuatro clases diferentes. Eso significa que si cada sensación diferente experimentada por un ser humano en el transcurso de su existencia fuese «archivada» en su cerebro como una combinación de unidades ARN diferente, bastaría para tal tarea una molécula de veinticinco unidades.

Pero las moléculas de ARN contienen muchos centenares de unidades y no sólo veinticinco. Por tanto, no hay duda de que la molécula de ARN representa un sistema de archivo completamente capaz de manejar cualquier cantidad de enseñanza y de recuerdos que pueda contener el ser humano… y aún un billón de veces más.

Supongamos que dibujamos una especie de «memoria ARN». Todas las células pueden fabricar rápida y fácilmente moléculas de ARN, pero las células ordinarias sólo pueden prepararlas en una variedad limitada, a fin de realizar ciertas tareas también limitadas. ¿Y si las células cerebrales pueden prepararlas en series ilimitadas de combinaciones? Cada sensación diferente podría provocar la producción de una molécula de ARN ligeramente diferente. Y el uso de tal molécula en el futuro podría hacer regresar la sensación asociada como un recuerdo.

Hyden descubrió que el ARN, en sus ratas estimuladas para el aprendizaje, cambiaba de naturaleza lo mismo que aumentaba su producción. Cambiaban los promedios de las cuatro unidades diferentes, aunque las ratas, al formar nuevas combinaciones, utilizaban las diferentes unidades en proporciones distintas de las requeridas ordinariamente.

¿Cómo responde una célula cerebral a una sensación formando una molécula de ARN? ¿Forma cualquier combinación al azar y dicha combinación es «asignada» al recuerdo de la sensación particular que provocó su formación? Si tal fuera el caso, ¿no podría formarse una molécula de ARN ya formada en otra ocasión, confundiéndose de este modo los recuerdos? La respuesta a esta última pregunta es: probablemente, no. El número de combinaciones posibles es tan enorme que las probabilidades de un duplicado por casualidad son virtualmente nulas.

No obstante, también cabe la posibilidad de que sea fija la combinación ARN para una sensación particular; que dicha sensación particular dé lugar a la misma molécula ARN en cada ser humano, en cada ser vivo.

La posible elección entre estas alternativas surgió de la labor de James V. McConnell, en la Universidad de Michigan. Experimentó con gusanos planos (planarias), de unos cuatro centímetros de longitud. Los expuso a un rayo de luz y después a un shock eléctrico. De este modo quedaron condicionados, aprendiendo que después del rayo de luz venía el shock eléctrico, y presumiblemente formaron nuevas moléculas ARN para ocuparse de este nuevo aprendizaje, de este nuevo recuerdo.

Esas planarias fueron seleccionadas y dadas como alimento a otras sin entrenar, que después quedaron expuestas al mismo proceso. En 1961 McConnell informó que las planarias no entrenadas que comían secciones de las entrenadas aprendían a reaccionar más de prisa por efecto de la luz que las ordinarias. Se habían incorporado las nuevas moléculas ARN de su alimento y «habían comido memoria».

Esto significa que una molécula ARN particular se halla unida a una sensación particular. Las combinaciones moleculares no se eligen al azar, puesto que la molécula ARN formada por la planaria 1 en respuesta a ciertas sensaciones, «tenía sentido» para la planaria 2.

Alian L. Jacobson, que colaboró con McConnell, continuó dichos experimentos en la Universidad de Chicago. Si una planaria se come a otra, es difícil decir qué molécula de la comida se utiliza. ¿Por qué, entonces, no extraer ARN de planarias condicionadas e inyectarlo sólo a las no condicionadas? Esto también daría el mismo resultado. Se inyectó el condicionamiento junto con el ARN naturalmente.

¿Por qué limitarse a las planarias? (Algunos investigadores afirmaron que la respuesta de una planaria era tan difícil de observar que nadie podía asegurar cuáles estaban condicionadas, o si lo estaban en absoluto.) Jacobson condicionó ratas y conejillos de indias, que respondían al sonido de un chasquido o al rayo de luz, acudiendo al comedero. Una vez condicionados eran matados y se inyectaba el ARN del cerebro a otros animales no condicionados. Los animales que recibían tales inyecciones resultaban más fáciles de adiestrar, ya que poseían una parte del necesario ARN que precisaban formar. De modo interesante, la inyección servía asimismo de una especie a otra, de manera que una rata se beneficiaba si se le inyectaba ARN de un conejillo de indias.

Cuando se publicaron los resultados de la labor de McConnell con las planarias, se produjeron algunas bromas y chistes (y ojalá fuesen sólo bromas) respecto a que si un estudiante se comía a un profesor ingería también todos sus conocimientos.

También existen otras alternativas. Tal vez serviría de ayuda cualquier suplemento de ARN… como material crudo adicional. Se ha afirmado que una inyección de ese ARN no condicionado ha producido mejorías en la frontera de la habilidad de aprender.

Bien, ¿por qué no alentar al organismo a formar mayores cantidades de ARN por sí mismo? Cierta droga llamada Cylert (su nombre químico es pemolina de magnesio) aumenta la producción de ARN del 35 al 40 por ciento. Si se emplea con ratas, mejora notablemente su condicionamiento.

Se han efectuado, con gran cautela, esta clase de experimentos en seres humanos, específicamente en pacientes que padecían senilidad prematura. El doctor Ewen Cameron, del Centro Médico de Albany, informó que al menos 17 de cada 24 pacientes presentaban señales de mejoría.

Existe, pese a todo, una declaración publicada el 5 de agosto de 1966, en un ejemplar de Ciencia, a cargo de un grupo de científicos de ocho laboratorios diferentes que aniquila la euforia de estos resultados positivos. Todos sus intentos de transmitir el condicionamiento junto con el ARN de unas ratas adiestradas a otras sin entrenamiento fracasaron absolutamente.

Sin embargo, esto no debe desesperar ni desesperanzar o confundir a los que aguardan o ansían adelantos sorprendentes. Los científicos se encuentran a medio camino en este aspecto de la investigación, aspecto tremendamente sutil y difícil. Generalmente, diferentes laboratorios llevan a cabo experimentos complicados con variaciones que parecen carentes de importancia, si bien podrían resultar vitales cuando se conozcan todos los factores. La medicación de la habilidad de aprender es, además, un proceso especialmente engañoso, pues lo que un investigador cree enseñanza, a otro no se lo parece.

Ciencia lo expresó de esta forma: «El fracaso en reproducir los resultados no es, al fin y al cabo, extraordinario en las prístinas fases de una investigación cuando todavía no se han especificado todas las variables relevantes.»

Los resultados negativos no indican necesariamente que el ARN no se halle relacionado con el mecanismo de la memoria, o que la misma no pueda ser transmitida. Indica, eso sí, que la técnica de tal transmisión aún no ha sido perfeccionada, lo que, en esta fase tan temprana del juego, no es sorprendente.

No es posible considerar en sí mismas a las moléculas de ARN. Proceden de algún sitio. Se sabe, por ejemplo, que las moléculas específicas de ARN se forman como copias de otras similares, pero más complicadas, llamadas ADN, en el núcleo celular. Se ignora si se forman combinaciones de ARN recientes dentro de la célula, y muchos científicos dudan que las sensaciones recibidas puedan formar moléculas ARN directamente.

Las moléculas ADN constituyen los genes, o unidades de la herencia, y éstas se transmiten de padres a hijos mediante un mecanismo muy complicado aunque efectivo.

Cada célula contiene una larga cadena de moléculas ADN, y cada parte de tales moléculas puede producir una copia ARN de una estructura determinada. Tal vez algunas moléculas ADN sirvan de modelo desde el principio, y a través de ellas, las células pueden formar los tipos ARN que necesitan para el funcionamiento ordinario de su maquinaria química.

Otras partes de las moléculas ADN quizá se hallan bloqueadas desde el principio. Entonces, una sensación dada podría servir para desobstaculizar una sección especial de las moléculas ADN, formándose una molécula ARN semejante a la sección no bloqueada.

Esto significaría que todo ser humano lleva consigo un gran depósito de posibles memorias, un «banco de recuerdos», en la molécula ADN con que ha nacido, un depósito suficiente para cuidarse de todas las contingencias razonables. La naturaleza de este «banco de recuerdos» sería semejante entre individuos de una especie o de varias especies estrechamente vinculadas entre sí. Esto haría comprensible por qué una molécula ARN que produce un recuerdo particular en un individuo, provoca otro recuerdo similar en otro, y por qué las enseñanzas aprendidas pueden transmitirse.

Si el ARN no es el principio, tampoco es el final. La principal función de las moléculas ARN, por lo que sabemos, es llevar información a las moléculas de proteínas. Cada molécula diferente de ARN está implicada en la formación de una molécula diferente de proteína. ¿Sería posible que fuese la molécula proteica y no la ARN la que verdaderamente se hallase relacionada con la función de la memoria?

Una forma de comprobarlo es utilizando una droga llamada puromicina. Esta interfiere con la maquinaria química, por medio de la cual la célula produce proteína mediante el ARN, pero no afecta a la formación de éste.

Louis B. Flexner y su esposa Josepha, llevaron a cabo experimentos con puromicina en la Universidad de Pennsylvania. Primero, condicionaron ratones de un grupo simple enseñándoles a seguir el Camino A para evitar un shock. A los ratones así condicionados se les inyectó puromicina y pronto olvidaron lo que habían aprendido. La molécula ARN seguía presente, pero no podía ya formarse la molécula clave de proteína. (Una vez desvanecidos los efectos de la puromicina, los ratones pudieron volver a ser adiestrados.

La pérdida de la memoria dependía del momento en que se había inyectado la droga. Si los Flexner esperaban más de cinco días, la puromicina no inducía al olvido. Era como si se hubiese formado algo permanente; como si la memoria a corto plazo se hubiera trocado en otra a largo plazo, y sólo la primera pudiera verse afectada por la puromicina.

Otro ejemplo se refería al aprendizaje invertido. Después de haber aprendido el ratón a seguir el Camino A para evitar el shock, lo recibía de repente cada vez que se internaba en dicho Camino A. Una vez el ratón había aprendido esta función inversa, se le inyectaba puromicina. La memoria del Camino B, aún de corto plazo, quedaba borrada, y no quedaba afectada la memoria de largo plazo del Camino A. El ratón volvía a seguir este último Camino.

Todo esto puede estar relacionado con los ciclos de corriente mencionados al principio de este capítulo. Supongamos que el ARN forma proteínas, éstas contribuyen a la formación de nuevas dendritas o, tal vez, a la activación de las antiguas. Si éste es un efecto gradualmente fortalecedor, durante los primeros días los nuevos ciclos son débiles y pueden cortarse con facilidad si queda interferido el flujo de una proteína específica con, por ejemplo, inyecciones de puromicina. Eventualmente, las dendritas se alargan hacia el punto donde el ciclo de corriente es firme e intenso, y no requiere más proteínas. Después de esto, la puromicina no ejerce ningún efecto.

Pero esto implica que la memoria a corto plazo se convierte en otra de largo plazo. ¿Y cuando es a la inversa?

Jacobson (que transmite ARN de un ser vivo a otro) condicionó unas planarias y después invirtió este condicionamiento. Transmitió el nuevo ARN a otras planarias y descubrió que había transmitido la tendencia al condicionamiento, pero no al condicionamiento invertido.

Es posible, por tanto, que cuando se obliga a una planaria a olvidar lo que ha aprendido, las moléculas ARN formadas en el proceso no se eliminan, sino que únicamente quedan bloqueadas de alguna manera. Si sólo se transmite el ARN y no el agente obstructor (sea cual sea), sólo se transmite la memoria y no el olvido. Lo cual nos devuelve a la hipótesis de que toda la memoria es de largo plazo y que las células nerviosas pasan el tiempo disponiendo un mecanismo de olvido y no de recuerdo… a la Freud.

Toda esta labor sobre la memoria resulta muy excitante y alienta toda clase de esperanzas (y temores) para el porvenir. ¿Podemos mejorar nuestra memoria tomando pildoras? ¿Podemos aprender más de prisa y educarnos mejor mediante el estímulo químico? ¿Podemos llegar a ser más inteligentes? ¿Podemos ajustar las mentes, por medio de manipulaciones externas, a nuestras necesidades? ¿Podemos, por medio de nuestro esfuerzo, cambiar al Hombre en un Hombre Mejor? ¿O algunos decidirán que lo necesario es cambiar al Hombre en un Hombre Más Dócil?

Las consecuencias, en bien o en mal, no son inminentes. A pesar de todo el entusiasmo y la excitación de los últimos años, sólo nos hallamos en el primer tramo de un camino rocoso que se desvanece en el horizonte visible.

3. La gente hambrienta

Es tremendamente fácil sermonear a los que pesan demasiado. Se les puede asustar con la posibilidad de una muerte temprana, y ordenarles bruscamente que coman menos. Se les puede insinuar amablemente que realicen ejercicios especiales, como apartar la silla de la mesa a media comida o girar la cabeza vigorosamente a derecha e izquierda, cuando se les ofrece un segundo plato.

No hay nada tan sencillo, al parecer, como seguir tales consejos. Entonces, ¿por qué tanta gente aumenta de peso, aunque la gordura se considere incómoda, poco atractiva y peligrosa para la salud? ¿Qué le obliga a la persona gruesa a seguir comiendo? Existe la popular explicación de que la gordura es esencialmente un asunto de psicología. La gordura es de «origen psicogénico».

Si esto es verdad, las personas moderadamente gruesas que no padecen ningún grave trastorno hormonal, son víctimas de problemas de personalidad que les obliga a comer con exceso, en contra del consejo de los amigos y los médicos, y contra su propio sentido común y, a menudo, en contra de sus deseos conscientes.

Tal vez se vieron excesivamente protegidos y sobrealimentados de niños hasta que se estableció en ellos el hábito de comer, de manera irrevocable. Tal vez, por otra parte, se vieron rechazados y se aficionaron a la comida como compensación. Quizás el trauma del destete los indujo a buscar solaz en la comida. O quizá se vieron atrapados en un período de erotismo oral, del que ya jamás se liberaron. O, aún más complicado, quizá comían impulsivamente para ocultarse a sí mismos un deseo todavía más profundamente escondido de rechazar la comida y a la madre.

Ciertamente, a los psiquiatras no les faltan explicaciones y psicoanálisis que parecen ofrecer la posibilidad de una curación. Sin embargo, el creciente número de psiquiatras de las dos últimas generaciones no ha logrado derrotar con eficacia a la gordura. Al contrario, hay ahora más personas gruesas que antaño. A juzgar por los resultados, parece como si el abordarlo psicológicamente fuese un fracaso completo.

Hace unos años, dos investigadores de la Facultad de Iowa informaron sobre el intento de comprobar la teoría del origen psicogénico de la gordura. Estudiaron a más de un centenar de muchachas que asistían a escuelas rurales, dividiéndolas en las que eran gordas desde hacía al menos tres años, y las demás de peso normal.

Para demostrar que la teoría psicogénica era correcta, las chicas gordas hubieran tenido que presentar más señales de trastorno emocional que las de peso normal; sus calificaciones escolares habrían estado por debajo del promedio general, y hubiesen tenido que obtener muy malas notas en los análisis destinados a medir su estabilidad mental, sus actitudes sexuales, y demás.

Pero al realizar las comparaciones, las muchachas gordas no presentaron ninguna diferencia como grupo. Su escolaridad, su estabilidad, sus actitudes sexuales no se distinguían de las jóvenes más delgadas. En realidad, sólo hallaron una definida diferencia entre los dos grupos. Los padres de las gruesas eran, por término medio, bastante más robustos que los de las muchachas de peso normal.

Esto último no es sorprendente. Los primeros estudios de muchos casos han demostrado que sólo el 10 por 100 de niños de padres de peso normal engordan. Cuando uno de los padres es grueso, el 50 por 100 de los niños tiende a ser grueso. Si ambos padres lo son, el 80 por 100 de sus hijos comparte la misma tendencia.

Esto parece indicar que comer demasiado puede ser el resultado del ejemplo paternal. Y sin embargo, mellizos idénticos tienden a pesar lo mismo, aunque hayan sido criados por separado y se les hayan inculcado otros hábitos alimentarios.

Por tanto, existen buenas razones para mirar suspicazmente a los motivos hereditarios. Puede existir un impulso heredado en el conjunto físico del organismo que conduce a comer con exceso, y la causa predominante de la gordura puede ser fisiológica.

Algunos dietetistas lo reconocen y se quejan de que las causas fisiológicas se entorpezcan por la actitud popular hacia la gordura. Los que no son gruesos (y esto incluye a varios médicos y dietetistas) suponen a menudo que el hábito de comer demasiado puede superarse mediante el simple uso de la voluntad.

El fracaso en ejercer la voluntad a este respecto es la «gula». Al fin y al cabo, descubrir una causa física conducente a la voracidad exonera al glotón…, lo cual le parece casi inmoral a mucha gente.

Sin embargo, es imposible prescindir de la fisiología.

Se ha reconocido y estudiado entre los animales la tendencia a la gordura, y es muy difícil que en los animales existan complicadas motivaciones psicológicas. Hay razas de ratas de laboratorio que, si se les permite comer libremente, lo hacen hasta alcanzar dos veces el tamaño de una rata normal (que come menos incluso cuando no se le coarta la comida). Esta tendencia a «engordar» se hereda y puede seguirse de generación en generación.

Nuestros animales domésticos están, en muchos casos, alimentados de tal forma que desarrollan, en algunas razas, la tendencia a engordar. El cerdo casero apenas es otra cosa que una máquina de engorde, sin parecerse casi en absoluto al jabalí, mucho más delgado, del que desciende. ¿Por qué no considerar asimismo los factores heredados que se relacionan con los seres humanos? ¿Por qué no indagar si entrañan algún funcionamiento defectuoso de la maquinaria orgánica? Todos sabemos que la ingestión de alimentos viene regulada por el apetito. Se come cuando se tiene hambre y se deja de comer cuando se tiene el estómago lleno. En la mayoría de individuos, estos reajustes automáticos funcionan lo suficientemente bien para mantener constantemente su peso (dentro de un 2 ó 3 por 100), de manera indefinida. Estas personas felices no necesitan preocuparse conscientemente de qué o cuánto comen. Su peso sabe cuidarse por sí solo.

Pero esto no le ocurre a todo el mundo. Hay otras personas que ganan peso constantemente si no prestan atención a su dieta. Para no aumentar de grasas han de realizar un esfuerzo consciente para restringir los alimentos, comiendo menos cada vez, llevando a veces una existencia desdichada por esta causa.

La persona cuyo apetito excede a las necesidades de su organismo puede encontrar un plano de gordura del que ya no varía mucho. Si gana peso, tiene que levantar, extender y mover esos kilos de más a cada paso que da y a cada movimiento que ejecuta. Eso significa el desgaste de más energía, pudiendo bastar para equilibrar su moderada gula. En otros casos, no obstante, una persona gruesa come más para compensar esta pérdida de energía, de forma que va ganando peso insensiblemente, hasta el momento en que decide hacer algo al respecto.

Esto tampoco es absoluto con respecto a la ingestión de alimentos. Una encuesta reciente realizada entre estudiantes demostró que la mayoría de los gordos comían mucho menos que los de peso normal. Pero se sentían más inclinados a estar horas y horas sentados ante el televisor, en tanto que los últimos dedicaban su tiempo libre a actividades más vigorosas.

La pérdida o ganancia de peso viene determinada por el equilibrio entre la ingestión de alimentos y el desgaste de energías. Entre las personas gruesas existe la tendencia a comer sólo un poco más de lo necesario para reponer la energía gastada, sea ésta poca o mucha. Y ese «poco más» forma la grasa.

Entonces, ¿no hay ningún fallo en el control del apetito de esas personas? Comparemos el control del apetito con el termostato de un calentador (en realidad, al control del apetito algunos dietetistas lo llaman «apestato»). Lo mismo que un termostato puede estar graduado para diferentes temperaturas (pudiendo, por tanto, quedar graduado para mantener una habitación demasiado caldeada para la buena comodidad), también es posible graduar un «apestato» a diferentes niveles. La persona cuyo «apestato» está graduado demasiado alto pertenece al grupo de los Hambrientos. Pronto tiene hambre y le dura mucho más, no tardando en engordar.

Eso es triste, ya que en algunos países consideran la gordura como signo de fealdad, y sabemos que es un peligro para la salud. Es cuatro veces más probable que la gente gorda contraiga la diabetes que la gente de peso normal, y casi dos veces que desarrollen dolencias cardíacas y circulatorias. En bien de la salud y la apariencia estética, tales personas han de intentar rebajar de peso usualmente mediante una dieta adecuada. Pero para la persona gruesa cuyo «apestato» está graduado muy alto, esto se convierte en una tortura. Para empeorar el asunto, su organismo compensa automáticamente la restricción de alimentos con la restricción de actividades, de modo que a pesar de la agonía que sufre pierde menos peso que una persona ordinaria con la misma cantidad de comida.

Una persona gruesa con una dieta rebaja manualmente el apestato. Tiene que mantener un dedo sobre el control constantemente, porque tan pronto como relaja la vigilancia el apestato vuelve a su posición anterior y el individuo sigue ganando peso. El mundo está lleno de personas gordas que habían adelgazado y han acabado por engordar nuevamente.

Es posible controlar el apestato mediante otros medios, aparte de la fuerza de voluntad. El individuo puede tomar píldoras que rebajan el apetito. También puede engañar al control comiendo muy lentamente, o varias veces, en pequeñas cantidades, durante el día. Hay trucos dietéticos como los que entrañan los alimentos con alto contenido en grasas y bajo contenido en hidratos de carbono, puesto que la grasa, al parecer, rebaja el apetito más de prisa y por períodos más prolongados que los hidratos de carbono. Pero haga lo que haga el individuo, una vez conseguido el peso requerido, si se prescinde del truco empleado para ello, la persona vuelve a recuperar su peso anterior.

Mas, ¿dónde se encuentra el apestato y cómo funciona? Parece hallarse localizado en la parte del cerebro llamada hipotálamo (ver Capítulo 1). Si el hipotálamo de un animal de laboratorio queda dañado química o quirúrgicamente, el apestato sube drásticamente. El animal empieza a comer vorazmente y engorda.

Respecto a cómo funciona el apestato, existe una gran controversia. Si sus cambios se hallan controlados, no por los trastornos personales, sino por influencias físicas y materiales, ¿qué tenemos? Una teoría desarrollada por Jean Mayer, fisiólogo de la Facultad de Medicina de Harvard, da lugar a una posibilidad muy interesante. Dicha teoría se refiere a la cantidad de glucosa en la sangre. La glucosa es un tipo de azúcar siempre presente en la sangre en pequeñas cantidades, y almacenada en el hígado como una sustancia semejante al almidón, denominada glicógeno. Las células orgánicas absorben la glucosa del fluido sanguíneo y la emplean para producir energía. A medida que se emplea la glucosa, en el hígado se produce más, procedente del glicógeno, y se vierte en la sangre sólo en la medida justa para compensar la que van absorbiendo las células. Cuando el ser humano realiza alguna actividad, las células absorben mayor cantidad de glucosa y, para compensar esto, el glicógeno del hígado se convierte de nuevo en glucosa, naturalmente, a más velocidad. Cuando el individuo está en reposo, las células usan menos glucosa, por lo que la conversión del glicógeno también se reduce. El resultado de estas operaciones es un equilibrio perfectamente controlado. Pero el equilibrio no siempre es perfecto. La glucosa de la sangre decrece lentamente durante el ayuno y aumenta cuando se ingieren alimentos. Mayer sugirió que esta variación afecta al apestato del hipotálamo. Las células del apestato se hallan constantemente comprobando el nivel de glucosa en la sangre. Cuando dicho nivel baja, el hombre siente apetito; cuando sube, desaparece la sensación de hambre.

Si se acepta la teoría de Mayer como una hipótesis plausible, hay que formular la siguiente pregunta: ¿Qué regula la cantidad de glucosa en la sangre y qué conserva tan perfectamente el equilibrio entre las tendencias opuestas de la formación y la absorción de la glucosa? Por lo que sabemos, este equilibrio está principalmente controlado por la actividad de dos hormonas producidas por unas células del páncreas. Una de éstas es una hormona muy conocida: la insulina. Ésta tiende a mantener bajo el nivel de glucosa, haciendo, por lo visto, que las células la absorban con más facilidad. Si por cualquier motivo, el nivel de glucosa amenaza con aumentar excesivamente, se produce más insulina, que es vertida al fluido sanguíneo. Como consecuencia de esta presencia de más insulina, las células absorben la glucosa con más rapidez y baja su nivel en la sangre.

La segunda hormona es el glucagón, que actúa de manera contraria. Tiende a conservar alto el nivel de glucosa, aparentemente alentando la conversión del glicógeno almacenado en el hígado en glucosa. Si el nivel de ésta es demasiado bajo, el páncreas entra en acción, produce glucagón, que convierte el glicógeno en glucosa, y ésta entra en el flujo sanguíneo, con lo que el nivel de aquélla vuelve a aumentar. Gracias a la actuación bien conjugada de ambas hormonas, el nivel de glucosa siempre es fijo, excepto por las fluctuaciones de orden menor empleadas por el hipotálamo para controlar el apetito.

Pero, ¿y si las hormonas salen fuera de control? A menudo, por desgracia demasiado a menudo, el organismo pierde su capacidad de formar insulina en la cantidad requerida. La tendencia a sufrir esta pérdida es una característica hereditaria, y la condición resultante se llama «diabetes mellitus».

Si se forma insulina en cantidad inferior a la normal, las células orgánicas no pueden absorber fácilmente la glucosa, y el nivel de ésta sube en la sangre, condición sumamente peligrosa para la salud. A pesar de esta elevación de nivel en la sangre, señala Mayer, las células del apestato sólo pueden absorber una pequeña cantidad de glucosa, puesto que hay poco suministro de insulina. Por tanto, se comportan como si la sangre presentara un nivel muy bajo de glucosa, y el apetito aumenta. Por este motivo, un diabético siempre tiene hambre y, si se le permite, come vorazmente. (Como utiliza la glucosa derivada de una alimentación ineficaz, pierde peso.) ¿Y el glucagón, la otra hormona que sirve para mantener fijo el nivel de glucosa en la sangre? Las inyecciones de glucagón provocan la elevación del nivel de glucosa en la sangre, con la consiguiente pérdida rápida de apetito.

Lo cual concuerda con la teoría de Mayer.

En este punto resulta tentador efectuar una especulación. Una disminución en la capacidad del organismo para formar glucagón, ¿podría mantener el nivel sanguíneo de glucosa demasiado bajo y, por tanto, conservar el apestato demasiado alto? ¿Acaso las personas que padecen de gordura son también las que presentan tendencia a tener un nivel bajo de glucagón? ¿Se trata de una tendencia hereditaria? En tal caso, ¿pierde el control con más facilidad la insulina cuando se descontrola también la producción de glucagón? ¿Es éste el motivo de que la gente gruesa sea más propensa a la diabetes que la de peso normal? ¿Es posible que la gordura pueda algún día controlarse mediante una terapia de hormonas, como se hace ya con la diabetes? A todas estas preguntas sólo podemos, por el momento, contestar con la ignorancia.

Pero sea cual fuere la respuesta, es inútil considerar a una persona gruesa como débil mental, retardada o simplemente glotona. Las recriminaciones, las amenazas, los sustos psicológicos, el análisis de los sueños, casi nunca ayudan, salvo temporalmente. Aunque la teoría de Mayer esté equivocada en algunos detalles, es casi seguro que en la gordura se halla mezclado algún mecanismo fisiológico que, es de desear, llegará a descubrirse y a comprenderse. En cuyo momento, podrá desarrollarse una terapia hormonal con que tratar la gordura, puesto que en realidad es solamente una enfermedad. Pero, ¿y mientras tanto? Existe un medio. La gente que desea perder peso lo conseguirá si comprende que, por encima de todo, necesita mucha paciencia. No deben jamás relajarse por completo y, para mantener el apestato bajo un control manual por tiempo indefinido es preferible que ejerzan sobre el mismo una presión sutil, no pesada.

Cierto, la persona gruesa puede emprender el espectacular procedimiento de una dieta estricta o un tratamiento gimnástico, sorprender con ello a sus amigos y entusiasmarse también ella con los resultados a corto plazo.

Mas, ¿con qué fin, si la fuerte presión sobre el control manual del apestato no ha de tardar en relajarse (usualmente muy pronto), y el organismo retomará a su hábito de comer con exceso? Es mejor comer un poco menos a cada comida de lo que sugiera el impulso personal, y hacerlo constantemente. Añadir a esto un poco más de ejercicio o de actividad de lo que sugiera asimismo el impulso, y continuar haciéndolo de modo constante. Unas cuantas calorías menos a diario, y unas cuantas más consumidas, ayudarán a rebajar de peso, lenta, pero seguramente, y sin excesivo pesar.

Y con mejores resultados a largo plazo, puesto que con el tiempo una presión sutil sobre el apestato es fácil de mantenerla indefinidamente.

Al menos, una investigación extensiva demostró que si una persona gruesa puede mantener un peso normal durante seis meses al año, es probable que siga manteniéndolo por siempre. Una presión sutil sobre el apestato se convierte en una costumbre fácil de seguir.

Es la célebre frase, tan de moda: lento pero seguro.

Nota especial: Cuando redacté el capítulo anterior, hace ya algún tiempo, yo pesaba unos veinte kilos de más. Gozaba de perfecta salud, sin ningún síntoma de diabetes o trastornos circulatorios. Además, mi energía era normal, usualmente me hallaba de buen humor y no veía, por tanto, la menor razón para limitar mi placer digestivo.

Tras haber escrito este capítulo empecé a reflexionar.

Animado por quienes me aprecian, practiqué lo que había predicado. Eventualmente, perdí quince kilos. Actualmente, llevo ya dos años con este nuevo peso sin ningún esfuerzo. Naturalmente, estoy a dieta perpetua en el sentido de que como menos que en otros tiempos, y espero poder seguir haciéndolo, puesto que no es difícil.

Claro está, todavía me quedan cinco kilos de exceso.

Bien, después de releer este capítulo quizás haga otra prueba para intentar rebajar dicho exceso.

4. La sangre lo dirá

Llevamos en nuestras venas una enciclopedia personal que los médicos y los bioquímicos empiezan a saber leer. Todavía forcejean con los pasajes oscuros, pero todo lo que ya han descifrado ha servido para prolongar la existencia humana.

La sangre lo dirá, dice el antiguo proverbio, con una significación errónea. Puesto que lo que la sangre dice, de acuerdo con el significado de los tiempos pretéritos, se refiere a los modales y la crianza, el valor y la honradez… o al revés; en resumen: todo aquello determinado por la enseñanza y el ambiente, y no por la herencia.

Fue al alborear el siglo xx cuando el verdadero código de la sangre empezó a ser desvelado por la investigación. La sangre lo dirá, siempre, según se ha averiguado, si se le hacen las preguntas adecuadas.

En 1901 se descubrió que existen cuatro tipos principales de sangre. Estos tipos no presentan señales externas. Nadie puede decir, sólo mirando a otra persona, o estudiando alguna parte de su cuerpo, aparte de la sangre, a qué tipo pertenece.

La diferencia que nos muestra la sangre es ésta: cuando se mezclan diferentes tipos de sangre, las células de una de las muestras se agrupan en una masa pegajosa.

Esto no ocurre nunca si se mezclan dos muestras sanguíneas del mismo tipo.

Es interesante observar este agrupamiento en el tubo de ensayo. Sin embargo, en las venas de un ser humano este agrupamiento puede ser fatal, porque los coágulos de células rojas obstruyen los capilares vitales de los ríñones, el corazón o el cerebro.

Al menos, ésta fue la respuesta a los versátiles resultados que acompañaron todos los intentos de transfusión sanguínea antes del siglo xx. De vez en cuando, a lo largo de toda la historia de la Medicina, algún médico ha intentado remplazar una pérdida de sangre con la introducción de la sangre de un donante en las venas del enfermo. En algunas ocasiones, esto ha ayudado al paciente; pero con más frecuencia lo ha matado.

En el siglo xx, la transfusión se ha convertido ya en una rutina segura. Sólo es necesario utilizar la sangre de un donante del mismo tipo sanguíneo que el del enfermo o, a lo sumo, de otro tipo de sangre compatible con el primero.

Los tipos sanguíneos se heredan de acuerdo con una norma fija, de modo que la sangre no sólo dice las posibilidades de una transfusión, sino también sobre el parentesco.

Así, un hombre y una mujer, ambos del tipo A, no pueden tener un hijo del tipo B. Si dicho hijo muestra sangre del tipo B, sólo caben dos posibilidades: o que el niño haya sido cambiado inadvertidamente en la clínica maternal, o que el marido no sea el padre verdadero. No importa que los diversos parientes afirmen que el niño tiene la barbilla y la nariz del padre. Esto es posible, pero por pura coincidencia. La sangre dice la verdad, y jamás miente. (Aunque sea posible, y lo admito, que los técnicos se equivoquen al analizar un tipo de sangre.) También se esconden en la sangre las relaciones de carácter más amplio. Por ejemplo, un tipo de sangre llamado Rh negativo se presenta en un número muy apreciable de europeos y entre sus descendientes de otros continentes. En cambio, apenas aparece entre los nativos de Asia, África, Australia y ambas Américas.

En Europa, la incidencia de este tipo sanguíneo es alta entre los vascos de los Pirineos españoles, donde un tercio de la población es Rh negativo. Por tanto, es posible que los vascos representen a un estrato muy antiguo de la población europea, sumergida por una oleada posterior de inmigrantes de Asia o Norte de África, que desde hace miles de años son «los europeos modernos».

Que los vascos son el último vestigio de los «antiguos europeos» viene indicado por el carácter de su lenguaje, que no está relacionado con ningún otro de la Tierra, y por su sangre.

En realidad, el promedio de cambios de los diferentes grupos de sangre se ha empleado para seguir las emigraciones del pasado. Una marea de tipo B retrocede lentamente a medida que se viaja hacia el Oeste a través de Europa, desde los Urales hacia el Atlántico, lo cual señala el paso de los invasores asiáticos, como los hunos y los mongoles, puesto que el porcentaje de tipo B es mayor en Asia Central. También pueden seguirse en la sangre de su población las invasiones de Australia desde el Norte, y de Japón desde el Oeste.

No obstante, si únicamente utilizamos los principales tipos sanguíneos, los que tienen importancia en la transfusión, para tales propósitos, los limitamos a una técnica muy pobre. Dichos tipos se extienden demasiado ampliamente y los promedios de uno a otro difieren por un margen muy estrecho. Por fortuna, en esta última mitad del siglo se han descubierto muchos tipos de sangre adicionales. Ninguno de éstos tiene importancia en las transfusiones, pero todos son claramente definidos y se heredan de acuerdo con una norma fija.

Hasta hoy día se han identificado más de sesenta tipos sanguíneos. El número de combinaciones posibles entre los mismos que pueden existir en un ser humano (incluso concediendo que algunos de tales tipos no son muy corrientes), se calcula en 1.152.900.000.000.000.000.

Esta cantidad es cuatrocientos millones de veces mayor que la población total de la Tierra. Es, por tanto, muy probable que un laboratorio equipado para analizar todos los posibles tipos sanguíneos (y por desgracia, no existe ninguno aún en estas condiciones), pudiera diferenciar la sangre de cualquier ser humano de la de otro…, salvo en el caso de mellizos idénticos.

Potencialmente, pues, cada individuo lleva consigo de manera constante su tarjeta de identidad. Una vez analizado completamente el grupo sanguíneo, todo ser humano queda debidamente «fichado» y controlado.

Como resultado de un análisis completo, las relaciones, en el estricto sentido de la paternidad, o en el más amplio de las emigraciones tribales, podrían ser precisadas detalladamente. Esos problemas relativos a las emigraciones de los polinesios o a la ruta seguida por los indios para penetrar en América, quedarían solucionados al punto.

(Los antropólogos interesados en tales emigraciones, no obstante, deben apresurarse a realizar tales deducciones. El automóvil apresuró la mezcla de las personas dentro de las naciones, y actualmente el avión a propulsión introduce una mayor facilidad de movimientos y desarraigos a escala intercontinental. Si este progreso continúa, la historia de las emigraciones tal vez dentro de unas generaciones será algo que sólo pertenecerá al pasado.) En esencia, todo esto significa que, al menos potencialmente, la sangre puede decirnos exactamente quiénes somos.

Sería interesante que, además, pudiese decimos qué somos. Supongamos, por ejemplo, que pudiera decirnos si estamos sanos o enfermos, y en este último caso, hasta qué punto y de qué modo. Aún sería más fascinante que pudiese predecir el futuro y decir si somos propensos a enfermar y, en tal caso, de qué forma.

Repito que, al menos en potencia, la sangre puede decir todo esto. A las debidas preguntas puede dar las más acertadas respuestas.

Naturalmente, no se trata de un asunto de mera curiosidad, como si fuéramos un pueblo primitivo deseoso de consultar una bola de cristal. Todos somos propensos a las enfermedades, y cuanto más sepamos a este respecto, más fácil nos resultará impedir su progreso o incluso cambiarlo. Mejor aún, podríamos prevenir las enfermedades, en primer lugar.

Es regla de carácter general que cuanto antes se detecta una enfermedad, más fácil es de curar. Toda enfermedad, si progresa bastante, produce síntomas visibles, de lo contrario no se reconoce como tal enfermedad. Pero el organismo lucha ferozmente para conservar su equilibrio contra el comienzo de una enfermedad, y cuando los síntomas son visibles, el organismo ha perdido ya la batalla, al menos por el momento. Por tanto, para su mejor tratamiento, habría que descubrir una enfermedad antes de la aparición a simple vista de los síntomas.

Bien, cada fase de las actividades orgánicas, tanto en salud como en enfermedad, se refleja en la compleja química de la sangre. Por tanto, volvamos a ella. Y tomemos como ejemplo la diabetes.

El diabético avanzado pierde peso a pesar de comer vorazmente (ver Capítulo 3); y ha de beber y orinar copiosamente. Está torturado por granos y sarpullidos, y una cantidad de trastornos más graves aunque menos observables. Cuando ocurre tal cosa, el diabético está ya muy mal y fuera de toda ayuda.

La diabetes es una enfermedad provocada por la falta de la hormona insulina. Ésta controla el nivel de concentración de la glucosa, que es una forma del azúcar, en la sangre. Cuando decrece la producción de insulina, la concentración de glucosa se eleva hasta que una parte de la misma se vierte en la orina. Detectar los primeros signos de glucosa en la orina sirve para probar la existencia de la diabetes antes de que el paciente llegue a los últimos extremos.

Pero entonces ya es demasiado tarde, en el curso de la dolencia, para conseguir un cierto bienestar. Es posible analizar directamente la sangre y ver si la concentración de glucosa, aunque todavía no lo bastante elevada para verterse en la orina, se halla por encima de lo normal.

Mejor aún, es posible poner bajo tensión los aparatos químicos del cuerpo que se ocupan de la glucosa. Entonces podemos observar si el organismo, aunque todavía capaz de controlar el nivel de glucosa en condiciones ordinarias, mostraría signos de fallo en una emergencia. Si el organismo presenta tales signos, habremos descubierto la diabetes en sus comienzos.

Esto se logra mediante un «ensayo de tolerancia de la glucosa». Al paciente se le administra como bebida una gran dosis de solución de glucosa, y se analiza antes y después su sangre varias veces.

La glucosa es absorbida rápidamente por el intestino, y llega a la sangre. Como resultado, la concentración de glucosa aumenta al momento. En respuesta a tal elevación, no obstante, se produce insulina en cantidad mayor de lo normal y la concentración de glucosa desciende a su justo nivel en poco tiempo. En los individuos normales, la concentración de glucosa es de unos 100 miligramos por 100 mililitros de sangre. Unos cuarenta y cinco minutos después de tomar una comida con glucosa, esta cifra se eleva el doble, si bien al cabo de una hora ya ha descendido al nivel normal.

Si la cifra aumenta marcadamente por encima del doble después de comer y tarda varias horas en volver al estado normal, ello significa que el organismo tiene dificultades en producir insulina en cantidades de emergencia y que, por tanto, existen algunas probabilidades de que se presente la diabetes. Cuando se detecta la enfermedad en esta fase, una dieta razonable y un régimen de ejercicios adecuado pueden mantener la normalidad por tiempo indefinido. Entonces, puede evitarse el uso de las inyecciones de insulina.

Un segundo ejemplo podría referirse a la glándula tiroides, que controla el promedio a que se «desgasta» la maquinaria química del organismo. A esto se le llama «promedio metabólico basal», o BMR. Hasta hace unos años, la forma de medir el BMR consistía en hacer que el paciente respirase por un cilindro de oxígeno, y el promedio a que se consumía dicho oxígeno era la medida aproximada del BMR. Pero la glándula tiroides produce ciertas hormonas que controlan el BMR. Éstas contienen átomos de yodo que son transportados al resto del cuerpo.

Tan pronto como se inventó el método para determinar el «yodo unido a las proteínas» (PBI) de la sangre, la prueba, mucho más lenta, del cilindro de oxígeno quedó descartada. Una pequeña punción, y la sangre habla.

Las enfermedades del riñón, como la diabetes, son fáciles de detectar cuando se hallan muy avanzadas. Al comienzo, es preciso algo que ayude a detectarlas. Bien, la función primordial de los riñones es filtrar los residuos de la sangre, el más importante de los cuales es la urea.

No es difícil medir la concentración de urea en la sangre, y cuando la misma supera el nivel normal, el riñon puede empezar a fallar…, tal vez a tiempo todavía de poder curarse.

El hígado es la fábrica química más atareada del organismo y su buen funcionamiento es crucial para la existencia. Pero todas las sustancias sumamente necesarias que fabrica han de ser distribuidas por la sangre, y según el aumento o disminución de concentración de aquéllas, es posible averiguar la historia exacta de los fallos hepáticos. La ictericia es una condición, por ejemplo, en que un pigmento amarillo llamado bilirrubina se concentra anormalmente en la sangre. Ello puede deberse a un trastorno de las células rojas de la sangre que, al descomponerse con excesiva rapidez, forman cantidades anormales de bilirrubina. O puede deberse a un trastorno hepático, al estar el hígado bloqueado y no poder verter la bilirrubina al intestino, como es normal, debiendo en cambio verterla a la sangre. Comprobando la cantidad de bilirrubina mediante dos métodos químicos distintos, el bioquímico puede al momento saber si el trastorno se debe a la sangre o al hígado.

Si bien la sangre es un libro abierto, es, no obstante, un libro complicado. Los bioquímicos pueden detectar cualquiera de las diversas docenas de sustancias de la sangre, y una variación en la concentración de cualquiera de ellas puede ser sintomática de cierto número de enfermedades. Una elevación del nivel de una de las proteínas amilasas puede pregonar una pancreatitis; el aumento de otra denominada fosfatasa básica puede indicar un cáncer óseo; la elevación del nivel de la fosfatasa ácida puede ser sintomática del cáncer de la próstata. Cierta proteína transaminasa puede, con una concentración excesiva, indicar una dolencia cardíaca. La elevación del nivel de cierto tipo de sustancias grasas convierte en posibilidad la arterioesclerosis. Y hay docenas de estos ejemplos.

Ningún análisis es la indicación clara de una enfermedad, pero cada uno estrecha el campo de las posibilidades, y una combinación de análisis puede considerarse como seguro. Gracias a ellos, el médico sabe en qué dirección mirar, cuando el paciente todavía no presenta síntomas visibles de una enfermedad dada y cuando el tratamiento puede ofrecer aún grandes esperanzas de curación o, al menos, la detención del progreso de la dolencia.

¿Qué nos reserva el futuro? Existen muchas razones para pensar que el valor de la sangre como instrumento diagnosticador seguirá en aumento. Desde la Segunda Guerra Mundial, se han inventado constantemente nuevas técnicas para analizar mezclas cada vez más complejas con mayor precisión. Ahora es posible ya separar los componentes de la sangre con mayor certidumbre.

Pero no todas las variaciones de la composición de la sangre son de orden patológico. Los grupos sanguíneos son un buen ejemplo de esto. Por lo que sabemos, una persona del grupo A es tan normal como otra del grupo B, siendo probable que viva una existencia tan larga y sana como la segunda. Pero los dos individuos son igualmente diferentes, y cuando se trata de una transfusión hay que tener en cuenta dicha diferencia.

Puede haber otras diferencias dentro de los límites de la normalidad, que requieren ligeras gradaciones de tratamiento. Por ejemplo, una de las funciones primordiales de la sangre es la de suministrar a las células las sustancias necesarias para fabricar los tejidos. Las principales necesidades son veinte compuestos estrechamente relacionados entre sí llamados aminoácidos. Éstos pueden darse por separado o estar combinados en unas moléculas gigantes, que son las proteínas. La composición de los aminoácidos de la sangre de un individuo puede tener suma importancia en su tratamiento clínico (ver Capítulo 5).

Tal vez el siglo venidero nos mostrará la bioquímica humana, como un asunto verdaderamente individual. La sangre de una persona no será ya solamente su tarjeta de visita, sino que constituirá una ficha de su historia pasada, presente y futura.

El Sherlock Holmes del porvenir será el técnico sanguíneo. Es posible incluso imaginar una época en que el análisis sanguíneo será perfeccionado mediante el empleo de microanálisis, tal vez con ordenadores, en que una sola gota de sangre bastará para retratar al individuo, como la cartulina de la balanza automática. Dicha tarjeta no dirá si el individuo en cuestión conocerá a una rubia despampanante o si ha de realizar un crucero en un próximo futuro. En cambio, aconsejará la dieta a emplear, prevendrá respecto a los peligros que amenacen la salud del analizado, y de los pequeños desajustes de la máquina orgánica que, al ser ignorados, pueden convertirse en graves.

Para nuestros nietos, la información conseguida por medio de una gota de sangre podrá ser la clave de una existencia sana y prolongada.

5. El tú químico

Damos por descontado que no existen dos personas exactamente iguales. Un niño no tiene dificultad en reconocer a su madre, y un adolescente asegura que ninguna otra mujer se parece a su amada. Incluso los mellizos idénticos presentan ciertas diferencias. Y lo que es aparente a nuestro sentido de la vista lo es al olfato del perro. Pero el aspecto sólo se halla a flor de piel, dijo el poeta. Y también el olor, según los anuncios de la televisión. ¿No podemos profundizar un poco más? ¿No hay diferencias en las funciones internas del organismo que resulten aparentes en el mundo fríamente imparcial del tubo de ensayo del químico? En realidad, todos nosotros utilizamos la hemoglobina para absorber el oxígeno, y ciertas para producir energía. Todos poseemos pulmones, corazón y ríñones. Podemos alimentamos con la misma comida, sufrir las mismas enfermedades, y todos terminamos en la muerte. Pero hay algo más.

En el capítulo anterior ya examinamos el papel de la sangre en la individualidad química del ser humano… Bien, pasemos más adelante.

En las dos primeras décadas de este siglo, un médico inglés llamado Archibald E. Garrod, estudió la pauta del metabolismo de los seres humanos. Estudió la secuencia de las reacciones químicas por las que el organismo descompone los alimentos para obtener energías y fabricar los tejidos. Y halló casos de personas que carecían de la capacidad de ejecutar una u otra reacción, a veces con resultados catastróficos (ver un ejemplo en el Capítulo 9).

Esos errores químicos acompañan al individuo desde el nacimiento. El equipo, o la falta del mismo, con que el individuo debe conducir su química interna es suyo desde el principio (al menos, en potencia, ya que en algunos casos la deficiencia sólo se observa más adelante). Garrod se refirió a las desviaciones de lo que parecía un metabolismo normal, como «errores innatos del metabolismo».

Naturalmente, los más fáciles de ver son los que producen enfermedades graves como la diabetes (ver Capítulo 4); o los síntomas espectaculares, como los de la relativamente inofensiva alcaptonuria, en que la orina, en ciertas condiciones, se torna negra.

Comprendiendo que los mecanismos químicos dentro de las células son altamente complejos, Garrod presintió que podía existir cualquier número de desviaciones que no producían síntomas espectaculares ni peligrosos. Dicho de otro modo: cada individuo podía seguir una senda química distinta de los demás, sin perjudicarse. Desde este punto de vista, todos somos individuales no sólo en el aspecto, sino en la parte química.

Fijémonos en ello. El organismo fabrica unas proteínas defensivas especiales (los anticuerpos) que reaccionan ante las moléculas extrañas, neutralizándolas. Ésta es una de nuestras mejores defensas contra las bacterias y los virus invasores. Una vez el individuo ha fabricado un anticuerpo contra los virus del sarampión, está ya inmunizado contra nuevos ataques de esta enfermedad. La vacuna Sabin anima al organismo a fabricar anticuerpos contra el virus de la polio, ofreciéndole los virus necesarios en una forma que no producen la enfermedad. De este modo, el organismo queda inmunizado sin sufrir el riesgo de contraer la polio.

Un ejemplo negativo del mismo uso de las proteínas es el hecho de que el cuerpo puede casualmente tornarse sensible a ciertas sustancias extrañas, inofensivas en sí mismas; por ejemplo, a las proteínas de ciertas clases de polen, o a ciertos tipos de alimentos. En tal caso, la persona padece una fiebre del heno o una alergia.

Un anticuerpo dado puede hacer distinciones entre una sustancia extraña y otra (por ejemplo, entre la proteína del pollo y la del pato), aunque tal diferencia no sea aparente para el químico. Siempre puede distinguir entre una sustancia extraña y las moléculas presentes en el organismo a que pertenece.

Si un anticuerpo puede distinguir entre dos proteínas, éstas han de ser diferentes en cierto modo. Siendo así, no puede haber dos seres humanos, aparte de los mellizos idénticos, cuyas proteínas sean exactamente iguales. La prueba de esto es que un injerto dérmico fracasará a menos que se haga con piel de otra parte del mismo cuerpo o, a lo sumo, del cuerpo del mellizo idéntico, si existe.

El organismo del paciente reconoce y forma anticuerpos contra las proteínas de la epidermis de cualquier otro ser humano. Estos anticuerpos impiden que el injerto «arraigue», y demuestran, con grave inconveniente o peligro para él, que el paciente es un individualista en el aspecto químico.

Gran parte de nuestra magia clínica está limitada a métodos que sorprenden al común denominador de toda la Humanidad. La aspirina alivia el dolor en casi todos los seres humanos, y la penicilina impide el crecimiento y la multiplicación de los gérmenes patógenos en casi todos también. Naturalmente, el médico debe tener cuidado con la pequeña minoría sensible a estas panaceas universales, pero en general puede prescribirlas libremente.

A medida que aumentan los conocimientos, es posible añadir un control más sutil, cuidadosamente dirigido a las necesidades de cada individuo, a la terapia de conjunto. El médico ha de saber, y de hecho sabe ya, que aparte de la individualidad psicológica y biológica, existe una individualidad química en cada uno de nosotros.

El primer paso en la dirección de un control más sutil se refiere indudablemente a las proteínas. Al fin y al cabo, la mayoría de las sustancias que inducen una reacción de anticuerpos son proteínas, siéndolo asimismo los anticuerpos. Está claro, pues, que las proteínas presentes en el cuerpo difieren entre sí de modo sutil, y que el organismo puede fabricar otras para aprovecharse de tales diferencias.

¿Cuáles son estas sutiles diferencias? En primer lugar, las proteínas están formadas de moléculas grandes. Incluso una molécula proteica de un tamaño normal está formada por la aglomeración de unos cuatrocientos mil átomos. En comparación, una molécula de agua está formada por sólo tres átomos, y la de azúcar por cuarenta y cinco átomos.

Los átomos dentro de la molécula de proteína se hallan dispuestos en combinaciones llamadas aminoácidos, cada uno de los cuales se compone de diez a treinta átomos. Los aminoácidos se unen, como las cuentas de un collar, formando una molécula de proteína.

Aunque la estructura general de los aminoácidos sea semejante, hay diferencias de detalle. Una proteína individual está formada por una cantidad que oscila entre quince y veintidós aminoácidos, colocados en un cierto orden como una cadena.

Naturalmente, si dos proteínas están formadas por diferente número de aminoácidos, son diferentes, y esta diferencia la notan los anticuerpos. También son diferentes si están formadas por el mismo número de aminoácidos, pero con distintas proporciones de los diversos tipos.

Lo más interesante, no obstante, es que si dos moléculas de proteína están formadas por el mismo número de los mismos tipos de aminoácidos, también son diferentes, si es distinto el orden en que los aminoácidos se suceden en las respectivas cadenas. Es lo mismo que si tomásemos un collar de cuentas: cinco rojas, cinco azules, cinco amarillas y cinco verdes. Según el orden en que éstas se colocasen, podríamos formar doce mil millones de combinaciones diferentes.

Pero el caso de las proteínas no es tan sencillo. Una proteína normal contiene quinientos aminoácidos, no veinte; y los aminoácidos pertenecen a veinte tipos distintos, no a cuatro. El número, por tanto, de formas posibles en que aquéllos pueden disponerse en una gran molécula de proteína daría por resultado una cantidad de seiscientos ceros.

Siendo así, es obvio que una persona posee sus propias proteínas, muy distintas del resto de la Humanidad. En realidad, cualquier ser vivo puede tener unas proteínas completamente diferentes de las de los demás existentes o que hayan existido desde el alborear de los tiempos…, y, por lo tanto, también una química especial.

Pero si cada ser vivo tiene proteínas diferentes y el organismo es sensible a las proteínas extrañas, ¿cómo podemos comer? Por fortuna, la comida no penetra en el organismo en su forma original, sino que permanece en el tubo digestivo hasta haber sufrido diversos cambios. Sólo entonces cruza las paredes intestinales y penetra en el organismo.

Las proteínas, al ser digeridas, se descomponen en aminoácidos individuales, siendo absorbidos sólo éstos. Solamente con que el organismo absorbiese un diminuto fragmento de proteína intacta, el cuerpo se sensibilizaría al mismo tiempo y daría muestras de una poderosa reacción alérgica a tal proteína. Los aminoácidos, sin embargo, son inofensivos.

Aparte de los aminoácidos absorbidos, el organismo fabrica las proteínas individuales de sus propios tejidos, utilizando el exceso de aminoácidos para producir energía. Naturalmente, utiliza los aminoácidos en ciertas proporciones para fabricar las proteínas orgánicas, y dichas proporciones es posible que no sean aquellas en que los aminoácidos se presentan en las proteínas alimenticias ingeridas. Por suerte, el cuerpo humano puede combatir esta discrepancia cambiando la estructura molecular de algunos aminoácidos hacia la de otros. Un aminoácido de excesiva estructura molecular puede convertirse en otro deficiente, preferible para una combinación más eficiente.

Sin embargo, hay límites a esto. Hace casi cien años se descubrió que las ratas morirían si su única fuente de proteínas fuese el maíz, pero que vivirían solamente con añadir una partícula de proteína láctea a su alimentación. La explicación a esta aparente anomalía consiste en que la proteína del maíz carece de un aminoácido llamado triptofano, que las proteínas de la leche poseen en gran cantidad. Aparentemente, la rata no puede extraer el triptofano de los otros aminoácidos y, al no ser capaz de mantener sin aquél el nivel de proteínas de sus tejidos de forma normal, se muere.

En el año 1930, el bioquímico norteamericano William C. Rose, mediante experimentos dietéticos en estudiantes universitarios, halló que el organismo humano no puede fabricar ocho aminoácidos. A los mismos se les denomina «aminoácidos esenciales», debido a que su presencia en la dieta es muy importante para la salud.

En una dieta razonablemente variada, por encima del nivel de la extenuación por hambre, es improbable que el ser humano padezca gravemente por la deficiencia de alguno de los aminoácidos más esenciales. Pero sí puede sufrir trastornos debido a que nuestra dieta nos suministra una pauta equivocada de estos aminoácidos.

Los dietistas pueden analizar fácilmente los aumentos en busca de su contenido de aminoácidos, y también conocen las necesidades cotidianas. Así, es posible saber qué aminoácidos faltan en una dieta y cómo suplir esta falta con alimentos o pastillas. Esto constituye una terapia de conjunto. Aunque hoy en día sea ya posible un tratamiento más individualizado.

Existe un método muy simple para calcular la individualidad química de un paciente. El organismo proporciona las proteínas para sus propias células, que circulan en la corriente sanguínea y presumiblemente tienen la pauta particular de aminoácidos requerida por el individuo. Pueden analizarse las proteínas de algunas gotas de sangre, y ser tomada la fórmula de los aminoácidos, lo que constituye una especie de huella dactilar. Cuando esto se compara con la fórmula de los aminoácidos de la dieta del paciente, pueden efectuarse las sugerencias específicas para los alimentos suplementarios.

Cualquier persona puede ser tratada individualmente a este respecto, y no necesita ser una víctima como miembro de un mítico grupo normal. Esto puede aplicarse a gran escala a todas las áreas donde un nivel de vida bajo requiere un suplemento eficaz en la dieta.

A medida que aumenten los conocimientos clínicos y químicos, llegará el día en que será posible analizar metabólicamente a cada individuo en la infancia y periódicamente más adelante; y llegará el día en que los archivos de cada centro médico contendrán las fichas de las fórmulas químicas de todos sus pacientes. Tal vez entonces, ningún médico tratará a su paciente, salvo en casos de emergencia, sin un estudio preliminar de tales fórmulas.

Al fin y al cabo, el médico no se enfrenta con la Humanidad en abstracto cuando se acude a él, sino con el individuo, con el químico que todos llevamos dentro.

6. Supervivencia de la molécula másapta

Como expliqué en el capítulo anterior, las proteínas son una de las claves principales de la individualidad química, gracias a las complicaciones de su estructura molecular. Y entre las proteínas más significativas se hallan las enzimas, que ya se mencionaron en el Capítulo 3.

Por tanto, no es raro que los bioquímicos se hayan dedicado al estudio de la estructura de las enzimas, también con buenos resultados.

Las enzimas, como todas las proteínas, están constituidas por unidades relativamente simples: aminoácidos. Los veinte tipos diferentes de aminoácidos que existen aproximadamente se presentan en las moléculas más pequeñas de enzimas en la cantidad de dos a seis en cada una; en las mayores llegan a varias docenas.

Los bioquímicos conocen la estructura detallada de cada aminoácido. También conocen la forma exacta en que un aminoácido se halla relacionado con otro para formar una «cadena peptídica». Para extraer la fórmula exacta de una proteína tenemos que determinar antes qué aminoácidos, y cuántos de cada uno, se encuentran en una «cadena peptídica».

Dicha cadena puede ser hidrolizada calentándola con una solución ácida. Este proceso sirve para descomponer la cadena en aminoácidos individuales. Entonces, es posible analizar la mezcla y el número de cada variedad de aminoácidos presentes en la cadena determinada.

Sin embargo, esto no es suficiente. ¿En qué orden se hallan los aminoácidos presentes en la cadena peptídica? El número de ordenaciones posibles es enorme incluso en la proteína más pequeña. Por ejemplo, existe una hormona llamada oxitocina que es una de las más pequeñas de las proteínas corrientes. Su molécula consta de una cadena peptídica que contiene cada una solamente ocho clases diferentes de aminoácidos. Sin embargo, los mismos pueden disponerse de 80.220 formas distintas.

La situación resulta excesivamente complicada para las moléculas mayores de proteínas…, aunque no sea una situación desesperada tampoco. Cortando la cadena peptídica en pequeños fragmentos, cada uno conteniendo dos o tres aminoácidos, y verificando el orden de un solo fragmento a la vez, puede deducirse el orden de toda la cadena. En 1953, se logró establecer por completo el orden de los cincuenta aminoácidos que componen las moléculas de la hormona insulina (ver Capítulo 4).

La insulina fue la primera molécula de proteína conquistada de esta forma, conquista que costó ocho años. Sin embargo, conociendo ya la técnica detallada, se pudo dominar a las moléculas mayores en mucho menos tiempo. Así, no se tardó en descubrir todo lo referente a la molécula de la proteína ribonucleasa (una enzima que comporta la descomposición del ácido ribonucleico, el famoso ARN del que se habló en el Capítulo 2), formada por una cadena peptídica de ciento veinticuatro aminoácidos.

La forma en que la ribonucleasa (o cualquier enzima) da lugar a una reacción química es muy sutil e interesante, y de ello hablaré en el capítulo siguiente. Como es natural, una vez los químicos conocieron la composición de aminoácidos de una molécula de enzima, se sintieron interesados por saber a qué se debía la asombrosa facilidad de dichas moléculas para provocar tales reacciones químicas como, en el caso presente, la descomposición de la molécula de ARN.

Los químicos empezaron, cuidadosamente, a alterar los diversos aminoácidos de la ribonucleasa a fin de encontrar los «puntos activos», las partes que estaban directamente relacionadas con la acción de la enzima. Resultó que algunos aminoácidos especiales, al ser alterados incluso ligeramente, quedaban asociados con una pérdida de la actividad de la enzima, en tanto que otros podían continuar laborando sin efecto alguno. Los puntos clave resultaron ser el aminoácido 12 (de la variedad denominada «histidina»), el aminoácido 14 («lisina»), y el 119 (también «histidina»).

Es probable que, a pesar de la amplia separación de esos aminoácidos en la cadena, representen un solo lugar activo. La cadena peptídica no es, en realidad, una varilla larga y recta, sino más parecida a una cuerda flexible que puede doblarse de forma que la molécula de ribonucleasa adopte conjuntamente las posiciones 14, 21 y 19. De este modo, se forma una fórmula específica de tres aminoácidos.

La molécula de enzima se conserva en su forma doblada mediante enlaces particulares entre agrupaciones atómicas. Uno de los más importantes se relaciona con el aminoácido «cistina». La cistina es una especie de molécula doble. Cada mitad es un aminoácido completo en sí, y ambas mitades se hallan conectadas por una cadena que incluye dos átomos de azufre (un «puente S-S»). Una mitad de la cistina puede formar parte de una cadena peptídica, y la otra mitad de otra cadena. De esta forma, dos cadenas separadas (o dos sectores de una misma cadena) se hallan firmemente sujetas por un puente S-S.

La molécula de ribonucleasa posee cuatro puentes que enlazan diferentes partes de la cadena. Existen tipos más débiles de enlace, claro está, y todos contribuyen a doblar apropiadamente la cadena peptídica, a fin de crear un lugar activo.

Pero si un grupo pequeño de aminoácidos forma el lugar activo, ¿a qué se debe la necesidad de que existan más de cien aminoácidos más? En parte, se han analizado ya algunas razones de esta aparente anomalía.

Si la ribonucleasa se separa en dos partes en la posición del aminoácido 20, cada parte queda inactiva. Si se mezclan las soluciones de las dos partes, se restaura casi toda su actividad. Es como si ambas partes pudiesen alinearse debidamente, a pesar de que existen trillones de maneras posibles de alinearse indebidamente. Aparentemente, la disposición del aminoácido en la enzima es tal que los pliegues naturales aparecen en la cadena, pliegues que juntan a los debidos aminoácidos, formando un lugar activo apropiado. Por tanto, parece como si la larga cadena fuese necesaria a fin de componer un proceso natural de pliegues que conduzca a la formación automática del lugar activo.

Mas, ¿por qué edificar una cadena larga sólo para formar un lugar activo? ¿Por qué no juntar a los aminoácidos de dicho lugar activo y descartar el resto de la molécula? En primer lugar, porque no es deseable mantener constantemente activa a la enzima.

Consideremos las enzimas corrientes llamadas tripsina y quimotripsina. Son enzimas digestivas que actúan sobre el alimento de los intestinos, descomponiendo las moléculas de proteínas de la comida y convirtiéndolas en diminutos fragmentos, que a su vez se descomponen en aminoácidos para ser absorbidos.

Tales enzimas forman parte de un equipo complicado y han de realizar su tarea en un momento apropiado. Por tanto, son segregadas en formas inactivas llamadas tripsinógeno y quimotripsinógeno. Las cadenas peptídicas de estas formas inactivas no pueden doblarse con facilidad a fin de obtener un lugar activo. Sin embargo, si se rompe la cadena en un punto dado, el resto se dobla adecuadamente y torna a ser una enzima activa; el quimotripsinógeno se convierte en quimotripsina, y el tripsinógeno en tripsina.

De modo semejante, la ribonucleasa, que debe doblarse para formar un lugar activo, sólo lo hace de manera adecuada cuando se cumplen ciertas condiciones. Entonces, puede estar activa o inactiva según las circunstancias.

Un lugar activo, formado con suma facilidad, lo sería siempre, y esto no encaja en las necesidades del tejido vivo, que requiere una flexibilidad enormemente sutil en la conducta de sus componentes.

Volvamos a las enzimas digestivas mencionadas. La molécula de tripsina contiene 233 aminoácidos divididos en tres cadenas peptídicas unidas por puentes de cistina. La de quimotripsina es un poco mayor. Sin embargo, se ha averiguado ya el orden de los aminoácidos en ambas enzimas.

La tripsina y la quimotripsina resultaron poseer lugares activos idénticos, y la mitad aproximadamente de los aminoácidos de la tripsina se hallan en el mismo orden que en la quimotripsina. En vista de esta semejanza, no es sorprendente que las dos enzimas tengan una función similar; que ambas descompongan las moléculas de proteína como parte del proceso digestivo.

Pero también existen algunas diferencias. Y esas diferencias en el orden de los aminoácidos hacen posible que la tripsina se dedique a las moléculas de proteína de una forma, y la quimotripsina de otra. De esta manera, las proteínas están orientadas de forma diferente con respecto al lugar activo, por lo que ambas enzimas no son precisamente duplicados exactos.

Debido a esta diferencia de orientación, la tripsina sólo divide a ciertos tipos de enlaces aminoácidos, incluyendo a los de la lisina, ya mencionada, u otro aminoácido semejante, la arginina, muy parecido a la lisina en ciertos aspectos. La quimotripsina divide los enlaces relacionados con aminoácidos tales como la fenilalanina, la tirosina y el tritofán (los cuales poseen en común la presencia de un anillo de seis átomos de carbono en la molécula).

Como la tripsina y la quimotripsina tienen lugares activos de estructura idéntica, el propósito del resto de la molécula se muestra bajo otra luz. Dominando la forma en que la enzima se combina con las moléculas sobre las que influye, se posibilita una flexibilidad de conducta adicional, que no existiría si el lugar activo estuviese aislado en presencia.

La semejanza de la tripsina y la quimotripsina sugiere que ambas proceden de una misma molécula ancestral. Las diferencias entre ambas proceden de que aunque haya sido heredada la capacidad de formar cadenas peptídicas específicas, esta capacidad se vio ocasionalmente distorsionada en el tránsito («mutación»).

El proceso de la evolución por selección natural se aplica a las cadenas peptídicas, presumiblemente, lo mismo que al conjunto de un organismo. Si se forma una cadena peptídica con un método ineficaz de funcionamiento, o ninguno en absoluto, los organismos que la poseen muestran tendencia a extinguirse. Una nueva cadena con una función ligeramente alterada, o una completamente nueva, sobrevivirá, y el organismo que la posea quedará ligeramente modificado, a fin de subvenir a la nueva función. Esta es la supervivencia de las moléculas entre las más aptas, lo mismo que entre los organismos más adecuados.

La evolución entre las moléculas puede arrojar cierta luz, por tanto, entre la evolución de los organismos. La estructura de las moléculas de la enzima citocromo C (referente a la oxigenación de los tejidos), se ha estudiado recientemente en trece especies diferentes, desde el hombre a la levadura. Casi la mitad de los ciento cuatro o ciento ocho aminoácidos de esta enzima están presentes en un orden idéntico en todas las especies. Lo cual es una prueba en favor de la creencia de que toda la vida procede de un factor ancestral común.

Las diferencias existentes son más notables a medida que se distancian las especies. La cadena peptídica de la molécula de citocromo C del hombre difiere en sólo un aminoácido de la de un mono Rhesus. Sin embargo, existen veintiuna diferencias entre la del hombre y el atún; y 48 entre la célula del hombre y la de la levadura, con respecto a dicha molécula (aunque en todas las especies su función sea la misma).

Indudablemente, si los químicos pudieran simplificar sus técnicas hasta el punto de poder estudiar numerosas enzimas de muchas especies, las enormes diferencias serían lo bastante complejas para revelar la fórmula evolutiva con todo detalle.

7. Enzimas y metáforas.

Un experimento clásico que a menudo sirve para iniciar un curso general de química en el Instituto es aquel en que se prepara oxígeno por descomposición de clorato potásico (que contiene átomos de aquel elemento en su molécula). Las orientaciones para llevar a cabo el experimento son muy explícitas. El estudiante no calienta simplemente el clorato potásico. Primero tiene que añadir bióxido de manganeso (que también contiene átomos de oxígeno en su molécula). Sin esto, el clorato potásico ha de calentarse enormemente y la liberación de oxígeno es muy lenta. Con el bióxido de manganeso, la mezcla puede calentarse moderadamente y el oxígeno se desprende con rapidez.

Es necesario explicarle al estudiante que el bióxido de manganeso no interviene en la reacción, para impedir que piense que el oxígeno procede de aquél, quedando sólo el manganeso en su forma metálica. La función del bióxido de manganeso es simplemente la de acelerar la descomposición del clorato potásico, pero de forma que no consuma el bióxido de manganeso. Basta con la sola presencia de éste. Se trata de un catalizador, y al proceso de influir por su sola presencia se denomina catálisis.

Esto puede inducir a un principiante a una asociación prolongada e innecesaria de la catálisis con el misterio. La noción de influir por la sola presencia y no por participación es muy incómoda, como una especie de fuerza psi molecular, una percepción extrasensorial y no por parte del clorato potásico, estando presente el aura influyente del bióxido de manganeso, o tal vez una telequinesis, una acción supernatural a distancia por parte de la molécula de bióxido de manganeso.

En la ciencia resulta muy poco deseable cualquier aura de misterio, puesto que la ciencia está dedicada a tornar el Universo menos misterioso. El hecho de que cualquier estudiante de química se halle constantemente frente a una catálisis y que todo aquel que se dedica a la bioquímica se halle delante de esas proteínas catalizadoras sumamente útiles, las enzimas, hace que este misterio resulte particularmente muy poco deseable.

Naturalmente, es imposible suspender un curso introductorio el tiempo preciso para sondear en la superficie química con el detalle necesario para eliminar el misterio. Por una parte, los estudiantes carecen de suficientes antecedentes para ello, y por otra, no se requiere nada de esto. Lo que hay que hacer al principio es ahuyentar el misterio; ya habrá tiempo más adelante para racionalizarlo.

Para alejar el misterio sólo es necesario ofrecerle al estudiante ejemplos corrientes de cómo puede acelerarse una reacción mediante la simple presencia de una sustancia externa; ejemplos que no entrañan brujería alguna. En resumen: un estudiante tal vez no siempre esté dispuesto a estudiar química, pero sí metáforas.

Dada ya la metáfora, el estudiante la recordará indefinidamente, si es lo suficiente espectacular. Y aunque no continúe con los estudios de química, evitará a este respecto el azote del misticismo, contribuyendo de este modo a una perspectiva razonada del Universo, que es uno de los objetivos de la ciencia. Si el estudiante sigue otros cursos de química en los que vuelva a presentarse el fenómeno de la catálisis, y es colocado sobre una base teórica más firme, tendrá al menos un comienzo adecuado y podrá abordar el tema con mayor confianza.

Por ejemplo, ¿cómo puede influir un catalizador en una reacción, por su sola presencia? ¿Qué existe en la vida ordinaria que pueda ofrecer una analogía a un fenómeno tan esotérico? Supongamos que hacemos uso de la metáfora de «el ladrillo y el plano inclinado».

En vez de descomponer el clorato potásico para liberar el oxígeno, imaginemos un ladrillo deslizándose por un plano inclinado, liberando energía. Ambos son procesos espontáneos, que sólo necesitan el empuje inicial. El clorato potásico necesita el estímulo del calor; el ladrillo requiere el impulso inicial de la mano.

Supongamos que el plano inclinado donde descansa el ladrillo es áspero, de modo que se produce una mayor fricción entre su superficie y el ladrillo. A pesar de la atracción de la gravedad y de la mano, el ladrillo se detiene tan pronto como aquélla deja de empujarle.

Bien, supongamos ahora que recubrimos la superficie del plano inclinado y la del ladrillo con hielo. El ladrillo, de pronto, se deslizará con mucha más facilidad. Un empujoncito al empezar…, o tal vez sin empujón, y el ladrillo se deslizará.

Pero el hielo no impulsa al ladrillo; no aumenta la atracción de la gravedad; no proporciona energía en ninguna forma; no juega el menor papel activo. Basta con su presencia. No necesita siquiera estar presente, sino sólo recubrir la superficie del ladrillo y la pendiente, en donde entran ambas en contacto. Idealmente, tampoco se desgasta el hielo en este proceso. Al apartar el ladrillo, el hielo continúa allí; de este modo, es posible empujar otro ladrillo recubierto de hielo, y otro y otro…

Un catalizador se define como una sustancia capaz de acelerar una reacción química por su sola presencia en pequeñas cantidades, sin sufrir ningún cambio por su parte en el proceso. Eliminemos el término «química», y el hielo que envuelve la pendiente es un catalizador perfecto.

Existe otra metáfora similar, conocida como la «mesa de escribir». Imaginemos a un hombre con papel y lápiz, y nada más, de pie en medio de un desierto, sólo con arena suave bajo sus pies. Este hombre desea escribir una nota en el papel.

Sabe cómo hacerlo y posee los elementos necesarios para la escritura. Sin embargo, sólo consigue garabatear unos trazos ilegibles, y ciertamente se le romperá el papel en el proceso.

Imaginémosle de pronto provisto de una tabla de madera pulimentada. ¡Cuán distinta es ahora su situación! Dicho hombre no ha aumentado sus conocimientos de escritura. La nota a escribir es la misma que antes, el papel y el lápiz. Sin embargo, ahora puede redactar el mensaje cómodamente, con toda claridad, sin esfuerzo…, sólo gracias a una mesa de escribir, a un pupitre, que acelera el proceso con su sola presencia, y no se modifica durante el mismo. Tanto el papel como el lápiz se desgastan, y hasta el mismo escritor pierde unas calorías, pero la tabla no ha sufrido ninguna pérdida significativa, y puede utilizarse un sinnúmero de veces de la misma forma. En resumen, es un catalizador.

Además, ambas metáforas sirven para introducir la noción de que la catálisis es un fenómeno esencialmente superficial; que acelera una reacción (ya sea un ladrillo deslizante, la redacción de una nota o, por extensión, la descomposición del clorato potásico), mediante la presencia de una superficie que es específicamente apta para las actividades que entraña tal reacción.

En el curso de química, ya más adelante, se le dirá al estudiante que la catálisis acelera una reacción sin cambiar la posición del punto de equilibrio. Supongamos, por ejemplo, que se empieza con dos sustancias, A y B, que reaccionan y forman C y D. Dejemos que la reacción llegue hasta el instante en que se produzca un punto de equilibrio en el que A, B, C y D estén presentes en unas proporciones fijas. La presencia de un catalizador acelera la velocidad con que se alcanza el punto de equilibrio, pero no altera su posición.

Ademas, si se empieza con C y D, reaccionarán parcialmente, llegando al punto de equilibrio -el mismo punto de equilibrio-, estando A, B, C y D presentes en proporciones fijas. Y el mismo catalizador acelerará la reacción inversa.

Al estudiante que se enfrenta por primera vez con este fenómeno, tal vez le parezca diabólico que una sustancia inanimada pueda actuar en ambas direcciones, como si supiera por anticipado dónde se halla el punto de equilibrio.

Es sencillo, no obstante, demostrar que el catalizador no actúa en dos direcciones sino en una sola, si volvemos a la metáfora del ladrillo y el plano inclinado. Imaginemos una doble pendiente en forma de V, formada por una sustancia áspera, de alta fricción. La capa de hielo también servirá de catalizador, permitiendo el deslizamiento del ladrillo. Observemos, entonces, que éste se deslizará por las dos laderas de la V, deteniéndose en ambos casos en el mismo punto: el fondo.

Si denominamos a la cúspide de un brazo de la V como A y B, y a la del otro brazo como C y D, y contemplamos el conjunto desde arriba, nos parecerá que el catalizador actúa de derecha a izquierda, o al revés y en cada caso parecerá detenerse en un punto medio misterioso que no semeja diferenciarse de los demás.

Visto desde un lado este proceso, no obstante, se percibe al instante que la reacción se produce sólo en una dirección: hacia abajo, hacia el centro de gravedad. El hielo catalizador acelera dicho movimiento descendente. El punto final (o de equilibrio) es el más inferior de la V, el punto de menor potencial gravitatorio, y por tanto único. Incluso el estudiante menos listo comprenderá que el hielo actúa sin ningún conocimiento previo de la posición de equilibrio. Simplemente, el ladrillo se desliza hasta el fondo.

Asimismo, el estudiante comprenderá por qué el catalizador de una reacción reversible no cambia el punto de equilibrio y por qué acelerar una reacción en una dirección dada no le obliga a moverse en dicha dirección. Naturalmente, el ladrillo envuelto en hielo y el plano inclinado igual, permiten que el primero se deslice más rápidamente sin alterar la posición del fondo de la V, ni permitir que el ladrillo pase de largo y suba por el otro lado.

En cursos más avanzados, el estudiante aprenderá que existe algo denominado «potencial químico» que puede compararse, en cierto modo, con el potencial gravitatorio, y la imagen que capte al principio le servirá de mucho más adelante.

Como es natural, la utilidad de una metáfora no necesita quedar restringida a las nociones más elementales. Eventualmente, el estudiante aprenderá que un catalizador consigue los resultados rebajando la energía de activación.

Dicho de otro modo: la sustancia sobre la que actúa la enzima forma primero un compuesto intermedio, inestable, que se descompone para formar el producto final. Dicho compuesto intermedio e inestable necesita el impulso de una cantidad de energía relativamente grande, mas hasta que se ha formado no se producen productos finales, aunque éstos no ostenten una energía particularmente elevada. Toda la reacción procederá con la misma rapidez con que se forme el producto intermedio.

El catalizador, estabilizando más el producto intermedio, permite su formación con un impulso menor de energía. Esto acelera la velocidad de formación del producto intermedio y, en consecuencia, apresura la reacción en conjunto.

A menudo, la energía de activación (la energía requerida para formar el producto intermedio) se representa como una «giba» de energía entre los productos y los reactivos. La enzima rebaja dicha «giba», aumentando el tráfico por encima de la misma. Supongamos una carretera con automóviles pasando en ambas direcciones, y la metáfora resultará interesante. Sin embargo, no demuestra de qué modo un catalizador puede rebajar una «giba».

Esto sí se logra claramente por medio de la metáfora del «cordón de zapato», Imaginemos a un hombre que está de pie en un campo fangoso de una extensión indefinida y que tiene que atarse el cordón del zapato. Mientras se halle de pie, con el cordón suelto, no hay peligro de que se hunda en el barro o se ensucie. Una vez atado el cordón, tampoco corre tales peligros. Las dos son posiciones estables.

Sin embargo, durante el proceso de anudarse el cordón del zapato, tiene que agacharse, inclinarse o levantar el pie, permaneciendo en un equilibrio muy precario. En una de esas alternativas, se aumenta el riesgo de ensuciarse o de perder el equilibrio. Por tanto, ha de actuar lentamente y con sumo cuidado durante toda la posición intermedia e inestable.

Si imaginamos una serie de hombres, todos los cuales han de atarse el cordón de un zapato en condiciones semejantes, uno tras otro, y sin que ninguno empiece hasta que haya terminado el anterior, todo el proceso tardará bastante tiempo en quedar terminado, debido únicamente a la lentitud del estado intermedio.

Bien, ahora tomemos una silla y permitamos que un hombre se siente en ella. Una vez sentado, puede levantar el pie sin perder su estabilidad. Podrá atar el cordón del zapato sin peligro alguno y volver a levantarse. La silla no es sólo un catalizador (puesto que sirve a su propósito ofreciendo una superficie conveniente), sino que sirve específicamente para estabilizar la posición intermedia.

Rebaja la «giba» de energía de forma clara.

De esta forma, una serie de individuos pueden atarse el cordón del zapato, uno tras otro, de manera mucho más rápida usando la silla en el proceso. Estabilizando la posición intermedia, el catalizador-silla apresura la reacción de atarse el zapato.

Cuando un estudiante aprende por primera vez algo relacionado con las enzimas, se enfrenta con los catalizadores que, de repente, están más íntimamente relacionados con la vida y los «mayores misterios». Pues, aunque son proteínas por su naturaleza, las enzimas comparten todas las propiedades fundamentales de los catalizadores en general. Las metáforas del ladrillo y el plano inclinado, de la tabla de escribir y del cordón de zapato, se aplican a todas las enzimas tan directamente como al bióxido de manganeso.

Pero las enzimas introducen otros refinamientos. Una forma en que las proteínas catalizadoras (enzimas) difieren de los catalizadores minerales es que las primeras son mucho más específicas. Es corriente hallar una enzima que sólo catalice una reacción de entre otras muchas posibles. Aunque no debe aceptarse esto como un ejemplo del dulce misterio de la vida. Incluso un conocimiento muy superficial de la estructura de las proteínas demostrará que es posible edificar superficies muy complejas de moléculas proteicas variando la naturaleza y disposición de los componentes aminoácidos. El valor de una superficie altamente especializada puede demostrarse mediante una extensión de la metáfora del cordón de zapato.

Una silla es una silla, pero hay sillas y sillas. Una silla de cocina ordinaria es adecuada como catalizador para acelerar la reacción de anudarse los zapatos. Pero imaginemos ahora una silla especialmente diseñada con respaldo, brazos y apoyo para los pies, motorizada y capaz de un movimiento automático. Al sentarse, el peso del individuo sobre el asiento establece un contacto que levanta el apoyo para un pie, el cual queda así situado a la altura debida. Simultáneamente, el respaldo avanza, inclinando al individuo convenientemente, mientras los brazos de la silla se mueven hacia dentro, doblando los brazos del hombre por el codo y juntando gentilmente sus manos. En una fracción de segundo, y sin el menor esfuerzo corporal, el individuo ha asumido la postura más apropiada para anudarse el zapato. Una vez hecho lo cual, la silla vuelve a su posición normal y una palanca expulsa suavemente al individuo fuera del asiento. La silla queda lista para recibir a otro individuo.

Obviamente, una silla semejante apresuraría la reacción de atarse el zapato mucho más que la generalizada silla de cocina. Además, estabilizaría más aún la posición intermedia. Y por su propia especialización sería, en cambio, menos útil para otros propósitos. Tal vez un joven intentaría utilizarla para mantener sobre sus rodillas a su amada. Pero el movimiento de sus distintas partes le sorprendería y aunque lograse soportarlo bajo circunstancias placenteras, con toda seguridad se vería asombrado ante la expulsión final, lo mismo que la dama en cuestión.

Si sólo se intentase utilizar dicha silla para leer un periódico, se la abandonaría con disgusto antes incluso de verse arrojado de ella. En cualquier caso, el individuo buscaría una silla más adecuada en la próxima ocasión, una silla especialmente destinada a sostener una chica sobre las rodillas o a leer el diario.

O sea que la silla especialmente diseñada (la enzima) es un catalizador más eficaz y específico que la diseñada ordinariamente (un mineral cualquiera); las características de cada cual implican casi necesariamente las demás.

No necesitamos imaginar sillas de varias clases para dilucidar este punto. Con un poco de fantasía, el lector puede referirse a distintas sillas, como la del barbero, del dentista, o la silla eléctrica, y compararlas con la silla de cocina para comprender hasta qué punto aquéllas aumentan su eficacia y su rapidez en una reacción especial.

La noción de especificación se interfiere con la idea de inhibición competitiva (ver Capítulo 2). Una enzima puede catalizar específicamente la descomposición de una sustancia A, por ejemplo. Y no catalizará la descomposición de una sustancia diferente, B, ni de otra sustancia similar A' (pero no idéntica), en tanto que la presencia de A' interferirá en el funcionamiento normal de la enzima respecto de A, y en cambio, no ocurrirá lo mismo en presencia de B.

Aquí podemos emplear la más familiar de todas las metáforas referentes a las enzimas: la metáfora de «la cerradura y la llave». Una enzima que actúe sobre una sustancia A específica puede compararse con una cerradura que A sea la llave. La sustancia B, distinta a A, es una llave con cabeza diferente a la de A. Por esto, ni siquiera puede insertarse en la cerradura. Con respecto a ésta, la presencia de B carece de significado.

Bien, tomemos ahora una sustancia A' semejante a A. Representa, en este caso, una llave con una cabeza similar a la de A. Por tanto, A' puede insertarse en la cerradura. Pero, el dentado no es semejante al de A. Por tanto A' no girará en la cerradura. Mas sí rellena su agujero. Está allí dentro, sin girar ni permitir que se inserte la llave A. La cerradura está inutilizada temporalmente o, si lo preferimos, la enzima está inhibida.

El estudiante no sólo estudiará las enzimas, sino grupos de ellas. Y llegará el día en que aprenderá que los compuestos internos del organismo ceden energía transportando dos átomos de hidrógeno a la vez, de compuesto a compuesto, hasta que al fin se agregan al oxígeno para formar agua. La mayor parte de esta energía desprendida en este proceso queda almacenada en forma de unos compuestos llamados «esteres de fosfato de alta energía», formándose unos tres de ellos por cada par de átomos de hidrógeno transportado.

El hidrógeno se transporta de una posición a otra lo mismo que un cubo en una fila, catalizando cada traslado con una enzima distinta.

¿Por qué esta serie de pasos y de enzimas? ¿No sería mejor y más sencillo combinar directamente los átomos de hidrógeno con el oxígeno molecular en un solo paso y usar una sola enzima para catalizar la reacción? Como de costumbre, también para esto existe una metáfora: la de la «escalera».

Supongamos que un hombre tuviese necesidad de bajar desde el quinto piso de una casa a la planta baja y almacenase el potencial gravitatorio así desprendido, usando la energía de su movimiento descendente dando cuerda a tres relojes. Podría hacerlo tirando de la cadena de cada reloj de pared al pasar, levantando sus pesas con el tirón de su propio peso al moverse hacia abajo.

Si baja desde el quinto piso a la planta baja por medio de cinco tramos de peldaños (un sistema de multienzimas), puede, en este proceso, más lentamente, asir las cadenas de los relojes con toda seguridad y tirar de ellas suavemente sin modificar su paso.

También podría bajar desde el quinto piso hasta la acera saltando por el pasamanos y el hueco de la escalera (método de una sola enzima). Llegaría a la acera más simple y rápidamente y perdería un potencial gravitatorio con la misma seguridad que bajando por la escalera como es debido. Sin embargo, le resultaría difícil asir las cadenas de los relojes al pasar. Desprendería energía, pero no la almacenaría.

Asimismo, el método de descender desde el quinto piso a la planta es reversible. Es posible subir desde abajo hasta el quinto piso sin gastar excesiva energía. Sin embargo, a pesar de haber bajado de un solo salto no podría de modo alguno (suponiendo que estuviera en situación de intentarlo) regresar al quinto piso de un solo salto.

De modo semejante, la reacción multienzima, en cada paso componente entraña un cambio relativamente pequeño de energía, permite un almacenaje más eficaz de aquélla y es, al mismo tiempo, más fácilmente reversible y, por ende, más simplemente controlable por el organismo. El mayor cambio de energía en el método de un solo paso (aunque aparentemente sea ésta la alternativa más sencilla) dificulta el almacenamiento eficaz de energía, y aún más invertir el proceso según las necesidades.

Estas metáforas no pretenden ser exhaustivas, ni muestras excelentes o perfectas; sólo sirven para atraer nuestra imaginación. No valoro cada metáfora por sí misma, sino el principio contenido en cada una. La metáfora en sí es un catalizador. Con su sola presencia y sin aumentar el contenido científico de su curso, acelera el proceso de la enseñanza, sin desgastarse ni descomponerse.

8. Un pellizco de vida

Según hemos visto en los primeros capítulos de este libro, resulta justificado decidir que la vida es un fenómeno sumamente complicado y sutil, cuya comprensión lleva al límite el ingenio humano…, y aún más allá. Pero ¿de qué están compuestos los organismos vivos que posibilitan este maravilloso fenómeno? Si el cuerpo humano, por ejemplo, se descompusiera en átomos separados y estas diferentes clases de átomos fuesen cuidadosamente segregadas, dos cosas resultarían obvias:

todos o casi todos los átomos pertenecerían a media docena de variedades;

y las variedades serían muy corrientes.

En primer lugar, el cuerpo está compuesto casi exclusivamente de agua, y cada molécula de agua se compone de dos átomos de hidrógeno y uno de oxígeno. Estos átomos se hallan asimismo en casi todas las demás moléculas del cuerpo. Aparte del agua, el cuerpo humano está formado principalmente de compuestos orgánicos, o sea, que contienen carbono. Los compuestos orgánicos más importantes son las proteínas, que contienen átomos de nitrógeno, junto con hidrógeno, oxígeno y carbono.

Los principales componentes del cuerpo de carácter inorgánico, o sea minerales, son los huesos. Sus átomos más comunes, aparte de los ya mencionados, son los de calcio y fósforo.

Si tuviésemos que contar el número de átomos del cuerpo humano veríamos que en cada diez mil hay:

6.300 átomos de hidrógeno

2.550 “ “ oxígeno

940 “ “ carbono

140 “ “ nitrógeno

30 “ “ calcio

21 “ “ fósforo

19 “ “ otros cuerpos

Esta lista no está llena de maravillas. El oxígeno es el tipo de átomo más corriente en la Tierra. El carbono, el calcio y el fósforo se cuentan entre los doce elementos más comunes en la corteza terrestre. Casi todos los átomos que forman los océanos son de hidrógeno, y la mayoría de los que componen la atmósfera son de nitrógeno.

Bien, pongamos ahora en primer lugar esa media docena de variedades de átomos que son, en realidad, los verdaderos elementos de la vida. ¿Y los otros diecienueve que pertenecen a otras variedades? ¿Por qué los necesitamos? Si estamos formados, en las 9.981/10.000 del cuerpo, con seis elementos, ¿no podríamos prescindir de las 19/10.000 restantes? Aparentemente no. La Naturaleza es como un buen cocinero que sabe que, aunque un pastel se componga de harina, leche y huevos, necesita al menos unos pellizcos de otras sustancias.

Veamos, pues, cuáles son esos diecinueve átomos de otras variedades. En lugar de contar los átomos de cada diez mil, contémoslos de cada millón. En este caso, hallamos que de cada millón de átomos del organismo hay: 998.100 átomos de los tipos antes mencionados

570» potasio

490» azufre

410» sodio

260» cloro

130» magnesio

38» hierro

2» otras variedades.

De esta forma, tenemos otra media docena de elementos presentes, podríamos decir, a modo de pellizcos. Cada uno es un elemento común del que tampoco podemos prescindir.

Los átomos de azufre son parte esencial de casi todas las proteínas del organismo, por lo que no podríamos vivir sin ellos.

El sodio, el cloro y el potasio están presentes como átomos cargados eléctricamente («iones»), disueltos en el fluido orgánico. Los iones de sodio y potasio llevan una carga eléctrica positiva. El ion de sodio se halla principalmente en el fluido que rodea a las células y el ion de potasio en el fluido interno de aquéllas. Los átomos de cloro llevan carga eléctrica negativa y son llamados «iones de cloro». Éstos están dentro y fuera de las células, equilibrando la carga positiva de los iones de sodio y de potasio.

Estos iones positivos son los responsables, entre otras cosas, de los fenómenos eléctricos del organismo. Los cambios de distribución de los iones de sodio y de potasio dentro y fuera de las células nerviosas son los responsables de las diminutas corrientes eléctricas que acompañan a los impulsos nerviosos. Sin ellos, no existirían tales impulsos, y la vida resultaría imposible.

La mitad del magnesio orgánico se halla en los huesos. El resto se presenta como iones cargados positivamente en los fluidos orgánicos. El magnesio está relacionado con las reacciones de energía del cuerpo. Las diminutas cargas de energía química pasan de un compuesto a otro, usualmente por medio de la acción de una sustancia conocida como adenosín trifosfato (ATP). Toda reacción que esté relacionada con el ATP requiere la presencia de un ion de magnesio, necesario para el manejo de la energía y, por tanto, de la vida.

Las moléculas de hemoglobina de la sangre contienen cuatro átomos de hierro cada una. La hemoglobina se apodera de las moléculas de oxígeno en los pulmones y las conduce a todas las células del organismo. Esos átomos de hierro de la molécula son los que se encargan del transporte, de modo que tampoco podríamos subsistir sin el hierro.

Si consideramos la hemoglobina y el ATP, vemos por qué el cuerpo sólo necesita unos cuantos átomos de ciertos elementos. Cada molécula de hemoglobina lleva cuatro moléculas de oxígeno de los pulmones a las células, y regresa en busca de un nuevo suministro. De igual forma, cada molécula de ATP lleva una carga de energía, siendo reformada a fin de que pueda ir en busca de otra.

Imaginemos unos albañiles construyendo un edificio. No es necesario un albañil para cada ladrillo. Trabajando bastante, un solo hombre puede colocar un millón de ladrillos. De este modo, aunque se necesiten muchos ladrillos, la casa se construye con unos cuantos albañiles.

De igual forma, nosotros necesitamos mucho oxígeno, pero sólo una pequeña cantidad de hierro; o mucha energía, y solamente una mínima cantidad de magnesio…, sólo para ayudar respectivamente a la hemoglobina y al ATP.

Naturalmente, no siempre sabemos por qué necesitamos un elemento dado. Por ejemplo, ¿por qué necesitamos los iones de magnesio? ¿Por qué no podrían los iones de calcio ayudar al ATP, si químicamente son semejantes a los de magnesio? Una buena pregunta, que aún no tiene respuesta.

El razonamiento de los albañiles también se aplica a otros elementos esenciales que sólo se necesitan en cantidades mínimas, aún menores que la de hierro. Se trata de los «elementos rastro».

Si contamos los átomos, no por diez mil o un millón, sino por cada mil millones, hallamos que en cada mil millones de átomos del organismo hay:

999.998.000 átomos de los tipos ya mencionados

1.500» cinc

170» manganeso

170» cobre

125» flúor

20» yodo

10» molibdeno

5» cobalto.

De éstos, el flúor se halla casi enteramente en los dientes, y no es realmente necesario para la vida, sino para poseer una buena dentadura. Los demás elementos rastro sí son esenciales para la existencia.

Los átomos de yodo forman parte de las moléculas hormonales fabricadas por la glándula tiroides. Las hormonas de esta glándula controlan la velocidad que el organismo produce y usa la energía. Para esta labor se precisa únicamente una pequeña parte de hormona, lo mismo que un termostato diminuto puede controlar un gran calentador. La hormona no podría efectuar su tarea sin la presencia del yodo, por lo que este elemento es esencial para nuestro organismo.

De todos los elementos esenciales, el yodo es el más raro de la Naturaleza. A pesar de la escasa cantidad que necesitamos, a veces se halla presente en el suelo de algunas regiones en cantidades insuficientes y, por tanto, en los alimentos vegetales de dichas comarcas y en los animales que de los mismos se nutren. Entonces, es necesario añadir unos pellizcos de yodo al depósito de agua de una ciudad, o utilizar una sal yodada (sal de mesa a la que se han añadido rastros de sustancias que contienen yodo).

El manganeso, el cobre, el cinc, el molibdeno y el cobalto están asociados cada uno con algunas de las enzimas que el organismo necesita para catalizar ciertas reacciones esenciales (ver el capítulo anterior). Por esto, sin tales elementos, las enzimas no podrían actuar.

Tal vez el lector se pregunte de qué puede servirle el cobalto al cuerpo humano, cuando de cada mil millones de átomos, el organismo sólo posee cinco átomos de tal metal.

Pero, ¿son tan pocos esos cinco átomos por cada mil millones? Se calcula que el cuerpo humano contiene unos cincuenta trillones de células, y un átomo es tantas veces más pequeño que una célula, que en cada una, a pesar de ser microscópica, alberga al menos a cien trillones de átomos.

Si cinco de cada mil millones de tales átomos son de cobalto, veremos que cada célula alberga un promedio de quinientos mil átomos de tal metal. Lo que demuestra que ni siquiera el más pequeño de los pellizcos de átomos es tan pequeño.

Y ahora que ya poseemos la receta de los tejidos humanos, que sabemos cuáles son las probabilidades de absorber diversas cantidades de los átomos necesarios para la vida, juntémoslos en la debida proporción y…

Bien, esto formará el tema del capítulo siguiente.

9. La construcción de un hombre

En setiembre de 1965, los químicos de la Ciento Cincuenta Asamblea Nacional de la Sociedad de Química Americana fueron exhortados por su presidente, el doctor Charles C. Price, como sigue:

–Me gustaría sugerir una cuestión de gran importancia pública, a la que la comunidad científica y el Gobierno prestan hoy día una gran consideración: el logro de la síntesis de la vida como objetivo racional…

»Yo creo que hemos progresado en el camino de, al menos, las síntesis parciales de los sistemas vivos como se ha progresado desde los años veinte en la liberación de la energía nuclear…, o desde los años cuarenta en el lanzamiento de un hombre al espacio.

¡Qué atrocidad! ¡La síntesis de la vida! Es tema tan viejo como la civilización.

En los tiempos antiguos existieron las jóvenes de Oro que (según Homero) ayudaron a Hefestos, el dios griego de la fragua, a formar la armadura de Aquiles. En los tiempos medievales, se originó el cuento del golem, un ser semejante a un autómata, hecho de arcilla, al que le infundió vida el rabino Löw, de Praga, mediante el sistema de emplear el nombre inefable de Dios. Y en los tiempos modernos, tenemos el bien conocido cuento de Pinocho, el títere de madera que consigue vivir como un ser humano.

¿Se convertirá en realidad este sueño dorado, o continuará eternamente siendo tan sólo un tema de ciencia-ficción? Esta misma pregunta fue formulada a una asamblea de científicos interesados en el problema. Los científicos son de por sí individuos cautos y por eso algunos situaron la síntesis de la vida en un futuro de miles de años; otros, más atrevidos, dieron un plazo de cientos de años; y otros, mucho más optimistas, sólo de decenios.

Pero cuando le formularon la misma pregunta a Hermán J. Muller, el ginecólogo ganador del Premio Nobel, contestó con firmeza:

–¡Se logró en 1955!

Seguramente, parece ridículo afirmar que la vida se sintetizó ya en 1955. ¿A qué se refirió Muller?

Bien, si la contestación de Muller suena como una paradoja, ésta se apoya en la definición de la vida, y en la simplicidad del sistema vital.

El individuo no científico, cuando piensa en la «vida», tiende a pensar en sistemas muy complicados. Así, piensa en él como hombre. Si medita en la fabricación de la vida sintética, conjura recuerdos de Frankenstein. Puede imaginarse el cuerpo de un hombre artificial, yacente sobre una mesa de operaciones, mientras que el científico le «insufla» la vida por medio de una radiación exótica o un producto químico.

Y sin embargo, con toda seguridad, no es así como será creada jamás la vida.

¿Por qué moldear a un ser humano, ya completo, de carne y huesos, músculos y cerebro, glándulas y venas? La Naturaleza no lo hace así…, en absoluto. Nadie inicia su existencia como ser adulto. Todos los organismos vivos de cualquier complejidad que sean, incluyendo los seres humanos, son máquinas de construcción propia, que empiezan con suma sencillez (al menos, en comparación con el producto final).

Los organismos vivos se componen de células, de diminutas (usualmente microscópicas) cargas de vida. El cuerpo humano se compone de unos cincuenta trillones de células, pero las formas de vida más sencillas, como la ameba, se componen de una sola célula: son cuerpos unicelulares.

Incluso los organismos multicelulares, que llegan a albergar trillones de células, empiezan con una sola célula: el óvulo fertilizado. Un hombre o una mujer, en realidad, se forma de una burbuja de gelatina viva, una burbuja que apenas puede verse a simple vista bajo una luz muy potente. Desde este óvulo fertilizado, debidamente alimentado en el útero femenino por la placenta materna, en un período de nueve meses se forma un niño que contiene trillones de células.

Para crear a un hombre, pues, sería suficiente crear un óvulo fertilizado. Sintetizar el óvulo es difícil, mas no tanto como sintetizar un ser adulto, un hombre ya completamente formado. Una vez formado el óvulo, puede continuar su expansión. Claro que ha de ser alimentado constantemente, pero en la actualidad conocemos casi la capacidad de hacerlo.

Los biólogos pueden ya mantener los órganos aislados, y hasta fragmentos de tejido vivo, durante un tiempo considerable. Antes de la Segunda Guerra Mundial, el famoso cirujano Alexis Carrel consiguió mantener el corazón de un embrión de pollo vivo y en crecimiento (tenía que reducirlo periódicamente) durante más de treinta y dos años. Fue una proeza, puesto que tenía que adoptar precauciones especiales para impedir que los tejidos se infectasen bacteriológicamente. Hoy día, con el descubrimiento de los antibióticos, esto ya no sería problema y los tejidos podrían vivir con más facilidad.

En cuanto al óvulo fertilizado, también en esto se han realizado grandes progresos. Con la tecnología actual es posible transferir un óvulo fertilizado de un cuerpo a otro, y desarrollarlo en éste. Hace setenta años ya se hizo esto con unos conejos. Y se ha realizado en casi todos los laboratorios del mundo, con animales de granja; y, dentro de la misma especie, un útero extraño produce a menudo una cría normal. Una oveja maravillosa dio a luz a once corderos en una sola temporada, cuando lo normal son uno o dos.

Lo que impide hacer lo mismo con los seres humanos es más el respeto que la falta de medios o conocimientos. En 1961, el doctor Danielle Petrucci, de Bolonia, Italia, extrajo un óvulo femenino sin fertilizar de un ovario de mujer, lo fertilizó dentro de un útero artificial de cristal, y allí vivió y creció por algún tiempo el embrión.

Se ha sugerido en varias ocasiones que las células espermáticas de un hombre podrían congelarse y mantenerse vivas a fin de que sus genes pudiesen transmitirse a muchos más retoños de los que podría producir en una existencia ordinaria.

Hasta ahora, los óvulos fertilizados sólo se han desarrollado fuera del claustro materno en las primeras fases. El proceso es detenido antes de que se formen los órganos. Si pudiese obtenerse el equivadente de una placenta, no habría la menor dificultad en formar un ser vivo completo desarrollado artificialmente de la unión de una célula óvulo y otra espermática. A este proceso se le denomina ya «ectogénesis».

La ectogenesia sería, plenamente desarrollada, de un gran valor científico para aprender cómo evoluciona la vida, a través de una observación continua.

Naturalmente, aquí nos enfrentamos con los aspectos antiutópicos de este posible suceso futuro. ¿Quién podría decidir cuáles serían los mejores padres potenciales? ¿Qué utilizaríamos como base para tal calificación? En la actualidad, nuestros conocimientos todavía son muy escasos para poder implantar una sociedad ectogénica.

Claro que una sociedad ectogénica no realiza el sueño de la vida creada por el hombre. No es suficiente tomar una vida que ya existe…, en forma de un óvulo fertilizado, y hacerla progresar. En tal caso, sólo efectuamos dentro del cristal lo que el cuerpo humano hace con carne.

¿Cómo podríamos fabricar una célula partiendo de materiales no vivos? De esta manera conseguiríamos formar un espécimen vivo totalmente nuevo, que no le debiera nada a una vida anterior.

Lo cual es muy fácil de decir…, pero muy difícil de hacer. Una simple célula es un sistema muy complejo, mucho más, a pesar de su tamaño microscópico, que los transatlánticos y los rascacielos más gigantescos que el hombre pueda construir.

Sí, cierto, podríamos investigar en la Naturaleza y tratar de averiguar cómo se forma una célula en ella. La respuesta, por lo demás, es sencilla. Todas las células existentes en la actualidad se han formado de otras células. Todas las células de un cuerpo humano se han formado del original óvulo fertilizado que fue el comienzo de dicho cuerpo. Cuyo óvulo también se formó de un gameto masculino y otro femenino, los que a su vez procedían de otras células…, y así sucesivamente durante millones de años.

Para volver al verdadero principio, las células deberían formarse de ninguna célula… ¿Y cómo sería esto posible? No lo sabemos. En este aspecto, sólo podemos formular suposiciones razonables.

Sería preciso poseer una mente muy osada para que un científico empezara a sospechar que el paso de la no célula a la célula, de la no vida a la vida, pudo tener lugar como un proceso químico, casual, ciego. Nuestra cultura occidental se halla demasiado imbuida por la sacramentalidad y la exclusividad de la vida humana para creer que se trata de un producto casual.

Un bioquímico de la Unión Soviética, país oficialmente ateo en su filosofía de la vida, A. I. Oparin, empezó a escribir sobre este tema en 1924, afirmando que las células se iniciaron a través de unos fenómenos simples e inevitables, a la par que naturales.

Consideró, por ejemplo, la formación natural de las gotitas de un líquido en suspensión dentro de otro, en las condiciones que prevalecen en el océano primitivo.

Avanzando en esta dirección, casi una generación más tarde, Sidney W. Fox, del Instituto de la Evolución Molecular de la Universidad de Miami, preconizó aún más esta teoría.

El profesor Fox empieza con un sistema químico destinado a representar las condiciones que los químicos suponen que se hallaban en la Tierra primitiva hace varios miles de millones [3] de años, y sujeta todo el sistema al calor…, calor que en la Tierra era formidable gracias al Sol.

Empezando con compuestos sencillos del tipo que podían existir hace millones de años, halla que el calor solo basta para formar aminoácidos, y luego les fuerza a unirse en largas cadenas para producir unos compuestos semejantes a las proteínas, a los que denomina «proteinoides».

Éstos actuaban mucho mejor a temperaturas por encima del punto de ebullición del agua, y algunos biólogos dudaron de que tal proceso pudiese tener lugar en la Tierra primitiva, sin que los proteinoides se descompusieran ya al formarse. Fox, no obstante, traza una imagen de proteinoides formándose sobre las cenizas calientes de un volcán, siendo disueltos y alejados por una lluvia cálida mucho antes de que tengan ocasión de descomponerse.

Fox halló que cuando sus proteinoides se disolvían en agua caliente, y dejaba luego enfriar la solución, las mayores moléculas semejantes a proteínas tendían a aglomerarse en forma de pequeños globos a los que llamó «microsferas».

Dichas microsferas semejan, en ciertos aspectos, células muy simples. En tamaño y forma son como diminutas bacterias. Se hallan rodeadas de una especie de membrana igual que las células. Pueden aumentar y disminuir mediante cambios apropiados del fluido ambiente, lo mismo que hacen las células. Pueden producir vástagos, que a veces crecen y se desgajan. Pueden dividirse en dos, o unirse en cadenas. El material que se halla dentro de estas microsferas presenta ciertas reminiscencias con las enzimas del tejido vivo.

Sin embargo, bajo ningún concepto pueden considerarse vivas a las microsferas, pero ¿es posible hablar de vida o no vida, como si ambos extremos estuvieran separados por una inmensa frontera? Muchos biólogos no lo creen así. La vida y la no vida se hallan separadas por una amplia zona dentro de la cual hay objetos que pueden considerarse como progresivamente más vivos y menos no vivos. En cuyo caso, las microsferas, aunque muy lejos de residir en el lado completamente vivo de la zona límite, se hallan al menos muy lejos de la parte no viva.

Tal vez Fox y otros científicos logren perfeccionar más las microsferas, consiguiendo trasponer las fronteras de la vida indudable. Y tal vez no. Es muy difícil predecirlo.

Tal vez sea un error tratar de saltar de la nada a la célula. Quizás una célula no sea el objeto más idóneo como meta inmediata de los sintetizadores de la vida. Es probable que la célula no fuese el primer producto de la evolución natural de la vida. La célula, tal como la conocemos en la actualidad, tal vez no sea un ejemplo de la vida primitiva, ni mucho menos, y sí el producto final de un largo período evolutivo. Durante muchos millones de años, antes de que surgiese la primera célula, pudo haber en existencia otras estructuras más simples. Una vez formadas las células, no obstante, su superior eficiencia hizo desaparecer a las demás estructuras «precelulares», dejándonos hoy en un mundo de vida en que la célula nos parece el principio más simple sólo porque ha borrado a todos sus competidores.

Pero las estructuras precelulares no desaparecieron sin dejar rastro.

Dentro de cada célula hay cuerpos más pequeños. Por ejemplo, el núcleo de una célula contiene los cromosomas que controlan todo lo relativo a la herencia. Fuera del núcleo se encuentran las mitocondrias, que contienen la maquinaria relacionada con la producción de energía. En las células vegetales se hallan los cloroplastos, que son versiones vivientes de la batería solar, destinadas a convertir la energía del Sol en la energía química del alimento almacenado.

Todos estos «organitos» (por diminutivo de «órganos»), podrían representar los restos de las primitivas estructuras precelulares; las cuales habrían concluido por existir en una cooperativa, formando estructuras mucho más complejas y eficaces que aisladamente. Estas cooperativas precelulares (que hoy llamamos células) acabaron por dominar al mundo.

De esos «organitos», los más fundamentales son los cromosomas. Cada especie contiene un número característico de ellos en todas las células. Cada célula humana posee cuarenta y seis cromosomas, como fideos romos, espesos y entremezclados, en ciertas fases del crecimiento de la célula.

Cada vez que una célula se divide en dos, cada cromosoma sufre unos cambios que producen dos cromosomas, réplicas perfectas del original. A este proceso se le llama «duplicación». Si seguimos el rastro de los cuarenta y seis cromosomas de cada una de los cincuenta trillones de células del cuerpo humano adulto, hallamos que se originaron de los cuarenta y seis cromosomas del óvulo fertilizado original. Éstos se obtuvieron de dos progenitores, la mitad de la célula espermátíca del padre o gameto masculino, y la otra mitad de la célula del óvulo materno, o gameto femenino. Dichos cromosomas derivaron de los gametos paternos…, y así sucesivamente.

Los cromosomas son los que vigilan la formación de enzimas dentro de una célula. En cada generación, los cromosomas de dos progenitores forman una nueva combinación; además, siempre se producen cambios de orden menor de los cromosomas cuando uno pasa de los padres a los hijos. Como resultado de esto, jamás puede haber dos individuos (aparte de los mellizos idénticos procedentes del mismo óvulo) que tengan precisamente los mismos cromosomas, ni dos individuos con las mismas enzimas.

Éstas supervisan la función química de cada célula, dándole a cada ser vivo su vida y su individualidad. Por tanto, podemos considerar a los cromosomas como el verdadero principio de la célula, lo mismo que ésta (en forma de óvulo fertilizado) en el verdadero principio del adulto global.

Tal vez sea éste el componente que todavía falta en las microsferas de Fox. Si pudiéramos sintetizar los cromosomas e introducirlos en las microsferas, habríamos creado indudablemente vida. O tal vez, si formásemos cromosomas podríamos impulsarlos a formar sus propias células.

Esto sería posible, puesto que existen pruebas (aparte de la simple lógica) de que los cromosomas son más fundamentales que las células. Éstas no existirían sin los cromosomas, y en cambio, éstos existen sin células.

Estos corpúsculos, que semejan cromosomas, son los que llamamos «virus». Son mucho más delgados que una célula y de estructura mucho más simple. Tienen el tamaño de un cromosoma y se parecen a éste en su estructura química y su función.

Los corpúsculos estilo virus existían hace ya miles de millones de años, antes de la evolución de las células, siendo capaces de reproducirse independientemente. Pudieron llevar en sí mismos toda la capacidad de crecimiento y multiplicación y pudieron, por tanto, haber sido más complejos que los modernos virus.

Puesto que los existentes hoy día han sido malogrados por la disponibilidad de las células. El virus moderno es un parásito completo que ha abandonado el material necesario para vivir con independencia, limitándose a mantenerse fuera de la célula. Una vez logra penetrar en una célula apropiada, emplea la maquinaria química para sus propósitos; se multiplica a expensas de las necesidades de la célula y a veces mata a su anfítriona en este proceso.

Al principio hubo dudas en si considerar como entes vivos a los virus, pero la mayoría de biólogos se han decidido en favor de su vitalidad. En parte, esto es lo que da pie al desacuerdo existente entre los científicos respecto a cuándo puede la vida ser sintetizada. Si por vida entendemos células complejas, la vida sintética puede aún hallarse muy lejos; si, por el contrario, consideramos como ser vivo a un virus, el objetivo está mucho más próximo.

Ordinariamente, por ejemplo, un virus se reproduce a sí mismo dentro de la célula, utilizando las enzimas necesarias, las materias primas, y las fuentes de energía presentes allí en abundancia. Pero supongamos que tomamos una pequeña cantidad de virus y le suministramos los materiales necesarios para que actúe fuera de la célula.

En octubre de 1965, el profesor Sol Spiegelman, de la Universidad de Illinois, informó respecto a su labor en esta orientación. Consiguió producir virus en un tubo de ensayo. En cierto sentido, esto significa una síntesis de la forma más simple de vida, aunque en el verdadero sentido no sea una síntesis completa. Tuvo que emplear un fragmento de virus como iniciador, de modo que el proceso se asemejó al de un pollito (o un ser humano) creciendo a partir del huevo. Lo que desearíamos realizar es una vida sintética perfecta: vida formada de un sistema que no contuviese vida en absoluto.

Para estudiar mejor estas posibilidades, examinemos más de cerca la estructura química del cromosoma o del virus.

El interior de un cromosoma o de un virus, está compuesto por una larga cadena enrollada de átomos formando una molécula de ácido nucleico. La variedad de este ácido en los cromosomas y los virus más complejos es el «ácido desoxirribonucleico», abreviado usualmente como ADN. A su alrededor, como protección, hay una capa de proteínas.

Las moléculas de ADN y de proteínas son muy complejas y en su interior poseen una gran capacidad para la variación (ver Capítulo 2). Los bioquímicos conocen desde hace más de un siglo la versatilidad de las proteínas, en tanto que los ácidos nucleicos son unos recién llegados en la conciencia biológica. Además, las proteínas están compuestas de veinte tipos de unidades diferentes, en tanto que los ácidos nucleicos sólo de cuatro. Por tanto, a mediados de la década de los 40, se dio por descontado que eran las proteínas, y no el ADN, la clave química del cromosoma o el virus. Pero a comienzos de 1944, todas las pruebas se inclinaron asombrosamente en favor del ADN.

Como ejemplo de un experimento de esta clase, podemos citar el llevado a cabo en 1955 por Heinz Fraenkel-Conrat, investigador bioquímico del ácido nucleico de un virus. De modo algo complejo, consiguió separar el núcleo de los virus de su envoltura. Separados de esta forma, ni la capa sola ni el núcleo solo podían infectar a las células. El virus parecía muerto. Después, mezcló de nuevo las envolturas y los núcleos, y algunos virus volvieron a poder infectar a las células.

Durante un tiempo, fue como si hubiesen matado a un organismo vivo, para resucitarlo después. Aunque los organismos objeto de tal experimento perteneciesen a las formas más simples de vida, la hazaña mereció ser destacada en los periódicos.

Sin embargo, resultó que ni se había matado la vida ni había sido resucitada. El núcleo del ácido nucleico poseía una vida propia. De vez en cuando, conseguía alguno penetrar dentro de una célula, infectándola eficazmente, sin la presencia de la proteína envolvente. La proteína ayuda al ácido nucleico a penetrar en las células (como un coche ayuda a un hombre a trasladarse de Nueva York a Chicago), pero, con ciertas dificultades, el ácido nucleico también puede hacerlo solo, lo mismo que un hombre podría recorrer a pie el trayecto Nueva York-Chicago, en caso de absoluta necesidad, o por capricho.

También se demostró que cuando un virus intacto invade una célula, sólo lo hace el ácido nucleico sin la proteína. Ésta, tras haber facilitado la entrada del núcleo, permanece fuera de la célula, aunque supervisa la formación de una envoltura de proteína (distinta a las proteínas que formaría la célula atacada por sí sola).

Los científicos empezaron a centrar su atención, por consiguiente, en el ácido nucleico, a partir de 1944, particularmente en el ADN, su variedad más importante. Un médico neozelandés, Maurice H. F. Wilkins, que fue uno de los científicos que trabajaron en la bomba atómica durante la Segunda Guerra Mundial, estudió el ADN bombardeando sus moléculas con rayos X. Las fotografías obtenidas fueron estudiadas por un colega inglés, el bioquímico Francis H. C. Crick, y su colaborador norteamericano, doctor James D. Watson (que en su juventud fue uno de los Chicos Quiz de la radio). En 1953, descubrieron la estructura del ADN, demostrando que se trataba de una cadena doble de cuatro unidades diferentes, pero estrechamente vinculadas entre sí, llamadas «nucleótidos».

La molécula ADN presenta innumerables fórmulas posibles, según el orden en que se hallen distribuidas las diferentes unidades. Watson y Crick demostraron cómo una molécula podía formar nuevas moléculas de la misma y exacta fórmula.

Otros bioquímicos, trabajando arduamente, descubrieron la manera en que la fórmula del ADN era trasladada a la fórmula análoga de una proteína, de forma que las porciones específicas de la molécula ADN produjese enzimas específicas, controlando de este modo la química celular. El traslado de «instrucciones» de la fórmula del ácido nucleico a la fórmula enzimática se denomina «código genético».

Aparentemente, pues, la reacción química fundamental de la vida es la capacidad de la molécula ADN de replicarse. Ésta es toda la ley, y lo demás es complementario.

Por lo tanto, si fuésemos capaces de formar una molécula ADN a partir de sustancias simples, no vivas, habríamos sintetizado el mismísimo principio de la vida. Naturalmente, existiría aún un abismo casi insondable entre esta síntesis y la del hombre, pero no dejaría de ser un auténtico comienzo. Ya habríamos cruzado el umbral entre la no vida y la vida.

¿Cómo cruzó la Naturaleza este umbral? Debió hacerlo hace miles de millones de años, cuando no existían aún enzimas que realizasen esta tarea, ni ácidos nucleicos que sirviesen de fotocopias.

Es posible que en la primitiva Tierra, falta de vida, sólo hubiesen estado presentes unas moléculas simples, en cierta cantidad en el océano, donde se cree que se originó la vida, y en la atmósfera. La naturaleza de dichas moléculas puede deducirse de la composición global de la primitiva Tierra (conocimiento basado en la composición del Sol y del Universo en general), y en las leyes conocidas de las combinaciones químicas.

Empecemos con tales moléculas: agua, amoníaco, metano, cianuro de hidrógeno, etcétera, y añadámosles energía en forma de radiación ultravioleta, radiactividad, corrientes electrónicas o rayos y relámpagos (todo lo cual pudo estar presente en la primitiva Tierra). ¿Qué ocurrirá?

Charles Darwin, el fundador de la teoría de la evolución mediante la selección natural, consideró esta cuestión hace cien años y se preguntó si los productos químicos de los tejidos vivos no se habrían construido con este sistema; si no habría habido una «evolución química» además de la evolución de las especies.

El primero que trató de investigar experimentalmente este asunto fue Melvin Calvin, de la Universidad de California. En 1951, comenzó a observar el efecto de la radiación energética al derivar compuestos complejos de los simples.

En 1952, Stanley L. Miller, de la Universidad de Chicago, avanzó aún más. Colocó los elementos químicos simples del tipo presente en la tierra primitiva en un recipiente absolutamente libre de materia viva y los sometió a la acción de una descarga eléctrica durante una semana. Una vez hecho lo cual, detectó la presencia de unas sustancias más complejas que las iniciales, incluyendo cuatro aminoácidos diferentes, cada uno perteneciente a una variedad presente entre las unidades de proteínas formadas por su propia naturaleza.

Desde entonces, otros químicos como Philip H. Abelson, del Instituto Carnegie, y Joan Oró, de la Universidad de Houston, han experimentado de forma semejante. Bajo el impacto de diversas formas de energía, se formaron compuestos complejos a partir de material inicial más simple. Luego, usando esos compuestos complejos como material inicial, lograron elementos compuestos más complejos todavía. Todos ellos eran similares a los componentes clave de los tejidos vivos. La ruta natural seguida por esta formación ciega y casual parecía apuntar directamente a la vida.

Un ceilandés-americano, el bioquímico Cyril Ponnamperuma, que trabajaba en el Departamento de Investigaciones Ames de la NASA, demostró la producción de porciones de moléculas nucleótidas, que son los bloques constructores de los ácidos nucleicos. Un nucleótido completo contiene átomos de fósforo. Por tanto, se añadieron a la mezcla sustancias que contenían fósforo simple. Junto con Carl Sagan y Ruth Mariner, Ponnamperuma inició un curso de experimentación que concluyó con la producción de una molécula nucleótida completa. En 1963, los nucleótidos se habían formado en la particular alta energía que podía usarse para producir ácidos nucleicos.

En septiembre de 1965, Ponnamperuma anunció que había dado otro paso adelante. Consiguió obligar a dos nucleótidos a juntarse en un «dinucleótido», que contenía la misma clase de enlace que el que une a los nucleótidos para formar ácidos nucleicos naturales.

Por consiguiente, está claro que los científicos poseen una cadena lineal de síntesis que empieza desde los compuestos simples que existían en la Tierra, cuando nuestro planeta comenzó a tomar su forma actual, hasta llegar a las moléculas que apuntan directamente a los ácidos nucleicos. Y en esta cadena no hay baches.

De esta forma se consigue la imagen de los cambios inevitables a través del nivel molecular. Se empieza con un planeta como la Tierra, con un complemento de compuestos simples que pueden existir en ella, se añade la energía de un Sol cercano, y se termina con el ácido nucleico. Esto no puede negarse, y lo único que necesitan los científicos es dirigir el proceso y acelerarlo.

La síntesis de los nucleótidos mediante los convenientes métodos químicos (no necesariamente como los procesos casuales que tuvieron lugar en los sistemas seguidos por Ponnamperuma), ya es algo viejo. El químico escocés, Alexander R. Todd (hoy día barón Todd de Trumpington) había ya sintetizado varios nucleótidos por los años cuarenta.

Pero, ¿y el paso de los nucleótidos a los ácidos nucleicos? En 1955, el científico español Severo Ochoa, en una Universidad de Nueva York, empezó con una solución de nucleótidos en forma de alta energía y con enzimas apropiadas, y formó moléculas muy semejantes a los ácidos nucleicos naturales…, aunque en la mezcla que sirvió de modelo no había una sola molécula de ácido nucleico.

Fue a esta síntesis del ácido nucleico partiendo de moléculas simples a la que Muller debió referirse al declarar que la vida se había sintetizado en 1955.

Naturalmente, las moléculas de ácido nucleico sintetizadas sin un modelo se juntan al azar y tienden a ser más simples que las naturales. Esos ácidos nucleicos sintéticos no encajan en las funciones de ninguna célula ni pueden penetrarlas y multiplicarse en ellas. Pueden poseer una vida en potencia, pero no pueden pasar de la potencialidad a la acción.

El biólogo se halla en una fase en que puede:

1) Formar moléculas de ácido nucleico modeladas sobre alguna molécula presente en el sistema. Tales moléculas pueden considerarse como vivas, pero no formadas de materias iniciales completamente no vivas.

2) Formar moléculas de ácido nucleico mediante materias iniciales completamente no vivas. Tales moléculas no pueden fabricarse para demostrar los fenómenos asociados con la vida.

Formar una molécula de ácido nucleico indudablemente viva con materiales iniciales completamente no vivos, se halla aún fuera del poder de la ciencia…, aunque seguramente no por mucho tiempo, siendo a esto a lo que el doctor Charles C. Price se refirió en la declaración con que comencé el presente capítulo.

Examinemos las posibles consecuencias que se derivarían del hecho de que los científicos lograsen un día formar ácidos nucleicos sintéticos, virus sintéticos, cromosomas sintéticos…, vida sintética.

¿Habría peligros inmediatos? Supongamos que los científicos fabricasen un virus nuevo que pudiese invadir una célula; un virus nuevo contra el que el hombre no tendría ni habría desarrollado tal vez ninguna defensa. ¿Podría este virus inimaginablemente mortal borrar de la Tierra a toda la Humanidad y quizás incluso toda la vida celular? Naturalmente, las probabilidades son muy escasas. La invasión y explotación de una célula por un virus es un fenómeno extraordinariamente complejo. Que pueda tener lugar es el resultado de miles de millones de años de evolución lenta, y los virus están usualmente adaptados a ser parásitos sólo de algunas células de ciertas especies.

Para suponer la formación de semejante virus destructor, casualmente, hay que dar por descontado que él mismo se armonizaría con todas las idiosincrasias de algunos tipos de las células humanas, y que poseería la capacidad de destruirlas a todas, lo cual es demasiado improbable. No es matemáticamente imposible, cierto, pero es sumamente improbable.

Entonces, examinemos otras posibilidades más constructivas y optimistas.

Es posible que esté alboreando el día en que podamos duplicar un prístino triunfo de la Humanidad, a nivel más sutil y sofisticado.

Una vez, en las nebulosas épocas prehistóricas, el hombre fue un acaparador de alimentos. Se comía a los animales salvajes que lograba matar o los frutos y bayas que conseguía coger. Si tenía poca suerte en la caza o en la recolección de frutos, pasaba hambre.

Luego, llegó el momento en que la Humanidad aprendió a domesticar a los animales, a alimentarlos, a cebarlos y a vigilarlos, a utilizar su leche, su lana, su trabajo, y a matarlos para saciar su hambre con su carne. También aprendió a cultivar los vegetales y a recolectarlos.

De devorador de alimentos pasó a ser pastor y agricultor, pudiendo conseguir más comida y con más facilidad. La Humanidad tuvo su primera explosión de población como resultado de estos descubrimientos hace unos diez mil años.

Respecto a las sustancias celulares, todavía nos hallamos en la primera fase de devorar comida. Por ejemplo, fijémonos en la insulina. Como se ha dicho, es una proteína producida por una glándula llamada páncreas. No es una enzima sino una hormona necesaria para el debido funcionamiento del organismo. En su ausencia, o casi ausencia, el organismo humano sufre de diabetes (ver Capítulo 3).

Un hombre diabético puede llevar una vida normal si se le administra inyecciones de insulina con regularidad. Ésta se obtiene del páncreas del ganado y los cerdos sacrificados. «Devoramos» la insulina del páncreas que tenemos más a mano…, un páncreas exactamente por cada animal sacrificado. Lo cual significa que el suministro es limitado.

En realidad, este suministro es suficiente, pero, ¿por qué extraer esta insulina si existe la posibilidad de poder obtenerla de las «hordas» de moléculas? Supongamos que no «sustraemos» la insulina de las células pancreáticas sino de las moléculas de ácido nucleico que presiden la formación de la insulina. Si «pastoreásemos» este ácido nucleico, manteniéndolo bien nutrido con las materias primas que necesita, podríamos formar la insulina en cantidades indefinidas, igual que la vaca produce la leche. Entonces, poseeríamos nuestro propio suministro de insulina y no dependeríamos de los animales que sacrificamos. Además, formaríamos réplicas de ácido nucleico, a buen seguro y nunca más tendríamos que recurrir a los animales.

¿Podemos prever un futuro en que se construyan factorías donde la maquinaria sean ácidos nucleicos submicroscópicos? ¿No podría la Humanidad reunir una serie de centenares o millares de enzimas complejas y otras proteínas? Algunas de las primeras se utilizarían para provocar las reacciones químicas de modo más conveniente que con los métodos actuales. Otras podrían usarse en medicina o para ayudar a la fabricación de vida.

Es incluso posible que algunos de los materiales formados sirviesen de comida. La proteína manufacturada podría utilizarse para fortificar los alimentos naturales en las partes subdesarrolladas del Globo. Al principio resultaría un proceso caro, pero los alimentos así obtenidos se compondrían de sustancias nutritivas puras, sin hueso, cartílagos ni grasas, y de un valor alimenticio muy elevado.

El hombre medio de la Tierra seguramente se resistiría a la introducción de estos alimentos «antinaturales» en su dieta, pero, ¿y en las colonias de la Luna o de Marte? En ausencia de ganado y de vegetales, y considerando el coste elevadísimo del transporte de ambos por vía espacial, seguramente sería preferible utilizar ácidos nucleicos. Las materias primas de las moléculas de ácidos nucleicos podría extraerse de los minerales existentes en dichos planetas. (A ello contribuirían en gran medidala caliza y los silicatos hidratados).

En realidad, la colonización del Sistema Solar no será una aventura práctica hasta que hayamos dominado adecuadamente las moléculas de ácido nucleico.

La Humanidad tampoco necesita seguir las hazañas de las células con tremenda exactitud. Al fin y al cabo, los ácidos nucleicos no siempre producen réplicas exactas de sí mismos. A veces, en la duplicación se introducen ligeros errores. Esto no es muy grave en sí, ya que los errores ocasionales dan por resultado una nueva clase de ácido nucleico útil para las células en que tal error concurre. Son precisamente estos errores casuales los que han dado por resultado el proceso evolutivo a lo largo de los dos mil millones de años, o más, en que tardó el hombre en surgir desde la ameba.

El hombre incluso puede alentar la aparición de tales cambios en los ácidos nucleicos durante sus duplicaciones. Tratándolos con calor, radiación o ciertos productos químicos, aumenta el número de errores. Los nuevos ácidos nucleicos forman moléculas de proteína (muchas de las cuales son enzimas), también modificadas, con fórmulas levemente distintas a la original. La mayor parte de tales proteínas serían inútiles, pero algunas poseerían propiedades nuevas e importantes que no se hallan en la Naturaleza.

(Los químicos ya han experimentado este proceso. Hace cien años aprendieron a fusionar productos químicos que no se encuentran en la Naturaleza. Con lo cual, descubrieron nuevos tintes, nuevos medicamentos y hasta nuevas moléculas gigantes, como las de las fibras sintéticas y los plásticos. En muchos casos, las nuevas sustancias perfeccionaron a la Naturaleza.)

¿Por qué, pues, no formar nuevos ácidos nucleicos que formarían proteínas nuevas que, a su vez, mejorarían a las de la Naturaleza, de una forma u otra? Aparte de «pastorear» a nuestros ácidos nucleicos, «criaríamos» nuevas variedades, tal como hacemos con el ganado o con el trigo.

¿Podría ser aplicada directamente a los seres humanos la nueva tecnología de los ácidos nucleicos?

Sigamos especulando.

Cada cromosoma está compuesto de centenares o de miles de unidades de ácido nucleico, cada cual capaz de alentar la formación de proteínas particulares. El nombre más antiguo de tales unidades es el de «genes». Todo ser humano posee sus propios genes, y cada uno de nosotros, probablemente, tiene en sus células algunos genes defectuosos, incapaces de formar ciertas enzimas de un modo apropiado.

A menudo, este defecto no es grave; a veces, sí. Los científicos están aprendiendo a identificar los genes mediante diversas técnicas. En 1962, Robert S. Edgar, del Instituto Tecnológico de California, identificó la mitad de los genes presentes en un virus particular, descubriendo la naturaleza de la enzima que cada uno producía.

Eventualmente, dada una serie de cromosomas de una célula, pueden desarrollarse técnicas que determinen la naturaleza de cada gen [4] presente. Todas las células de un individuo poseen la misma serie de genes, de modo que este «análisis genético» puede realizarse en las células blancas de una gota de sangre, residiendo todo el proceso en un simple pinchazo.

Tal vez llegue el momento en que todos los individuos pasen por este análisis al nacer. Y una vez analizada e identificada la serie de genes, ¿podría hacerse algo al respecto? Tal vez. Sería posible, seguramente, gracias a esta ficha de genes defectuosos, predecir el futuro estado de salud del recién nacido, y adoptar las adecuadas medidas preventivas; incluso podría proyectarse su carrera de acuerdo con sus potencialidades físicas. La ficha del análisis genético llegaría a ser una parte esencial del hombre, que llevaría constantemente encima, con un duplicado en un Departamento Central.

Aunque todas las células de un ser humano posean la misma serie de genes, éstos no se expresan siempre del mismo modo. Las células se especializan, y unas se convierten en células nerviosas, otras en musculares, y en fin, en epidérmicas, hepáticas, pancreáticas…, y así sucesivamente. Cada célula posee su propia serie de enzimas, lo que significa que en cada clase de célula unos genes no pueden actuar, en tanto que otros han de hacerlo en doble tiempo.

Los científicos aún ignoran exactamente qué es lo que obstruye la labor de unos genes y alienta la de los demás; pero éste es el problema más urgente con que se enfrentan hoy día los bioquímicos, problema que han atacado desde diversos ángulos. Unos buscan las proteínas contenidas en los cromosomas, que podrían constituir el agente obstructor. Otros estudian los productos de la acción enzimática; los mismos, podrían relajar la acción de las enzimas que los producen. Y este «retroceso» podría entrañar la obstaculización de algunos genes. Naturalmente, otros bioquímicos examinan otras posibilidades.

Supongamos que ya sabemos lo bastante para desobstruir a los genes. En este caso, tendríamos células poseedoras de todas las capacidades del primitivo óvulo fertilizado.

Si de este modo pudiéramos «desespecializar» el muñón de un brazo o una pierna amputada, ¿podríamos ser tratados de forma que el muñón volviera a convertirse en el brazo o la pierna completos? ¿Podrían regenerarse los nervios de modo que la parálisis fuese ya una cosa del pasado? ¿Podríamos reconstruir los ojos, para que la ceguera fuese ya sólo un mal recuerdo para la Humanidad? Retrocedamos más y llevemos el análisis de los genes al original óvulo fertilizado. Supongamos que a un óvulo fertilizado se le permitiera dividirse en dos, separando una de las células nuevas. No se ha causado ningún perjuicio, puesto que la otra célula podría dividirse de nuevo en dos, una y otra vez, produciendo un individuo completo.

(En realidad, los mellizos idénticos nacen cuando el primer par de células formadas por la división del óvulo fertilizado se separa, siguiendo cada célula su propio rumbo.) La célula separada podría utilizarse para el análisis genético. Entonces, sería posible decir desde el mismo principio si podía permitírsele a la célula restante desarrollarse en embrión o no.

Supongamos que hallamos que un gen clave del óvulo fertilizado es defectuoso, si bien la fórmula es muy buena, y dará vida a un ser humano superior. Sería una lástima perder esta posibilidad por culpa de un gen. ¿Podría entonces sustituirse el gen defectuoso por otro procedente de un «banco de genes»? En 1964, Muriel Roger, de la Universidad Rockefeller, manifestó haber transferido un gene individual de una célula bacterial a otra. La célula que recibió el gen pudo entonces producir una nueva enzima que antes no había podido crear. Por tanto, la idea del trasplante de los genes no es tan monstruosa.

Supongamos asimismo que un óvulo fertilizado tiene varios genes defectuosos, demasiados para salvar a un individuo completo. Sin embargo, podría ocurrir que ninguno de tales genes defectuosos impidiese la función del corazón o los ríñones. ¿Sería posible, entonces, bloquear varios genes, de manera que se especializase al momento y desarrollase sólo un corazón o un riñon? De este modo, tal vez podríamos utilizar un suplemento de nuevos órganos para su trasplante.

Todo esto parece una locura, cierto, pero el progreso avanza a una velocidad enorme. Y esos sueños tal vez sean una realidad en menos de diez años. Setenta años solamente después del primer vuelo tambaleante y desalentador de los hermanos Wright, los aviones a propulsión giran ya en torno a la Tierra. Cuarenta años después de que Robert H. Goddard lanzase al aire el primer cohete de impulsión líquida a unos sesenta metros de altura, los cohetes han llegado ya a Marte, y al lejano Júpiter, para ir a perderse en los insondables abismos de otras galaxias.

¿Quién sabe, pues, en qué fase de la bioquímica estaremos en el año 2000…, año que muchos de nosotros aún veremos? La capacidad de la bioquímica, naturalmente, puede causar cierta aprensión. ¿Sabemos ya lo bastante para jugar a dioses con la vida y con los seres vivos? Tal vez no, pero no sería la primera vez que el hombre ha corrido grandes riesgos. Desde que empezó a aplicar su inteligencia a la modificación de su ambiente ha estado jugando a dios. Cuando el hombre domesticó a los animales, inventó la agricultura y construyó las ciudades, creó la «civilización». Esto alteró profundamente su forma de vivir e introdujo problemas que antes no existían. Mas en conjunto, todo ello representó una mejoría, y nadie querría volver a los tiempos primitivos.

Cuando el hombre inventó la máquina de vapor, dominó la corriente eléctrica, diseñó el motor de combustión interna e ideó la bomba nuclear, creó una tecnología que nuevamente cortó las amarras de su forma de vivir. Bien sabe Dios que con ello se han creado inmensos problemas y, sin embargo, muy pocos de nosotros querríamos retroceder a la era preindustrial.

Sin la menor duda, una era bioquímica y bioindustrializada nos presentaría otra serie de cambios cruciales, de problemas aplastantes, pero a juzgar por las experiencias pasadas, el hombre conseguiría salir airoso de todo. Y los beneficios serían superiores a las catástrofes.

Además, si el hombre empezase realmente a programar una serie de mejoras para sí mismo, sería el hombre perfeccionado, o sea, casi el superhombre, el que buscaría las nuevas mejoras.

Cada logro resultaría más fácil que el anterior y, gracias a esta espiral ascendente, la Humanidad podría conseguir al fin su salvación y su salud, para emerger a las llanuras vivificadas por el sol del potencial humano.

Primera parte

(continuación)

Relativa a los más o menos conocido

2. NO VIDA

10. El elemento flamígero

Desde el momento de su descubrimiento, el gas inflamable, el hidrógeno, ejerció un efecto revolucionario sobre toda la Humanidad. Quebrantó viejas teorías y ayudó a formular otras. En dos ocasiones diferentes, condujo a los hombres hacia las estrellas. Ahora apunta hacia los interminables depósitos de energía para las necesidades del hombre futuro.

Su historia comenzó en llamas, pues en el siglo XVII, los primeros químicos produjeron un «aire» nuevo, con el hierro y un ácido, un «aire» que explotaba al ser calentado. Y lo denominaron «aire inflamable».

El químico inglés Henry Cavendish, que estudió la nueva sustancia en 1766, vio que producía algo más notable que una llama. Cuando dicho gas se quemaba y se combinaba con algo del aire (el oxígeno, según se averiguó más adelante), se formaban unas gotas líquidas que resultaron ser de agua. De la llama surgía el agua.

El mundo de la química se sintió maravillado. Durante miles de años se había creído que el agua era un elemento, y que, por tanto, no podía formarse de elementos más simples. Y sin embargo, la combinación de dos gases producía agua.

Al aire inflamable se le designó con un nombre, «hidrógeno», que en griego significa «el que produce agua». La formación de agua mediante el hidrógeno fue una de las claves que permitieron al científico francés Antoine-Laurent Lavoisier barrer las antiguas teorías y establecer los cimientos de la química moderna.

Pero el hidrógeno era un gas maravilloso en diversos sentidos. No sólo formaba llama y agua, sino que era increíblemente ligero. Un litro de aire ordinario pesa solamente un gramo y cuarto. Lo cual ya es muy poco. Pero un litro de hidrógeno sólo pesa una décima de gramo. En realidad, el hidrógeno es la sustancia más ligera que se conoce.

En 1783, los hermanos Montgolfier, de Francia, llenaron una bolsa de seda con aire caliente y la hicieron volar. El aire caliente era más ligero que el frío y la bolsa hinchada flotó por la atmósfera como un corcho flota en el agua. Cuando el aire caliente se enfrió, la bolsa de seda (el primer globo) descendió.

Pero, ¿por qué usar aire caliente? El hidrógeno, el nuevo gas, era muchísimo más ligero que el aire, incluso estando frío. Su poderosa fuerza de elevación podría transportar una barquilla… con hombres dentro.

En los primeros años del siglo XIX, en Europa y América centenares de globos llenos de hidrógeno fueron lanzados hacia el cielo. Para algunos, esto era solamente una aventura emocionante, excitante. Para los científicos, un nuevo modo de estudiar las capas altas de la atmósfera…, el primer paso hacia las estrellas.

También podía significar el viaje comercial si lograba hacerse a los globos independientes del viento. En 1900, el inventor alemán conde de Zeppelin, construyó unos globos en forma de cigarro puro, con estructuras de aluminio, y les añadió un propulsor a motor. El globo dirigible (o «Zeppelin») era un buque del aire, nacido y llevado en alas del hidrógeno.

Pero éste, tanto para bien como para mal, es un hijo de las llamas. El gigantesco globo de hidrógeno era un contenedor de explosivos, un blanco cierto para el enemigo- Y el enemigo era, a veces, una chispa de electricidad estática. En 1937, la bolsa de hidrógeno del dirigible gigante Hindenburg estalló en llamas. Y en unos minutos quedó totalmente destruido.

Sin embargo, el dirigible ya había tenido su época. El porvenir se apoyaba en unos aparatos más pesados que el aire, menores y más ligeros que el dirigible, y más capaces de soportar el mal tiempo.

Entonces, pareció como si el hidrógeno debiera limitarse a usos terrestres. Los químicos lo utilizaban para reducir o «hidrogenar» las materias orgánicas de mil formas distintas; por ejemplo, convirtiendo aceites vegetales inadmisibles para el organismo humano en sustancias sólidas. Se usó la llama del hidrógeno en forma de sopletes de oxi-hidrógeno que cortaban el acero como si fuese mantequilla.

¿Y qué más?

El hidrógeno no estaba aún derrotado. Si el dirigible ardió por las llamas, el cohete ha subido en llamas. Y cuando se extinguió el último dirigible, alboreó la época del cohete.

Los aviones ordinarios sólo pueden moverse en un aire, o medio atmosférico, que contenga una provisión adecuada de oxígeno para quemar el combustible en los motores. Este aire, además, ha de ser bastante denso para soportar el peso de la máquina.

Un cohete, sin embargo, lleva combustible y oxígeno. Ambos se combinan en una furia al rojo vivo, enviando un chorro de gases recalentados hacia abajo. Como parte del contenido del cohete, en forma de dichos gases, es enviado hacia abajo, el resto del aparato se mueve hacia arriba. (Esto se produce como respuestas a la ley de «acción y reacción», o «tercera ley de la Dinámica», formulada por el científico inglés Isaac Newton en 1683.)

Como los gases residuales siguen yendo hacia abajo, el cohete asciende, cada vez más de prisa. Eventualmente, llega más arriba del límite atmosférico (pues no necesita a la atmósfera para que soporte su peso o mantenga la combustión), y se lanza al espacio exterior.

La altura a la que llega el cohete depende, en parte del modo en que sean arrojados los gases residuales. Cuanto más rápidamente sean expulsados hacia abajo (cuanto más violenta sea la «acción») tanto mayor serán la velocidad y la altitud alcanzadas por el cohete (tanto más violenta será la «reacción»). Los científicos de cohetes tenían que encontrar el combustible que provocase la mayor reacción ascendente.

Los primeros cohetes, como los usados el día del Cuatro de Julio[5], y los utilizados en las guerras del siglo xlx (no mucho mayores ni mejores), utilizaban la pólvora. Ésta contiene un compuesto muy rico en oxígeno llamado «salitre». También contiene carbono y azufre que, al calentarse, se combinan violentamente con el oxígeno del salitre. Por tanto, la pólvora es un combinado de combustible y oxígeno.

Pero la pólvora no tiene mucha potencia. En 1926, el inventor norteamericano Robert H. Goddard comprendió que era mucho mejor trabajar con líquidos. El 16 de marzo de dicho año, en la granja de su tía Effie, en Auburn, Massachusetts, lanzó el primer cohete del mundo impulsado por un líquido. Su combustible, una mezcla de gasolina y oxígeno líquido, cedió cinco veces más energía, a igualdad de peso, que el TNT (trinitrotolueno). Gracias a la enorme energía de esta combinación, no se tardó en enviar cohetes por el aire a velocidades supersónicas.

Aunque fue un norteamericano el padre del cohete moderno, éste llegó a su edad adulta gracias a los alemanes, que construyeron los cohetes V-2 en la Segunda Guerra Mundial. Varios de dichos cohetes fueron llevados a Norteamérica en 1946, y los americanos los estudiaron atentamente. (Por desgracia, Goddard había fallecido el año anterior.) Continuó usándose la combinación de gasolina y oxígeno, si bien en modo alguno representa un límite superior de energía potencial. De todos los combustibles químicos conocidos, el hidrógeno (en combinación con el oxígeno o el flúor) ardía con más energía. Un cohete impulsado por hidrógeno podía subir mucho más alto y levantar una carga mucho mayor que otro del mismo peso impulsado por gasolina u otro combustible.

El hidrógeno parecía nuevamente hallarse a punto de emprender una carrera aérea…, pero había un fallo. No podía usarse el hidrógeno en su forma ordinaria. Un kilo de hidrógeno ocupa más de once metros cúbicos de espacio, y si algo le falta a un cohete es esto precisamente: sitio.

Había que obtener hidrógeno en forma compacta. Podía comprimirse bajo muchas atmósferas de presión, pero era muy difícil… y peligroso. Sin embargo, existe un medio de comprimir un gas sin gran presión: licuándolo.

No solamente necesitaban el hidrógeno comprimido en grandes cantidades en la Segunda Guerra Mundial, puesto que se estaba fabricando una nueva bomba nuclear. La bomba atómica ordinaria, obtenida por la fusión del uranio (la temible bomba «A», que acabó con la resistencia japonesa), se estaba transformando en una espoleta de ignición de una explosión mucho mayor. Esta explosión tan inmensa tendría lugar cuando los átomos de hidrógeno fuesen obligados a unirse (fusionarse) para formar helio. Sería una bomba de «fusión», una «bomba de hidrógeno», una bomba «H».

Entonces, lo único que hacía falta era hidrógeno líquido en cantidades fabulosas. Pero había varios obstáculos…

El hidrógeno es un gas muy común. Dos tercios de todos los átomos del petróleo y del océano son de hidrógeno. Tres quintas partes de los átomos del tejido vivo, incluyendo el organismo humano, son de hidrógeno. Casi un átomo de cada treinta de la corteza terrestre es de hidrógeno.

Sin embargo, los átomos de hidrógeno no existen por separado, sino en combinación con otros átomos. Separarlos era un proceso lento y costoso. Pero se logró haciendo reaccionar ciertos metales con ácidos, o pasando a través del agua una corriente eléctrica. Esto bastó para los usos del hidrógeno en el siglo xix, usos a pequeña escala.

Poco después de la Segunda Guerra Mundial, un grupo de empresas petrolíferas y de gas natural se unieron para instalar una planta donde extraer gasolina del gas natural. Desarrollaron un proceso para quemar el gas natural y apagar la llama en el debido punto, a fin de que la combustión fuese incompleta, produciendo monóxido de carbono e hidrógeno (en vez de dióxido de carbono y agua). Los dos primeros podían volver a combinarse de nuevo en condiciones convenientes, formándose gasolina.

El proceso tuvo éxito, pero resultó antieconómico para la producción de gasolina, en competencia con las reservas naturales de petróleo, al alcance del hombre después de la guerra. Sin embargo, aquella investigación tuvo importantes ramificaciones. El nuevo proceso demostró ser mucho más eficiente en la producción de hidrógeno que los métodos más antiguos.

En consecuencia, cuando se necesitó más hidrógeno a mediados del siglo xx, pudo satisfacerse tal necesidad. No obstante, conseguirlo en forma líquida era otra cuestión.

Durante todo el siglo xix, los químicos habían intentado licuar los gases. Algunos, como el cloro y el dióxido de azufre, cedían fácilmente. Cierto enfriamiento y los gases se licuaban. En realidad, bastaba un poco de presión sin enfriamiento.

Otros gases, como el oxígeno, el nitrógeno y el hidrógeno, no se licuaron a pesar de un gran enfriamiento y considerable presión. Por algún tiempo se los llamó «gases permanentes». En 1869, sin embargo, los químicos descubrieron que la presión no servía de nada si la temperatura no se hallaba por debajo de cierto «punto crítico». Para los gases como el oxígeno, el nitrógeno y el hidrógeno, esta temperatura crítica era muy baja.

Por lo tanto, los químicos se concentraron en rebajar la temperatura, y, hacia 1880, lograron licuar el oxígeno y el nitrógeno. El nitrógeno líquido hierve a -195º C, pero incluso a esta temperatura el hidrógeno sigue siendo gas.

Hasta 1895, no logró el químico inglés James Dewar obtener hidrógeno líquido. Éste hierve a -253º C, temperatura que está a unos 20° C por encima del cero absoluto…, que es el término de la escala termométrica.

Bien, era posible formar hidrógeno líquido, y con el debido esfuerzo, en gran cantidad; pero durante cincuenta años no fue más que una curiosidad de laboratorio.

El principal obstáculo era que este líquido superfrígido se evaporaba con suma facilidad. Ni el aislamiento más elaborado servía para mantenerlo en forma líquida más allá de cierto punto, puesto que el hidrógeno licuado engendraba su propio calor.

Esto requiere una explicación. En condiciones corrientes, el hidrógeno existe como una serie de moléculas, cada una de las cuales está formada por un par de átomos de hidrógeno.

Cada átomo de hidrógeno se compone principalmente de una diminuta partícula central llamada «protón», que gira constantemente sobre sí misma. En algunas moléculas de hidrógeno, los protones de los átomos de hidrógeno giran en la misma dirección. Es el «orto-hidrógeno». En otras moléculas, los protones giran en direcciones opuestas. Es el «para-hidrógeno». En el gas hidrógeno ordinario, las tres cuartas partes de moléculas son orto, y las restantes para.

El orto-hidrógeno contiene más energía que el parahidrógeno. Cuando se forma hidrógeno líquido, las moléculas orto se convierten lentamente en moléculas para, de menor energía. La energía extra de las moléculas orto se libera como calor.

Esta conversión lenta orto a para añade constantemente calor al hidrógeno líquido y lo evapora a uno por ciento a la hora, por muy bien aislado que esté. Además, si el contenedor no está debidamente ventilado, la presión podría originar explosiones.

Un modo de orillar estas dificultades es cambiar el orto en para, que, con el aislamiento más conveniente, podría conservarse durante largos períodos de tiempo. El problema estaba en que esta conversión era lenta y difícil.

Hay sustancias, no obstante, que actúan como catalizadoras y aceleran esta conversión. En 1929 se descubrió que el carbón común en polvo aceleraba la conversión, por ejemplo. En 1952, debido a las súbitas necesidades, se descubrió que un preparado de óxido de hierro convertía grandes cantidades de orto-hidrógeno, en para en escasos segundos.

Se adoptó este procedimiento para la fabricación en gran escala, gracias a lo cual puede hoy día prepararse el hidrógeno en una forma en que, con el debido aislamiento, se pierde un uno por ciento por medio de la evaporación, no en una hora sino en tres días. El precio ha bajado a medio dólar la libra y se han instalado plantas de hidrógeno líquido que producen más de veinte toneladas diarias. Así se obtuvo la respuesta a la llamada del hidrógeno líquido.

Las necesidades actuales de hidrógeno son iguales a su suministro, aunque dichas necesidades continúen creciendo.

Al parecer, el nuevo empleo del hidrógeno puede apoyarse en la producción de energía eléctrica. Ordinariamente, la electricidad se forma por medio de un generador impulsado por energía calorífica, quemando carbón o petróleo (o, claro está, mediante la energía hidráulica). En este paso del calor a la electricidad se pierde, de modo inevitable, mucha energía. Si fuese posible combinar el combustible con el oxígeno en una instalación de celdas eléctricas (las llamadas «celdas combustibles»), el proceso resultaría mucho más eficaz.

Se han probado varios combustibles -carbón pulverizado, monóxido de carbono, y metano-, como celdas combustibles. Las dificultades prácticas para que éstas sean económicas son enormes, aunque no insuperables. La posibilidad que promete más es la de la célula combustible de hidrógeno-oxígeno. Ya funcionan estas celdas a pequeña escala, y llegará el momento en que el hidrógeno abaratará la electricidad.

El hidrógeno líquido, en la época posbélica, tiene un nuevo uso particularmente exótico en las «cámaras de burbujas», empleado para descubrir las extrañas y muy perecederas partículas subatómicas producidas por las poderosas máquinas que hoy día aplastan los átomos. (Estas cámaras se inventaron en 1952, siendo su creador el físico americano Donaid W. Glaser.) Una cámara de burbujas de la Universidad de California tiene dos metros de longitud y contiene seiscientos setenta y cinco litros de hidrógeno líquido.

Pero las celdas de combustible y las cámaras de burbujas sólo pueden utilizar minúsculas cantidades de hidrógeno. El uso inmediato para todo el hidrógeno líquido se apoya en los cohetes y naves espaciales de hoy y de mañana. En particular, el hidrógeno líquido impulsa a las naves que llevan al hombre a la Luna.

Una de las razones posbélicas para disponer de ingentes cantidades de hidrógeno líquido se desvaneció rápidamente. Sí, las primeras bombas de hidrógeno experimentales utilizaron hidrógeno líquido, pero no así en su forma práctica. Como se necesitaba mucho espacio y peso para el aislamiento del líquido, la bomba era una creación inamovible y monstruosa.

El remedio consistió en utilizar, no hidrógeno líquido, sino un compuesto de hidrógeno y un metal ligero: el litio. Ese compuesto, litio hidratado, explota igual que el hidrógeno una vez impulsado por una bomba de fisión. Más aún: el litio hidratado es sólido a temperatura ordinaria y presenta el hidrógeno en forma compacta sin presiones ni aislamiento. Lo cual hace que tales bombas puedan ser transportadas por los aviones y los cohetes.

Sin embargo, a pesar de todas las esperanzas de que las bombas de hidrógeno no se utilizarán jamás en una guerra, otro aspecto del proceso de fusión inspira una sensación de bienestar para la Humanidad. Si se logra la fusión del hidrógeno bien controlada, lenta y seguramente (en lugar de explosivamente), durante un futuro indefinido quedarán solucionadas las necesidades de la Humanidad.

Para ello es necesario elevar la temperatura de una cantidad de hidrógeno hasta el punto donde empieza la fusión nuclear manteniéndola constante, y hacerlo sin la ayuda de la bomba de fisión. Lo ideal sería encontrar el medio de la fusión de los núcleos de hidrógeno a la temperatura más baja posible.

Mas para esto se requiere utilizar una clase de hidrógeno muy rara. Ya dije antes que el átomo de hidrógeno contiene una partícula central llamada protón. Un átomo de hidrógeno, de cada siete mil, lleva, junto con el protón, una segunda partícula denominada «neutrón». Este átomo de hidrógeno «protón-neutrón» es dos veces más pesado que los átomos de hidrógeno de protón solo, por lo que se le llama «hidrógeno pesado». También se llama «deuterio», derivado este nombre del término griego que significa «segundo» (por contener una segunda partícula junto con el protón).

El deuterio se descubrió en 1932, y su descubridor fue el químico americano Harold C. Urey. Debido a su doble peso, no fue difícil separar el deuterio del hidrógeno ordinario, pero durante diez años no fue más que una pieza curiosa de laboratorio. Luego, en la Segunda Guerra Mundial, se descubrió que el agua que contenía deuterio («agua pesada») podía ser un factor importante en los reactores nucleares.

Como si esto no fuese bastante, se descubrió, terminada ya la guerra, que el deuterio se fusiona más fácilmente que el hidrógeno ordinario. En consecuencia, se efectuaron grandes esfuerzos concentrados en el deuterio para dominar la reacción de fusión.

Aun así, se necesita una temperatura de cientos de millones de grados. A esta temperatura, los átomos de deuterio (y también todos los de otras clases) se descomponen en una mezcla de fragmentos subatómicos cargados, llamada «plasma». Éste es demasiado caliente para poder ser contenido en algo surgido de la materia, pero como está eléctricamente cargado, puede ser confinado mediante campos magnéticos.

Este problema es engañoso, pero año tras año elevamos el plasma del deuterio a mayores temperaturas, manteniéndolo confinado durante períodos de tiempo más largos cada vez. Por eso es de esperar que no tardemos mucho en dominar ya la fusión.

Tal vez antes del final del siglo xx, se instalarán en la Tierra nuevas plantas de energía. Los pequeños contenedores de deuterio líquido serán los proveedores de dichas plantas y llevarán a cabo las funciones hoy día ejecutadas por los camiones de carbón y los tanques de petróleo. Será el hidrógeno, en una forma u otra, no sólo el que llevará al hombre a las estrellas, sino el que ayudará a borrar el hambre y la miseria de la faz de la Tierra.

11. Una nueva luz

En 1960, el físico americano Theodore Harold Maiman, expuso una barra de rubí sintético a una luz muy potente. Poco después, la luz absorbida por la barra fue emitida de nuevo, mas con un cambio. Parecía un rayo muy fino, de profundo color rojo, llameando brevemente a un extremo de la barra.

Este rayo de luz era de una variedad desconocida. Por lo que sabemos, era una variedad de luz inexistente antes en la Tierra, y en cualquier parte del Universo conocido. La barra de rubí sintético de Maiman fue el primer «láser», un aparato que hoy día consideramos como un posible «rayo de la muerte» por una parte, y que ofrece milagros en tiempo de paz, en campos tales como la cirugía, la fotografía, las comunicaciones, la ciencia espacial y otra media docena más de aplicaciones.

Pero, ¿qué es lo que hace al rayo láser tan diferente, tan único? A simple vista, parece solamente un rayo fino de luz coloreada, nunca visto antes. ¿Qué hay que no percibe el ojo? Para contestar a esta pregunta, permítanme antes preguntar qué es la luz ordinaria.

Imaginemos la luz como una serie de ondas. «¿Ondas de qué?», cabría preguntar, y al momento nos hallaríamos en un apuro, mas no lo preguntaremos. Imaginemos que son unas ondas, y nada más.

No pensemos que, si queremos fabricar una imagen de ondas de un rayo de luz hay que trazar una línea ondulada que continúe en toda la longitud de dicho rayo. (Los rayos de luz que llegan desde las estrellas se hallan a muchos trillones de kilómetros de longitud, por lo que «en toda la longitud» representa una cifra enorme.) En cambio, podemos imaginar las ondas como interrumpidas de trecho en trecho, conteniendo cada uno unos altibajos, o sea «oscilaciones». Podemos referirnos a esos minúsculos trechos de ondas como fotones, expresión que se deriva de la griega por la palabra «luz».

Los fotones son extremadamente pequeños. Una bombilla de 40 vatios, que da una luz tenue, emite un quintillón de fotones cada segundo.

Los fotones no son todos iguales. La diferencia más importante es que unos contienen más energía que otros. También ahora podemos eludir preguntas tan embarazosas como «¿qué se entiende por energía?», y decir simplemente que un fotón más energético puede hacer cosas que otro menos energético no puede realizar.

Por ejemplo, la luz roja se compone de fotones una mitad menos energéticos que los de la luz violeta. Cuando los fotones de luz roja atacan la película fotográfica, carecen de energía para que los productos químicos de la película sufran cambios. Si atacan dicha película los fotones de la luz violeta, los productos químicos se descomponen y la película queda velada.

Por esto, el cuarto oscuro donde se revelan las películas y fotografías puede estar iluminado por una luz roja, que no estropea las imágenes.

La luz solar contiene fotones de muy diversas energías, desde el rojo al violeta, con todos los colores intermedios. Contiene fotones de todas las energías que afectan al ojo humano (la retina del ojo es una especie de película viva y muy complicada). Contiene fotones de luz infrarroja, que nuestros ojos no registran visiblemente y que son menos energéticos que cualquier forma de luz visible. También contiene fotones de luz ultravioleta, que no registra nuestra vista y son más energéticos que cualquier forma de luz visible. (Todas las formas de luz, visibles o invisibles, pueden ser llamadas «radiaciones electromagnéticas».) Los fotones de luz ultravioleta son tan energéticos que pueden dañar la retina humana, por lo que siempre resulta tan peligroso mirar directamente al sol. Los fotones de luz ultravioleta pueden producir cambios en la epidermis, causando las quemaduras del sol.

Los fotones de rayos X y rayos gamma, más energéticos aún que los de luz ultravioleta, pueden pasar a través de nuestro campo y atacar directamente a ciertas moléculas, produciendo graves y hasta mortales cambios químicos. Por esto las personas que trabajan con sustancias radiactivas o en las modernas plantas nucleares, donde se hallan estos superpoderosos fotones, han de adoptar precauciones extremadas contra los mismos.

Bien, si imaginamos a los fotones como diminutos «trechos» o «paquetes» de ondas, podremos indicar la diferencia existente entre uno de alta energía y otro de baja. Esto se logra alterando la longitud de cada oscilación. Podemos trazar una onda de un centímetro de longitud y curvar tan suavemente la línea de la onda que solamente haya una oscilación en cada centímetro. En otro caso, pueden trazarse diez oscilaciones.

El número de oscilaciones de una longitud dada se llama «frecuencia» de la luz. Un fotón de luz roja tiene unas 14.000 oscilaciones por centímetro, en tanto que otro de luz violeta tiene el doble, unas 28.000. (La diferencia en la frecuencia de los fotones de la luz visible afecta a nuestros ojos, produciendo la sensación de los colores.) Veamos cómo se producen los fotones. Para esto, hemos de referimos a la materia que forma el universo.

Ya sabemos que la materia del universo se compone de diminutas partículas llamadas átomos. Éstos, junto con otras partículas aún menores que los forman, y las mayores en que pueden agruparse, contienen energía. La energía contenida se hace evidente como movimiento. Una partícula de alta energía se mueve o vibra más rápidamente que otra de menor energía.

Las partículas de materia siempre poseen energía. Pueden poseerla en pequeña cantidad, y cada tipo diferente de partícula puede sólo poseer ciertas cantidades características del mismo y no de otro. Por lo tanto, cada partícula posee cierto «nivel de energía» característico. La partícula puede ostentar uno u otro nivel, pero jamás está situada en medio de dos niveles.

(La situación es semejante a la del sistema monetario. Un hombre, por ejemplo, sólo lleva en el bolsillo monedas sueltas. De este modo, podrá tener cuarenta y cinco o cincuenta centavos[6], pero nunca llevará cuarenta y siete centavos en el bolsillo. Si otro hombre sólo llevase monedas de cuarto de dólar, podría tener en el bolsillo cincuenta centavos, mas no cuarenta y cinco.) Si se quema un leño, la energía liberada por la combinación de las partículas de madera con el aire aumenta el contenido de energía de la madera y el aire en la vecindad del fuego. Todas las partículas aumentan el nivel de energía.

Sin embargo, no siguen en tal nivel. Siempre tienen, todas las partículas, una tendencia a llegar al nivel de energía más bajo posible. Las partículas que han alcanzado un nivel alto de energía, por tanto, descienden a uno inferior. Entonces, ceden la diferencia de energía entre ambos niveles, y esta energía cedida adopta la forma de un fotón.

Si todas las partículas existentes cerca del leño ardiendo fuesen idénticas y ostentasen el mismo nivel de energía, para después descender al mismo nivel inferior, todos los fotones emitidos tendrían el mismo contenido de energía, y serían de la misma frecuencia.

Sin embargo, esto no sucede jamás. Existen muchas partículas distintas, que ostentan distintos niveles de energía. El resultado es que se liberan los fotones de una amplia gama de frecuencias, algunos en la gama de la luz visible, y entonces tenemos ya la hoguera. La luz solar también se compone de una amplia variedad de frecuencias fotónicas, como en cualquier otra forma de luz natural.

Hasta hace unos veinte años, los científicos aceptaban esta increíble mezcla de frecuencias como una propiedad casi inevitable de la luz ordinaria.

Empecemos ahora con una clase de partículas y establezcamos las condiciones que permitan que todas las moléculas contengan el mismo nivel de baja energía. Supongamos, además, que dichas moléculas están expuestas a cierto tipo de energía que las envía al siguiente nivel de energía ascendente.

En tales condiciones, una partícula, de vez en cuando, absorberá suficiente energía para ascender al nivel superior de energía, y regresar al inferior, liberando la energía en forma de un fotón de una cierta frecuencia. Habrá siempre algunas partículas del grupo que absorberán la energía y estarán en el proceso del retroceso. Los fotones, siempre de la misma frecuencia, quedarán liberados, produciendo por consiguiente una radiación de frecuencia constante.

Se halló, por ejemplo, que el gas amoníaco podía emitir un tipo de radiación de «baja frecuencia» llamado de «microondas». Esta radiación de microondas del amoníaco sólo tiene ochenta oscilaciones por metro, en comparación con las catorce mil por centímetro de la luz roja.

Estas oscilaciones son muy regulares y no sufren variaciones. Son más constantes que las oscilaciones de un péndulo, y más constantes y regulares que los movimientos de los cuerpos celestes. En 1949, el físico norteamericano Harold Lyons demostró cómo podían usarse estas oscilaciones para controlar los aparatos que miden el tiempo, fabricando los «relojes atómicos», mucho más exactos que todos los demás. Pero dichas radiaciones sirven para algo más que para medir el tiempo.

Las partículas del amoníaco se trasladan de un nivel de energía inferior a otro superior cuando absorben un fotón que posee el debido contenido de energía. Pero, ¿qué sucede si un fotón exterior choca con una partícula que ya está en el nivel superior? ¿La obliga a pasar a un nivel aún más elevado? ¡No!

En 1917, Albert Einstein demostró, mediante consideraciones puramente teóricas, que si un fotón del tamaño debido choca con una partícula de nivel superior, no es absorbido. En cambio, la partícula con la que choca el fotón desciende otra vez al nivel inferior.

La partícula del choque, al descender al nivel inferior de energía, produce un fotón de igual tamaño que el fotón con el que chocó. Más aún: el fotón producido se moverá en la misma dirección que el fotón primitivo. Se empieza con un fotón que choca con una partícula y se termina con dos fotones de igual frecuencia y dirección.

¿Y si esos dos fotones chocan con una partícula de nivel superior? Cada partícula golpeada desciende de nivel y produce otros dos fotones, o sea cuatro, idénticos en frecuencia y dirección. Después, si cada uno de estos cuatro chocan con una partícula de nivel superior…

Pero en circunstancias ordinarias no es probable que ocurra tal cosa, porque las partículas permanecen, en un nivel inferior de energía, breves períodos de tiempo. En un instante dado, por tanto, la mayoría de las partículas de amoníaco están en el nivel inferior, y los fotones que van a su encuentro es más fácil que choquen con aquéllas que con las de un nivel superior.

El físico norteamericano Charles Hard Townes ideó una forma para separar las partículas de alta energía de las de baja, mediante un aparato cargado eléctricamente. En 1953, logró llenar un pequeño compartimiento solamente con partículas de amoníaco de alto nivel. Si penetraba un fotón del tamaño adecuado en aquel compartimiento, producía otro fotón. Los dos fotones producían otros dos; los cuatro, otros cuatro más y así sucesivamente.

Un solo fotón podía desencadenar un vasto alud de fotones idénticos en una fracción de segundo. De esta forma, podía usarse el aparato como un amplificador. Supongamos que hay una radiación muy débil en un punto del cielo; esta radiación tan débil no podría ser detectada por nuestros aparatos. Si la radiación chocase con el compartimiento de amoníaco de alto nivel, el alud de fotones resultante podría ser detectado fácilmente, y así deduciríamos la existencia del fotón primitivo (imposible de detectar de otro modo) que inició el alud.

El fotón original estimula la emisión de grandes cantidades de fotones de microonda, a fin de producir la amplificación. Por tanto, Townes se refirió a su aparato como a algo que producía «una amplificación de microondas mediante la emisión estimulada de radiación». Y las iniciales de las palabras que componen esta frase se combinaron para formar la palabra maser[7].

El maser de amoníaco sólo actúa con fotones de una cierta frecuencia, pero no hay necesidad de utilizar sólo el amoníaco. Así, se desarrollaron sustancias sólidas que entrañaban otras combinaciones de niveles de energía. Y en poco tiempo se desarrollaron masers con una gran variedad de frecuencias fotónicas.

Sin embargo, al principio, los masers sólo podían actuar con intermitencias. El sistema era ascendido a un nivel superior, y un fotón enviado contra él mismo provocaba el alud. Pero el sistema no volvía a actuar hasta una nueva ascensión.

Un físico holandés. Nicolás Bloembergen, logró inventar un maser que actuaba con un sistema de tres niveles: uno inferior, otro medio y el último superior. El sistema es bombeado por fotones de alta frecuencia capaces de elevar los átomos del maser desde un nivel inferior a otro superior. Una segunda serie de fotones de frecuencia menor lleva al sistema desde el superior al nivel medio, y después, desde éste al inferior. Ambos procesos actúan independientemente, así como continuamente, y el maser es bombeado hacia arriba por una serie de fotones tan de prisa como baja para producir otra serie. Por este motivo puede trabajar continuamente.

No hay ninguna razón para que sólo existan radiaciones de microondas. ¿Por qué no utilizar niveles de energía muy separados para producir más fotones energéticos? ¿Fotones de frecuencia suficientemente alta como para registrarse en la región de la luz visible? Un maser que produjese luz visible sería un «maser óptico». También podríamos referirnos a él como «creador de la amplificación de la luz mediante la emisión estimulada de radiación», remplazando la palabra «luz» a la de «microondas». Con este cambio, tendríamos la palabra «láser».

Townes afirmó en 1958 que un láser era totalmente posible en teoría, y Maiman construyó el primero en 1960, como indiqué al comienzo de este Capítulo. El primer láser de Maiman era intermitente y tenía que ser elevado de nuevo después de cada descarga. Sin embargo, antes de finalizar aquel año, el físico iraní Ali Javan ya preparaba láseres continuos en los Laboratorios Bell.

Veamos, ahora, de qué forma la luz láser es diferente de las demás formas de luz que conocemos.

Primero, el rayo láser es muy intenso. En todos los procesos ordinarios de producir luz, surge una vasta gama de frecuencias de fotón. De ellas, sólo una pequeña porción se halla usualmente en la gama de la luz visible. En el rayo láser, toda la energía liberada puede serlo en forma de luz altamente concentrada.

Segundo, el rayo láser es muy uniforme. Ordinariamente, la luz se compone de fotones de diversas frecuencias, en tanto que el rayo láser está compuesto por entero de fotones idénticos. Por tanto, es de un mismo matiz pero con un color particular. Es una luz «monocromática» (expresión griega que significa «del mismo color»).

Tercero, el rayo láser es muy compacto. Los fotones de luz ordinaria se mueven continuamente en todas direcciones. Es difícil impedir que un rayo de luz normal se difunda. Los fotones del rayo láser, por otra parte, se mueven todos en la misma dirección. La luz ordinaria puede compararse a una multitud en la que cada miembro va hacia la dirección que elige. El rayo láser puede, en cambio, compararse a una columna de soldados que marcha con absoluta precisión.

La tendencia natural de los fotones del rayo láser a moverse en la misma dirección, queda acentuada por el diseño del tubo que los produce. Los extremos son absolutamente lisos y paralelos. Uno es plateado para formar un espejo perfecto, y el otro sólo es ligeramente plateado. Cuando los fotones se producen por la acción del láser, pueden producirse varios aludes en direcciones diferentes. La mayor parte pasan por los costados del tubo instantáneamente. Sin embargo, los aludes que se mueven por toda la longitud del tubo, bombardean primero un extremo plateado, y después el otro, yendo atrás y adelante una y otra vez, produciendo constantemente más fotones y provocando un alud mayor cada vez. El fotón que, por cualquier causa, no se mueve exactamente paralelo a la línea general del alud, choca rápidamente con un costado del tubo, y sale del aparato.

Finalmente, cuando el alud es bastante abundante, surge por el extremo menos plateado y tenemos ya el rayo láser. Los fotones de este rayo son tan idénticos en frecuencia y dirección, que las oscilaciones de uno parecen engancharse en los fotones delanteros y traseros, y el resultado es como una larga serie de oscilaciones. Los fotones actúan como si estuvieran encajados entre sí, como cohesionados unos a otros. Por este motivo, se dice que el rayo láser está formado por «luz coherente».

Un rayo láser, formado por luz coherente, no posee tendencia a difundirse. Sigue en forma compacta y pierde muy poca energía de concentración al viajar por el espacio. Un rayo de luz coherente láser puede enfocarse de modo que caliente una cafetera a muchos miles de kilómetros de distancia. Los rayos láser llegaron en 1962 a la Luna, extendiéndose solamente en un diámetro de tres kilómetros, tras haber cruzado limpiamente unos 380.000 kilómetros en el espacio.

Las propiedades únicas de la luz láser han posibilitado una serie de aplicaciones muy interesantes.

Por ejemplo, la estrechez del rayo láser permite que se concentre una gran energía en una zona sumamente pequeña. En esta zona, la temperatura alcanza límites extremadamente altos tan rápidamente, que puede fundirse un punto antes de que el calor se irradie hacia fuera en cantidades suficientes para causar daños.

Así, un leve parpadeo de rayo láser en el ojo puede prevenir ciertas clases de ceguera, soldando la retina desprendida con tanta rapidez que los objetos circundantes no quedan afectados por el calor. De forma similar, pueden destruirse los tumores dérmicos sin quemar la piel.

Puede vaporizarse un diminuto fragmento de metal y analizar rápidamente el vapor por medios espectroscópicos. Pueden agujerear eficaz y velozmente los metales; incluso pueden tallarse los diamantes. Tal vez el rayo láser ayude eventualmente a producir las temperaturas extremas para iniciar una reacción controlada de fusión de hidrógeno, lo cual solucionaría conjuntamente los problemas energéticos de la Humanidad (ver Capítulo 10).

Natural y tristemente, a uno le asalta la idea de que lo que el rayo láser puede hacerle a un fragmento de metal, también puede hacerlo con un ser humano. En 1965, se desarrollaron unos láser que podían ser elevados a un nivel más alto gracias a la energía proporcionada por reacciones químicas. Por tanto, podemos ya imaginar una pistola que no use la energía química para impulsar a la bala sino que emita un destello láser. Podría chocar contra un hombre con efectos mortales sin hacer ruido ni dejar señales. Sería el «rayo de la muerte», tantas veces descrito en los relatos de ciencia-ficción.

Y si puede haber pistolas láser, ¿por qué no cañones láser? Un láser gigante podría agujerear la coraza de un tanque o de un buque. El «cohete» de luz viajaría a 300.000 kilómetros por segundo, en línea recta, sin quedar afectado por el viento, la temperatura, la rotación de la Tierra, el efecto de la gravedad, ni cualquier otro de los obstáculos que dificultan la buena puntería de los tiradores.

El rayo de la muerte tiene sus limitaciones como arma de largo alcance. Podría quedar debilitado o ser absorbido por las nubes, la niebla, el humo y el polvo. Además, su trayectoria en línea recta no seguiría la curvatura de la Tierra, por lo que no podría ser orientado contra un blanco situado más allá del horizonte.

Sin embargo, si consultamos la bola de cristal del porvenir, veremos el espectro de tal rayo de la muerte en el espacio. En el vacío, pasada la atmósfera, no hay nubes ni nieblas o polvillo que cree obstáculos, ni existen horizontes. ¿Llevará a cabo la Humanidad, dentro de unos años, batallas espaciales, con naves provistas de rayos láser, en que un contacto momentáneo significa un agujero? Estos rayos láser requerirían una enorme cantidad de energía, pero actualmente ya se desarrollan láseres que aprovechan la energía solar. En el espacio exterior, los láseres podrían ser accionados, sin límites, por el omnipresente sol.

Dejemos los adelantos de la sociedad actual en el punto en que jamás se necesiten tales armas, grandes o pequeñas. Ya existen suficientes usos en tiempo de paz para mantener el rayo láser constantemente ocupado. Este rayo puede aplicarse a las comunicaciones industriales, por ejemplo, industrias que hoy día dependen en gran parte de los fotones de microondas y ondas de radio de baja frecuencia.

Estos fotones de baja frecuencia pueden «modularse», o sea, que la corriente de fotones varía de forma regular, a fin de producir vibraciones mecánicas en un diafragma que, a su vez, produce ondas sonoras en el aire. O producen variaciones en una corriente eléctrica que, a su vez, producen luz de variada intensidad. De este modo obtenemos el sonido de la radio y la imagen y el sonido de la televisión.

Para impedir la interferencia de un mensaje con otro hay que enviar los diferentes mensajes en un «rayo transportador» de fotones de frecuencias muy diferentes. En la zona de la baja frecuencia no hay muchas frecuencias diferentes, y el número de estaciones de radio o canales de televisión ha de ser, por tanto, muy limitado.

Si se usaran fotones luminosos como ondas de transporte, sus frecuencias más altas nos permitirían tener sitio para un mayor número de mensajes. (Esto lo comprenderemos si consideramos que los números del 1 al 10 representan a las ondas de radiofrecuencia, y del 1.000.000.000 al 10.000.000.000 representan a las ondas luminosas. En ambos casos, el último número es diez veces mayor que el primero; pero del 1 al 10 sólo hay diez números dígitos, mientras que del 1.000.000.000 al 10.000.000.000 hay nueve mil millones y uno.) Para que la radiación actúe como una onda de transporte, ha de ser muy regular en la frecuencia y la dirección. Esto es posible para las ondas de radio, que oscilan suavemente, pero no para las de alta frecuencia, aunque esto ya no es así desde que se inventó el láser. Naturalmente, no es fácil modular las ondas de luz de un rayo láser, pero el problema se está ya solucionando. En 1965, los siete canales de televisión de Nueva York fueron transmitidos a través de la anchura de una habitación en un solo láser del ancho de un lápiz, y cada canal pudo ser separado de los demás.

¿Llegará el momento en que los rayos láser, reflejados y amplificados por los satélites de comunicación, servirán al mundo? Si esto fuera posible, habría sitio suficiente para todas las distintas estaciones de radio y canales de televisión del planeta, en la cantidad que fuese.

Las posibles interferencias atmosféricas para tal sistema no se aplican al espacio. Las naves y las estaciones espaciales podrían comunicarse entre sí y también con las estaciones situadas en la superficie de un mundo sin aire como la Luna, mediante los mensajes transmitidos por los rayos láser.

La información enviada, además, no estaría compuesta sólo de palabras. La línea absolutamente recta del rayo láser serviría para localizar la posición exacta de la nave o la estación respecto a otra en el instante preciso. Además, el rayo láser se reflejaría en la nave, y el rayo reflejado cambiaría de frecuencia muy lentamente, según que el objeto reflejado se alejase o acercase hacia el observador, y con dependencia de la velocidad. El rayo láser también se vería afectado si el objeto observado estaba girando, en qué dirección y a qué velocidad.

Naturalmente, podría realizarse lo mismo con la luz ordinaria si ésta pudiese comprimirse en un rayo compacto y de energía suficiente para viajar a través del espacio y retroceder sin demasiada pérdida. Sin embargo, la luz ordinaria contiene fotones de tantas frecuencias que los cambios ligeros de las mismas no podrían detectarse como ocurre con los fotones del rayo láser. (Si cada miembro de una muchedumbre en movimiento da un ligero paso lateral, ¿podría ser detectado tal paso? Si en una columna de soldados, marchando en una formación precisa, todos diesen un paso lateral, ¿podría ser detectado? No, en el primer caso; sí, en el segundo.) Cuando la era espacial alcance su madurez, es casi seguro que un gran volumen de comunicaciones e informaciones podrá ser transportada por los rayos láser, entre los diversos puestos avanzados establecidos por el hombre. Es posible que entonces se lleve a cabo la exploración espacial que, sin el láser, jamás saldrá de unos confines muy limitados.

Volviendo a la Tierra, hallamos que una aplicación reciente del láser se refiere a la fotografía. En la fotografía ordinaria, la luz queda grabada en las placas o la película mediante el efecto de la luz sobre unos productos químicos. Cuanto más intensa es la luz, mayor es el efecto. Los productos químicos, por tanto, graban la brillantez y producen una réplica de la forma claroscura de la luz emitida por un objeto, o de la luz reflejada por aquél. Esta réplica es la fotografía.

Supongamos que se envía un rayo láser contra un espejo y que aquél se refleja sin distorsión alguna sobre una placa fotográfica. Simultáneamente, es reflejado otro rayo láser desde un objeto ordinario que lo refleja, con alguna distorsión, a la placa fotográfica. (La distorsión se debe a que el objeto ordinario posee una superficie irregular, de modo que algunas partes del rayo láser son absorbidas, y otras no; unas son reflejadas en una dirección, y otras en la contraria.) En la placa fotográfica, los dos rayos se encuentran, uno distorsionado y el otro no. La intensidad total de la luz en cada punto queda grabada como en una fotografía ordinaria. Además, las ondas de los dos rayos se entrecruzan en una variedad de formas que depende de los detalles exactos de la distorsión del rayo reflejado por el objeto ordinario. A este entrecruzamiento se le llama «interferencia». La placa no sólo graba la intensidad de la luz sino la fórmula de la interferencia.

Los físicos sabían hace ya muchos años que esto era posible, pero con la luz ordinaria no lo lograban. Todas las ondas diferentes de la luz ordinaria, moviéndose con frecuencias diferentes y en direcciones distintas, producían una interferencia tan confusa que de la misma no podía extraerse ninguna información útil.

Con un rayo láser, sin embargo, se produce una fórmula de interferencia muy clara, que sólo depende de la naturaleza del objeto reflejante y de nada más. La placa posee toda la información, la intensidad y la interferencia, en forma absoluta, y a este proceso se le denomina «holografía». («Holo» significa «todo».) La placa, u «holograma», que lleva toda esta información no muestra nada al ojo humano, salvo, a veces, una pauta de círculos procedentes de las motas de polvo. La fórmula de interferencia es microscópica.

Si se envía un rayo láser a través del holograma, se crea una imagen del objeto original reflejante, y puede fotografiarse desde diferentes ángulos una imagen parcialmente tridimensional. Esto se llevó a cabo por primera vez en 1964, y en 1966 ya no fue necesario el rayo láser para crear la imagen, pues lo hacía la luz ordinaria, con lo que el proceso resulta más barato y más práctico. (No obstante, sigue siendo necesario el rayo láser para formar antes el holograma.) Un holograma puede estar formado por un objeto que se mueva velozmente o por uno de brevísima existencia, proporcionando una imagen permanente que puede estudiarse con mucho más detalle que en una fotografía. La holografía proporciona detalles más perfectos, más acusados, y los científicos ahora buscan la manera de conseguir un microscopio holográfico que permita estudiar con toda claridad el mundo de lo invisible.

Y avanzando en este sentido nos asalta la idea de que tal vez llegue a perfeccionarse la holografía hasta el punto de poder formar una imagen tridimensional perfecta, y el proceso se obtenga sobre una pantalla de televisión.

¿Llegará el día en que ya no nos contentaremos con la pantalla televisiva bidimensional, con sus líneas de luz y oscuridad tan confusas y ásperas, y en cambio podremos ver una representación en color y en una auténtica tercera dimensión? En el futuro, ¿concursará Miss América con las demás jóvenes del mundo a través de un cubo (el televisor), en nuestro salón, en tres dimensiones? Naturalmente, las jóvenes no serán más que unas imágenes, nada más que unos rayos de luz impalpables, enfocados de forma muy concentrada. No serán jóvenes de verdad. Pero incluso así… ¡resultaría muy agradable!

12. Una mina del océano

Nuestras minas se agotan. Nuestra población aumenta rápidamente y nuestra producción industrial, a marchas más forzadas todavía… y los recursos minerales del mundo entero sufren las consecuencias. Las mejores minas de cobre desaparecen. Nuestras mejores minas de hierro se agotan. Hemos de aprender a extraer el mineral de filones más pobres.

Pero la situación no es completamente negra. Ciertos recursos mineralógicos, es decir, la mina más rica y abundante que haya existido jamás, se halla a nuestra puerta, sin que se haya explotado apenas. Esta mina es el océano.

El océano abarca más de 36.000.000 de kilómetros cuadrados, o sea, unas siete décimas partes de la superficie del planeta. Su profundidad media es casi cuatro kilómetros, de modo que la cantidad total de agua, en todo el globo oceánico, es de 1.400.000.000 de kilómetros cúbicos.

Lo que convierte al océano en una mina es que esos mil y pico millones de kilómetros cúbicos no están formados sólo de agua. Cuando una persona se baña, comprende que el océano no contiene solamente agua. Porque «el agua» sola no sabe como sabe el agua del mar.

El 3,25 por ciento del océano es materia sólida que se halla en disolución dentro del 96,75 restante, que es agua.

Realmente, es una gran cantidad de materia sólida, y no hace falta que consideremos todo el océano del planeta para demostrarlo. La demostración la obtendremos con una pequeña cantidad de agua del mar; por ejemplo, una piscina.

Imaginemos una piscina de quince metros de longitud, nueve de anchura y una profundidad media de dos metros.

Llena de agua del mar, contendría 270 toneladas de líquido; de éste, casi nueve toneladas (8 %) serían minerales disueltos.

Dicho de otro modo: si se evaporase el agua de la piscina, casi nueve toneladas de materia sólida quedaría en el fondo. Para una piscina, se trata de una cantidad respetable.

Como es fácil deducir por el sabor del agua del mar, la mayor parte de la materia sólida es sal común, o sea cloruro sódico. Casi siete toneladas de nuestra piscina serían sal ordinaria, y tres cuartos de tonelada serían de átomos de cloro en combinación con otros metales distintos al sodio.

Dejando éste aparte, aún quedan unos tres cuartos de tonelada de materias en la piscina seca, que no son de sodio ni de cloro. Debidamente tratada dicha materia, esa cantidad nos produciría: 319 kilos de magnesio, 213 de azufre, 97 de calcio, 93 de potasio, 15 de bromo, y unos 12 de una miscelánea de minerales, incluyendo cobre, plata, oro, uranio y radio.

Naturalmente, hay una dificultad. Para extraer minerales del mar hemos de concentrar los átomos disgregados.

Esto representa un gasto de energía. Cuanto menos concentrado se halle el mineral, mayor cantidad de energía necesitaremos para extraerlo. Y esto no es posible marginarlo.

Por suerte, el sol ya nos ha facilitado en muchos casos esta labor. De vez en cuando, en el transcurso del tiempo, un brazo de mar de poca profundidad ha sido absorbido por el terreno al elevarse. Si el clima del mar interior así formado le ayuda a evaporarse más de prisa que el agua corriente allí vertida por los ríos, se encoge gradualmente.

Las sales que contiene se concentran más cada vez, y el mar acaba eventualmente por secarse, dejando sólo la materia sólida.

Las minas de sal son los residuos de porciones de océanos secas. Y todos conocemos el gran valor de la sal. No sólo es un ingrediente de la cocina (y muy esencial), sino que posee centenares de importantes usos industriales, siendo la fuente principal de productos químicos tan primordiales como el gas cloro, el ácido clorhídrico, el hidróxido sódico, el carbonato sódico, y muchísimos más sumamente utilizados en la industria moderna.

Si un mar interior se evapora lentamente, la sal queda depositada por capas. Este hecho se debe a que el cloruro sódico es una de las sales más solubles del océano. Asimismo, está presente en la mayor concentración. Cuando se seca un mar interior, por tanto, el cloruro sódico empieza a precipitar cuando todavía queda agua bastante para mantener a las demás sales en disolución. Luego, en las últimas fases de la evaporación, las demás sales precipitan encima del cloruro sódico. El sol no solamente nos ha ayudado a extraer los sólidos sino que también los ha separado por capas.

Los depósitos de sal cerca de Stassfurt, Alemania, son ejemplos bien conocidos de este proceso estratificador. Representan el mejor depósito de sales de potasio del mundo entero y por esto dichas sales son mucho más baratas en Alemania que en los demás países del Globo. En el norte de Chile, hay depósitos de sal que son ricas fuentes de nitrato sódico y nitrato potásico. Antes de la Primera Guerra Mundial, esos depósitos eran el origen principal de los nitratos para la fabricación de fertilizantes y explosivos.

Luego, existen mares interiores que están secándose. En las orillas de dichos mares se han formado ya depósitos de sal, y el agua que aún queda es muy densa, debido a las materias disueltas. Los ejemplos más conocidos de tales mares en estado de secarse son el mar Muerto, en la frontera jordano-israelí, y el gran lago Salado de Utah.

Los minerales del mar Muerto representan un ingreso muy valioso para los israelíes.

También hay numerosas marismas saladas y porciones subterráneas con bolsas de agua, con un gran contenido salino. Se les llama «pozos salados» y a veces se hallan asociados con los pozos de petróleo. En estos pozos salados es posible obtener yodo en cantidades comerciales.

Pero, ¿y las posibilidades de extraer minerales directamente del océano? ¿Es posible que los científicos inventen un proceso de «secado» artificial? Es posible. Hoy día se producen ya, al menos, dos elementos que el hombre extrae del mar en la cantidad necesaria para sus fines.

Uno de ellos es el magnesio. Sus átomos, después de los de sodio y cloro, son los más comunes en la materia sólida del océano. Para extraer magnesio, hay que bombear el agua de mar en grandes tanques, añadiendo óxido de calcio (cal). (El óxido de calcio también procede del mar, puesto que se forma tostando conchas de ostra.) El óxido de cal reacciona con los iones de agua y magnesio en disolución. Entonces se forma hidróxido de magnesio, que precipita en forma sólida.

El hidróxido de magnesio se filtra y se convierte en cloruro de magnesio por la reacción con el ácido clorhídrico. Luego, se pasa el cloruro de magnesio por filtros y secadores y finalmente se convierte, por medio de una corriente eléctrica, en magnesio metálico y gas cloro. (El cloro se convierte a su vez en ácido clorhídrico que se usa en la siguiente hornada de hidróxido de magnesio, con lo que no se pierde.) El otro elemento extraído del mar comercialmente es el bromo. Resulta mucho más difícil de extraer que el magnesio. En el océano sólo hay veinte veces menos bromo que magnesio. Sin embargo, los científicos han solucionado el problema de su extracción.

Para ello se acidula una gran cantidad de agua de mar, a la que se añade cloro en forma de gas. (Tanto el ácido clorhídrico como el gas cloro proceden de la sal, que a su vez proviene del mar.) El cloro reacciona con los iones de bromo en el agua salada y los convierte en gas bromo. Éste queda disuelto en el agua y es extraído con aire. Es decir, se hace pasar una corriente de aire por el agua de mar y, al pasar, se lleva consigo el vapor de bromo. Después, el aire pasa a través de unos tubos llenos de carbonato sódico; allí el gas bromo es absorbido, formándose bromuro sódico y bromato sódico. El bromo se concentra entonces en un volumen relativamente pequeño. Y puede ser, y es, separado del resto.

Del mar se obtiene un tercer elemento, aunque de forma más indirecta. Este elemento es el yodo. La cantidad de yodo en el océano no llega a la milésima de la de bromo. Nuestra imaginaria piscina, que nos daría dieciséis kilos de bromo, sólo nos proporcionaría doce gramos de yodo. Lo cual es demasiado poco para que nuestras técnicas industriales lo concentren provechosamente.

Es decir, demasiado poco para la técnica humana. Pero en el agua hay organismos vivos, como las algas marinas, que necesitan el yodo para sus procesos vitales. Con paciencia, esos organismos extraen los átomos de yodo del agua que pasa a su alcance y a través de sus filamentos.

Entonces, sólo es necesario que el hombre cultive en el mar las algas marinas. Luego, las algas se queman en pozos poco profundos, y las cenizas resultantes contienen más del uno por ciento del yodo. Las cenizas de las algas contienen el yodo en una concentración doscientas mil veces mayor que el agua de mar, siendo, por tanto, una buena fuente comercial de dicho elemento. (En 1810 ya se descubrió que había yodo en las cenizas de las algas marinas.) El mar es una fuente inagotable de esos elementos. No sólo las cantidades contenidas sirven para satisfacer las necesidades humanas sino que al ser extraídas esas sustancias, el océano no las pierde por completo. Los compuestos de todos los elementos son vertidos constantemente al mar por medio de las lluvias y los ríos, por lo que todo cuanto extraemos del mar a él vuelve.

Es posible que, aparte de los tres elementos mencionados, lleguen a extraerse otros del mar. Elementos que no necesitarán ser concentrados. Grandes extensiones del fondo del océano son muy ricas en pepitas metálicas de manganeso, y contienen cantidades razonables de metales tan valiosos como el cobalto, el níquel y el cobre. Las operaciones de dragado a muchos kilómetros de las costas tal vez lleguen a algo positivo en un futuro no lejano.

Para dar una idea, incidentalmente, de la vasta mina que es el mar en realidad, procederemos a efectuar unos cálculos. Un kilómetro cúbico de océano puede llenar casi cuatro millones de piscinas como la imaginada, y como ya he manifestado, el océano ocupa un volumen total de 1.400.000.000 de kilómetros cúbicos. Por tanto, no es sorprendente (o no debiera serlo) que el océano contenga cincuenta mil billones (50.000.000.000.000.000) de toneladas sólidas. En las cuales incluyen:

2.000.000.000.000.000 toneladas de magnesio,

100.000.000.000.000» bromo, y

75.000.000.000» yodo,

suficiente para satisfacer las necesidades de la Humanidad por largos milenios.

El océano contiene sorprendentes cantidades de otros metales en disolución (aparte de lo que puedan contener los nodulos del suelo oceánico). Por ejemplo, contiene:

15.000.000.000 toneladas de aluminio,

4.500.000.000» cobre,

4.500.000.000» uranio,

1.000.000.000» torio,

450.000.000» plata,

45.000.000 » » mercurio,

6.000.000» oro, y

45» radio.

Estas cantidades, aunque enormes, se hallan tan diseminadas en el océano, que todavía nos resulta imposible extraerlas provechosamente.

13. Nuestra atmósfera en formación

En los últimos veinte años se han ampliado nuestros conocimientos referentes a la atmósfera de nuestros planetas vecinos. Las observaciones de los globos que flotan por encima de nuestra atmósfera nos han dado pruebas suficientes para incitarnos a la creencia de que las nubes de Venus están formadas por partículas de helio. El «Mariner IV», que pasó cerca de Marte en 1965, nos dijo que su atmósfera era considerablemente más enrarecida de lo que creíamos.

Pero todas las observaciones posteriores sólo han servido para confirmar lo que ya sospechábamos: que la atmósfera de nuestro planeta es única, sin parangón posible con ninguna de las que se hallan al alcance de nuestros instrumentos.

Las atmósferas planetarias que conocemos se dividen en cuatro fases:

Primero, un planeta u otro cuerpo frío no puede tener atmósfera en absoluto, o será tan enrarecida que apenas se distinguirá del vacío espacial.

Segundo, una atmósfera puede ser rica en hidrógeno y otros compuestos relacionados con dicho gas, lo cual impulsa el tipo de reacciones químicas conocidas como «reducciones». Se trataría, por consiguiente, de una «atmósfera de reducción».

Tercero, una atmósfera rica en oxígeno libre sería una «atmósfera oxidante».

Cuarto, una atmósfera puede no contener hidrógeno ni oxígeno y sí sólo algunos gases que no provoquen la oxidación ni la reducción. Sería una «atmósfera neutra».

Los planetas de nuestro sistema solar (exceptuando a Plutón, sobre cuya atmósfera carecemos de información), pertenecen a las clases siguientes:

1) Poca o ninguna atmósfera: Mercurio.

2) Atmósfera de reducción: Júpiter, Saturno, Urano y Neptuno.

3) Atmósfera oxidante: Tierra.

4) Atmósfera neutra: Venus y Marte.

De los treinta y un satélites del sistema solar. Titán (el mayor de Saturno) es el único que posee atmósfera, según nuestros conocimientos una atmósfera de reducción. Los demás, incluyendo nuestra Luna, carecen de atmósfera o la poseen en cantidades mínimas.

En resumen: en ninguna parte del sistema solar, excepto en la Tierra, existe una atmósfera oxidante. En ninguna otra parte hay oxígeno libre.

¿Por qué?

Empecemos con la nube de polvo y gas de la que, según suposiciones, se desarrolló el sistema solar. Los astrónomos opinan que el 90 por ciento de la misma era hidrógeno y otro 9 por ciento, helio. El restante 1 por ciento estaba formado por oxígeno, neón, nitrógeno, carbono, silicio, magnesio, hierro, azufre y argón, probablemente por este orden en cantidades decrecientes, con elementos aún menos comunes, diseminados.

El carbono, el silicio, el magnesio, el hierro y el azufre se solidifican a la temperatura ordinaria, formando compuestos entre sí (carburos, silicatos y sulfuros). Cuando la nube giró en remolinos, los átomos y moléculas de esos elementos y compuestos tendieron a juntarse. Primero en guijarros, después en rocas, hasta formar los cuerpos llamados «planetasimales». Eventualmente, éstos constituyeron el núcleo sólido de un planeta. En la Tierra, un gran exceso de hierro se concentró en el centro del planeta, en tanto que las sustancias rocosas constituían la capa externa.

El hidrógeno, presente en grandes cantidades, se combinó con casi todo lo demás. Atrajo a las moléculas de oxígeno para formar las moléculas de agua (H2O): al nitrógeno, como moléculas de amoníaco (NH3); a gran parte del carbono como moléculas de metano (CH4) y, a parte del azufre como ácido sulfhídrico (H2S). El hidrógeno no pudo combinarse con el helio, el neón, ni el argón, ya que estos tres gases son «gases inertes», y no forman compuestos.

Estas sustancias -hidrógeno, helio, neón, argón, agua, amoníaco, metano y ácido sulfhídrico- tienen bajo punto de fusión, y a temperaturas ordinarias son gases o (caso del agua), líquidos fácilmente evaporables.

A las bajas temperaturas en que se formaron los planetas, algunas de estas sustancias, particularmente el agua y el amoníaco, pudieron ser sólidos, juntándose con los metales y las rocas de los planetasimales. Incluso las sustancias que siguieron en estado gaseoso pudieron quedar atrapadas en considerable cantidad dentro de las estructuras sólidas.

En el centro del sistema solar, la enorme masa interior de la nube se condensó hasta el punto de que las temperaturas internas provocaron el estallido en llamas nucleares. Había nacido el Sol.

El calor creciente del Sol vaporizó las sustancias de bajo punto de fusión, y los gases quedaron liberados de entre los fragmentos del planeta en formación. Estos gases no se adhirieron a la estructura del planeta por lazos químicos, sino únicamente por la fuerza de la gravedad. Si las moléculas de gas se mueven lentamente, tienden a ser sostenidas por las moderadas fuerzas de la gravedad; si se mueven más rápidamente, tienden a liberarse.

Cuanto más caliente está un gas, más velozmente se mueven sus moléculas, y con más facilidad se pierden. Los grupos de planetasimales más cercanos al Sol sintieron con mayor intensidad el calor solar y su atmósfera comenzó a desaparecer. Los gases fueron barridos por el viento solar (consistente en partículas emanadas del Sol a grandes velocidades), y transportados a las regiones exteriores, más frías, del sistema solar.

Al formarse los planetas exteriores, se acumuló en ellos el gas procedente de la parte interior del sistema solar. Por este motivo, Júpiter, Saturno, Urano y Neptuno (particularmente el primero, que recibió los primeros impactos gaseosos) son mucho mayores que los planetas llamados interiores. Están formados principalmente de hidrógeno y de los compuestos de este gas existentes en la nube original. Poseen densas atmósferas reductoras de hidrógeno, helio, amoníaco y metano.

Sin embargo, el grupo interior de los planetasimales perdió completamente su original «atmósfera primaria». El hidrógeno libre y los gases inertes desaparecieron para siempre. Algunas moléculas de agua, amoníaco, metano y ácido sulfhídrico consiguieron adherirse al núcleo sólido de los planetasimales, formando combinaciones químicas en su estructura.

Pero los grupos interiores aún estaban transformándose en planetas, y sus campos gravitatorios eran cada vez más intensos. Sus núcleos se calentaban, y las moléculas gaseosas eran separadas lentamente de sus combinaciones y lanzadas fuera del núcleo, mediante las presiones o la acción volcánica. Mercurio no llegó a poseer el volumen suficiente para aprisionar a esos gases contra la acción calorífica del cercano Sol, gracias a su pequeño tamaño y a su débil campo gravitatorio. Por consiguiente, actualmente casi carece de atmósfera.

Los demás planetas interiores. Venus, la Tierra, y Marte, se hicieron más grandes y se enfriaron más que Mercurio, consiguiendo retener algunos gases; Marte, que es relativamente pequeño, pudo aprisionar una ligera capa, pero la Tierra y Venus tuvieron más suerte. Las moléculas de amoníaco, metano y ácido sulfhídrico envolvieron a Venus, la Tierra y Marte con una fina «atmósfera secundaria», de naturaleza reductora.

El agua también fue desalojada del núcleo de cada planeta. Parte de la misma continuó en la atmósfera en forma de vapor, aunque la mayor cantidad se condensó como líquido. En la Tierra se formaron lentamente los grandes océanos, si bien resulta obvio que Venus, menor y más caliente, retuvo una cantidad de agua muchísimo menor, y aún menos Marte, mucho más pequeño aunque más frío.

De este modo, la vida se desarrolló en una atmósfera reductora. En realidad, la vida necesitaba esta clase de atmósfera para desarrollarse (ver Capítulo 9). A fin de poder formar la vida, han de construirse antes las moléculas complejas, principalmente de átomos de carbono e hidrógeno. Éstas no podían formarse espontáneamente en presencia del oxígeno libre que hoy día se halla en nuestra atmósfera.

Además, esas moléculas complejas sólo podían formarse a expensas de la energética radiación ultravioleta del Sol, que baña la atmósfera y el océano terrestres. De haber contenido oxígeno la atmósfera, la radiación ultravioleta se habría diseminado y perdido, y su energía no habría podido ser utilizada.

Naturalmente, la misma radiación ultravioleta que suministra la energía para la formación de las complejas moléculas de carbono-hidrógeno, tenderían a descomponer a las que se transformasen en particularmente complejas. Eventualmente, pues, las formas simples de vida surgieron de moléculas muy complejas que llenaban los mares a varias decenas de metros por debajo de su superficie, aunque planeando a un nivel hasta el que no podían penetrar los rayos ultravioleta. En el nivel superior, las moléculas moderadamente complejas comenzaron también a formarse y, descendiendo, servían de alimento a las formas de vida.

Pero, incluso cuando se estaba desarrollando la vida, la atmósfera continuaba en formación. La radiación ultravioleta, al llegar a la atmósfera, chocaba con las moléculas de agua y las separaba, liberando el oxígeno del hidrógeno («fotodisociación»).

Cuanto menor es la masa de una molécula gaseosa, tanto más rápidamente se mueve a cualquier temperatura, y más fácilmente escapa al campo gravitatorio. Los átomos de hidrógeno son los más ligeros que se conocen y se mueven con demasiada rapidez para quedar retenidos por el campo de gravedad de la Tierra. Los átomos de hidrógeno liberados por la descomposición de la molécula de agua se retiraron lentamente hacia el espacio interplanetario, desapareciendo de la Tierra.

Los átomos de oxígeno libre, bastante macizos para ser retenidos por el campo de gravedad de la Tierra, se combinaron, formando moléculas (cada una con dos átomos de oxígeno), las cuales se combinaron con otras sustancias y con las rocas del suelo para formar minerales oxidados, principalmente silicatos. También se combinaron con las moléculas de amoníaco, metano y ácido sulfhídrico de la atmósfera, formando nitrógeno y agua en el primer caso, dióxido de carbono y agua en el segundo, y azufre y agua en el tercero.

El agua formada en estas reacciones fue fotodisociada a su vez, lo cual sirvió para que continuase el proceso. El azufre se unió al núcleo sólido del planeta, formando sulfatos, en combinación con el oxígeno, o sulfitos. El amoníaco y el metano de la atmósfera se combinaron gradualmente en nitrógeno y dióxido de carbono, a expensas de un gradual decrecimiento de los depósitos de agua. Y la atmósfera reductora se convirtió en una atmósfera neutra.

Esto ocurrió en Marte, cuya delgada atmósfera está formada en la actualidad casi por completo por dióxido de carbono, y cuyas cantidades de agua han decrecido hasta poder formar escasas capas de escarcha y casquetes polares.

La atmósfera de Venus está compuesta actualmente, según todas las probabilidades, de nitrógeno y dióxido de carbono. Si bien Venus aún retiene bastante agua, se calcula que la cantidad total es solamente de 1/10.000 de la de los océanos terrestres.

Como Venus siempre tuvo una atmósfera mucho más densa que Marte, contiene ahora mucho más dióxido de carbono, lo cual es crucial.

El dióxido de carbono no absorbe la luz visible en gran cantidad, pero sí, y abundantemente, la radiación infrarroja. La luz solar pasa a través de una atmósfera que contiene mucho dióxido de carbono, incidiendo contra el suelo y los mares, y es absorbida como calor. La superficie calentada irradia parte del calor como luz infrarroja, pero esta radiación es absorbida y retenida por el dióxido de carbono de la atmósfera, que, en consecuencia, se calienta más.

Un planeta con una atmósfera pobre en dióxido de carbono y otros gases absorbentes, permite que la radiación infrarroja huya al espacio, y permanece frío, mientras que otro planeta con una atmósfera rica en dióxido de carbono retiene la radiación infrarroja y se calienta más, aunque ambos planetas se hallen a la misma distancia del Sol. A esta acción del dióxido de carbono se le llama «efecto de invernadero», porque el cristal de un invernadero también sirve para transmitir la luz y retener la radiación infrarroja, manteniendo caliente y húmedo el interior, incluso en invierno.

Cuando la atmósfera de Venus se transformó en neutra y se fue formando más dióxido de carbono, la atmósfera se fue calentando cada vez más. Eventualmente, la temperatura del planeta y su atmósfera llegó a un punto en que el agua comenzó a formar las nubes que hoy día cubren el planeta Venus. El vapor de agua también absorbió la radiación infrarroja, por lo que la presencia de esa capa acuosa sobre Venus intensifica el efecto de invernadero.

Al parecer, este proceso podría continuar indefinidamente, suponiendo que hubiese suficiente agua. Podría seguir vertiéndose oxígeno a la atmósfera, y cuando todo el amoníaco y el metano se hubieran convertido en nitrógeno y dióxido de carbono, y todas las rocas de la superficie en silicatos, se acumularían en la atmósfera mayores cantidades de oxígeno. Sin embargo, no es así. Tan pronto como el oxígeno libre penetra en la atmósfera, empieza a absorber la radiación ultravioleta. En este proceso, las moléculas de dos átomos del oxígeno ordinario se convierten en la molécula de tres átomos del ozono.

Entonces, se forma una capa de ozono en la atmósfera superior, y la radiación ultravioleta queda absorbida. Esta radiación, al penetrar en la capa y llegar a la atmósfera inferior, donde existe el vapor de agua, decrece gradualmente en intensidad, a medida que aumenta la concentración de ozono, y eventualmente termina la fotodisociación.

La fotodisociación es un «proceso autolimitado». Puede convertir una atmósfera reductora en otra neutra, que es lo que por lo visto hizo en Marte y Venus, pero no puede continuar formando una atmósfera oxidante.

Entonces, ¿cómo se originó la atmósfera oxidante de la Tierra? Al principio, debió de haber fotodisociación en la Tierra igual que en Venus, aunque probablemente a ritmo más lento, puesto que la Tierra se halla más lejos del Sol que Venus y recibe la radiación ultravioleta en menores dosis.

Aun así, la provisión de agua de la Tierra empezó a decrecer y su atmósfera a tornarse neutra, y al final tal vez se perdió la mitad de la provisión total de agua. Por fortuna, la Tierra podía permitirse esta pérdida, y retuvo agua suficiente para formar el océano actual.

Sin embargo, el proceso no concluyó igual que en Venus. Entró en liza un nuevo factor, que parece haber sido el desarrollo químico que apareció en algunas de las formas de vida oceánica, en la Tierra primitiva. Sin ese desarrollo, cualquier forma simple de vida aparecida en Marte no logró más que sobrevivir mientras el planeta se secaba lentamente. Y cualquier forma simple de vida desarrollada en Venus aún debió de ser menos afortunada, pues debió morir en tanto el planeta se calentaba lentamente hasta llegar casi al punto de ebullición.

Las formas de vida de la Tierra hubieran sufrido un fin semejante de no ocurrir una novedad. Se trataba por entonces de seres diminutos, de una sola célula, no mucho mayores ni complejos que las bacterias actuales. Derivaban perezosamente por debajo de las capas superiores del océano, viviendo gracias a la penetración de las moléculas alimenticias de más arriba. Vivían exclusivamente al ritmo ordenado por la lenta producción de sustancias nutritivas, formadas gracias a la radiación ultravioleta del Sol.

Después se originó una nueva molécula llamada «clorofila». La misma se formó en torno a un círculo de átomos complejo y estable, que debió construirse partiendo de moléculas más simples, por la acción de la radiación ultravioleta. Ocasionalmente, se añadieron adornos en forma de breves «cadenas secundarias» de átomos, unidos espaciadamente al círculo. Y una combinación especial de esas cadenas secundarias dio origen a la clorofila. Ésta era capaz de absorber la luz visible, particularmente en su gama roja. El verde era reflejado, por lo que la clorofila presenta un color verde muy vivo. Cuando la clorofila absorbió la luz visible, quedó cargada de energía, que introdujo ciertos cambios químicos.

Una vez las células incorporaron la clorofila a su estructura, poseyeron ya un instrumento importante para producir unos cambios antes imposibles. En efecto, podían ya usar la energía de la luz visible, después de haber sido almacenada en la molécula de clorofila, para provocar una serie de cambios que terminaron en la formación de las complejas moléculas alimenticias, de las que las células se nutrieron sin tener que aguardar la llovizna molecular en el océano. A este proceso se le conoce con el nombre de «fotosíntesis».

Una de las consecuencias de la fotosíntesis es que la energía de la luz visible descompone las moléculas de agua en hidrógeno y oxígeno. La luz visible, al contrario que la radiación ultravioleta, de más energía, no podría efectuar esta descomposición sin la ayuda de la clorofila.

El agua se descompone más rápidamente mediante el efecto de las concentraciones de los sistemas clorofílicos dentro de las células que mediante la acción de choque de la radiación ultravioleta. Las células que utilizaron la clorofila obtuvieron más alimentos y pudieron multiplicarse más rápidamente que las que no utilizaron la clorofila.Gradualmente, a través de muchos siglos, el uso de la clorofila se generalizó y la fotosíntesis fue el modo de vida prevalente. Como las células que contenían clorofila eran de color verde, el mundo vivo se tornó verde lentamente. Y nuestro planeta es hoy día el del color verdoso.

La fotosíntesis hizo algo más, aparte de acelerar la descomposición del agua y apresurar la conversión de la atmósfera reductora en neutra.

La evolución ya podía superar la fase de la atmósfera neutra. Una vez formada ésta completamente, y el oxígeno existiendo en forma libre, se originó en la atmósfera superior una capa o sombrilla de oxígeno, y ocasionalmente una capa de ozono. La radiación ultravioleta empezó a ver impedida su penetración, y la fotodisociación decayó. Pero la luz visible podía atravesar la capa de ozono, de modo que continuó la fotosíntesis. Al revés que la fotodisociación, la fotosíntesis no es un proceso autolimitado a este respecto. Por esto, cada vez hubo en el aire más oxígeno, y la atmósfera terrestre pasó del estado neutro al oxidante.

Aun así, ¿por qué la alta concentración de dióxido de carbono siempre presente en la atmósfera no atrapó el calor del Sol, haciendo hervir a todos los océanos de la Tierra, lo mismo que ocurrió en Venus? Por fortuna, la descomposición de las moléculas de agua no es el único efecto de la fotosíntesis. Las moléculas de hidrógeno que se formaron en el curso del proceso no penetraron en la atmósfera y se perdieron en el espacio. En cambio, el hidrógeno sufrió una serie de reacciones químicas que finalizaron en su combinación con el dióxido de carbono, para formar almidones y otros componentes de las células vegetales.

Así, mientras la fotosíntesis llenaba la atmósfera de oxígeno, no permitía la fuga del hidrógeno sino que lo utilizó para eliminar el dióxido de carbono, y al final, la atmósfera terrestre quedó casi exclusivamente compuesta de nitrógeno y oxígeno.

Se ignora cuándo tuvo lugar este cambio. La mejor suposición, basada en la química de las rocas antiguas, es que el oxígeno libre empezó a formar parte de la atmósfera terrestre entre mil y dos mil millones de años atrás, cuando ya hacía otros mil o dos mil millones de años que existía la vida en la Tierra.

Hace unos 600.000.000 de años que la cantidad de oxígeno de la atmósfera era aproximadamente ya una décima parte de la que hay hoy día. Esto produjo un cambio biológico, ocasionando lo que los biólogos denominan «período cámbrico».

Durante el período precámbrico, en el que había muy poco o ningún oxígeno en la atmósfera, las formas de vida ya habían obtenido energía de las moléculas orgánicas complejas descomponiéndolas en estructuras más simples, sin ningún cambio radical en la naturaleza de su estructura química. Éste es el proceso de la «fermentación».

Sin embargo, con un porcentaje razonable de oxígeno en la atmósfera, las formas de vida pudieron obtener unas veinte veces más de energía, y de esta manera desarrollaron sistemas para combinar los alimentos con el oxígeno.

Disponiendo de grandes cantidades de energía, la vida floreció y proliferó. Durante los cien millones de años del período cámbrico, las formas de vida crecieron y se transformaron en miríadas de formas más complejas y desarrolladas.

Las células se unieron para formar organismos multicelulares. Los diferentes grupos de células en el interior de esos organismos pudieron especializarse. Algunos desarrollaron métodos de contracción rápida, y otros para conducir los impulsos eléctricos, de modo que se formaron los músculos y los nervios. Crecieron conchas y otros agentes duros para proteger las grandes masas de células, así como a todo el organismo de sus enemigos. De pronto, no tuvo fin al parecer el ingenioso desarrollo de las formas de vida, una vez dispusieron de la energía necesaria.

Las conchas y otras estructuras duras se conservaban incluso después de morir el organismo. Era entonces cuando asumían una estructura pétrea al correr de los siglos, y las rocas del período cámbrico son muy ricas en tales restos, llamados «fósiles», mientras que las rocas pertenecientes a tiempos más remotos están libres de fósiles.

Hace unos 400.000.000 de años, el contenido de oxígeno de la atmósfera alcanzó probablemente su nivel actual. La sombrilla de ozono era compacta, y la cantidad de radiación ultravioleta que llegaba a la Tierra era suficiente para permitir que las formas de vida recibiesen directamente la luz del Sol durante razonables períodos de tiempo.

Por primera vez las formas de vida pudieron aventurarse por la tierra seca, colonizando los continentes.

Pero la evolución atmosférica no llegó a su final con la formación de lo que ahora poseemos. Hubo fluctuaciones en la cantidad de los componentes, y éstas ejercieron unos considerables efectos con respecto, particularmente, al dióxido de carbono.

Actualmente, sólo el 0,03 por ciento de la atmósfera se compone de dióxido de carbono, mas su importancia es superior a lo que indica tal cantidad, no sólo por ser el último alimento de la vida vegetal (y, por tanto, también de la vida animal), sino a causa de su efecto de invernadero. Incluso pequeños cambios en la concentración de dióxido de carbono pueden ejercer un efecto poderoso en la temperatura terrestre.

Hay periodos en la Historia en que las grandes acciones volcánicas en toda la corteza terrestre arrojaron cantidades desusadas de dióxido de carbono a la atmósfera, aumentando su concentración. Entonces, la atmósfera retuvo más el calor, y la temperatura de la Tierra fue mayor. Con este calor y las mayores cantidades de dióxido de carbono, floreció la vida vegetal y los bosques cubrieron la Tierra. Seguramente fue después de tales períodos que se formaron los grandes depósitos de carbón y los pozos de petróleo.

Otras veces, los períodos de formación de montañas trajeron grandes masas de rocas a la superficie. Estas rocas, que jamás habían estado expuestas al aire, se combinaron con el dióxido de carbono para formar los carbonatos. El contenido de dióxido de carbono del aire se redujo anormalmente a un valor bajo, el efecto de invernadero dismi nuyó, y la Tierra se enfrió. Entonces, al enfriarse más de lo debido, apareció uno de los períodos glaciares. Actualmente, nos hallamos al final de un prolongado período de construcciones montañosas y heladas. Pero la Humanidad ya está a punto de introducir un nuevo factor, jamás presente en la Tierra hasta ahora: su propia tecnología.

El hombre cava la tierra en busca de carbón y petróleo, en los yacimientos originados hace tantos millones de años, y lo quema desde hace casi un par de siglos. Forma, de nuevo, el dióxido de carbono que las plantas consumieron hace tiempo para formar sus tejidos y que, ocasionalmente, se transformaron en más carbón y petróleo.

Seis mil millones de toneladas de carbón, petróleo y gas se queman al año, y la cantidad de dióxido de carbono en la atmósfera aumenta lentamente (aunque la mayor parte se disuelve en el mar y es consumida por las plantas). Se calcula que a este promedio, la cantidad de dióxido de carbono del aire será en el año 2000 un 25 por ciento mayor que en la actualidad. Y hacia el 2300 se habrá duplicado.

No nos envenenará la presencia de un 0,26 por ciento de carbono en el aire, pero, ¿y el efecto de invernadero? Si la Tierra aumenta lentamente de temperatura, cabe suponer que se fundan los icebergs polares, lo cual aumentará el nivel del océano. Aun teniendo en cuenta que el mayor peso del agua tenderá a deprimir el fondo de los océanos, es de esperar que el nivel del mar tenga sesenta metros más de altura que hoy día, una vez fundidos todos los casquetes polares.

Por tanto, todas las regiones costeras de los continentes, precisamente donde se agrupan las mayores concentraciones de seres humanos, quedarán cubiertas por las aguas. Se ha calculado, no obstante, que hasta en las condiciones más extremas, los hielos tardarían unos cuatrocientos años en fundirse completamente, por lo que la Humanidad tendría tiempo de prevenirse. También podría ser una gran ayuda pasar del carbón y el petróleo a los combustibles nucleares. Los aparatos que limpiaran la atmósfera de grandes cantidades de dióxido de carbono ayudarían a enfriar la Tierra, y podría inventarse un sistema para diseminar en los océanos sustancias destinadas a reflejar más eficazmente la luz solar.

Como último recurso, podrían producirse traslados de población, ya que ciertas regiones cercanas a los polos y los desiertos, que actualmente apenas mantienen vida, serían entonces capaces de acoger a grandes poblaciones.

La atmósfera terrestre, que ha mantenido formas de vida en crisis pasadas, tal vez se halle a punto de ofrecernos otra en un futuro próximo.

14. La atmósfera de la Luna

Ahora que los satélites se dirigen a la Luna, que rodean la Luna, que aterrizan en la Luna, ahora que ya hemos enviado hombres a la Luna, resulta grata cualquier información respecto a la misma. Por ejemplo, ¿qué hay de la atmósfera lunar?

¡Oh, la Luna carece de atmósfera!, alegará el lector.

Ciertamente, carece de ella en el sentido terrestre. Pero tiene algo. Ha de tener algo. Y esto es demostrable de esta forma:

La Tierra se compone de dos secciones de composición radicalmente diferente (como un huevo, formado por la yema central y la clara.) La «yema» de la Tierra es el núcleo de hierro y níquel, con una densidad diez veces mayor, aproximadamente, que la del agua. A su alrededor, la «clara» de la Tierra es la corteza de silicatos, con una densidad menor, unas tres veces la del agua. La densidad media de la Tierra se halla entre dichas dos cifras. O sea, unas cinco veces y media la del agua (5,5 gramos por centímetro cúbico).

La densidad de la Luna es de 3,3 gramos por centímetro cúbico. Para poder ser mucho menos densa que la Tierra, la Luna ha de carecer bastante de hierro y níquel. Hablando vulgarmente, ha de ser toda «clara», conteniendo principalmente silicatos.

Es razonable suponer que la composición elemental de la Luna es la misma, por consiguiente, que la de las rocas terrestres. Las dos fueron formadas al mismo tiempo, con los mismos materiales. La corteza terrestre, por ejemplo, tiene un 2 ½ por ciento de potasio, y podemos suponer que en la Luna la cifra es la misma. La masa de la Luna es de unos 73.430.000.000.000.000.000.000 kilogramos, o sea unos ochenta trillones de toneladas. La masa de potasio lunar ha de situarse, por tanto, en los 1.800.000.000.000.000.000.000 de kilogramos, aproximadamente, o dos trillones de toneladas.

Existen tres variedades de átomos de potasio. Dos de ellos, el potasio-39 y el potasio-41, componen el 99,99 por ciento del total. Sin embargo, el restante 0,0119 es un isótopo raro, el potasio-40, el más interesante. La masa total del potasio-40 en la Luna debe de ser de unos 214.000.000.000.000.000 de kilogramos, o sea 214 billones de toneladas.

El potasio-40 es radiactivo. Tiene una vida media de 1.200 millones de años, lo que significa que en este período de tiempo, la mitad de sus átomos se descomponen.

La mayoría de átomos descompuestos (el 89 por ciento para ser exactos) ceden un electrón y se convierten en átomos estables de calcio-40. Los núcleos del restante 11 por ciento, no obstante, absorben los electrones del ambiente y se convierten en átomos estables de argón-40.

Una vez conocida la vida media de una sustancia radiactiva, puede calcularse fácilmente su promedio de desintegración por unidad de tiempo. En la Luna, 3.600 gramos (unas 8 libras) de potasio-40 se desintegran cada segundo. Como resultado de esta desintegración, 3.240 gramos (7 1/6 libras) de calcio-40 y 360 gramos (5/6 de onza) de argón-40 se forman cada segundo.

Éste, el argón-40, es el que nos interesa, puesto que se trata de un gas, y esto significa que la Luna está formando constantemente una atmósfera a su alrededor. Naturalmente, 360 gramos de argón es poca cosa, pero si se forman a cada segundo, y éstos se suceden como es normal…

Además, había más potasio-40 presente en la Luna antaño que en la actualidad. Hace unos 1.200 millones de años, había casi el doble, y cuatro mil millones de años atrás, unas ocho veces más que hoy día.

Si calculamos la cantidad de argón formada durante los cuatro mil millones de años en que la Luna ha sido un cuerpo sólido, y pensamos en la mayor cantidad de potasio-40 de tiempos remotos, resulta que la cantidad de argón que se ha formado durante todo este tiempo es de 150.000.000.000.000.000 de kilogramos, o sea unos 150.000 billones de toneladas de argón.

Para tener una idea de la enormidad de tal cantidad, diremos que representa casi tres veces al argón presente actualmente en nuestra atmósfera (argón que, incidentalmente, se formó y se forma de nuestro potasio-40).

Si todo el argón estuviera presente en la superficie de la Luna, nuestro satélite tendría una atmósfera con una masa 1/30 de la nuestra. Además, como la superficie lunar es sólo 1/16 de la terrestre, su atmósfera sería más compacta, hasta tener la mitad de la densidad de la atmósfera terrestre.

Pero la Luna no tiene tal atmósfera, como sabemos. Entonces, ¿qué ha sido del argón de la Luna?

Primero, el potasio-40 se propagó por todo el volumen lunar. El argón formado en las capas externas de las rocas lunares se abrió paso hacia la superficie, mas el formado a mayor profundidad quedó atrapado. (Esto también cuenta para el argón terrestre. La cantidad de argón atrapado en las entrañas de la Tierra es cinco veces, y podría serlo quince, mayor que la de la atmósfera.)

Pero aunque solamente 1/15 del argón lunar llegue a su superficie, la Luna debería tener una atmósfera cuya densidad sería del 3 por ciento respecto a la terrestre, y no obstante, ni esto tiene.

Aquí se presenta otra cuestión. El campo gravitatorio de la Luna es sólo 1/6 del terrestre, o sea, que no posee fuerza suficiente para retener al argón. La Luna, pues, pierde este gas hacia el espacio exterior casi tan rápidamente como surge de las rocas.

¡Casi! Sí, el argón tarda algún tiempo en abandonar por completo la Luna, de modo que siempre hay alguna cantidad de argón (no mucha) presente cerca de la superficie lunar.

En realidad, los astrónomos que observan las ondas de radio emitidas por diversos cuerpos celestes, han estudiado el comportamiento de las ondas que se deslizan por la superficie de la Luna, camino de la Tierra. Estas ondas de radio quedan ligeramente perturbadas, y se calcula que la perturbación se debe a una atmósfera lunar de partículas cargadas, de densidad igual a una dieztrillonésima de la terrestre.

No es mucho…, pero es algo.

15. El hombre y el Sol

El Sol era un dios para los antiguos. Ikhnaton, faraón de Egipto desde 1375 a 1358 a. de C., adoró al Sol y compuso un himno que subsiste hoy día. Quince siglos más tarde, cuando la cristiandad comenzó a apoderarse del Imperio romano, su mayor competidor era el mitraísmo, el culto al Sol.

Y con toda seguridad, si algún objeto inanimado es digno de adoración, éste es el Sol. Fue él quien produjo la progresión del día y la noche que le dio al hombre primitivo la primera noción del tiempo. El Sol trajo el calor y la vida a este mundo, y cada amanecer era una alegría al desvanecerse las tinieblas, los terrores de la oscuridad. Si la luz del Sol fuese pálida y empañada como en los meses de invierno, el hielo y la muerte rondarían cerca. Por lo tanto, no es de maravillar que si alguna vez quedó eclipsado su brillo y su resplandor, se apoderase el pánico de quienes presenciaban tal fenómeno.

La ciencia moderna ha intensificado nuestra comprensión respecto hasta qué punto dependemos del Sol. Salvo por el calor volcánico y las reacciones nucleares, todo el origen de las energías necesarias para el hombre procede en último término del Sol. Los océanos se mantienen líquidos por el calor del Sol, y el vapor formado por dicho calor es devuelto en forma de lluvia, mientras que el calentamiento de la atmósfera nos proporciona el viento y los cambios climatológicos.

Los rayos del Sol proporcionan la energía requerida por las plantas verdes, a fin de que puedan fabricar el almidón partiendo del dióxido de carbono, y liberar el oxígeno del agua. De esta forma, la comida que ingerimos y el aire que respiramos son un don directo del Sol.

¿Y qué es el Sol, al que tanto debemos? Una bola de luz, una bola de luz pura y perfecta, sin peso y divina, según juzgaban los antiguos. Un astrónomo griego empleó proporciones geométricas para demostrar que el Sol era mayor que la Tierra, y que ésta debía moverse a su alrededor, pero muy pocos hicieron caso de esta aparente tontería.

Sin embargo, dieciocho siglos más tarde, el astrónomo polaco Nicholas Copérnico, publicó en 1543 un análisis detallado de la forma en que la Tierra tenía que girar en torno al Sol, si había que explicar convenientemente los movimientos de los cuerpos celestes. Al cabo de un siglo de debates, se aceptó su opinión. En 1610, el científico italiano Galileo, ayudó a ello detectando puntos negros en el Sol, unas manchas en su supuesta perfección, lo que sirvió para demostrar que era un cuerpo material y no una sustancia semidivina, completamente extraña a la terrestre.

En 1683, el científico inglés Isaac Newton formuló la teoría de la gravitación universal, y la Humanidad tuvo otra deuda con el Sol. Su gigantesco cuerpo propagaba un enorme campo de gravitación, que se extendía miles de millones de kilómetros en todas direcciones. Atrapada en este campo, la Tierra daba vueltas en torno al Sol constantemente sin acercársele jamás demasiado, ni apartarse con exceso, quedando mantenida a la distancia requerida con la misma gentileza que un niño en brazos de su madre.

Según la ciencia moderna, el Sol es un globo material de 1.392.000 kilómetros de diámetro, que gira en torno a su eje cada veinticinco días. Comparada con él, la Tierra es como un pequeño guisante ante una pelota de béisbol. Si el Sol fuese una cascara vacía, en su interior cabrían 1.300.000 planetas del tamaño de la Tierra, sin llenarla. La materia es algo más compacta en la Tierra que en el conjunto solar. Se necesitaría la materia de 333.000 Tierras para formar la materia del Sol.

Las partes más pequeñas que del Sol podemos ver son enormes y monstruosas. La materia de sus capas superficiales, a una temperatura de 5.500° C, se arremolina y burbujea, con secciones que se levantan y se hunden, dándole al conjunto el aspecto de un grano de arroz. Si bien cada grano tiene un diámetro de miles de kilómetros.

En la superficie solar se forman grandes remolinos de materia, con fuertes propiedades magnéticas. La energía empleada en construir este magnetismo y en producir otros vastos trastornos se extrae de su propio calor. Por tanto, los tornados se calman a 3.900° C. Se trata de mucho calor según la pauta terrestre, pero no tanto como el existente en la superficie que rodea al Sol, que en comparación aparece negra. Se trata de las manchas descubiertas por Galileo.

Estas manchas, los remolinos solares, tienen miles de kilómetros de diámetro. Una de ellas, medida en 1947, medía 150.000 kilómetros de diámetro. Tres docenas de planetas como la Tierra no habrían bastado para llenar aquel gigantesco embudo.

Las manchas del Sol aparecen por ciclos, aumentando en número de año en año hasta alcanzar una cúspide, durante cuyo tiempo el Sol queda ampliamente manchado. Luego, declina esta incidencia, hasta que algunos años el Sol está despejado por completo. Las cúspides se producen con once años de intervalo, y en tales épocas el Sol parece trastornado de muchos modos.

En los momentos álgidos de la actividad solar, por ejemplo, el Sol es particularmente activo en la erupción de materiales a miles y cientos de miles de kilómetros hacia arriba, contra su propia gravedad. Estas «prominencias» forman gotas de brillantes llamas rojas que ascienden o se arquean hacia arriba, invisibles a la vista ordinaria, más aparentes contra el borde del globo solar cuando el resplandor de su disco queda obstaculizado en los instrumentos modernos.

Tiene lugar un bloqueo natural de la luz del Sol cuando la Luna pasa directamente por delante del mismo. Por extraña coincidencia, la diminuta Luna se halla a la distancia exacta de la Tierra para adoptar el tamaño aparente del gigantesco Sol. Cuando la Luna pasa por delante de aquél, por tanto, lo tapa por completo.

Cuando esto sucede (por desgracia para los astrónomos con poca frecuencia), el resplandor blanquecino del sol queda ensombrecido, y la atmósfera exterior del astro rey se torna visible como una serie perlina de gallardetes luminosos y difusos. Esta «corona» se extiende fuera del disco solar como un gas muy caliente aunque muy tenue. Las observaciones de estos últimos años nos han permitido medir la temperatura de la corona, que ha resultado ser de 1.112.000° C aproximadamente. O sea, temperatura suficientemente elevada como para irradiar rayos X junto con la luz ordinaria. Sin embargo, la materia de la corona se halla extendida por el espacio en forma muy tenue, y a pesar de su alta temperatura, el contenido calorífico total es muy reducido.

Los astrónomos suponen que en la infancia del sistema solar, la materia que lo formaba consistía principalmente en polvillo de gases que giraban lentamente, y fueron contrayéndose bajo su propio impulso gravitatorio.

A medida que la materia se tornaba compacta hacia el centro, la temperatura del mismo se iba elevando. Éste es un fenómeno inevitable. La compresión del aire mediante una bomba de mano lo calienta y el centro de la Tierra, comprimido por el peso de todas las rocas y las demás materias de la superficie, se halla a una temperatura de miles de grados.

La materia comprimida del Sol, mucho más maciza que la terrestre, elevó su presión interna y su temperatura hasta unos límites insospechados.

Los átomos se movieron allí con más energía, hasta llegar a un punto en que las colisiones fueron tan monstruosas que los electrones que ocupaban las órbitas extemas de los átomos abandonaron su lugar dejando al descubierto los diminutos núcleos en el centro de los átomos. Entonces, los materiales se unieron drásticamente, y el Sol se encogió hasta alcanzar el tamaño actual.

Casi toda la materia del primitivo Sol era hidrógeno, y el núcleo del átomo de hidrógeno es una partícula sola, increíblemente pequeña, llamada «protón», según ya sabemos. En tanto se iba elevando la temperatura, estos protones, ya sin capa protectora, fueron chocando cada vez con más ímpetu, hasta que empezaron a ejercer una interacción que formó unos núcleos más complicados, con cuatro partículas: los núcleos de helio.

Esta fusión del hidrógeno para formar helio liberó una gran cantidad de energía. Se trata del mismo proceso que tiene lugar en la bomba de hidrógeno. En resumen: el Sol se incendió para formar una hoguera nuclear y se transformó en una colosal bomba de hidrógeno, gracias a cuya luz y calor vivimos. El Sol, al revés que las bombas de hidrógeno terrestre, no estalla y se desvanece pocos instantes después de la explosión, porque la gigantesca gravedad solar mantiene junta a su sustancia contra toda la fuerza de la fusión nuclear.

Tampoco estamos sujetos a la peligrosa radiación de esta enorme bomba de hidrógeno del cielo, porque la mayor parte del peligro queda enterrado muy adentro del Sol. En su centro, donde tiene lugar la fusión nuclear, la temperatura es de unos 14.000.000° C, pero este calor increíble está contenido allí y sólo surge muy lentamente a través de los centenares de miles de kilómetros de materia solar. La superficie del Sol sólo está ligeramente caliente en comparación con el centro, y la parte de radiación superviviente es absorbida por la atmósfera terrestre antes de llegar a nosotros.

Probablemente habrán transcurrido unos cinco o seis mil millones de años desde que el centro compacto del Sol se incendió en un fuego nuclear, mas en todo este tiempo sólo una mínima porción de su inmenso contenido de hidrógeno se ha fusionado en helio. Aun hoy día, muchísimo más de la mitad de la masa solar es de hidrógeno, y posee bastante combustible nuclear para continuar ardiendo igual que ahora al menos durante diez mil millones de años más.

Del Sol, hasta tiempos muy recientes, nos llegaba más materia de lo que la gente supone. Porque no toda la materia arrojada «hacia arriba» desde su superficie vuelve al Sol. Una parte de la misma (como las rociadas del mar llevadas a tierra por el viento) deja el Sol y se propaga por el exterior en forma de filamentos muy finos.

Este material, en forma de protones y electrones cargados eléctricamente, alimenta a la corona, que se extiende en torno al Sol, cada vez más ancha, hasta que se pierde en las vastedades del espacio…, siendo constantemente renovado por la nueva materia procedente del Sol. Esta materia, sumamente fina, siempre arrojada del Sol, es el «viento solar», que incluso se nota en la Tierra, o sea, a una distancia de 150.000.000 de kilómetros del Sol.

La materia solar, cerca de la Tierra, es sumamente tenue, aunque bastante densa para impedir que el espacio que nos rodea sea un vacío absoluto. La Tierra, dicho de otro modo, es como un objeto que se mueve en una órbita dentro de la corona solar.

Las partículas cargadas del Sol son atraídas por los campos magnéticos de la Tierra, que avanzan desde los polos magnéticos a las regiones polares y alcanzan su mayor altitud en las regiones ecuatoriales.

Los electrones y los protones del Sol se unen en el campo magnético terrestre, y forman una especie de círculo en forma de buñuelo en torno a la Tierra. Se trata del Cinturón de Van Allen, que descubrió en 1958 el físico norteamericano James van Allen.

Cerca de los polos magnéticos, las partículas cargadas eléctricamente se dirigen hacia la atmósfera superior de la Tierra, donde sufren interacciones que crean la esférica belleza de las auroras boreales y australes.

El viento solar no es constante. De vez en cuando, se torna más intenso, de forma imprevisible. Esto sucede principalmente en las épocas de mayor actividad de las manchas solares, hallándose especialmente asociado con los «destellos». Ocasionalmente, la vecindad de una mancha solar puede tornarse mucho más brillante durante una hora, y este destello descarga una enorme rociada de partículas hacia el espacio.

Si esta rociada adopta la dirección de la Tierra, la nube de partículas invade nuestra atmósfera superior en menos de un día. Entonces, las auroras boreales son más resplandecientes, y se produce lo que se llama una «tormenta eléctrica».

Esta clase de tormentas puede afectar gravemente a la tecnología moderna. Las comunicaciones por radio dependen del contenido de fragmentos de átomos cargados eléctricamente, llamados iones, en la atmósfera superior, por lo que esta región se llama «ionosfera». Estos iones pueden reflejar las ondas de radio. Sin embargo, cuando las partículas cargadas eléctricamente invaden la ionosfera por enjambres, esta acción reflejante se toma versátil. Los medios de comunicación a larga distancia por medios electrónicos se descomponen en una serie de enjambres estáticos que pueden persistir durante más de treinta horas.

Asimismo, el viento solar puede afectar más a la Tierra día a día, con efectos intrínsecamente importantes. La lluvia no es sólo un efecto de la humedad del aire, ni siquiera las nubes, según sabemos hoy día. Las gotas de lluvia tienen que formarse, y esto no es sencillo. Usualmente se forman en torno a alguna partícula de polvo, del tamaño, forma y propiedades químicas adecuados. Los modernos creadores de lluvia tratan de suministrar este polvillo rociando los productos químicos más apropiados hacia las nubes.

Los iones también forman núcleos naturales para las gotas de lluvia, por lo que la probabilidad de que llueva se apoya en la riqueza de iones de la atmósfera superior. En conjunto, los iones son más numerosos en los años de actividad de las manchas solares, en que el viento solar es más intenso. Por tanto, las lluvias son más abundantes en tales años.

Así, algunas mediciones han indicado que el nivel de agua del lago Erie es más elevado durante la actividad máxima de las manchas solares. Los estudios de los círculos de árboles del sudoeste de Estados Unidos demuestran, al parecer, que aquéllos son más espesos (y la lluvia, por tanto, más copiosa) en ciclos de once años, como el de las manchas del Sol.

Cuando meditamos hasta qué punto la vida de nuestro planeta depende de la lluvia, podemos achacar casi todas sus variaciones a las manchas del Sol. Los períodos carentes de lluvias pueden significar años de carestía en los alimentos y, por consiguiente, años de inquietud política y de agresiones periódicas en el mundo entero. No es extraño que algunos sabios hayan intentado formular ciclos de guerras y depresiones, armonizándolos con el crecimiento y descenso de la frecuencia en las manchas solares. Sin embargo, dicha frecuencia es muy irregular, y la conducta humana es lo suficientemente complicada para tornar fútiles tales intentos.

Con la llegada de la era espacial, la conducta del Sol ha de ser fuente de grandes preocupaciones para los astronautas. La atmósfera terrestre absorbe gran parte de la radiación peligrosa para la vida, y fuera de la atmósfera el margen de seguridad es mucho menor. Mientras los astronautas salgan sólo de la inmediata vecindad de la Tierra por cortos períodos de tiempo, las paredes de la cápsula espacial (y, aún más importante, el campo magnético de la Tierra) los protegerá, pero en períodos más largos el peligro se agudiza.

Camino de la Luna, han de estar protegidos contra la intensa radiación del Cinturón de Van Allen[8]. Tal vez será posible evitarlo, pasando por entre las brechas polares de tal Cinturón.

En el espacio abierto, los astronautas no pueden contar con ninguna seguridad, ni siquiera bajo condiciones en que el nivel de radiación a su alrededor parezca ser extraordinariamente bajo. Un súbito destello en la superficie del Sol podría propagar partículas peligrosas en su dirección, que no podrían esquivarse en modo alguno. Varios destellos han sido tan feroces que han enviado cantidades de la radiación más enérgica que se conoce: rayos cósmicos.

Los mismos exploradores de la Luna, que no tiene atmósfera que proteja (ver Capítulo 14), hallarán que éste es uno de los mayores peligros contra los que tendrán que prevenirse: la conducta insospechada del viento solar y sus imprevistos ataques mortales.

Naturalmente, es prudente saber muchos más detalles con respecto al Sol. Un descubrimiento importante que puede enseñarnos mucho se refiere a una diminuta partícula llamada «neutrino». Las reacciones de fusión que tienen lugar en el centro del Sol liberan tales partículas como reacción ordinaria.

La radiación normal tarda tanto en llegar a la superficie del Sol y sufre tantos cambios en este proceso, que lo que vemos del mismo sólo nos proporciona datos con respecto a la superficie del globo y nada sobre su interior, salvo lo que podemos deducir indirectamente. Sin embargo, los neutrinos son tan minúsculos y tan indiferentes a la materia ordinaria, que surgen del centro del Sol a la velocidad de la luz sin verse afectados en absoluto por la restante materia solar. Llegan a nosotros ocho minutos después de haberse formado, procedentes directamente del centro del Sol.

Los científicos se hallan ahora inventando unos «telescopios de neutrinos», que pueden consistir, por ejemplo, en grandes depósitos de ciertos productos químicos, capaces de detener unos cuantos neutrinos surgidos del Sol. Por el número de los detenidos y otras informaciones obtenidas gracias a ellos, será posible deducir la temperatura y otras condiciones existentes, en el centro del Sol, con una certeza muy superior a la actual.

Quedando expuesto a nuestro estudio el centro solar, gran parte de lo que hoy en día es misterioso dejará de serlo. Las manchas solares, el viento solar, los destellos, las prominencias, todo quedará registrado en detalle y, tal vez, por anticipado. Con este nuevo conocimiento, podremos avanzar con mayor seguridad por las profundidades espaciales, tal como la brújula guiaba antaño a los exploradores europeos por los terribles peligros del océano ignoto.

16. Las insólitas estrellas

Existe una anécdota muy popular sobre el joven de ojos saltones que asistió a una conferencia astronómica y que después comentó:

–Comprendo bien cómo descubren los astrónomos la distancia a que están las estrellas y sus grados de temperatura. Lo que no entiendo, sin embargo, es cómo averiguan sus nombres.

En realidad, muy pocas estrellas tienen un nombre. La mayoría se conocen por su número de catálogo, y en vez de nombre ostentan una serie de números.

Incluso la mayor parte de las que pueden apreciarse a simple vista se conocen por una letra griega aplicada a la constelación de la que forman parte. La estrella más próxima, Alfa del Centauro, se llama asi por ser la más brillante de la constelación del Centauro, por lo que mereció ser designada con la letra Alfa, primera del alfabeto griego. También existen, naturalmente. Beta del Centauro, Gamma, etcétera.

Sin embargo, hay unas doscientas estrellas con nombre propio, con un nombre real, un nombre agradable, pero para el público en general apenas se ha popularizado una media docena de tales nombres. Es una lástima, porque con toda seguridad sería grato poder hablar de una estrella llamada Ruchbah, y de otra conocida como Benetnasch. Los cuales son ciertamente nombres de estrellas.

Incluso las más conocidas, aquellas cuyo nombre conocen hasta los menos aficionados a la Astronomía, consiguen una nueva vitalidad si se consideran sus nombres.

La estrella más brillante del firmamento y la que ostenta el nombre más conocido es Sirio. Se halla en la constelación Canis Major (Can Mayor, ya que los nombres oficiales de todas las constelaciones están en latín), y a veces se la llama la «Estrella del Can», por este motivo.

Como posee tanto brillo, los antiguos suponían que su calor se añadía al del Sol, cuando salía a mediados de verano. A esta parte del año todavía se la llama «días del perro», y el nombre Sirio debió su origen a ello, ya que procede de una palabra griega que significa «despellejar».

(Incidentalmente, Sirio, la Estrella del Perro, tiene otra compañera extremadamente pequeña, con un diámetro igual a la cuarta parte del de nuestra diminuta Tierra. A veces, a esta compañera se la conoce con el nombre de «la Cachorra».)

Una estrella muy brillante, situada al oeste de Sirio, pertenece a Canis Minor (Can Menor). Como se halla al oeste de Sirio, repito, naturalmente sale y se pone un poco antes que aquélla. Esta estrella, que sale antes que Sirio, se llama Procyon, cuyo significado griego es «antes que el Perro».

Cerca de las dos constelaciones Ursa Major (Osa Mayor) y Ursa Minor (Osa Menor), se halla la constelación de Boötes (Pastor). Los antiguos representaban a esta constelación como a un hombre sosteniendo dos perros entraillados. Los perros eran representados por estrellas de una pequeña constelación situada entre Boötes y Ursa Major, la constelación Canes Venatici (Los Perros de Caza).

Boötes y los perros protegen obviamente al resto del cielo contra las feroces osas. En consecuencia, se llamó Arcturus a la estrella más espléndida de Boötes, que en griego significa «guardián de los osos».

Los antiguos se tomaban con gran seriedad los dibujos que ellos mismos trazaban. Por ejemplo, la constelación Auriga (Cochero), la trazaron como un viejo empuñando una brida con una mano y una cabra y sus crías sostenidas con la otra. Las estrellas que se hallan a un lado de la constelación se llaman, en consecuencia, «Las cabrillas», y a la más brillante entre ellas (y de toda la constelación), Capella, que en latín significa «cabra pequeña». Por este motivo, a la Capella suele llamársele Estrella de la Cabra.

La constelación Virgo (Virgen) se representa como una joven con unas espigas en la mano. Presumiblemente, se debe a que el Sol entra en Virgo a principios de otoño, cuando el grano está maduro y dispuesto para su cosecha. La estrella de dichas espigas es Spica, que en latín significa precisamente «espiga».

A veces, los nombres dependen menos del dibujo de las constelaciones. La de Géminis (Gemelos) contiene dos estrellas muy brillantes, y muy poco separadas en el espacio (lo cual probablemente inspiró el nombre de la constelación). Los romanos les dieron los nombres de los famosos mellizos de su mitología. Castor y Pólux.

Régulus es la estrella más brillante de Leo (León) y procede del latín, por «reyezuelo», apropiado como ornamento principal del «rey de los animales salvajes». Ares es el dios romano al que los griegos llamaban Marte. Y Antares es una estrella que rivaliza con Marte en el color rojizo.

El nombre más apropiado de todos es el de la Polar, la estrella que señala el norte celeste, por lo que también se la llama la Estrella del Norte, o Estrella Polar.

En cambio, hay nombres totalmente inadecuados. La constelación de Orión («Cazador») se describe como un gigante sosteniendo su mano izquierda hacia arriba para detener al feroz Taurus («Toro»), mientras se dispone a golpearlo con el palo que tiene en la mano derecha. Bellatrix es la estrella de su hombro izquierdo y su nombre latino se refiere a «guerrero femenino», el cual no creo que le guste mucho a Orión.

Sin embargo, la mayoría de nombres estelares no son griegos ni latinos, sino árabes (de aquí tantas estrellas que empiezan con la partícula Al, (el artículo él árabe).

Consideremos, por ejemplo, las siete estrellas del Carro Mayor. Todo el mundo las ha visto; se trata de un grupo de estrellas que todo el mundo del hemisferio Norte puede señalar, aunque se ignore todo lo referente a las demás estrellas. Pero, ¿cuáles son sus nombres?

Sí, muchos denominamos a las dos estrellas que están al final del grupo, y forman como una línea apuntada a la Estrella Polar, «los Punteros», pero, ¿cuáles son sus nombres verdaderos?

Bien, éstos son sus nombres, empezando desde el extremo del manillar del carro y terminando con los Punteros: Alkaid, Mizar, Alioth, Megrez, Phecda, Merak y Dubhe.

La primera de la lista, Alkaid, tiene un nombre que suena a tableta antiácida, pero es árabe (como todos los demás), y significa «jefe», puesto que es la primera de las siete estrellas.

La segunda, Mizar, significa «velo». Este nombre tiene una historia propia. Cerca de Mizar se halla una estrella mucho más débil. Si ésta estuviera sola podría ser contemplada a simple vista, mas la presencia de otra más brillante la oscurece. Para distinguir la más débil de ambas estrellas hay que poseer una vista muy penetrante, y durante siglos este par de estrellas ha servido para distinguir una buena de una mala vista. La estrella más débil es Alcor, que en árabe significa «débil».

El nombre de la tercera estrella del Carro Mayor, Alioth, significa en árabe la gruesa cola de una oveja. Si esto parece raro, hay que recordar que los griegos pintaron al Carro Mayor como la Osa Mayor, de modo que las cuatro estrellas que forman el fondo del Carro eran sus cuartos traseros, en tanto las tres del manillar o pescante del Carro eran una cola. Naturalmente, ya sabemos que las osas carecen de cola, o casi, y los griegos también debían saberlo. Debieron, no obstante, heredar el dibujo de los babilonios, y a pesar de las colas, llamaron osas a esas constelaciones. Los árabes continuaron con las colas, y como carecían de palabra para expresar la cola de una osa, llamaron a la estrella con el nombre aplicado a la cola de una oveja.

La cuarta estrella, que inicia el Carro, es Megrez, que significa «raíz», presumiblemente por ser la raíz de la cola.

No he hallado el significado de Phecda, pero en cuanto a los Punteros, Merak, la más alejada de la Polar, significa «ingle», pues está situada en la ingle de la osa, mientras que Dubhe quiere decir «oso».

De modo semejante, las cuatro estrellas del famoso «Cuadro de Pegaso» (Pegaso es el «Centauro» o caballo volador), tienen los nombres árabes de Alpheratz, Algenib, Markab y Scheat. Alpheratz, en el flanco del caballo, significa «la yegua»; Algenib, más arriba, es «el costado»; Markab, aún más alto, es «la silla». Scheat, justo encima de una pata delantera, no es un nombre aclarado. Puede derivarse del término árabe «buena suerte», mas no veo razón para ello.

Varios nombres de la estrella más familiar también son de origen árabe. La segunda estrella más brillante de Orión, la de la pierna izquierda del cazador, es Rigel, que en árabe significa «pie». Betelgeuse, la estrella más resplandeciente de la constelación, está en el brazo derecho del cazador, y es la corrupción de un término árabe que significa «brazo de Orión».

Otras estrellas también ostentan nombres derivados de las constelaciones a que pertenecen. Altair, la más brillante de la constelación Aquila. («Águila»), significa en árabe «pájaro». La constelación Piscis («Peces»), se representa por dos peces unidos por una larga cuerda. En el centro de la misma se halla la estrella más refulgente de la constelación a la que los astrónomos árabes dieron el nombre de Al Richa («la cuerda»).

La estrella más brillante de la constelación Cignus («Cisne») es Deneb. Está situada en la parte posterior del Cisne, según se representa usualmente, y procede de una palabra árabe que significa «cola». Era un nombre favorito entre los astrónomos árabes (los mejores estudiantes del cielo en la Edad Media, por cuyo motivo tantas estrellas llevan nombres árabes), de modo que en el firmamento hay varias estrellas con el mismo nombre de Deneb.

Los árabes hicieron distinciones entre ellas, añadiendo una segunda palabra. Esto persiste en varios casos. Por ejemplo, Deneb Albedi, de Capricornio («Cabra»), significa «cola de la cabra», y Deneb Kaitos, de Cetus («Ballena»), significa «cola de ballena». La segunda estrella más brillante de Leo es Denebola, donde el sufijo «ola» es lo que queda de la expresión árabe que significa «del león».

Por otra parte, para demostrar que los árabes no se limitaban a un extremo del ser, la estrella más brillante de Píscís Australis («el Pez del Sur»), es Formalhaut, frase que en árabe significa «boca del pez». Igualmente, la estrella más resplandeciente de Ophiucus («Sostenedor de la Serpiente»), que naturalmente representa a un hombre que sostiene una serpiente, es Rasalhague, que en árabe significa «cabeza del encantador de serpientes».

Aldebarán, la estrella más brillante de Taurus, es una especie de Procyon al revés. Aldebarán se halla algo al este del bien conocido grupo de estrellas llamado las Pléyades, y en consecuencia las sigue al salir y al ponerse. El nombre de esa estrella, en árabe, significa «el seguidor».

Tal vez el nombre más colorista, aplicado por los árabes a una estrella, sea el que ostenta la segunda más brillante de la constelación de Perseo. Es una de las pocas estrellas del firmamento que cambia visiblemente de brillo con regularidad. Esto era muy sorprendente para los antiguos, que, por lo general, creían que las estrellas eran perfectas e inmutables. El caso de esta estrella pudo, por tanto, guiar el trazado de la constelación. El dibujo muestra a Perseo sosteniendo la cabeza cortada de Medusa, una horrible diablesa a la que él mató, una diablesa tan horrible, con un rostro odioso y serpientes vivas en lugar de cabellos, que convertía a los hombres en piedra cuando la miraban.

La estrella en cuestión está en la cabeza de Medusa, y los árabes le aplicaron el nombre de Algol, que significa «diablo». En consecuencia, a Algol se la conoce como la «Estrella del Demonio».

Todo esto da una leve idea de la riqueza del firmamento. Entre los doscientos nombres, aproximadamente, que no he mencionado, hay ejemplos tan genuinos como Tazared, Pherkad, Mesartim, Kochab, Izar, Caph, Dschubba y Azelfafage.

17. Medición del espacio

La Humanidad, por su propia conveniencia, utiliza diferentes unidades de acuerdo con sus necesidades. Así, la longitud de una habitación se hace por metros o palmos, una pista de carreras suele medirse por yardas o por metros y un trayecto automovilístimo por kilómetros en algunos países y por millas en otros.

Esto se hace, principalmente, para que las cifras no crezcan desmesuradamente. Sería ridículo decir que una habitación mide 0,004 kilómetros de longitud, en lugar de cuatro metros; o que la distancia desde Bostón a Nueva York es de 1.500.000 palmos, y no de 370 kilómetros.

Sin embargo, ninguna de las unidades de medición inventadas para su empleo en la superficie de la Tierra resulta conveniente para la medición de las distancias astronómicas. La mayor unidad de longitud común en la Tierra es la milla, o el kilómetro en los países que utilizan el sistema métrico decimal. Pero ambas unidades son tremendamente cortas para los astrónomos (una milla equivale a 1,609 kilómetros).

El objeto más cercano a nosotros del espacio es la Luna, y el segundo en tamaño. Venus. Pero la distancia de la Tierra a la Luna, expresada en nuestras unidades terrestres, es de 237.000 millas (380.000 kilómetros), en tanto que Venus se halla a 25.000.000 de millas (40.000.000 de kilómetros) cuando está más cerca.

Para impedir que estas cifras astronómicas sumen millones, lo cual resultaría muy molesto, más aún cuando se tratase de billones y trillones, los astrónomos han inventado unas unidades de medición, inútiles en la Tierra, aunque, en esta era espacial en que vivimos, cada vez se escuchan con más frecuencia. Por esto, hemos de aprender a entender este código de distancias astronómicas.

Por ejemplo, los astrónomos utilizan la distancia entre la Tierra y el Sol como una «yarda espacial». Ésta varía en varios millones de millas según la posición exacta de la Tierra en su órbita elíptica, pero la distancia media es de 92.870.000 millas (149.450.000 kilómetros).

Los astrónomos llaman a esta vara de medición la Unidad Astronómica, abreviadamente U. A. De esta forma, es posible decir que la distancia media entre la Tierra y el Sol es de 1 U. A. La ventaja de este sistema es poder medir distancias astronómicas en U. A., resultando de esta manera unas cifras más al alcance de todos.

Por ejemplo, la distancia media de la Tierra a la Luna es 0,00255 U. A., mientras que Venus se halla a 0,27 U. A. de la Tierra. Con lo cual sabemos al momento que la distancia lunar es 1/400 de la del Sol, y que Venus se halla a 1/4 de la distancia solar.

TABLA 1

La Tabla 1 presenta las distancias medias de los diversos planetas al Sol en millas, kilómetros y U. A. No solamente los números de la columna U.A. son de más fácil manejo, sino que también dejan comprender claramente y con suma rapidez la relación existente en los diversos valores, puesto que la columna de las millas y la de los kilómetros no puede leerse tan fácilmente.

Si uno oye decir que Neptuno se encuentra a 2.800.000.000 millas del Sol, sólo tenemos un número confuso. Si, por otra parte, nos dicen que esa distancia se de 30,07 U.A. sabemos al instante que Neptuno se halla 30 veces más lejos del Sol que la Tierra.

Con las cifras U.A. es posible afirmar a simple vista que Saturno se encuentra dos veces más lejos del Sol que Júpiter, y que Plutón está (por término medio) dos veces tan lejos del Sol como Urano. La misma información la podemos conseguir en las columnas de las millas y kilómetros, pero unas cifras tan desmesuradas confunden con más facilidad.

Una unidad de suma importancia para los astrónomos es la relacionada con la velocidad de la luz.

En un segundo, la luz (o cualquier otra forma de radiación electromagnética como las ondas de radio) viaja a 186.282 millas, aproximadamente 300.000 kilómetros por segundo. En lugar de llamar a esto «la distancia a que viaja la luz en un segundo», decimos simplemente «segundo-luz». De esta manera resulta mucho más fácildecir que la distancia de la Luna a la Tierra es de 1,27 segundos-luz, o que Venus está a unos 13,5 segundos-luz de la Tierra.

De esta manera, al establecer contacto por radio con una expedidión a la Luna, nuestras señales tardar en llegar hasta allí 1,27 segundos. Una señal de radar, saliendo de Venus a su más próxima distancia que es de 13,5 segundos-luz, tardaría 27 segundos en el viaje de ida y vuelta. Las distancias medidas con estas unidades encajan mejor dentro de las comunicaciones por radio.

El sistema solar puede medirse en segundos-luz, pero esto resulta menos claro que utilizando las Unidades Astronómicas o U.A. Un U.A. es igual a unos 500 segundos-luz. En consecuencia, la distancia media del Sol a Neptuno, que es de unos 30 U.A., resulta ser de 15.000 segundos-luz. Esta última cifra es mayor y, por tanto, menos conveniente.

Sin embargo, el uso de la velocidad de la luz como unidad de medición no se limita a los segundos-luz. Existe también la distancia que la luz recorre en un minuto, o en una hora (minuto-luz y hora-luz). Naturalmente, un minuto-luz es igual a 60 segundos-luz, mientras que una hora-luz es lo mismo que 60 minutos-luz o 360 segundos-luz.

TABLA 2

En la Tabla 2, las distancias medias de los planetas al Sol se expresan en minutos-luz y horas-luz. En realidad, la primera es una unidad de medición muy comveniente para distancias planetarias como la de la órbita de Júpiter, mientras que la unidad de horas-luz es más apropiada para los planetas situados más allá de Júpiter.

La anchura del sistema solar desde un extremo de la órbita de Plutón al otro es de unas 11 horas-luz, o casi medio día-luz. Fuera de esta zona no hay nada, según nuestros conocimientos (salvo los insubstanciales fantasmas que llamamos cometas, y tal vez algunos meteoritos errantes), hasta llegar a las estrellas.

Un gráfico de nuestra familia de planetas se forma de este modo en nuestras mentes. La luz, que puede llegar de la Tierra a la Luna en 1 ¼ segundos y llegar a nosotros, procedente del Sol, aproximadamente en ocho minutos, debe viajar durante once horas a fin de llegar hasta la órbita de Plutón.

Sin embargo, el sistema solar no es más que una mota de polvo en la enormidad del espacio, y los astrónomos sondean sus límites con sus telescopios. Por fortuna, la velocidad de la luz sigue ofreciéndoles una continua sucesión de unidades de medición. Pero al suponer las semanas-luz y los meses-luz como unidades convenientes, nos equivocamos.

Esto se debe a que las ondas luminosas, después de pasar la órbita de Plutón, pueden viajar semanas y meses en la misma dirección sin hallar materia en el vacío del espacio exterior.

No existe ningún objeto conocido en dicho espacio exterior cuya distancia al Sol pueda medirse adecuadamente en términos de semanas-luz o meses-luz.

Cuando se miden distancias estelares hay que emplear el año-luz, o sea, 300.000 kilómetros multiplicados por el número de segundos (más de un millón) que tiene un año.

Esta distancia es muy larga: 5.890.000.000.000 millas o 9.450.000.000.000 kilómetros. Hablando en general, un año-luz equivale casi a seis billones de millas o diez billones de kilómetros.

A pesar de ser ésta una distancia enorme, no existe ningún cuerpo fuera del sistema solar que se halle a un año-luz de nosotros. La estrella más cercana, Alfa del Centauro, se halla a 4,3 años-luz.

Otra unidad de medición para distancias estelares no se basa en la velocidad de la luz. Esta nueva unidad se refiere al aparente cambio de posición de las estrellas más próximas contra el fondo de las más lejanas. Este cambio se produce cuando la Tierra viaja de un punto de su órbita al extremo opuesto, lo que tiene lugar seis meses más tarde. La mitad de este cambio aparente en la posición de una estrella se llama «paralaje estelar».

Es posible observar un paralaje tosco manteniendo el índice a unos quince centímetros de la nariz y mirar hacia algún objeto distante con sólo un ojo abierto. Sin mover el dedo, miradlo con el otro ojo. El dedo cambia de posición contra el fondo porque el individuo ha cambiado el punto de vista de un ojo a otro.

Cuanto más distante está el objeto de referencia, menor es el paralaje. Extended el índice a la distancia del brazo y veréis que cambia mucho menos de posición respecto al fondo. Por esta razón, es posible calcular la distancia de un objeto celeste a la Tierra, gracias a su paralaje. Este método se emplea para calcular la distancia de las estrellas más cercanas, desde hace más de un siglo, pero es una tarea muy engorrosa, puesto que hasta las estrellas más próximas tienen un paralaje muy pequeño.

Imaginemos un paralaje de un segundo de arco (que es 1/60 de un minuto de arco, que, a su vez, es 1/60 de un grado de arco, de los 360 grados que tiene la circunferencia). Un segundo de arco es igual al diámetro aparente de una moneda sostenida a la distancia de cuatro kilómetros, o sea excesivamente pequeño. Una estrella con este paralaje se halla a la distancia de un «parsec». (La palabra «parsec» es una contracción de «paralaje-segundo».) Pero incluso un paralaje diminuto es demasiado grande.

No hay ningún objeto conocido fuera del sistema solar que se halle sólo a un parsec de distancia. En consecuencia, ninguna estrella conocida posee un paralaje que tenga un segundo de arco. La estrella más próxima. Alfa del Centauro, tiene un paralaje que vale 0,76 segundos de arco.

Un parsec equivale a 3,26 años-luz. Así, Alfa del Centauro está a 4,3 años-luz de distancia, o sea, a 1,3 parsecs, dividiendo 4,3 por 3,26.

TABLA 3

En la Tabla 3, se expresa la distancia de algunas de las estrellas más conocidas en años-luz y parsecs.

Aunque quepa pensar que los astrónomos poseen ya todas las unidades de medición necesarias para su labor, las estrellas enumeradas en la Tabla 3 pertenecen todas a la vecindad más inmediata a nosotros, siendo sólo un pequeño sector de un brazo en espiral de nuestra galaxia.

Toda la Vía Láctea es mucho mayor que el pequeño volumen de soles que podemos observar a simple vista, incluyendo las estrellas cuyo paralaje aquí se expresa.

El núcleo, o centro, de nuestra galaxia, que contiene el 90 por ciento de todas sus estrellas (y que ni siquiera podemos ver con un telescopio debido a las masas de polvo interestelar interpuestas entre ellas y nosotros), se halla a más de 30.000 años-luz del sistema solar. En realidad, el diámetro de nuestra galaxia, que tiene forma de disco, es de unos 100.000 años-luz, mientras que su mayor grosor (en el centro) es de unos 30.000 años-luz. Estas cifras resultan también muy elevadas.

Una de las maneras de impedir este crecimiento numérico es emplear el siglo-luz (100 años-luz) y el milenio-luz (1.000 años-luz o 10 siglos-luz). De esta forma podemos decir que Deneb se halla a 4 siglos-luz de nosotros y que las medidas de nuestra galaxia son 100 milenios-luz de diámetro por 30 milenios-luz de grosor.

En realidad, estas unidades pocas veces se emplean. Los astrónomos tienden a expresarse en parsecs para distancias excesivas. Así como en el sistema métrico decimal un kilómetro equivale a 1.000 metros y un kilogramo a 1.000 gramos, los astrónomos han inventado el «kiloparsec», igual a 1.000 parsecs. Utilizando esta compacta unidad de medición, podemos decir que las medidas de nuestra galaxia son de 31 kiloparsecs de diámetro por 9 kiloparsecs de grosor.

Sin embargo, nuestra galaxia no es más que un punto perdido en la inmensidad del universo, lleno de otras galaxias, miríadas de galaxias, en realidad. En las galaxias más cercanas a la nuestra, a la Vía Láctea, se hallan las relativamente pequeñas «galaxias satélites», llamadas la Gran Nube de Magallanes y la Pequeña Nube de Magallanes. Se hallan respectivamente a 150.000 y 170.000 años-luz de distancia, o sea, a 47 y 53 kiloparsecs.

La galaxia de gran tamaño más cercana a la nuestra es Andrómeda, que está a 2.300.000 años-luz, o 700 kiloparsecs. Otras galaxias (incluyendo, por ejemplo, un famoso grupo galáctico en la constelación de Coma Berenice, y una galaxia espectacular en el Cisne, que se cree son dos galaxias en colisión), se hallan mucho más distantes. Incluso el kiloparsec resulta demasiado pequeño para medir tales distancias.

En cambio, podemos utilizar el «megaparsec», equivalente a un millón de parsecs, o mil kiloparsecs (o a 3.260.000 años-luz). Usando esta unidad, este grupo de galaxias de Coma Berenice se halla a 25 megaparsecs de distancia. Las galaxias en colisión del Cisne están a 80 megaparsecs de nosotros.

¿Tenemos ya todas las medidas espaciales necesarias? No, en absoluto. En 1963, los astrónomos comprendieron que existían objetos en el universo que se hallaban mucho más lejos que las galaxias más apartadas. Estos objetos nuevos, los más distantes conocidos, se llaman quasars (ver Capítulo 19).

El quasar más distante detectado por nosotros se llama 3C9, que se halla a la posible distancia de 9.000 millones de años-luz. Esta distancia equivale a 2.800 megaparsecs.

Demos un paso adelante e introduzcamos el «gigaparsec», igual a mil millones de parsecs o a mil megaparsecs. Entonces, diremos que el 3C9 está a 2,8 gigaparsecs de nosotros.

En realidad, los astrónomos tienen motivos para pensar que la máxima distancia a que podemos llegar con nuestros mejores instrumentos es de 25.000 millones de años-luz. En cuyo caso, la anchura de todo el universo alcanzable con nuestros instrumentos más perfeccionados, en las mejores circunstancias, es de 25.000 millones de años-luz, o sea, unos 7,5 gigaparsecs.

Y, naturalmente, no es preciso ahondar más.

18. Viajar en el tiempo: en unsentido

En 1905, Albert Einstein adelantó un nuevo modo de considerar el universo, que pareció trascender y subvertir el «sentido común». En efecto, aquella teoría parecía rara, pues en ella los objetos cambiaban al moverse, tornándose más cortos y más densos. En esta nueva teoría, una persona vería, mediría y juraría lo que otras no. En esta teoría, todas nuestras creencias parecían disolverse.

El único consuelo para el hombre medio en esta teoría era que, en circunstancias ordinarias, los cambios eran tan mínimos que pasaban inadvertidos.

Por ejemplo, empecemos por construir un tren de mercancías imaginario que, estando parado, tiene exactamente una milla de longitud y una masa de un millón exacto de toneladas. Si pasa ante nosotros a la velocidad de sesenta millas por hora, y en el caso de que pudiésemos efectuar las medidas necesarias con toda precisión, hallaríamos que el tren se ha acortado en diez millonésimas de pulgada, y se ha hecho más macizo en cien milésimas de onza.

Una persona que, no obstante, estuviese en el tren de mercancías, al efectuar las mismas mediciones, vería que la longitud y la masa del tren no han variado. Para él, el tren mediría exactamente una milla de largo y pesaría un millón de toneladas. En cuanto al hombre del tren, éste nos vería a nosotros, los observadores del tren en marcha, ligeramente distorsionados en forma y masa.

Pero, ¿a quién diablos le importa discutir por unas mil millonésimas de pulgada o de onza? En efecto, por una teoría tan complicada del universo, relativa a unos cambios tan insignificantes, no vale la pena molestarse, al parecer, en considerarla.

Pero estos cambios no son siempre tan insignificantes. Unos años antes de que Einstein formulase su teoría, se descubrió que los átomos radiactivos disparan al exterior unas minúsculas partículas subatómicas que viajan a velocidades muy superiores a la de nuestro imaginario tren de carga. Las velocidades de esas partículas subatómicas son de 16.000 a 297.000 kilómetros por segundo. Para ellas, la longitud y la masa cambian radicalmente; cambian lo bastante como para que dichos cambios sean observados; cambian lo bastante como para que resulte imposible ignorar la variación de la masa. Por tanto, la antigua noción de un universo en el que la longitud y la masa no quedaban afectados por el movimiento ya ha sido abandonada. En cambio, se ha adoptado la teoría de Einstein.

Naturalmente, si imaginamos trenes de carga, o cualquier tren, con velocidades tan grandes como para que sus cambios de longitud y masa resulten visibles, escaparán instantáneamente a la tracción del campo gravitatorio de la Tierra. Por este mismo método, nosotros nos hallaríamos en el espacio exterior, y puesto que es así, imaginemos que ya estamos allí.

Imaginemos que nos hallamos en una nave espacial llamada A, de 1.000 metros de longitud y una masa de 1.000 toneladas. Frente a nosotros se halla otra nave espacial B, de igual masa y longitud, que pasa a 260.000 kilómetros por segundo.

Cuando B pasa ante nosotros, si empleamos algún instrumento muy sofisticado para medir su longitud y su masa, comprobaremos que sólo mide 500 metros de largo y su masa es de 2.000 toneladas. En otras palabras, su longitud es la mitad que antes y su masa se ha duplicado.

Al momento, radiamos a B y les informamos de esta anomalía, pero B nos comunica que, según sus propias mediciones, su nave no ha cambiado en absoluto. En cambio, al pasar frente a nosotros, han tomado las medidas de nuestra nave, la A, y ésta sí que sólo mide 500 metros de longitud y su masa es de 2.000 toneladas.

Después, las naves cambian de rumbo, se acercan y se detienen lado a lado. Se toman las correspondientes medidas, y las dos naves resultan normales. Las dos tienen 1.000 metros de longitud y 1.000 toneladas de masa. ¿Qué serie de mediciones son las correctas? Todas. Recordemos que las medidas cambian con el movimiento.

Para la dotación de la nave A, la nave B pasaba a la velocidad de 260.000 kilómetros por segundo; y para B, A iba a la misma velocidad en dirección contraria. Cada dotación veía cómo la nave contraria se movía a esta velocidad, y sus mediciones daban la mitad de la longitud normal y el doble de la masa. Una vez puestas una al lado de otra, las naves no se movían y las medidas volvían a ser «normales».

Si preguntamos: ¿se acortaban o no las naves en realidad?, hemos de considerar que al tomar una medida no estamos comprobando necesariamente la «realidad». Estamos leyendo solamente la situación de un objeto, situación que varía en condiciones distintas.

La teoría de Einstein se refiere a algo más que a longitud y masa; se refiere también al tiempo. Según él, en un objeto en movimiento todo se retarda. El péndulo de un reloj en movimiento se mueve más lentamente; el muelle de un reloj de pulsera pulsa de forma más lenta. Todos los movimientos se retrasan.

Y nosotros medimos este movimiento periódico al medir el tiempo, una vibración, una pulsación, un latido regulares. Si todos los movimientos con los que medímos el tiempo se retrasan, tenemos razón al afirmar que el tiempo en sí también se retrasa.

Para algunas personas esto resultará difícil de digerir. Más aún que los cambios de masa y longitud. Al fin y al cabo, ambas pueden cambiar de muchas formas. Podemos acortar un objeto martilleándolo; podemos aligerarlo extrayendo agua de su contenido total, mediante la evaporación. Pero no conocemos la forma de lograr que cambie la velocidad a que se mueve el tiempo. Damos por sentado que la velocidad del tiempo es inmutable; algo que, por encima de todo, permanece inalterable.

Y sin embargo, la teoría de Einstein postula el cambio de velocidad del tiempo con el movimiento, cambio que ya ha sido medido. Incluso con velocidades de unos centímetros por segundo, un fenómeno físico llamado efecto Mossbauer (por el nombre de su descubridor), nos permite medir los increíblemente minúsculos cambios en la velocidad del tiempo. Aquí también, las partículas subatómicas nos ofrecen velocidades bastante grandes para que el cambio resulte conmesurable y significativo.

Hay una partícula llamada «mesón mu» que dura dos microsegundos (un microsegundo es una millonésima de segundo) antes de descomponerse. Al menos, dura dos microsegundos si se mueve a velocidades moderadas. A veces, no obstante, se forma un mesón mu en lo alto de la atmósfera debido a los rayos cósmicos y, en el choque de su creación, desciende a la Tierra a una velocidad de más de 290.000 kilómetros por segundo.

Si el mesón mu tuviese una existencia de dos microsegundos a esta velocidad, tendría tiempo de moverse sólo 580 metros. Como se forma a muchos kilómetros en lo alto de la atmósfera, no debería llegar jamás hasta la superficie de la Tierra.

Pero llega. Un mesón mu realmente veloz puede viajar cinco kilómetros o incluso más antes de descomponerse.

Esto se explica suponiendo que para él el tiempo se retrasa. Sigue viviendo dos microsegundos según su propia observación (si la tuviese), pero se trata ahora (según el observador terrestre) de dos microsegundos lentos, que exceden de veinte microsegundos ordinarios.

Este cambio en la velocidad del tiempo del mesón mu encaja exactamente con la predicción de Einstein, por lo que debemos de aceptar que el tiempo no es inmutable sino algo que posee unas propiedades que dependen de nuestro propio punto de vista.

Volvamos a las naves A y B. Supongamos una vez más que B adelanta a A, e imaginemos asimismo que a bordo de A hay un instrumento capaz de permitir a su dotación que observe un reloj colocado en B, exactamente durante una hora, según un reloj de A.

El reloj de B parecerá retrasarse ante los ojos de la dotación de A, porque B se mueve. Al cabo de una hora, según la medición del tiempo efectuada por el reloj de A, el que está a bordo de la nave B habrá medido algo menos de una hora. Cuanto más de prisa viaje B, más lenta será la velocidad de su tiempo, y menos tiempo medirá por tanto su reloj.

Hay una fórmula que se emplea para averiguar la velocidad del tiempo con el movimiento, fórmula que da la tabla siguiente.

¿Qué ocurre si B viaja frente a A a una velocidad superior a 300.000 kilómetros por segundo? ¿Registra su reloj el tiempo al revés? ¿Va hacia atrás? ¡No! Podemos eludir la posibilidad de que el tiempo vaya hacia atrás, ya que 300.000 kilómetros por segundo es la velocidad máxima capaz de ser alcanzada. Es la velocidad de la luz en el vacío y, según la teoría de Einstein, esta velocidad relativa no puede ser superada por ningún objeto material.

Pero hay algo que no debemos olvidar. La dotación de A observa cómo la nave B pasa veloz ante su radio visual, mas la tripulación de B ve pasar a la nave A como un rayo en dirección contraria. Para cada una de las dotaciones, es la otra nave la que se mueve. Por tanto, si la tripulación de B midiese el reloj que hay a bordo de A, hallaría que es este reloj, el de A, el que se ha retrasado.

Esto es grave, mucho más grave que el desacuerdo entre las longitudes y las masas, estudiado antes. Si dos naves estuviesen juntas en reposo después de haber cruzado una frente a otra en un experimento masa-longitud, sus dotaciones discutirían de esta forma:

–Cuando habéis pasado frente a nosotros, vuestra nave era más pequeña y menos pesada que la nuestra.

–¡Oh. no!, era la vuestra más corta y menos pesada que la nuestra.

–i No, no…!

Esta argumentación no tendría fin. Si un objeto se encoge a la mitad de su longitud y después retorna a su tamaño normal, o duplica su masa y luego recobra su peso adecuado, este experimento no deja huellas. No queda ningún rastro que demuestre que el encogimiento ha sido o no temporal. Por tanto, las discusiones al respecto son fútiles e innecesarias.

Pero si el reloj de una nave A corre más despacio que el de otra nave B, estando las dos naves juntas, los relojes presentarán las señales de tal retraso. Si se sincronizan ambos relojes al comienzo del experimento, al final ya no estarán sincronizados.

Un reloj, por ejemplo, debido a su retraso en la velocidad del tiempo, pierde el total de una hora. Por tanto, cuando se acercan las naves otra vez, un reloj marca las 2,15 y el otro las 3,15.

Pero ¿qué reloj marca el tiempo real? La tripulación de la A jura que es el reloj de la B el que se atrasa, mientras que la dotación de la B afirma lo contrario. Y como las dos dotaciones no pueden tener razón, éste parece un problema insoluble, que comúnmente se llama la «paradoja del reloj».

En realidad, no existe paradoja alguna. Si una nave pasó frente a otra y ambas tripulaciones aseguran que el reloj de la otra se atrasa, no importa cuál sea el reloj que atrasa, porque ambas han de estar eternamente separadas.

Los dos relojes nunca se hallarán en el mismo lugar ni a la misma hora para ser comparados, por lo que jamás se presentará la paradoja del reloj.

Por otra parte, supongamos que las dos naves se juntasen después del experimento, a fin de comparar los relojes. Para esto, hay que añadir un nuevo detalle. Al menos, una nave tiene que acelerar, es decir, cambiar su velocidad. Si acelera B, dicha nave ha de viajar trazando una gran curva, para volver hacia A, y después moderar la marcha hasta el punto donde pueda situarse inmóvil junto a A.

Esta aceleración estropea la simetría de la situación. B cambia de velocidad, no sólo respecto a A, sino a todo el universo, a todas las galaxias y a todas las estrellas. La tripulación de B insiste en que su nave está inmóvil y que es la A, la nave que se les aproxima, y en tal caso también han de saber que todo el universo cambia de posición respecto a su nave. La tripulación de A, no obstante, sólo observa el cambio de velocidad de B, y el universo no cambia en relación con la velocidad de A.

Como B acelera respecto a todo el universo (no sólo respecto de A), esto hace que el reloj de B se atrase, cosa que todos pueden observar. Cuando se juntan las dos naves, el reloj de B marcará las 2,15, en tanto el de A señalará las 3,15.

Si, por otra parte, B ha seguido viajando sin cambiar de velocidad, mientras que A ha acelerado súbitamente a fin de alcanzar a B, esta aceleración hará posible que todos los observadores afirmen que el atrasado es el reloj de A.

Este efecto, por el que todos los observadores pueden afirmar que el objeto acelerado es el que ha sufrido el atraso en la velocidad del tiempo, se llama dilatación del tiempo, y tiene una gran aplicación en esta era espacial.

La estrella más cercana, repito, es Alfa del Centauro, que está a unos 4,3 años-luz de distancia, o sea a 40.000.000.000.000 kilómetros (ver Capítulo 17). Y como la velocidad de la luz es la velocidad límite del universo, un viaje desde la Tierra a Alfa del Centauro nunca podría tardar menos de 4,3 años.

En realidad, ninguna nave espacial podría alcanzar velocidades ni remotamente parecidas a la de la luz, excepto mediante una larga y gradual aceleración, de modo que durante un periodo de tiempo muy considerable, la nave viajaría a una velocidad muy por debajo de la que es propia de la luz, o sea que tardaría mucho más de 4,3 años en llegar a la estrella Alfa.

Pero gracias a la dilatación del tiempo, no sería así. Supongamos que la nave acelerase a 1 g (aceleración a la que todos los tripulantes experimentarían una sensación de peso dirigido hacia el fondo de la nave, igual a la experimentada en la Tierra). La combinación de la aceleración y la velocidad rápida introduce un retraso en la velocidad del tiempo que todos pueden observar.

Para los moradores terrestres, transcurrirán diez años estando la nave en route, pero para la gente a bordo de la nave, la medición del tiempo con unos relojes que a medida que la velocidad aumentase se atrasarían más cada vez, sólo transcurrirían 3,2 años antes de llegar a Alfa del Centauro.

Si continuaban acelerando y su velocidad se aproximase a la de la luz (aun sin llegar jamás a igualarla), el efecto de la dilatación del tiempo iría siendo mayor. La nave podría cubrir distancias enormes en un tiempo relativamente corto para la tripulación.

Recordemos, no obstante, que el efecto de la dilatación del tiempo sólo tendría lugar en la nave; no en la Tierra, que seguiría a su velocidad acostumbrada, por lo que el tiempo pasaría normalmente.

Esto se demuestra claramente mediante la tabla siguiente, aplicada a una nave que viaje desde la Tierra al espacio exterior a una aceleración continua de 1 g:

De modo que podemos imaginarnos a nuestros astronautas visitando, no sólo otras estrellas, sino otras galaxias, en un viaje cuya duración fuese de un simple cuarto de siglo.

Y este cuarto de siglo no tiene nada que ver con la medición del tiempo hecha por el reloj. No es sólo el reloj ni otros instrumentos de medición del tiempo los que se atrasarían a bordo de la nave, sino todos los movimientos.

Todos los movimientos atómicos, y por tanto, la velocidad de todas las acciones químicas, incluyendo las internas de un astronauta. La química corporal iría a una velocidad mucho menor en sus reacciones. La mente pensaría y experimentaría con más lentitud.

Esto significa que, bajo el efecto de la dilatación del tiempo, en un viaje a la galaxia Andrómeda, los astronautas no sólo verían que transcurrían 28 años, sino que experimentarían el paso de este tiempo. Más aún, sus cuerpos envejecerían 28 años y no más, aunque en el mismo intervalo, transcurrirían dos millones de años en la Tierra.

Además, el efecto de la dilatación del tiempo es algo en lo que todos los observadores pueden estar de acuerdo, de forma que al volver los astronautas a la Tierra, los habitantes de ésta (en el supuesto de que hubiesen sobrevivido a los dos millones de años envejeciendo naturalmente) tendrían que reconocer que dichos astronautas sólo habían envejecido unos treinta años.

Éste es el fundamento de la «paradoja de los gemelos». Supongamos que una persona viaja en una nave espacial que acelera constantemente a gran velocidad, mientras que su hermano gemelo se queda en casa. El viajante gradualmente se atrasa, llega a un alto, acelera y atrasa de nuevo al llegar a la Tierra. Gracias a la dilatación del tiempo, ha envejecido diez años mientras que su hermano sedentario (igual que los demás terrestres) ha envejecido cuarenta años. Cuando el viajero regresa, es treinta años más joven que su hermano gemelo.

En realidad, el viajero no se ha rejuvenecido, no se ha hecho más joven. Es imposible que el tiempo retroceda, por lo que el viajero ha envejecido con menos rapidez que si se hubiese quedado en la Tierra.

Tampoco ha ampliado el viajero su límite existencial. Si tanto él como su sedentario hermano tuviesen que vivir hasta una edad fisiológica de 70 años, el sedentario se moriría, digamos, en el año 2050, mientras que el viajero sobreviviría hasta el 2080. Pero, aunque el viajero fuese testigo de los sucesos de treinta años más que su hermano, no experimentaría en absoluto estos treinta años de más. Mientras viajase, sólo experimentaría diez años, en tanto que su hermano experimentaría cuarenta. Y ambos morirían exactamente a los setenta años de recuerdos.

Aunque el viajero hubiese ido a Andrómeda y vuelto de allí, y hubiese muerto, por tanto, varios millones de años después que su hermano terrestre, sólo experimentaría setenta años de vida y de recuerdos.

Naturalmente, hay experiencias y experiencias. Resulta atractiva la idea de pasar setenta años moviéndose por el espacio en un viaje de ida y vuelta, y llegar a la Tierra al cabo de cincuenta mil años, según la medición del tiempo terrestre. No se trata sólo de la experiencia del viaje espacial sino de lo que, virtualmente, es el viaje en el tiempo. Este astronauta poseería la capacidad de asistir a la historia futura de la Humanidad, como por medio de un telescopio.

Sin embargo, en esto hay un fallo. El viaje en el tiempo, según la paradoja de los gemelos, sólo existe en un sentido: hacia el futuro. Una vez en la ruta de la dilatación del tiempo, no es posible arrepentirse, no es posible volver atrás. El siglo del nacimiento del astronauta ha desaparecido para siempre y jamás ha de volver.

19. Nacimiento y muerte del universo

Pocos científicos saltan a los titulares de los periódicos por haber formulado una teoría, pero Fred Hoyle, el astrónomo inglés, lo consiguió en 1965. Formuló la teoría de la «creación continua», basándose en objetos existentes a siete mil trillones de kilómetros de distancia, y a diez millones de años en el tiempo.

Para llegar a esta teoría hay que recorrer un largo camino, pero esto es necesario para poder establecer el choque más grandioso de las teorías de toda la historia de la Ciencia. Se refiere nada menos que al nacimiento (o no-nacimiento) y a la muerte (o no-muerte) del universo.

Todo empezó hace medio siglo, cuando los astrónomos aún sabían muy poco de lo que ocurría y había fuera de nuestra Vía Láctea, un conglomerado en forma de lente de unos ciento treinta mil millones de estrellas, con un diámetro de cien mil años-luz. En el cielo, en algunos lugares, es posible vislumbrar pequeños grumos de luz neblinosa que, entonces suponían algunos astrónomos, eran otras aglomeraciones de estrellas o galaxias. Podían estar a muchos millones de años-luz de distancia (siendo cada año-luz equivalente a nueve billones de kilómetros).

La luz de estas galaxias, o de cualquier otro objeto celeste luminoso, puede reunirse mediante los telescopios, después diseminarse en un leve arco iris (o «espectro»), cruzado por diversas líneas oscuras. Cada una de estas líneas la origina un producto químico particular, y tiene su lugar especial en el espectro, si la fuente luminosa está estacionada respecto a nosotros. Si la fuente luminosa se aleja de nosotros, dichas líneas cambian de lugar hacia el extremo rojo del espectro; cuanto mayor sea la velocidad de retroceso, mayor es la extensión de este «desplazamiento hacia el rojo». Si la fuente luminosa se acerca a nosotros, las líneas oscuras se aproximan al extremo violeta del espectro.

En 1912, el astrónomo americano Vesto Melvin Slipher, comenzó a recoger luz de diversas galaxias, a fin de medir la naturaleza y extensión del desplazamiento de las líneas oscuras. Esperaba descubrir que, aproximadamente, la mitad de las líneas se agruparían en el extremo rojo, y la otra mitad en el violeta, o sea, que la mitad de los objetos celestes luminosos, en este caso galaxias, se apartaría de nosotros, y la otra mitad se aproximaría.

En realidad, no fue así. Ante la sorpresa de Slipher, sólo unas cuantas galaxias, las más cercanas, presentaron un acercamiento a la luz violeta. Las demás se agruparon en la zona roja del espectro. En 1917, halló dos galaxias que se aproximaban a nosotros y trece que retrocedían.

Más aún, el tamaño del desplazamiento hacia el rojo era excesivamente alto. Las estrellas de nuestra galaxia muestran las líneas oscuras en el espectro rojo, lo que indica que retroceden a menos de 160 kilómetros por segundo, pero Slipher detectó retrocesos galácticos a más de 600 kilómetros por segundo, a juzgar por la magnitud de los desplazamientos hacia el rojo.

Otros continuaron esta labor. Así, por ejemplo, otro astrónomo americano, Milton La Salle Humason, empezó a realizar exposiciones de película fotográfica, noche tras noche, a la luz de galaxias muy débiles, dejando que sus débiles rayos se acumulasen hasta el punto en que un espectro detectable se fijara en la película. De esta forma logró medir los movimientos de algunas galaxias sumamente distantes. Todas las débiles galaxias detectadas mostraron un desplazamiento hacia el rojo, sin ninguna excepción. Y las más débiles (con toda seguridad, las más lejanas), fueron las que mayor desplazamiento hacia el rojo presentaron. En 1936, confirmó velocidades de fuga del orden de los 40.000 kilómetros por segundo.

Ya a finales de la década de los años veinte, el astrónomo americano Edwin Powell Hubble generalizó el tema, desarrollando lo que hoy día se conoce como «Ley de Hubble». La misma establece que las lejanas galaxias retroceden de la nuestra a una velocidad proporcional a su distancia de la Vía Láctea.

Según las actuales teorías, este incremento constante de velocidad de fuga alcanza un valor igual a la velocidad de la luz, a una distancia de 12.500 millones de años-luz. Si una galaxia se aparta de nosotros a la velocidad de la luz, la que ella emita jamás llegará hasta nosotros, lo cual significa que, hagamos lo que hagamos, y por muy perfectos que sean nuestros instrumentos, nunca podremos detectar tal galaxia. No podemos ver su luz, recibir partículas subatómicas suyas, ni siquiera detectar su campo gravitatorio.

La distancia de 12.500 millones de años-luz representa, por consiguiente, el límite del «universo observable». Que haya o no algo más alejado, por el momento, no puede afectarnos en modo alguno.

Entonces, éste es nuestro universo: una gigantesca esfera de espacio, esmaltada de galaxias, estando la nuestra en su centro, y un borde de 12.500 millones de años-luz en todas las direcciones.

Parece raro, no obstante, que nosotros nos hallemos en el centro del universo, y que las demás galaxias se vayan alejando de dicho centro. ¿Por qué nosotros somos tan especiales? Por nada, naturalmente. De haber algo especial, sería mera ilusión.

La teoría de Einstein sobre la relatividad, formulada en 1916, se adecúa con la opinión de que el universo se expande. Como ello es así, las galaxias de su interior se alejan constantemente en un volumen de espacio cada vez mayor. (Las galaxias, unidas entre sí por la fuerza de la gravedad, no se expanden en el interior.) Cada una se va apartando cada vez más de sus vecinas, a medida que el universo se expande.

En este universo, a un observador ha de parecerle, estando situado en una galaxia, que las demás se apartan de él (salvo, posiblemente, las dos o tres más cercanas, que podrían formar parte de un grupo común de galaxias). Más aún, al observador situado en una galaxia cualquiera, le parecería que otras galaxias retroceden a una velocidad proporcional a la distancia.

Por tanto, es posible que el aspecto general del universo continúe igual, sin tener en cuenta la posición en el espacio, a los ojos del espectador. A esto se le denomina «principio cosmológico», siendo «cosmología» el nombre aplicado a la rama de la Ciencia que estudia las propiedades del universo en su conjunto.

Esta expansión podría ser simplemente una propiedad intrínseca del espacio, pero en 1927, un astrónomo belga, Georges Edouard Lemaitre, formuló una explicación física. El universo podría ensancharse debido a los efectos de una explosión colosal que tuvo lugar hace miles de millones de años. Originalmente, sugirió Lemaitre, toda la materia del universo formaba una masa sólida, densa, de materia: el «huevo cósmico». Éste explotó en un cataclismo de proporciones inimaginables y se rompió en miles y millones de pedazos, que ocasionalmente formaron las galaxias actuales. Éstas, en consecuencia, se separan unas de otras desde la primitiva explosión, creando lo que parece un universo en expansión constante.

Desde 1927, otros astrónomos han adoptado esta idea, desarrollando sus consecuencias con gran detalle. Tal vez el mejor propulsor de esta teoría «big-bang», como se la llama popularmente, sea el físico ruso-americano, George Gamow.

La teoría «big-bang» formula un universo que cambia drásticamente con el tiempo. Al principio (hace unos 12.000 millones de años, calculan los astrónomos), el universo era un globo de materia superdensa. Después, llegó la explosión en una serie de fragmentos, muy juntos. Con el tiempo, dichos fragmentos se fueron enfriando, separándose en estrellas y galaxias, y continuaron apartándose entre sí. Actualmente, los fragmentos se hallan separados por millones de años-luz y, a medida que pasa el tiempo, se separan aún más.

La teoría «big-bang», con su necesario punto de vista sobre un universo modificado con el tiempo, no satisfizo a todos los astrónomos. Para tres de ellos, ingleses, Hermann Bondi, Thomas Gold y Fred Hoyle, ya en 1948, el principio cosmológico (según el cual, el universo tenía que parecer el mismo a todos los observadores), era incompleto si sólo se refería a observadores situados en distintos lugares del espacio. Dichos astrónomos ampliaron la teoría a los observadores situados en diferentes momentos del tiempo, a cuyo resultado lo denominaron el «perfecto principio cosmológico». Mediante esta opinión, el universo en su conjunto no cambia con el tiempo, sino que sigue siendo esencialmente el mismo en apariencia, a través de todos los tiempos.

Aunque admitieron que el universo se expande. Las galaxias se separan entre sí. Para salvar su teoría, Bondi, Gold y Hoyle sugirieron que, a medida que el universo se expande y las galaxias se van alejando unas de otras, continuamente se crea nueva materia en todas partes, a una velocidad excesivamente lenta, a una velocidad tan lenta que resulta indetectable para nuestros más delicados instrumentos. Cuando dos galaxias hayan duplicado la distancia entre ellas como resultado de la expansión espacial, se habrá creado entre ambas bastante materia, incluso a velocidad tan lenta, para aglomerarse en una nueva galaxia.

De este modo, aunque el universo se expanda eternamente, la distancia entre las galaxias vecinas siempre será la misma, ya que nuevas galaxias se forman dentro de la esfera del universo observable, a la misma velocidad con que otras se retiran, cayendo ya fuera de los límites observables. Por tanto, el aspecto del universo continúa siendo siempre el mismo, tanto en el pasado como en el futuro.

Ambas opiniones -la teoría «big-bang» y la de la «creación continua»- tienen sus mantenedores y sus detractores, acaudillados respectivamente por George Gamow y Fred Hoyle. Incluso entre individuos que no eran astrónomos se formaron dos bandos. Algunos se dejaron seducir por el superespectáculo colosal de una gigantesca explosión; en tanto, otros hallaban una austera gloria en la teoría de un universo sin principio ni fin, un universo de continuo cambio, y no obstante, siempre en el mismo lugar. ¿Cuál es la teoría correcta? ¿No existe un intermedio entre ambas? La distinción entre las dos teorías resultaría fácil si los astrónomos poseyeran una máquina del tiempo. Lo único que tendrían que hacer sería entrar en dicha máquina y moverse diez o doce mil millones de años en el pasado (o en el futuro), y echar una ojeada al universo. Si en tales fechas tenía el mismo aspecto actual, la teoría «big-bang» estaría equivocada, siendo correcta la de la «creación continua». Si, en cambio, el universo se veía radicalmente diferente, la equivocada sería la teoría de la «creación continua», y la razonable la «big-bang».

De manera extraña, los astrónomos ya poseen una máquina del tiempo.

La luz (o cualquier otra forma de radiación) no puede viajar a mayor velocidad que la propia, o sea, a 300.000 kilómetros por segundo. A escala terrestre es una velocidad tremenda, pero no es más que un simple salto en el conjunto del universo. La luz de las galaxias más distantes que podemos apreciar con nuestros más potentes telescopios tarda mil millones de años en llegar hasta nosotros. Esto significa que la luz que vemos procedente de las galaxias más lejanas de nuestro universo salió de alli hace mil millones de años.

Entonces, lo único que tenemos que decidir es que lo que vemos tan lejos, es esencialmente lo mismo que vemos en nuestras proximidades. Si las distantes galaxias son semejantes a las más cercanas, sin mostrar cambios, podemos olvidamos de la teoría «big-bang» (que postula el cambio). Si las galaxias más distantes son diferentes de nuestras vecinas, está claro que se ha producido un cambio con el tiempo, por lo que podemos olvidar la «creación continua» (que no postula los cambios).

Pero hay un obstáculo. Es muy difícil ver nada a mil millones de años-luz o más lejos. A lo sumo, sólo percibimos diminutas brumas luminosas. Si hubiese diferencias significativas en la estructura fina de estas galaxias lejanas, en comparación con la nuestra, es probable que no las distinguiésemos. A fin de que una diferencia sea detectable desde una distancia de mil millones de años-luz, tendría que ser inmensa y de carácter muy general.

Hacia 1950 no se había aún detectado nada semejante. Pero, entretanto, se había inventado una nueva técnica, un nuevo instrumento para atisbar en las profundidades más lejanas del espacio.

En 1931, un ingeniero de radio norteamericano, Karl Jansky, estaba ocupado en un problema no astronómico en absoluto, consistente en contrarrestar los efectos intermitentes de la estática en las comunicaciones por radio. Había una fuente de ruidos estáticos que no lograba eliminar y que, finalmente, decidió como procedente del espacio exterior.

En aquella época, su anuncio no creó impacto. Era algo interesante pero nada práctico. Las ondas de radio del espacio exterior eran extracortas y todavía no se habían inventado aparatos para detectar las débiles radiaciones de esta clase. Después, los aparatos de radar se fundaron en la detección de tales radiaciones, y al terminar la Segunda Guerra Mundial, el esfuerzo para poner a un alcance práctico el radar, dio por resultado una nueva capacidad para detectar las ondas de radio del espacio exterior. De esta forma nació la «radioastronomía», y los colosales aparatos de recepción («radiotelescopios») fueron encarados al firmamento.

Se detectaron ondas de radio del Sol y de unos objetos brumosos que parecían restos de estrellas, que en tiempos muy remotos habían estallado ferozmente. Incluso se detectaron ondas de radio procedentes del centro de nuestra galaxia, centro oculto a la vista (con respecto a la luz ordinaria) por la existencia de grandes nubes de polvo que absorben la luz, entre el centro y nosotros, nubes de polvo que, no obstante, las ondas de radio pueden penetrar.

Hacia 1950, se habían detectado en el cielo un millar de fuentes diferentes de emisiones de ondas extracortas, aunque sólo fue posible relacionar unas cuantas con algo visible. Lo malo era que, incluso, las ondas de radio eran mucho mayores que las ondas de luz corrientes; y cuanto mayores son las ondas, más borrosa es la visión. Tratar de encontrar la fuente exacta de un débil haz de ondas de radio era como querer descubrir la fuente exacta de un haz de luces visto a través de un cristal esmerilado. Lo único que se percibe es una mancha luminosa.

Sin embargo, hacia 1951, gracias a una gran perseverancia y dedicación, pudo limitarse a una pequeña zona una poderosa fuente de radiación de ondas de radio (llamada «Cisne A»). Dentro de esa zona, el astrónomo germano-americano Walter Baade, observó una galaxia de forma especial. En un estudio más minucioso, la galaxia resultó estar formada por dos en colisión. Y ésta pareció ser la fuente de aquella radiación de ondas extracortas: un par de galaxias en choque a 700.000.000 años-luz de distancia.

Por primera vez se puso en claro que las ondas de radio podían detectarse a enormes distancias. En realidad, las «radiogalaxias» que emitían ondas tan poderosas como las de Cisne A podían detectarse fácilmente a distancias tan inmensas que hasta con los más potentes telescopios era imposible distinguir su luz.

Los radiotelescopios podían penetrar a unas distancias fabulosas y sin precedentes y, por tanto, retroceder en el tiempo un número de millones de años inimaginable.

Esto planteó una posibilidad muy excitante para los astrónomos. Formularon de este modo la suposición de que todas, o virtualmente todas, las fuentes de ondas extracortas eran galaxias muy lejanas que emitían ondas de radio de gran intensidad por estar chocando o explotando, o sufriendo otra ingente catástrofe. Naturalmente, sólo un pequeño contingente de galaxias debían de hallarse complicadas en tales desastres, pero el universo contiene muchos miles de millones de galaxias, por lo que puede contener con suma facilidad algunos millares de «radiogalaxias». Y esos pocos millares son suficientes.

Parecía razonable suponer que cuanto más débil fuera la fuente de las ondas, más distante estaría la galaxia. En cuyo caso, era posible contar el número de tales fuentes a diversas distancias. Si la teoría de la «creación continua» es correcta, el universo es siempre generalmente el mismo a través del tiempo, por lo que debería de haber el mismo número de cataclismos constantemente. De esta forma, el número de fuentes de ondas de radio en un volumen de espacio dado debería de ser un valor fijo para distintas distancias.

Si, en cambio, es correcta la teoría «big-bang», el juvenil universo que se detecta a inmensas distancias debió ser más caliente y más poblado que en la actualidad. Es de suponer, razonablemente, que en un universo más joven que el nuestro se produjesen más catástrofes que ahora. Por tanto, el número de fuentes de ondas de radio para un volumen de espacio dado debería aumentar con la distancia.

A mediados de la década de los años cincuenta, el astrónomo inglés Martin Ryle emprendió una cuidadosa cuenta de las fuentes de radio y anunció que la cantidad aumentaba con la distancia, de acuerdo con la teoría «big-bang».

Sin embargo, la labor de Ryle no fue completamente convincente. Se apoyaba en la detección y la medición de fuentes de radio muy débiles, y unos levísimos errores, que podían sufrirse fácilmente, bastarían para destruir por entero la base de las conclusiones del astrónomo. De esta manera, los sostenedores de la «creación continua» del universo no perdieron la fe en sus creencias.

Como las fuentes de radio iban detectando cada vez zonas más limitadas, algunas en particular atrajeron la atención de los astrónomos. Las fuentes parecían tan pequeñas que podía tratarse de estrellas individuales y no de galaxias. Si era así, tendrían que estar muy juntas (las estrellas individuales no pueden hallarse separadas por distancias inconmesurables) y la suposición de Ryle, según la cual todas las fuentes de ondas de radio eran galaxias, se venía abajo, y con ello su conclusión. Entonces, cobraría nueva vida la teoría de la «creación continua».

Entre las fuentes de ondas de radio compactas había algunas conocidas como 3C48, 3C147, 3C196 y 3C286. (El prefijo «3C» es la abreviación de «Tercer Catálogo de Cambridge de radioestrellas», lista compilada por Ryle y su grupo, mientras que los otros números representan la situación de tales fuentes en la lista.) Se efectuaron grandes esfuerzos para detectar las estrellas que podían ser origen de dichas fuentes 3C. En América, Allan Sandage investigó meticulosamente las zonas sospechosas con el telescopio de 200 pulgadas de Monte Palomar, dispuesto a registrar cualquier estrella de aspecto sospechoso. En Australia, Cyril Hazard mantuvo su radiotelescopio enfocado hacia la fuente 3C273, mientras la Luna se hallaba en su dirección. Cuando la Luna se movía por delante de la 3C273, quedaba cortado el haz de ondas.

En el instante del corte, el borde lunar había cortado obviamente la situación exacta de tal fuente.

En 1960 se habían descubierto ya dichas estrellas. En realidad, no eran descubrimientos nuevos, ya que tales estrellas se hallaban registradas en anteriores fotografías celestes, aunque siempre tomadas por simples estrellas débiles de nuestra galaxia. Una nueva investigación más completa, propulsada por sus emisiones de ondas de radio, demostró que no se trataba en absoluto de estrellas ordinarias. Débiles nubes de materia parecían planear sobre un par de ellas, y la 3C273 presentaba señales de un débil surtidor.

Aún más: sus espectros, cuando fueron obtenidos por dos astrónomos, el americano, Jesse L. Greenstein y el holandés Maarten Schmidt, demostraron ser muy peculiares. Las escasas líneas oscuras presentes estaban situadas en lugares que no podían ser identificados con ningún elemento conocido. Era un misterio intrigante que por un tiempo quedó marginado.

En 1963, Schmidt volvió a estudiar el espectro de la 3C273. Había seis líneas, y de repente observó que cuatro de ellas estaban espaciadas de una forma que recordaba una serie muy conocida de líneas que hubieran debido hallarse en otra zona del espectro. A fin de que tales cuatro líneas estuviesen donde estaban, habían debido sufrir un desplazamiento hacia el rojo sin precedentes. ¿Era eso posible? Examinó los demás espectros. Si presentaban grandes desplazamientos hacia el rojo conseguiría identificar cada una de las líneas implicadas.

Al cabo de dos o tres años, gracias a una investigación concentrada del firmamento, se logró descubrir cuarenta objetos con estas mismas características. Se obtuvieron los espectros de más de la mitad, y todos mostraron enormes desplazamientos hacia el rojo. Uno de esos cuerpos, en realidad, retrocede a la velocidad récord de 240.000 kilómetros por segundo, y se calcula que se halla a 9.000 millones de años-luz de distancia.

Sin embargo, con la existencia real de tales desplazamientos hacia el rojo, las aparentes «estrellas» tenían que estar a distancias remotas, puesto que, sobre la base del universo en expansión, un gran desplazamiento hacia el rojo siempre va asociado con inmensas distancias. En efecto, esos cuerpos celestes se hallaban mucho más lejos que todos los demás del universo conocido.

A esas distancias, ciertamente no podía tratarse de estrellas. No es posible divisar ninguna estrella corriente a tan gran lejanía, y por esto se denominó a dichos objetos con el nombre de «quasi-estelares», nombre que pronto quedó abreviado a «quasar».

Los quasars son una fuente estupenda de intrigas para los astrónomos. De interpretarse sus desplazamientos hacia el rojo a la teoría del universo en expansión, y, si, en efecto, los quasars están a muchos miles de millones de años-luz y sus propiedades son muy especiales. Para aparecer tan brillantes como aparecen a tanta distancia, han de resplandecer con una luminosidad de diez a cien galaxias. Y sin embargo, hay muchos motivos para suponer que no son tan grandes. Es posible que sólo tengan un diámetro de uno a diez años-luz, y no los cien mil años-luz de una galaxia ordinaria.

¿Qué clase de cuerpos son, para tener su materia acumulada en una fracción tan diminuta de volumen galáctico, y no obstante brillar con el resplandor de docenas de galaxias? Existen casi tantas teorías al respecto como astrónomos, pero con relación a la teoría de la «creación continua» del universo, esas opiniones no cuentan. Ya es suficiente que existan los quasars.

El punto clave es que haya tantos quasars tan alejados y ninguno a una distancia inferior a mil millones de años-luz. Esto significa que en el juvenil universo, en el universo primitivo, había muchos quasars, y ahora no. El número de quasars (que pueden ser la fuente de todos o casi todos los haces de ondas de radio estudiados por Ryle), puede aumentar con la distancia y, por consiguiente, con la juventud del universo. Esto presupone que hemos detectado un cambio muy importante del universo con el paso del tiempo: la disminución del número de quasars.

Lo que es suficiente para eliminar la teoría de la «creación continua».

Es suficiente si, en realidad, los quasars son objetos sumamente lejanos. La creencia de que lo son se apoya en la suposición de que los inmensos desplazamientos hacia el rojo que presentan son parte de la expansión del universo… Mas, ¿y si no son objetos tan distantes? Supongamos que los quasars fuesen pequeñas porciones de galaxias cercanas, expulsadas de los centros galácticos por medio de enormes explosiones. En los últimos años se han detectado ejemplos de «explosiones galácticas», y los astrónomos investigan atentamente las galaxias que, por algún motivo -formas raras, neblinas, signos de convulsiones internas-, parecen insólitas. Se han detectado algunos quasars no muy lejos de esas galaxias extrañas.

¿Coincidencia? ¿Se hallan los quasars en la misma línea visual que las galaxias raras? ¿O fueron expulsados del interior de las mismas a velocidades monstruosas como resultado de explosiones ocurridas entre millones de estrellas? En este caso, los quasars no estarían tan lejos de nosotros. Algunos podrían incluso estar cerca, otros lejos, y su distribución no nos permitiría desdeñar absolutamente la teoría de la «creación continua».

Esto es posible, mas también hay argumentos en contra. Supongamos que los quasars fuesen objetos arrojados de algunas galaxias con tal fuerza que viajasen a enormes fracciones de la velocidad de la luz. Algunos habrían sido incluso arrojados de nuestra galaxia y mostrarían un gigantesco desplazamiento hacia el rojo de carácter equívoco, si se interpretaban como representantes de un retroceso causado por la expansión general del universo y no por la explosión especial de una galaxia.

Sin embargo, también una cantidad aproximadamente igual podría haber sido enviada hacia nosotros, acercándose a grandes fracciones de la velocidad de la luz. Entonces, éstos presentarían un gran desplazamiento hacia la zona violeta.

Podría haber otros todavía que ni se alejasen ni se acercasen a nosotros, sino que viajasen, por efecto de las explosiones, en una línea diagonal, en dirección lateral.

Estos quasars sólo presentarían, si acaso, un pequeño desplazamiento hacia el rojo o hacia el violeta; mas al considerar cuan cerca podrían estar y lo muy velozmente que podrían moverse, alterarían sus posiciones en el cielo en una leve, pero considerable cantidad por encima del par de años de haber sido observados.

Sin embargo, lo cierto es que no se ha detectado ningún quasar con un desplazamiento hacia el violeta, y ninguno que altere su posición celeste. Sólo se han observado quasars con desplazamientos hacia el rojo, con grandes desplazamientos hacia el rojo. Y suponer que unas explosiones relativamente cercanas hubiesen arrojado a los quasars sólo en el sentido adecuado para producir desplazamientos hacia el rojo exclusivamente, es demasiada coincidencia.

Por tanto, el peso de las pruebas se inclina a favor de la inmensa distancia de los quasars y de la eliminación de la teoría de la «creación continua»… Naturalmente, Fred Hoyle se derrumbó.

La eliminación de la «creación continua» no significa necesariamente el establecimiento de la teoría «big-bang». Supongamos que existe una tercera posibilidad que aún no ha sido sugerida. Para fortalecer la teoría «big-bang» contra todas las posibilidades aún no sugeridas, sería justo considerar algún fenómeno profetizado por aquella teoría, algún fenómeno que pudiera ser observado actualmente.

Supongamos, por ejemplo, que el universo empezó como un huevo cósmico increíblemente denso, que estalló. En el momento de la explosión, el calor debió de ser tremendamente intenso, posiblemente de 10.000 millones de grados centígrados.

En este caso, si nuestros instrumentos pudiesen penetrar lo bastante lejos, hasta llegar al borde del universo observable, podríamos retroceder lo suficiente en el tiempo para captar un vislumbre de la radiación que acompañó al «big-bang».

A temperaturas de miles de millones de grados, la radiación tendría lugar en forma de rayos X muy enérgicos.

Sin embargo, el universo en expansión alejaría esta fuente inusitada de rayos X casi a la velocidad de la luz. Esta increíble velocidad de retroceso tendría el efecto de debilitar grandemente la energía de la radiación; debilitaría hasta el punto de que llegaría a nosotros en forma de ondas de radio con cierto conjunto de propiedades. Por los años sesenta se calcularon cuáles podían ser esas propiedades.

Después, a principios de 1966, se detectó en el cielo un fondo muy débil de radiación extracorta; esta radiación encajaría con la teoría «big-bang». Esto se ha comprobado y no sólo parece haberse eliminado por completo la teoría de la «creación continua», sino haberse detectado ya la inmensa explosión del universo primitivo.

Si es así, hemos perdido algo. Al enfrentamos con nuestra propia muerte individual, era posible, incluso para los que no tenían fe en una existencia posterior, hallar algún consuelo. La vida continuaría. En un universo de «creación continua», sería posible concebir una Humanidad que se trasladase, en caso necesario, de una galaxia vieja a otra joven, existiendo así la Humanidad a través de todo lo infinito y por el infinito. Es ésta una visión colosal, divina, que señalaría la muerte del individuo como algo carente de consecuencias.

Sin embargo, en el esquema «big-bang», nuestro universo tiene un principio…, y un final. O ha de extenderse de manera más tenue y delgada mientras todas las galaxias envejecen y las estrellas perecen una a una, o ha de llegar a su máxima extensión y sufrir otro cataclismo, volviendo al cabo de tantísimos miles de millones de años de existencia momentánea a ser un huevo cósmico.

En cuyo caso, la Humanidad, tal como la conocemos, ha de dejar de existir, y acabarse el sueño de la divinidad. Hemos vuelto a descubrir la muerte y el Homo sapiens, como especie, igual que los hombres como individuos, han de aprender a enfrentarse con el inevitable fin.

Aunque si el universo oscila, y si el huevo cósmico se renueva cada 100.000 millones de años, aproximadamente, explotará otra vez; y entonces, quizás en un número infinito de universos sucesivos, se formará una Humanidad inteligente (o varias), que pueda preguntarse respecto al principio y al fin de todo esto.

Segunda parte

RELATIVA A LO MAS O MENOS

DESCONOCIDO

3. OTRA VIDA

20. Una ciencia en busca de un

sujeto

Supongo que todos habréis escuchado el áspero comentario sobre el milagro de producirse drogas en tal profusión que algunas son útiles para enfermedades que todavía no se han descubierto. Esta idea de una cura sin enfermedad es análoga al estado de la nueva ciencia llamada «exobiología», un campo de estudios sin nada que estudiar.

La palabra exobiología fue inventada por el biólogo norteamericano, ganador del Premio Nobel, Joshua Lederberg. Significa «fuera de la biología», o sea el estudio de las formas de vida más allá de la Tierra.

¿Cuáles son las formas de vida fuera de la Tierra? Éste es el fallo. No conocemos ninguna, aunque sospechamos que exista alguna. En el espacio habrá estrellas o soles como el nuestro en torno a los cuales unos planetas den vueltas como ocurre en nuestro sistema solar…

¿Y cuáles serán sus formas de vida? ¿Como las nuestras? ¿O casi iguales? ¿Como algo jamás soñado? No lo sabemos.

En el sistema solar puede haber formas de vida en Marte…, puesto que en la Luna ya sabemos que no hay ninguna… ¿O sí? Las exploraciones lunares del programa «Apolo» han sido, en realidad, muy limitadas en sus radios de acción y en el tiempo, y aunque la conclusión casi indudable es que en la Luna no existe ninguna forma de vida, la exploración del planeta no ha sido todo lo exhaustiva que debía ser para afirmar tal cosa.

La especulación es libre, y si ahora, en general nos faltan sujetos y temas que estudiar a este respecto, no faltan conceptos a considerar en la quietud de nuestras mentes. En este sentido, Lederberg es un exobiólogo; lo mismo que los astrónomos William M. Sinton, del Observatorio Lowell; Stephen H. Dole, de la Corporación Rand; Carl Sagan, del Observatorio de Harvard, y el químico Harold C. Urey, de la Universidad de California.

Dole, por ejemplo, en su obra Planetas habitables para el hombre, llega a la conclusión (ver Capítulo 22) de que solamente en nuestra galaxia es probable que existan unos 640.000.000 de planetas semejantes a la Tierra, capaces de mantener formas de vida. (Tengamos en cuenta que existen miles de millones de galaxias en el universo.) Sagan va más lejos todavía. Piensa que es razonable suponer que puede haber en nuestra galaxia hasta 1.000.000 de planetas que no sólo sostengan formas de vida, sino vida inteligente y civilizaciones avanzadas. Incluso se pregunta si las formas de vida inteligente de otros mundos visitaron la Tierra en un pasado distante, y cita los antiguos mitos de Babilonia, al efecto de que la civilización de la primitiva Tierra se fundó gracias a seres no humanos de profunda sabiduría.

Mas, ¿cómo especular cuando no hay nada en que apoyarse? ¿Cuando no existe ni la menor vida exterior que sirva de guía? La respuesta es que necesitamos tener algo con que continuar adelante. Conocemos un planeta plenamente infestado de vida: el nuestro. Aunque se suponga que es arriesgado extraer conclusiones respecto a la vida en general del universo, derivando tales conclusiones de la vida de nuestro planeta, y que hacerlo sería excesivamente egoísta, en realidad existen argumentos que justifican esta postura.

En primer lugar, la Tierra no es un planeta raro ni insólito, químicamente hablando. Los astrónomos, en su estudio sobre la composición de las estrellas y de la materia existente entre las mismas (basado en la naturaleza de la luz emitida o absorbida), han llegado a tener nociones bien definidas respecto a la abundancia relativa de los diferentes productos químicos del universo.

Los dos elementos más abundantes son los gases ligeros: hidrógeno y helio. La gravedad terrestre era demasiado débil y sus temperaturas demasiado elevadas durante el proceso de la formación planetaria para que dichos gases fuesen retenidos. Otros gases como el neón y el argón también se perdieron, mas aparte de ésos, la estructura terrestre es semejante en naturaleza y proporciones a la general del universo.

La Tierra, por tanto, es un planeta normal y típico…, que no está formado por elementos raros que, a través de un fallo o capricho de la Naturaleza, sirve para mantener vida. En realidad, si descubrimos un planeta en el universo con una masa y una temperatura similares aproximadamente a las de la Tierra, podremos estar casi seguros de que estructural y químicamente será igual a aquélla.

Entonces, con un planeta como la Tierra, ¿qué clase de vida podemos esperar hallar en él? Para responder a esto, veamos antes qué clase de vida es posible que contenga el universo.

En toda la Tierra sólo hay una forma básica de vida.

Toda la vida terrestre, desde el virus más simple a la mayor ballena y a un árbol de palosanto, se basa en las proteínas y los ácidos nucleicos (ver Capítulo 6). Toda la vida utiliza las mismas vitaminas, los mismos tipos de cambios químicos, los mismos métodos para liberar y utilizar la energía. Toda la vida sigue un solo sendero, por mucho que varíen las distintas especies en sus detalles particulares.

Además, la vida terrestre, que se inició en el mar, se compone precisamente de los elementos que son, y fueron, comunes en el mar. No existen «ingredientes misteriosos», cosas raras y mágicas incluidas en el conjunto gracias a un extraño azar.

Otro planeta, con la masa y la temperatura de la Tierra, también debería poseer océanos de agua, y el mismo tipo de sales disueltas. Por tanto, debería desarrollar una vida basada en los mismos elementos químicos que la nuestra. ¿Se deduce de ahí, pues (habiendo llegado ya tan lejos), que también debe moverse por el mismo sendero general por el que lo hace la vida en la Tierra? En esto no podemos estar seguros. Los elementos químicos de la vida pueden combinarse de muchas maneras diferentes. Supongamos que en los primitivos tiempos de la Tierra, cuando la vida se estaba formando en los océanos, se propusieron mil esquemas distintos de vida. Supongamos asimismo que un esquema particular venció sobre los demás, tal vez por suerte; la supervivencia de este esquema podría ahora damos una falsa impresión de que es el único e inevitable esquema.

Naturalmente pudo ser así, mas las pruebas que poseemos apuntan en otra dirección. Desde los años cincuenta, los químicos han tratado de imitar las condiciones químicas existentes en la primitiva Tierra, y han observado que moléculas muy complejas se han desarrollado espontáneamente de las simples sustancias que a la sazón existían (ver Capítulo 9).

Los componentes que se formaban en tales experimentos eran los mismos elementos familiares que componen nuestro cuerpo: los aminoácidos, de donde surgen nuestras proteínas; los nucleótidos, de donde se derivan los ácidos nucleicos; los anillos de porfirina, de donde se forman la clorofila y la hemoglobina.

Todas las sustancias formadas de sistemas que imitaban el océano primitivo se hallan en un amplio camino conducente a nuestra particular clase de vida. No hay la menor señal de cambio, ninguna insinuación de un camino lateral. Aún podría presentarse en un futuro más o menos lejano, mas un experimento tras otro han disminuido tal posibilidad.

En un planeta como el nuestro, la base química de la vida ha de ser, por tanto, similar a la de la Tierra. No tenemos motivos para pensar lo contrario. Además, la tendencia general de la evolución habría de ser la misma. Las presiones de la selección natural tienden a llenar todas las posibles regiones de un planeta con organismos adaptados a tales regiones. En la Tierra, después del desarrollo de la vida en el mar, hubo una invasión gradual del agua corriente por organismos que podían conservar la sal, una invasión de la tierra seca por organismos adaptados para conservar el agua, y una invasión del aire por organismos adaptados al vuelo.

Todo esto ha de haber sucedido igualmente en otro planeta semejante al nuestro, por lo que la novedad tendría su límite. En un planeta como la Tierra, un ser volador sólo podría tener un tamaño dado si el aire ha de sostenerle; un ser marino tendría que ser de líneas aerodinámicas o de movimientos lentos, y así sucesivamente.

Es muy razonable, por tanto, suponer que la vida de otros mundos desarrolle unos rasgos reconocibles, basados en la utilidad general. Dichos seres tendrían que conservar la simetría derecha-izquierda. Deberían tener una cabeza diferenciada, en donde se concentrasen el cerebro y los órganos sensoriales. Entre los mismos, se hallarían los de la vista, como nuestros ojos. Las formas más activas cometerían otras formas, como vegetales, y es probable que respirasen oxígeno, o lo absorbiesen de alguna forma.

En resumen, la vida de un planeta semejante a la Tierra no sería completamente extraña a la nuestra. Indudablemente, diferiría drástica y completamente en sus detalles. (¿Quién podía predecir la forma del platipus antes del descubrimiento de Australia, o las de los peces de las profundidades abisales antes de ser vistos por vez primera?) La vida puede variar en muchos detalles y en muchas direcciones. Aunque la química sea la misma y similar el plan de estructura general, las variaciones posibles de este tema son tan inmensas que es extremadamente improbable que, ni siquiera por casualidad, ocurran las mismas variaciones en otro planeta como la Tierra. Sería demasiada coincidencia que una criatura extraterrestre se pareciese a un hombre; incluso sería esperar demasiado un vago parecido. Sin embargo, habría entre ellos y nosotros diversos factores en común, que nos obligaría a aceptarlos si no como hermanos, al menos como primos lejanos nuestros.

Mas, por desgacia, no existe a nuestro alcance ningún planeta semejante a la Tierra. Dentro del sistema solar, Venus se parece a la Tierra en la masa, pero está demasiado caliente para que pueda haber allí algo semejante a nuestra clase de vida. Marte, por otra parte, tiene una temperatura parecida a la de la Tierra (algo más fría), pero sólo tiene una masa de algo más de la décima parte terrestre y, por tanto, retiene muy poca atmósfera. Especialmente, no posee oxígeno y apenas agua.

Mas, ¿es necesario el oxígeno para la vida? El oxígeno de nuestra atmósfera es muy probable que esté ahí sólo por ser producido por las plantas verdes (ver Capítulo 13). Antes de desarrollarse los vegetales verdes probablemente no había oxígeno en el aire, y la vida se había iniciado sin él. Incluso hoy día, existen formas bacterianas que no necesitan oxígeno para sobrevivir. Para algunas, el oxígeno resulta incluso venenoso. Dichas bacterias podrían ser restos de vida, restos supervivientes del período de la primitiva Tierra carente de oxígeno.

No tenemos pruebas de que haya existido nunca una vida sin oxígeno más avanzada que las bacterias, pero no estamos seguros. Sin embargo, es mejor suponer que la vida en Marte, puesto que no hay oxígeno o muy poco, ha de ser muy simple.

Hacia 1960, hubo grandes esperanzas de que en Marte pudiesen existir formas vegetales muy simples. En aquel planeta hay zonas de color verde que varían con la estación del año, como si se tratase de una vegetación que, a veces se extiende, y otras se retrae. Sinton estudió la luz reflejada desde Marte y dedujo la presencia de elementos químicos semejantes a los hallados en los vegetales terrestres. Ciertas formas simples de vida vegetal se han desarrollado en la Tierra bajo condiciones marcianas: frío intenso, poca agua, sin oxígeno…, y han sobrevivido. En realidad, formas simples de vida, como bacterias y hongos, también han sobrevivido en condiciones semejantes a las de la atmósfera aún más hostil de Júpiter, cargada de metano y amoníaco, gases ordinariamente venenosos.

Por desgracia, los signos de vida de Marte son inciertos y han quedado bastante desacreditados. Sinton descubrió que la luz reflejada de Marte podía interpretarse en forma que no implicaba una vida vegetal. Sagan ha desarrollado una teoría que explica la propagación y retirada de las zonas de verdor sin postular formas de vida. Peor todavía, la sonda de Marte: Maríner IV que voló hacia allí en julio de 1965, y tomó fotografías de su superficie, nos demostró la existencia de muchos cráteres en el planeta rojo. La existencia de tales cráteres parece indicar la ausencia de erosión y, por tanto, la ausencia permanente de agua, algo que rebaja las posibilidades de que alguna vez haya habido vida en Marte.

Sin embargo, no se han perdido todas las esperanzas. Algunos astrónomos, incluyendo al propio Sagan, siguen argumentando sobre la posibilidad de la vida en Marte; y aunque las probabilidades no sean muchas, ni aun para los más optimistas, una de las perspectivas más fascinantes de la exploración marciana estriba en la oportunidad de estudiar la vida exterior. Si tal vida se halla presente en Marte, aunque sea en formas muy simples, la ciencia de la exobiología habrá dado un gran paso adelante.

Suponiendo que la estructura química de la vida marciana (si existe) fuese igual básicamente a la nuestra, que las formas de vida se componen de proteínas y ácidos nucléicos edificados según los mismos bloques de construcción, la suposición de que toda la vida es una básicamente, en cualquier planeta remotamente semejante al nuestro, se vería tremendamente fortalecida.

Por otra parte, si las formas de vida marcianas son básicamente distintas en su aspecto químico, ello aún sería mejor. Por primera vez, los científicos podrían estudiar un esquema vital diferente al nuestro. Y los conocimientos así obtenidos sobre la vida de la naturaleza en general (los factores comunes en dos esquemas de vida básicamente distintos) podrían ser de una importancia incalculable.

Por todo esto, los científicos no desean aguardar a que el hombre aterrice en Marte para determinar si allí existe vida. Es por esto que actualmente se están desarrollando instrumentos que pueden aterrizar automáticamente en Marte para comprobar la presencia de vida. (Éste es el propósito de la «exobiología aplicada».) Estos instrumentos se construyen con el fin de expulsar cuerdas o cintas pegajosas u otros aparatos que recojan partículas y polvillo marciano. Dicho polvillo y partículas de Marte, posiblemente conteniendo células vivas, serían sumergidas en líquidos con sales en suspensión junto con elementos nutritivos, capaces de soportar la vida terrestre, y los instrumentos registrarían y transmitirían a la Tierra datos sobre cualquier cambio producido en la basicidad o acidez de los líquidos[9]. O registrarían la formación de dióxido de carbono o la presencia de reacciones específicas que sólo pueden tener lugar mediante enzimas.

Dichos cambios, o algunos de ellos, constituirían una prueba absoluta, no sólo de la presencia de vida en Marte, sino de la presencia de una vida basada en los mismos principios químicos que los de la Tierra.

Mas, ¿y si no se detecta cambio alguno? ¿No tendría entonces vida el planeta Marte? ¿O los instrumentos habrían ido a parar a una zona estéril? ¿O las formas químicas de Marte se niegan a vivir y crecer en los elementos químicos enviados? No podemos saberlo con certeza. Para esto tendremos que esperar a poner el pie en Marte.

La Luna nos ha dado ya algunas insinuaciones. El hombre ha estado ya en nuestro satélite natural, sin haber hallado vida en absoluto…, al menos por el momento. No obstante, puede haber aire y hasta restos de agua bajo la superficie de algunas zonas o en el interior de algunos cráteres, pudiendo incluso existir formas de vida muy simples.

Los datos obtenidos hasta el presente parecen indicar lo contrario, pero todavía están todas las muestras lunares bajo estudios especiales y rodeados del mayor secreto. Además, repetimos, las exploraciones lunares, a pesar de su espectacularidad, debida en gran parte a la televisión, han sido breves y sumamente limitadas por razones obvias de comprender. Si hubiese, a pesar de todo, una vida lunar básicamente diferente a la de la Tierra, el resultado sería tan satisfactorio como el obtenido con un viaje a Marte.

Si, en cambio, la vida lunar estuviese basada en la química terrestre, no podríamos estar tampoco seguros de su significado. En la Luna han aterrizado ya, desde hace tiempo, objetos terrestres y, a pesar de nuestros esfuerzos por esterilizarlos, pueden haber contaminado la superficie lunar.

Peor aún: algunos astrónomos creen que en el pasado, cuando la Tierra y la Luna estaban más cerca entre sí y el bombardeo meteórico era más intenso, materias de un planeta pudieron pasar al otro. Recientemente, Urey especuló con la idea de que pasó a la Luna una cantidad suficiente de agua terrestre para formar lagos de breve tiempo de duración. En tal caso, la Luna pudo haberse contaminado con formas de vida terrestre millones de años antes de iniciarse el programa «Apolo», por lo que cabría esperar de Marte una visión más clara de la verdadera exobiología.

A pesar de todos los cálculos, tenemos que volver a la declaración inicial de que a la exobiología le falta, en realidad, un sujeto de estudio. Hasta ahora sólo poseemos especulaciones, muy atractivas, cierto, pero nada sustanciales.

Muchos biólogos (especialmente el importante zoólogo de Harvard, George Gaylord Simpson, lector de ciencia-ficción, y muy imaginativo por consiguiente, y Theodosius Dobzhansky, de la Universidad de Rockefeller, también hombre de gran inteligencia y osadía mental) se hallan impacientados al tomar con entusiasmo excesivo una ciencia todavía falta de contenido.

Indudablemente, los exobiólogos han de proceder lentamente, paso a paso.

Paso 1: Tienen que aferrarse firmemente al único tipo de vida que conocemos: el nuestro.

Paso 2: Han de indagar en cómo sus modestas conclusiones, basadas en las pruebas reunidas en la Tierra, se sostienen en contra de la realidad de la Luna y Marte, después de haber sido ambos mundos estudiados a conciencia por el hombre y sus instrumentos.

Paso 3: Antes de dar este paso, aguardemos a haber dado el paso anterior.

21. Nosotros, los intermedios

En la Tierra, la vida se ha desarrollado en muchas direcciones, adecuándose a una tremenda variedad de ambientes, y adoptando formas que apenas hubiera podido inventar la más desatada de las imaginaciones.

Mas todas las variaciones y modificaciones existentes en la Tierra son, en cierto modo, superficiales. Pese a todas sus maravillosas diferencias, la vida en la Tierra es solamente una variación imaginativa de un solo tema químico (ver Capítulo 20), y la vida en cualquier planeta de apariencia terrestre tendría que ser una serie de variaciones sobre el mismo y perpetuo tema.

Tal vez no sea esto excesivamente sorprendente. Tal como nosotros entendemos la vida, ha de consistir en moléculas bastante grandes y complejas para adaptarse a las necesidades flexibles y numerosas del tejido vivo. Deben de ser muy estables, a pesar de su complejidad, para retener su estructura bajo ciertas condiciones, y bastante inestables para cambiar caleidoscópicamente bajo otras condiciones. Dichas moléculas, grandes y complejas, estables e inestables a la vez, no se producen fácilmente. En los seres y cosas vivos de la Tierra, las moléculas más importantes de este tipo son las proteínas y, por lo que sabemos, nada más puede sustituirlas. Además, los cambios sufridos por estas proteínas en el proceso vital sólo pueden tener lugar en un fondo acuático. La vida se inició en el océano, y hasta las diversas formas de vida de la tierra seca todavía contienen del 50 al 80 por 100 de agua.

El tema químico, por tanto, sobre el cual la vida interpreta sus variaciones, aquí y posiblemente en todos los planetas sostenedores de vida y del tipo terrestre, es la proteína en el agua. (Con la estructura proteica supervisada por un complejo sistema de ácidos nucleicos.) Si hemos de tropezar con seres vivos de un planeta tipo Tierra, no podemos predecir si tendrán alas, tentáculos, la piel verde, diez pies, cabezas en forma de cúpula o colas bifurcadas. Pero sí podemos adelantar que, sea cual fuere su forma, estarán formados por proteínas en el agua bajo la supervisión de los ácidos nucleicos.

Pero, ¿y la vida en otros planetas distintos de la Tierra? ¿Y en los planetas tan próximos a su Sol que su superficie está lo bastante caliente para fundir el plomo? ¿Y los planetas tan alejados de su Sol que el agua está eternamente helada? ¿Están condenados tales mundos a una eterna esterilidad? Así tendría que ser, ciertamente, si toda la vida estuviera basada sólo en las proteínas en agua.

Pero, ¿toda la vida es igual? ¿Estamos seguros de que no existen otros temas vitales? Supongamos, por ejemplo, que en un mundo en que no existe, ni ha existido jamás, agua líquida, gracias a una temperatura eternamente helada, haya una sustancia que a temperaturas bajísimas pueda ocupar el lugar del agua. En realidad, tal sustancia existe y se llama «amoníaco».

Todo el mundo está familiarizado con el amoníaco embotellado, que parece agua y tiene un olor picante. Esto no es amoníaco verdadero, sino una disolución de amoníaco en agua.

El verdadero amoníaco es un gas a temperatura ordinaria, un gas picante, lacrimógeno, venenoso. En las condiciones terrestres no se licúa hasta una temperatura de 33° C ¡bajo cero! No se hiela hasta una temperatura de 78° C bajo cero. La amplitud térmica de su fase líquida cambia con la presión atmosférica de un planeta, pero bajo cualquier condición sigue siendo un líquido a 50 grados por debajo del punto de solidificación del agua.

Los mundos fríos de nuestro sistema solar, como Júpiter y Saturno, poseen atmósferas densas, compuestas principalmente de hidrógeno y helio, mas también contienen mezclas ricas en amoníaco y metano. Tal vez algunos de los mayores satélites de dichos planetas contengan también tales atmósferas. En efecto, existen buenos motivos para creer que todos los grandes planetas fríos poseen esta clase de atmósfera.

Es concebible, entonces, que esos planetas, incluso con el agua convertida en hielo muy sólido, puedan poseer océanos de amoníaco líquido, donde se haya desarrollado una vida completamente diferente a la nuestra.

La conducta química del amoníaco se parece mucho a la del agua. Los químicos han desarrollado y demostrado una química de sustancias disueltas en amoníaco, análoga a la química ordinaria de las sustancias disueltas en el agua; por lo que el tema proteínas en amoníaco resulta muy fascinante en condiciones en que la temperatura sea demasiado fría para las proteínas en agua.

Una química vital basada en este nuevo tema tendría que diferir radicalmente de todo lo que conocemos. Nuestras proteínas, suficientemente activas para participar en los procesos vitales a las temperaturas acostumbradas, se tornan demasiado perezosas a las temperaturas del amoníaco líquido, demasiado inertes para soportar las complejidades de los rápidos cambios requeridos por nuestra vida. Sin embargo, hay muchas estructuras químicas demasiado activas, demasiado inestables, para existir durante más de una fracción de segundo a la temperatura de congelación del agua. Estas estructuras se toman estables a temperaturas inferiores, pudiendo entonces poporcionar una base práctica para la vida.

Los organismos terrestres ingieren alimentos que contienen moléculas complejas de átomos de carbono e hidrógeno. (Las plantas no ingieren tales alimentos, pero fabrican moléculas complejas utilizando la energía solar.) Los átomos de hidrógeno se combinan con el oxígeno de la atmósfera, y la energía liberada soporta la vida.

Pero en los planetas fríos no hay oxígeno en la atmósfera. En cambio, hay hidrógeno. Tal vez el alimento de los seres amoniacales serían moléculas complejas ricas en átomos de carbono y oxígeno…, moléculas de tipo demasiado inestable para existir a la elevada temperatura de la Tierra. Los átomos de oxígeno de tales alimentos se combinarían con el hidrógeno absorbido de la atmósfera. La energía, al fin y al cabo, se obtendría tan fácilmente como en el proceso «inverso», o sea, en este caso el nuestro.

Aunque un planeta fuese demasiado frío para que el amoníaco permaneciese en estado líquido (y la mayoría de los planetas exteriores de nuestro sistema solar, por ejemplo, Urano y Neptuno, son demasiado fríos), no se habría perdido toda esperanza de alguna forma de vida.

Existe el metano que, en la Tierra, es el elemento principal del «gas natural». Si se quema, sirve para guisar y calentar las viviendas. El metano cuesta más de licuar que el amoníaco, ya que se convierte en líquido a 184 grados centígrados bajo cero.

El metano, sin embargo, es una sustancia completamente diferente del amoníaco o el agua en sus propiedades químicas. Las proteínas ordinarias no se combinan bien con él. Mas sí las sustancias grasas, y tal vez en los planetas muy fríos sean las moléculas grasas las que reemplacen a las proteínas. En realidad, existen complejas moléculas grasas, incluso en organismos terrestres, y algunas son tan complicadas como las proteínas; por lo que no es una fantasía el tema vital de grasa en metano.

¿Y los planetas calientes que están cerca de un sol? Esos mundos son demasiado pequeños y carecen de una atmósfera normal…, normal según la nuestra. Puede haber en pequeñas cantidades gases poco apetecibles, como el vapor de azufre y el de mercurio. Ciertamente, no hay agua, pues de existir alguna en su principio, hace millones de años que ya se habría evaporado.

Tal vez la vida pudiera desarrollarse en sustancias líquidas a temperaturas muy elevadas. El azufre (parecido químicamente al oxígeno), es líquido entre las temperaturas de 119 y 444 grados centígrados. ¿Podría existir una vida fundada en el azufre? En este caso, seguramente no se basaría en las proteínas ordinarias, demasiado inestables a temperaturas altas.

La proteína ordinaria y todas las complejas moléculas del tejido vivo, incluyendo las del ácido nucleico que presenta el mismo proceso, se componen en su mayor parte de átomos de hidrógeno y carbono, con algo de oxígeno, nitrógeno, azufre y fósforo, como átomos menores. Nuestras moléculas ordinarias son, en realidad, derivados de hidrocarbonatos.

Durante la Segunda Guerra Mundial, no obstante, como resultado de una investigación sobre la bomba atómica, los químicos descubrieron que los átomos de hidrógeno de tales moléculas podían sustituirse por átomos de flúor (el flúor es un gas venenoso y muy corrosivo). Las moléculas de fluorcarbonato resultantes poseían ciertas propiedades como la de los hidrocarbonatos, pero eran mucho más estables. Los complejos elementos químicos formados de derivados de los fluorcarbonatos serían excesivamente estables para componer un ser vivo, pero a la temperatura del azufre líquido podrían resultar bastante inestables. (Es difícil juzgar por las moléculas simples de un tipo especial cuáles serían las propiedades de las variedades más complejas del mismo tipo.) Por ejemplo, una molécula fabricada por el hombre, como la de nylon, tiene semejanzas básicas con las combinaciones atómicas de las proteínas.

Si el nylon, tan estable e inerte, fuese el único compuesto de su tipo a estudiar, ¿quién hubiese podido nunca predecir la posible existencia de las complejas e inestables proteínas con su reactividad y versatilidad? Otro tipo de moléculas que podrían concebiblemente formarse como estructuras complejas y capaces de sobrevivir a temperaturas elevadas son las siliconas. Están formadas, esencialmente, por cadenas de átomos de sílice y oxígeno, como las rocas de nuestro planeta. Unidos a dichas cadenas para otorgarles versatilidad, hay grupos de hidrocarburos (o grupos de fluorcarbonatos a elevadas temperaturas).

Tales siliconas se han desarrollado en los laboratorios, en las últimas décadas. Las siliconas sólidas sirven, entre otros fines, como una especie de goma artificial, mientras que las líquidas se han empleado como fluidos hidráulicos.

¿Podemos imaginarnos la vida en los planetas calientes en unas formas de tejidos esponjosos y una sangre de tipo hidráulico, viviendo en charcos de azufre líquido? En los planetas calientes ningún ser vivo necesitaría utilizar reacciones químicas como fuente de energía. Con un sol diez veces mayor y más brillante que el nuestro (es decir, visto desde la Tierra), los seres vivos de fluorcarbonato o siliconas, podrían absorber la energía directamente del horno solar.

¿Es posible que en un futuro más o menos distante nos encontremos con esos seres? Al fin y al cabo, aunque nunca lleguemos a las estrellas, por la época en que vivan nuestros nietos habremos explorado ya los planetas de nuestro sistema solar. Y ahí, con excepción de Marte y sus posibles formas vegetales simples, habremos investigado, o investigaremos, mundos completamente distintos a nuestra Tierra. ¿Qué hallaremos en un planeta caliente como Mercurio? ¿Sólo rocas muertas y azufre humeante? ¿Qué encontraremos en un mundo como Titán, el mayor satélite de Saturno? ¿Sólo hielo endurecido y vientos de metano? No lo sabemos con certeza.

Ya hemos cambiado radicalmente de ideas al aceptar el hecho de que tal vez no somos las únicas criaturas vivas del universo; ni siquiera, tal vez, las únicas inteligentes. ¿Ensancharemos algún día nuestras ideas más aún, y nos aceptaremos como ejemplo de uno solo de los distintos y posibles temas vitales? En este caso, hasta es posible que estudiemos, con plena fascinación, la extraña química vital de los fluorcarbonatos o las siliconas calientes, y de los metanos y amoníacos fríos, considerándonos a nosotros mismos como meros ejemplos de intermedios formados por proteínas en agua.

¿Por qué no? En Ciencia, como en todo lo humano, es la posibilidad de lo inesperado lo que da sabor a la investigación.

22. ¿Hay alguien ahí?

Sit, Jessica. Look how the floor of heaven

Is thick inlaid with patines of bright gold;

There's not the smallest orb which thou behold'st

But in his motion like an angel sings,

Still quiring to the young-eyed cherubins.

Such harmony is in immortal souls;

But whilst this muddy vesture of decay

Doth grossly close it in, we cannot hear it[10].

Así habla Lorenzo en El mercader de Venecia, de Shakespeare, como añorando oír la música de las esferas, pero sabiendo que es imposible.

Desde los tiempos del bardo de Avon, el hombre ha superado parte del obstáculo que representa su «envoltura de podredumbre carnal», gracias a nuevos instrumentos: telescopios, espectroscopios, cámaras fotográficas y amplificadores de microondas. Hoy día podemos captar la canción de las esferas en una forma literal, ya que el universo radia ondas extracortas. Traducidas en sonido, parecen ruidos estáticos (los llamados parásitos atmosféricos), ásperos y bruscos, pero para los hechizados oídos de los astrónomos, representan un cántico angélico.

Desde algunas zonas invisibles del cielo llegan a nosotros ondas que no provienen de otras partes. Dos zonas de esta clase fueron descubiertas hacia 1960, incluidas en una lista de fuente de ondas de radio celestes, formulada por el Instituto Tecnológico de California. Según su numeración en dicha lista, a las fuentes en cuestión se les llamó CTA-21 y CTA-102. En 1963, un equipo de astrónomos angloamericanos indicó que tales fuentes eran dignas de estudio, y en octubre de 1964, un famoso astrónomo soviético, Nikolai S. Kardashev, proporcionó algunos resultados de este estudio.

Llegó a la conclusión de que los fenómenos naturales del universo inanimado no podían ser responsables de las radiaciones de CTA-21 y CTA-102. Sugirió, en cambio, que tal vez estuviésemos observando ondas de radio enviadas por seres inteligentes de alta eficiencia tecnológica.

¿Hay que achacar tales declaraciones a la fantasía exaltada de un astrónomo? ¡En absoluto! Es sumamente improbable, naturalmente (como admitió el propio Kardashev), mas no una fantasía. Desde la Segunda Guerra Mundial, los astrónomos se han ido convenciendo de que en las inmensas profundidades del espacio existen seres inteligentes. Esto se debe, principalmente, a las teorías actuales sobre el origen del sistema solar y la vida.

Existen dos teorías generales sobre el origen del sistema solar: la catastrófica y la evolucionista. Según la primera, cuando dos estrellas pasan cerca una de otra, son extraídas de cada una enormes mareas de materia, que se condensa en forma de planetas. Según la segunda teoría, una estrella se forma de una enorme nube de polvo y gas en torbellino, y de la materia extraída de los bordes de esta nube se forman automáticamente los planetas, en tanto que en el centro toma forma una estrella.

Durante la primera mitad del siglo xx, se aceptó generalmente la teoría catastrófica. Pero, a medida que se iba comprendiendo mejor la naturaleza interna de las estrellas, los astrónomos fueron descartando tal teoría. La materia extraída del Sol por medio de una estrella cercana no podría condensarse en forma de planetas, ya que estaría demasiado caliente.

En 1944, el astrónomo alemán Carl F. von Weizsäcker, dio a conocer una nueva versión de la teoría evolucionista que mereció una gran aprobación. Los astrónomos discuten actualmente sobre el modo de contrarrestar varias dificultades, pero de manera virtual todos están de acuerdo en que la teoría evolucionista es más plausible que la otra.

Esto es muy importante para la cuestión relativa a la existencia de otros seres inteligentes. Si los planetas se originasen por medio de cataclismos, habría muy pocos en el universo, ya que las estrellas, virtualmente, jamás se acercan unas a otras.

Sin embargo, si los planetas se forman como parte de los naturales cambios evolutivos sufridos en la formación de una estrella, serían excesivamente comunes. Prácticamente, cada estrella tendría una serie de planetas, teoría aceptada hoy día por los astrónomos.

¿Cuántos planetas pueden ser semejantes a la Tierra para que sean moradas de la vida, tal como la conocemos nosotros? El doctor Stephen H. Dole, de la Corporación Rand, trató de contestar a esta pregunta sobre la base de los conocimientos actuales.

En nuestra galaxia, la Vía Láctea, indicó, se calcula que existen 135.000 millones de estrellas. De éstas, no obstante, sólo las de cierto tamaño pueden ser soles de planetas convenientes, como el nuestro. Dichos planetas, además, han de poseer ciertos tamaños, estar a cierta distancia de su estrella, girar con un determinado período de rotación, etc., antes de poder ser considerados semejantes a la Tierra.

Tomando en cuenta todas estas consideraciones razonables, el doctor Dole concluyó que existen unos 640 millones de planetas semejantes a la Tierra esparcidos por nuestra galaxia.

Si dichos planetas se hallasen distribuidos regularmente por toda la galaxia, el más cercano se hallaría a 27 años-luz de distancia (equivalente a 255 billones de kilómetros). Dentro de un radio de cien años-luz en torno a la Tierra, habría entonces unos cincuenta planetas semejantes a ella.

¿Podrían tales planetas contener vida? La conclusión es ciertamente afirmativa. Experimentos recientes parecen demostrar que la vida no es un accidente casual derivado de una combinación casual de productos químicos, sino que tiende a originarse allí donde las condiciones son similares a las de la primitiva Tierra (ver Capítulos 20 y 21).

Mas, ¿cuántos de esos planetas soportarían una vida inteligente?

La ciencia tropieza aquí con el mayor escollo. Es imposible predecirlo. La vida en la Tierra existía ya desde dos o tres mil millones de años antes de que se desarrollaran especies inteligentes. Y, ¿no pudo ser este desarrollo un accidente casual? ¿No hubiera sido posible que la vida hubiese continuado existiendo en la Tierra sin desarrollar inteligencia?

No conocemos la respuesta a esa pregunta (y el doctor Dole no se aventuró a sacar conclusiones), pero aunque la inteligencia sólo se presente en uno entre un millón de planetas apropiados para mantener vida, todavía tendríamos casi mil especies inteligentes diseminadas por esta galaxia. Y en este caso, sus actividades tal vez podrían ser conocidas si nosotros prestamos oído atento y sutil, especialmente, si por algún motivo, dichas especies inteligentes tratan de hacerse oír. No es probable que oigamos algo semejante prestando atención al universo, pero tampoco es imposible.

Si quisiéramos enviar un mensaje a una forma de vida de un planeta que orbite en tomo a una estrella, o recibir un mensaje de aquél, algunas señales tendrían que cruzar vastos abismos espaciales. Por nuestra parte, recibimos tres clases distintas de señales desde el espacio exterior. Las mismas son:

1. efectos gravitatorios;

2. corrientes de partículas subatómicas; y

3. radiación electromagnética.

De las tres, la fuerza de la gravedad nos llega a nosotros con más fuerza desde el Sol y la Luna. Nuestra órbita en tomo al Sol es una respuesta a la gigantesca atracción ejercida por él, y las mareas oceánicas son la respuesta a la atracción de la Luna. En los pequeños movimientos de nuestro satélite podemos observar las fuerzas de atracción ejercidas por Venus y Marte.

Sin embargo, la fuerza de gravedad es la más débil de la Naturaleza, y llega hasta nosotros desde las otras estrellas de manera tan disminuida, que prácticamente es imposible detectarla. Tampoco podríamos enviar un haz gravitatorio aunque fuese una fuerza más potente, puesto que ignoramos el modo de encender y apagar (hablando en términos eléctricos) la gravedad, con el fin de enviar un código basado en puntos y rayas de gravitación, por ejemplo.

Las corrientes de partículas subatómicas nos llegan en forma de protones y electrones desde el Sol, y en forma de rayos cósmicos (protones de energía muy elevada y partículas más macizas cargadas eléctricamente) desde el espacio exterior. Nosotros podemos producir estas corrientes de partículas con bastante facilidad, y hasta detener y reiniciar tales corrientes, pero sólo en cantidades mínimas.

Aunque pudiésemos producirlas con la fuerza suficiente para que llegasen de estrella a estrella, no podríamos enviarlas al espacio en una línea perfectamente dirigida.

Las líneas de partículas cargadas eléctricamente se curvarían y desviarían al pasar a través de los campos magnéticos que llenan el espacio. Además, junto con partículas sin cambiar, quedarían absorbidos y cambiados por la atmósfera que indudablemente rodea a todo planeta semejante a la Tierra.

Un tipo de partícula subatómica, el neutrino, no padece ninguna de estas desventajas. Podría viajar en línea recta de estrella en estrella, sin quedar afectado por la gravedad, los campos magnéticos o las atmósferas. Por desgracia, esta partícula es casi imposible de detectar.

Esto deja a la radiación electromagnética sola, de la cual dos tipos penetran en nuestra atmósfera. Uno es la luz ordinaria, y el otro las ondas de radio de alta frecuencia, de una clase denominada usualmente «microondas». Ambos son fáciles de producir, fáciles de detectar, no quedan afectados por los campos magnéticos ni las atmósferas y, en resumen, son casi ideales para este fin.

De los dos tipos, la luz podría ser la primera elección. Es fácil imaginar un faro inmenso enviando señales en morse a las estrellas. Pero también esto tiene sus dificultades.

Primero, existen infinitas fuentes luminosas en la galaxia, considerando sus miles de millones de estrellas, de modo que una señal diminuta se perdería entre ellas. Especialmente, la luz originada en un planeta distante, se vería absorbida por la luminosidad más potente de su sol. Cierto que esto puede ser discutido. Supongamos que el rayo de luz procediese de un láser gigantesco (ver Capítulo 11). La luz característica de un láser podría diferenciarse de la de una estrella y hasta la sola existencia de la luz láser podría considerarse como un signo de inteligencia al otro extremo. Otra sugerencia más atrevida aún es que una civilización suficientemente avanzada podría aprender a usar las estrellas como proyectores. Así, algunos quasars (ver Capítulo 19) varían su intensidad lumínica con el tiempo. ¿No podrían algunos superseres utilizarlos para enviar una especie de código morse? No es muy probable, añadiré al momento, pero resulta interesante meditar sobre ello.

Otra dificultad que ofrece la luz es que no puede penetrar la espesura del polvo cósmico que invade grandes zonas de nuestra galaxia. Nosotros no logramos divisar el glorioso estallido de luz de los miles de millones de estrellas del centro de la Vía Láctea por culpa de las nubes de polvo que lo ocultan.

Lo cual deja el camino expedito para las microondas. Éstas atraviesan impunemente las nubes de polvo cósmico, y nosotros podemos detectarlas sin molestias si proceden del centro de nuestra galaxia.

Las fuentes de microondas del cielo («radiofuentes»), algunas de las cuales son visibles por la luz que emiten, aun cuando la mayoría todavía no se haya asociado con objetos visibles) se hallan en menor cantidad que las fuentes luminosas. Esto hace que una radiofuente sea más fácil de descubrir que una luz. Además, una radiofuente poderosa, de un planeta, no quedaría absorbida por su sol, ya que muy pocas estrellas son potentes emisoras de microondas.

Es fácil medir la longitud de las ondas individuales del haz de microondas que llega desde el espacio exterior. En casi todas las radiofuentes, la «longitud de onda» es asunto de metros. Sin embargo, para los fines de una comunicación, sería preferible utilizar microondas más cortas.

Las longitudes de onda deberían ser de 7 a 14 centímetros, idóneamente. Estas ondas sufrirían menos distorsiones o interferencias en sus largos viajes, y no quedarían ahogadas por otras fuentes naturales de microondas.

Por esto, las emisiones recibidas desde CTA-21 y CTA-102 despiertan tanto interés. Las microondas recibidas de tales fuentes pertenecen primordialmente a la extensión de 10 a 50 centímetros, con una cúspide de 30 a lo sumo. Esto no es ideal, pero sí es suficiente, y mejor que lo que se obtiene de otras radiofuentes. Además, como afirman los mejores astrónomos, estas microondas surgen de una «fuente celeste diminuta», como procedentes de un planeta. En el caso de radiofuentes normales, el origen se halla más extendido, indicando que la fuente es un gran volumen gaseoso.

Si las emisiones de microondas de CTA-21 y CTA-102 son el producto de vidas inteligentes, han de representar civilizaciones muchísimo más avanzadas que la nuestra.

Actualmente, la Humanidad terrestre produce energía con un promedio de 4.000 millones de kilovatios. Aunque toda la empleásemos en un proyector de microondas y la enviésemos al espacio, sería insuficiente. El haz se esparciría y diluiría, aunque lo fabricásemos lo más coherente posible, y cuando llegase a los seres inteligentes más próximos, sería demasiado débil para poder ser detectado.

Para producir haces bastante poderosos de una perfecta captación, se necesitaría una civilización capaz de producir mucha más energía que la nuestra.

La producción de energía de la Humanidad crece en la proporción de un 3 a un 4 por ciento anual. Si nada lo impide, dentro de 3.200 años produciremos energía al promedio del Sol, y entonces podremos anunciar nuestra existencia por medio de haces que cruzarán toda la longitud y la anchura de nuestra galaxia. Y si nosotros podemos detectar ahora los haces de otras formas de vida, es porque las mismas se hallan varios miles de años más avanzadas que nosotros en tecnología.

En realidad, no hay que tomar demasiado en serio los casos CTA-21 y CTA-102. Se trata de objetos sumamente distantes, probablemente quasars, y sin duda sus emisiones en microondas pueden explicarse sin suponer la existencia de vida inteligente allí.

Sin embargo, supongamos que alguna inteligencia de una estrella próxima intenta llegar hasta nosotros. O supongamos que nosotros pretendemos llegar hasta otras formas de vida. ¿Qué hay que decir en los anuncios enviados o recibidos? No podemos utilizar el morse ni esperar que una inteligencia foránea hable inglés. Por tanto, tenemos que buscar algo universalmente comprensible. Podemos presumir, por ejemplo, que los habitantes de cualquier civilización sepan matemáticas, y que todos los teoremas y postulados que aquí son exactos, también lo son en el resto del universo.

Por ejemplo, supongamos que enviamos dos pulsaciones de microondas seguidas por dos más y después cuatro. Más tarde, tras una larga pausa, enviamos tres, tres y nueve; luego, volvemos al primer grupo, y así sucesivamente. De esta forma, tendríamos el siguiente mensaje:

2,2,4… 3,3,9… 2,2,4… 3,3,9.

Si desde algún lugar del espacio, recibíamos el mensaje 4,4,16, aunque sólo fuese una vez, habríamos establecido una perfecta comunicación.

También podríamos intentar el lenguaje universal de la química. Existe un número fijo de tipos de átomos estables, los mismos en todo el universo. Cada tipo diferente está formado por una combinación definida de dos clases de partículas: protones y neutrones.

El más simple, el hidrógeno-1, tiene un solo protón mientras que el siguiente, el hidrógeno-2, posee un protón y un neutrón. Por consiguiente, pondríamos números que representasen los diferentes átomos en un orden de complejidad creciente: Podríamos empezar con el hidrógeno-1 y el hidrógeno-2. Así, emitiríamos 1 y 1-1. Después, seguiríamos con el helio-3 (2-1), helio-4 (2-2), litio-6 (3-3), y litio-7 (3-4).

Supongamos que repetimos la combinación número 1:1… 1-1… 2-1… 2-2… 3-3… 3-4… una y otra vez. Una inteligencia exterior que recibiese esta serie de combinaciones podría reconocerla como representativa de la estructura de los primeros átomos más simples y radiar señales sobre los siguientes átomos de la lista: berilio-9 (4-5) y boro-10 (5-5). En cuyo caso, se habría establecido la comunicación.

Sería posible asimismo intentar un abordamiento geométrico. Podríamos enviar una serie de pulsaciones rápidas entre las cuales hubiese una pausa, otra serie, y así sucesivamente. Cada serie tendría una fórmula diferente de pulsaciones especiales.

Al ser grabadas las series una sobre otra, las pulsaciones especiales podrían combinarse para formar un círculo u otra figura. De este modo, podrían transmitirse simples teoremas de geometría; un triángulo rectángulo con cuadrados a cada lado indicaría que el cuadrado de la hipotenusa es igual a la suma de los cuadrados de los dos catetos.

De esta forma podrían también enviarse dibujos que indicarían seres humanos con cuatro extremidades, con dos de ellas para sostenerse; que existían en forma de dos sexos, etc. Si la respuesta llegaba en dibujos semejantes, se habría establecido realmente la comunicación.

Tales comunicaciones serían terriblemente lentas, claro está, puesto que un planeta capaz de contestar podría hallarse en cualquier parte de nuestra galaxia, a miles de años-luz de distancia. Supongamos que la inteligencia detectada estuviese a 500 años-luz, suposición que, si acaso, es demasiado optimista.

En este caso, las ondas de radio, o cualquier otra forma concebible de señales informativas, tardarían quinientos años en viajar desde aquí hasta allí. Y transcurrirían otros quinientos años antes de recibir la respuesta.

¿De qué serviría un diálogo entre dos civilizaciones, con intermedios de mil años?

En primer lugar, el mero hecho de existir el diálogo revestiría una importancia tremenda. La Humanidad sabría que no es la única inteligencia, ni siquiera (muy probable) la máxima inteligencia del universo, lo cual ejercería un efecto profundo sobre la religión y la filosofía, las costumbres y nuestra aproximación al mundo que nos rodea.

En segundo lugar, ni nosotros ni ellos tendríamos necesidad de esperar una respuesta para continuar comunicando. Podríamos variar nuestros mensajes a voluntad una vez establecida la comunicación. Ellos harían lo mismo y el resultado final sería una conversación completa, consistente en comentarios que invocarían una respuesta futura, y en respuesta para comentarios sobre el pasado.

La espera tampoco sería inútil. Podría, en cambio, ser extremadamente fructuosa. Si enviásemos dibujos sencillos, podrían ir acompañados cada uno con el equivalente a una señal del morse. El dibujo de un hombre iría acompañado por las señales que indican hombre. Los hombres en diferentes actitudes podrían ser hombre andando, hombre de pie… y otros detalles que nos sugiriera nuestro ingenio.

En quinientos años podríamos enviar una gran cantidad de señales, y si la inteligencia fuese superior a la nuestra, no habría dificultades en interpretar nuestro código. Empezando con un vocabulario dado, ni siquiera se necesitarían más dibujos para poder deducir el significado de las palabras que no entendiesen.

Una vez transcurridos los quinientos años, cuando se iniciase la serie de respuestas, comprenderíamos que ellos lo habían captado todo fácilmente y tan sólo al cabo de un siglo, quizás, habían aprendido a expresarse en un idioma terrestre…

Es posible que incluso las formas más simples de comunicación con seres inteligentes sirviesen para fertilizar mutuamente el reino de las ideas. Si alistásemos las combinaciones de protones y neutrones, ellos podrían contestar de vez en cuando con una lista diferente de los átomos y, quizás, en la nueva lista hallaríamos una nueva regularidad que ahora no conocemos.

Ni siquiera es necesario suponer una información directa y específica. El mero hecho de la comunicación interestelar ya nos ayudaría en nuestro progreso tecnológico.

El esfuerzo de enviar haces cada vez más potentes, con mayor eficacia, o de detectar haces cada vez más débiles, nos haría progresar por senderos que tendrían otras aplicaciones, aparte de la comunicación con las estrellas.

Asimismo, el esfuerzo para concentrar la máxima información posible en unos cuantos símbolos nos alentaría a ahondar más en la teoría de la información. Al intentar llegar a las mentes extrañas de seres inteligentes situados a muchos años-luz, nos perfeccionaríamos quizás hasta poder comunicarnos con nuestros delfines terrestres. Más importante aún: el hombre podría aprender a comunicarse más eficazmente con el prójimo. Y esta sola consecuencia ya justificaría todos los esfuerzos realizados para ponernos en contacto con seres espaciales.

Aún queda una pregunta: ¿Es peligroso? ¿Es prudente atraer la atención de alguna supercivilización hacia nosotros? ¿Qué ocurriría si los chimpancés atrajeran nuestra atención hacia un fértil continente, donde ellos fuesen la forma de vida más elevada? ¿No trataríamos de conquistar tal continente, eliminando a los chimpancés sin el menor remordimiento?

Bien, quinientos años-luz es una enorme distancia a cruzar a cualquier nivel tecnológico, ya que cada cruce tardaría un mínimo absoluto de quinientos años en completarse. Y esta distancia sola ya nos salvaría.

Además, ¿estamos seguros de que una inteligencia espacial sólo pensaría en destruirnos? Hasta nosotros, una especie capaz de perpetrar los crímenes nazis, hemos llegado al punto de lamentar la extinción de cualquier clase de vida no inteligente, y haríamos todo lo posible para conservar a los chimpancés en su ambiente. ¿Han de ser unos seres supercivilizados menos decentes que nuestros instintos imperfectos? ¡No! Yo creo firmemente que un contacto de mentes a través de los grandes abismos del espacio sólo podría producir buenos resultados, no malos.

23. Anatomía de un marciano

Las condiciones son tan diferentes en Marte y -para nuestros sentimientos terrestres- tan inferiores a las de la Tierra, que los científicos confían en que allí no haya vida inteligente. Si en Marte existe algún asomo de vida (probabilidad mínima aunque no completamente negativa) probablemente se parecerá a los vegetales más simples y más primitivos de nuestro planeta (ver Capítulo 20).

Sin embargo, aun concediendo que la probabilidad de una vida compleja es virtualmente inexistente, podemos dejar volar nuestra fantasía. Supongamos que se nos dice llanamente:

–En Marte hay vida inteligente, con una forma burdamente humana.

¿Qué imagen razonable podríamos deducir sobre la base de lo que sabemos de Marte, teniendo siempre presente que las conclusiones a que podamos llegar no han de tomarse en serio, sino solamente como una graciosa fantasía?

En primer lugar. Marte es un mundo pequeño con una fuerza de gravedad igual a dos quintos de la terrestre. Si el marciano ha de ser un ser huesudo, sus huesos serían considerablemente más ligeros y esbeltos que los nuestros y no obstante podrían soportar una masa de materia semejante a la nuestra (consecuencia mecánicamente inevitable por la disminución de peso). Por tanto, aunque el torso tuviese el volumen humano, las piernas y los brazos de un marciano nos parecerían grotescamente delgados.

Los objetos caen más lentamente en un campo de gravedad débil, por lo que los marcianos podrían tener los reflejos más lentos. De esta forma, nos parecerían lentos y torpones (y debido a su lucha menor contra la gravedad podrían vivir más tiempo). Como los objetos son menos pesados en la superficie de un mundo de poca gravedad, el marciano tendría posiblemente más estatura que nosotros. Su columna vertebral no tendría por qué ser tan rígida como la nuestra y podría ostentar dos o tres articulaciones como nuestros codos, lo cual le permitiría agacharse con más facilidad, a pesar de su gran estatura de más de dos metros.

La superficie marciana, según los Mariner, está erizada de cráteres, mas las irregularidades seguramente no serían tan marcadas para un ser situado en su superficie. Entre los cráteres y en el interior de éstos, probablemente existan muchos desiertos. Nubes amarillas oscurecen la superficie, según sabemos, y por los años veinte, el astrónomo E. M. Antoniadi interpretó dichas nubes como tormentas de polvo. Caminar sobre arenas movedizas significa que el pie marciano (como el de los camellos terrestres) sería plano y ancho. Este tipo de pie, junto con la escasa gravedad, impediría que se hundiese en la arena.

Los pies podrían ser triangulares, con tres dedos separados por 120°, y membranas entre ellos. (Ninguna especie terrestre los tiene así, pero ello no es imposible. Los reptiles voladores, ya extintos, como los pterodáctilos, poseían alas membranosas, ligeramente semejantes a las de los actuales murciélagos, procedentes de una sola línea de huesos.) Las manos también serían triangulares, con tres dedos cada una, igualmente separados. Si los ligeros «dedos» del dedo fuesen numerosos, el dedo marciano equivaldría a un tentáculo. Cada uno podría terminar en un bulto romo (como el del lagarto terrestre llamado gecko), con muchas extremidades nerviosas, como la uña humana convirtiéndolo en un excelente órgano del tacto.

El día y la noche marcianos son tan largos como los nuestros, pero Marte se halla mucho más lejos del Sol, y carece de océanos y de atmósfera densa que le sirvan de depósitos de calor. La temperatura de la superficie marciana, por tanto, varía desde los 70° centígrados, en el mediodía ecuatorial, a unos 130° grados bajo cero de la misma escala termométrica, al final de la helada noche. El marciano necesitaría una capa aislante. Ésta podría consistir en una doble piel; la exterior, callosa, córnea, dura, impermeable al agua, como la de los reptiles terrestres: la interior, suave, flexible, muy rica en vasos sanguíneos, como la del hombre terrestre. Entre ambas dermis habría un espacio aéreo, que el marciano hincharía o deshincharía a voluntad.

Por la noche, dicho espacio estaría lleno y el marciano parecería una pelota. El aire encerrado serviría de aislante, para proteger el calor propio del cuerpo. Durante el caluroso día, el marciano deshincharía su cuerpo, pudiendo de esta forma perder calor con más facilidad. Durante este período deshinchado, la piel externa se juntaría en unos pliegues verticales, como un acordeón.

La atmósfera marciana, según los datos obtenidos por los Mariner, es extremadamente tenue, con tal vez una centésima de la densidad de la nuestra, constituida casi exclusivamente por dióxido de carbono. Así, el marciano no respiraría ni tendría nariz, aun cuando poseyera una ranura fuertemente musculada, probablemente en el cuello, por la que hincharía o deshincharía el espacio aéreo.

El oxígeno requerido para fabricar sus estructuras orgánicas tendría que obtenerlo de los alimentos. Le costaría mucha energía la obtención de este oxígeno, energía que para éste y otros fines podría captar directamente del Sol.

Podemos imaginarnos a un marciano provisto de una extensión en forma de capa, constituida por tejido vivo, unida tal vez a la columna vertebral. Ordinariamente, esta capa estaría plegada junto al cuerpo, a fin de no destacarse demasiado.

Durante el día, no obstante, el marciano pasaría unas horas al Sol (las nubes son poco frecuentes en la seca atmósfera de Marte), con la capa plenamente extendida, semejante a un par de alas ampliamente desplegadas a cada lado. Su rica provisión de vasos sanguíneos quedaría así expuesta a la acción de los rayos ultravioleta del Sol, que serían absorbidos a través de la piel tenue y traslúcida.

La energía conseguida por este método podría utilizarla en la noche para efectuar las necesarias reacciones químicas de su organismo.

Aunque el Sol está a gran distancia de Marte, la atmósfera marciana es demasiado tenue para absorber una gran parte de sus rayos ultravioleta, por lo que el marciano recibiría más rayos de este tipo que nosotros. Sus ojos estarían adaptados a esta absorción, y su principal par, centrados en su rostro, serían pequeños, como ranuras, para impedir la entrada de una radiación excesiva. Podemos intuir dos ojos delante, como en los seres humanos, puesto que se necesitan dos para conseguir la visión tridimensional.

Es probable que el marciano estuviese adaptado a la existencia subterránea, ya que las condiciones son mucho más equitativas bajo tierra. Por tanto, cabría esperar que el marciano tuviese asimismo dos ojos grandes, uno a cada lado de la cabeza, para poder ver en una iluminación más débil. Su función estribaría principalmente en detectar la luz, no en calcular las distancias, por lo que podrían estar situados uno a cada lado de la cabeza, como en los delfines terrestres (animales sumamente inteligentes), pudiendo de este modo quedar sacrificada la visión tridimensional con una luz débil. Los ojos podrían ser sensibles a los rayos infrarrojos, para que el marciano pudiese ver al prójimo por medio del calor irradiado. Esos ojos de visión disminuida serían lo bastante grandes para que la cara de un marciano resultase más ancha que larga. De día, naturalmente, estarían fuertemente cerrados debajo de unos párpados de piel dura, apareciendo sólo como meros bultos.

La atmósfera tenue transporta muy mal los sonidos, y si los marcianos quisieran oír, necesitarían unas orejas muy largas, en forma de trompetilla, como las de los conejos, aunque capaces de moverse independientemente, para abrirse y cerrarse a voluntad (por ejemplo, durante las tormentas de polvo).

Las partes de su cuerpo expuestas a la intemperie, como los brazos, las piernas, las orejas y algunas del rostro, que no quedaran protegidas por la concha exterior, podrían estar recubiertas de plumas, para conservarse caliente durante la helada noche.

La comida del marciano consistiría principalmente en simple vida vegetal, que sería dura y correosa, pudiendo incorporar sílice a los componentes de su estructura. El caballo terrestre posee unos dientes dispuestos para triturar hierbas duras y ásperas, pero el marciano tendría que tener unos dientes más extremados aún. La boca del marciano, por consiguiente, podría contener placas de sílice detrás de una abertura redonda, que se expandería y contraería como el diafragma de una cámara. Las placas funcionarían como un molino, triturando las plantas más duras.

El agua es la gran necesidad de Marte. Toda su provisión acuática es igual a la contenida en el lago Erie, según los cálculos del astrónomo Robert S. Richardson. En consecuencia, el marciano acumularía el agua consumida, sin desperdiciarla como sudor ni orina, por ejemplo. Sus desperdicios tendrían una forma absolutamente seca, y seguramente serían liberados con consistencia, aun con formaciones químicas, como las de un ladrillo terrestre.

La sangre marciana no transportaría oxigeno, por lo que no contendría hemoglobina, absorbente de ese gas, que colorea la sangre de los seres de la Tierra. La sangre marciana, pues, sería incolora. La piel marciana, adaptada a los rayos ultravioleta y absorbiéndola como fuente de energía, no contendría ningún pigmento para rechazarlos. O sea que el color del marciano sería cremoso.

La capa extensible para absorber la luz, particularmente destinada a la absorción de la ultravioleta, podría reflejar la luz visible de onda larga por inútil. La luz reflejada tendría un color amarillento. Lo cual haría que el marciano (cuando estuviese ocupado en absorber la energía de la radiación solar) fuese una asombrosa criatura de alas doradas y algunas plumas en su cuerpo.

Y aquí termina nuestra fantasía, con una visión de formas marcianas no muy distinta de las fantasías terrestres con respecto al aspecto de los ángeles.

24. Sobre los platillos volantes

Como frecuentemente me he entregado a especulaciones relativas a la vida extraterrestre (ver Capítulo 20 a 23 inclusive), y como es sabido que soy un escritor de ciencia-ficción, me han preguntado a menudo si yo «creo» en los platillos volantes. Naturalmente, mi interlocutor siempre espera que yo crea en ellos. Y por «creer» en los platillos volantes, el individuo preguntón se refiere usualmente a los vehículos espaciales maniobrados por inteligencias no humanas.

Bien, permítanme aclarar mi posición, pues no deseo que mis escritos se utilicen como base de un punto de vista que considero necio.

Yo no creo en los platillos volantes en el sentido de considerarlos vehículos espaciales guiados por extraterrestres. Como expliqué en capítulos anteriores, no existe virtualmente la menor probabilidad de que exista vida inteligente en ningún otro planeta del sistema solar, y las formas más próximas de vida inteligente capaz de manejar vehículos espaciales han de estar, si acaso, a muchos, muchísimos años-luz.

Afirmar que indudablemente existe vida inteligente en algún lugar invisible del universo (como creo firmemente), no es lo mismo que decir que dichas formas inteligentes nos visiten a grandes oleadas en unos vehículos espaciales disfrazados de platillos volantes, que nosotros vemos constantemente, según diversos informes, pero que jamás entablan el menor contacto con la Tierra.

La energía necesaria para un viaje interestelar es tan inmensa, que para mí resulta inconcebible que unos seres que pilotasen sus naves a través de las grandes profundidades del espacio, sólo lo hiciesen para jugar con nosotros durante unos años, y repetir la misma experiencia unas décadas más tarde. Si quisieran entrar en contacto, lo harían; de lo contrario, ahorrarían energía y se marcharían a otra parte.

Indudablemente, existen personas sinceras que entienden como perfectamente legítimos los fenómenos insólitos. Es posible que no se trate de vehículos espaciales (y yo estoy seguro de que no lo son), pero otras cosas aparte de las naves espaciales merecen ser investigadas. Indudablemente también, los científicos reaccionarían con más entusiasmo e investigarían con más ardor, si las pasadas experiencias no les dijesen que la historia de los platillos volantes está llena de fraudes, engaños, errores y contradicciones. Claro que esto no es culpa suya.

Por tanto, sin tachar a nadie de crédulo, debo afirmar que, hasta que un vehículo espacial con una dotación no humana sea exhibida en carne y metal (unas luces celestes, por muy misteriosas que sean, no son suficientes), yo continuaré suponiendo que cualquier visión de platillo volante es un error, un fraude, o algo que tal vez pueda explicarse mediante una teoría que no se relacione con los vehículos espaciales de las distantes estrellas.

4. LA VIDA FUTURA

25. El mundo de 1990

Predecir el futuro es una tarea imposible, muy poco agradecida, en la que se comete el más espantoso de los ridículos, y a menudo se obtienen solamente burlas y menosprecios. Sin embargo, como yo he escrito ciencia-ficción durante más de un cuarto de siglo, se espera de mí esta predicción, y sería una cobardía tratar de evadirla.

Para realizarla, no obstante, con toda impunidad, debo adivinar lo menos posible, y limitarme en lo posible a las condiciones que ciertamente existirán en el futuro, tratando de analizar las probables consecuencias. Consideremos, por ejemplo, la población de nuestro planeta.

Actualmente hay en la Tierra más de tres mil millones de habitantes[11]. Para las tres naciones más importantes del mundo, las cifras de población son aproximadamente de setecientos millones para China; doscientos cincuenta millones para la URSS, y doscientos millones para Estados Unidos.

¿Cuál será la situación dentro de una generación, o sea, hacia 1990, suponiendo que no iniciemos una guerra termonuclear? Es virtualmente seguro que la población mundial habrá crecido al menos en un 60 por ciento. La de Estados Unidos tal vez haya llegado a los trescientos millones de habitantes, al menos.

Muy bien, estudiemos el asunto. ¿Cómo será la vida cotidiana en América hacia 1990, ante tal explosión demográfica? Una de las consecuencias más obvias es la necesidad de conservar los recursos del planeta, no por idealismo sino por propia estimación. El aire es inagotable, por ejemplo, mas para que sea útil ha de estar limpio. El problema de la polución atmosférica ya es grave hoy día, y en 1990 será irrespirable por el humo y los gases del escape de los coches en la atmósfera, tal como hoy lo sería una cloaca vertida en los depósitos de agua de una ciudad.

Es posible que esto influya en los individuos en forma de prohibiciones sobre el fumar al aire libre. Probablemente se descubrirá que la solución atmosférica (incluyendo el humo del tabaco descargado de los pulmones de centenares de millones de fumadores) contribuye al cáncer de pulmón y al de piel, incluso entre los no fumadores.

Por tanto, es posible que se limite el fumar a los «fumadores», donde los habituales a esta costumbre podrán entregarse a ella a su placer sin afectar al resto de la población.

Hacia 1990, los apartamentos se hallarán cada vez más provistos de aire filtrado. La antigua expresión «aire fresco» tal vez será remplazada por la expresión «aire crudo», el cual se considerará poco beneficioso para los pulmones delicados, especialmente en las zonas urbanas.

El agua también es inagotable, mas no así el agua fresca. Actualmente ya existe demanda de agua fresca. De todos modos, es probable que antes de que termine este siglo se hayan descubierto métodos factibles para la desalinización del agua marina, o sea, que, en principio, el agua fresca es inagotable. Mas seguramente el agua de mar desalinizada será mucho más cara que el agua fresca natural. Hacia 1990, todavía lo será demasiado para otros usos que no sean guisar o beber, de manera que la lucha contra la contaminación del agua también revestirá un carácter grave.

Las fuentes de energía aún no significarán ningún problema inminente en 1990. Con un poco de suerte, quizá no llegue a ser ningún problema en absoluto. El petróleo y el carbón aún surgirán de la tierra, y las plantas de fisión nuclear serán muy comunes. El gran problema de disponer de los residuos atómicos con seguridad se habrá solucionado, según todas las probabilidades. (Yo creo que se conseguirá mezclando los residuos con bloques de cristal que podrán ser arrojados a las minas de sal o a las profundidades oceánicas.) Quizá se hayan efectuado ya experimentos con plantas de energía, basadas en la fusión del hidrógeno, en algún lugar del planeta, y se hablará ya considerablemente de las plantas de energía solar.

No es tan fácil ser optimista con los minerales. Las necesidades mundiales se elevarán agudamente y algunos depósitos minerales ya se hallan críticamente agotados. Los grandes recursos, por no aprovechados todavía, son el fondo de los estratos continentales donde, en algunos casos, aún yacen nodulos de compuestos metálicos para su extracción. Con los dragados oceánicos se explotarán estos recursos hacia 1990.

Lo que influirá más sobre el hombre medio serán las presiones del suelo y el espacio vital. No existen soluciones fáciles al problema de albergar a más gente en las ciudades, aunque opino que en 1990 asistiremos a un cambio en este sentido. El movimiento no será ya construir hacia arriba, como los rascacielos edificados por dos generaciones, sino hacia abajo. No se trata necesariamente de una idea grata para los acostumbrados a vivir en espacios abiertos, mas tal vez sea inevitable, acabando la gente por aceptar sus ventajas.

La gente ya vive y trabaja en colmenas, rodeada constantemente por luz artificial y aire acondicionado. Apenas conoce hoy día la diferencia el empleado que de repente es trasladado al subsuelo. Consideremos, asimismo, que la temperatura del subsuelo cambia tan poco que habría menos problemas para la refrigeración en verano o la calefacción en invierno. Si se edificase toda una ciudad subterránea, el tráfico no se vería jamás alterado por la lluvia o la nieve. La producción aumentaría en eficiencia, puesto que los turnos de labor diarios se adaptarían mejor a un ambiente donde es mínima la diferencia entre el día y la noche.

Además, la superficie de la Tierra no se hallaría directamente obstaculizada por una ciudad. El terreno situado encima de una urbe podría dedicarse a parques de recreo, a granjas y a pastos. Sin embargo, ni siquiera en 1990 se hallará este plan proyectado a un futuro próximo. De todas formas, se construirán más a menudo casas y factorías bajo tierra.

Las presiones de la población tornarán menos deseables las zonas de la superficie terrestre que hoy día lo parecen. Los que puedan permitírselo, se retirarán a la soledad de las montañas, donde los medios de comunicación y transporte los mantendrán en contacto con la Humanidad sin estar sujetos a agrupamiento físico.

La selva albergará menos terrores, ya que los grandes carnívoros o se habrán extinguido o se hallarán definitivamente en vías de extinción, y los insectos, lombrices y microorganismos más mortales, se hallarán bajo control.

El uso creciente de la energía nuclear empezará a abrir las costas árticas, señalando el camino de la colonización del vacío continente de la Antártida.

Posiblemente aún resultará más asombroso el inicio de un movimiento en dirección de los declives continentales.

Se duplicarán con toda seguridad bajo el agua las ventajas de vivir subterráneamente, con la posibilidad para quienes gustan de los deportes náuticos que, en ese caso, sólo tendrán que salir de casa para satisfacer este afán.

Bajo el agua también será posible conseguir la comida en el patio delantero del hogar, lo cual antaño sólo podían permitírselo algunos americanos. Tal vez en 1990 se construya un hotel submarino, seguramente en Miami, Florida.

Las presiones debidas al exceso de población aún no habrán impulsado a la Humanidad a ir en busca de otros planetas. Quizás exista ya una colonia en la Luna, compuesta de turnos de personal altamente especializado y entrenado, y es fácil que haya proyectos respecto al aterrizaje del hombre en Marte. El hombre medio de la calle, no obstante, se hallará aún muy lejos de poder viajar por el espacio libremente en 1990. Mas la era espacial estará, claro está, mucho más adelantada (ver Capítulo 30).

Las mayores aglomeraciones humanas de 1990, a pesar de los movimientos iniciales hacia el subsuelo y bajo el agua, y hasta hacia la Luna, se hallarán en las mismas ciudades que conocemos hoy; ciudades que, por entonces, serán muchísimo mayores. La costa oriental de Estados Unidos, en su parte norte, donde se asientan Nueva York, Baltimore y Filadelfia, será en realidad una sola ciudad de unos cuarenta millones de habitantes.

Para mantener a tantos millones cómodamente alojados, se necesitarán enormes refinamientos en el transporte y las comunicaciones. Abundarán los garajes, tanto arriba como debajo de la superficie terrestre. Su eficacia se acentuará mediante el uso creciente de coches de dos asientos para utilización personal. (Supongo que los impuestos se elevarán agudamente respecto a los automóviles de gran volumen, a fin de alentar el empleo de otros más pequeños.)

Los vehículos personales estarán separados de los comerciales en lo posible. La calle elevada será una cosa corriente en los centros congestionados de las inmensas urbes, siendo utilizadas por los coches pequeños, en tanto que los autobuses y camiones estarán confinados a lo que hoy día es una calle normal.

Los repartos de mercancías efectuados a base de helicópteros obtendrán una gran popularidad. Los edificios más modernos de 1990 tendrán en sus tejados pequeños heliódromos, aunque quizás ello sea tanto por prestigio y vanidad como por su posible empleo. Asimismo, existirá una tendencia creciente a utilizar tubos de aire comprimido para el servicio de Correos. Las oficinas postales estarán casi por completo automatizadas. Supongo que al menos en los grandes edificios se recibirá la correspondencia por medio de impulsos de aire, entregada a los apartamentos individuales por medio del mínimo contacto personal.

También los Metros serán cada vez más automatizados y, en 1990, habrá una acentuada tendencia a las cadenas continuas de Metros: una larga serie de coches que cubrirán toda la longitud de una línea, con amplias curvas a cada extremo. Esto, naturalmente, quedará limitado a las líneas cortas, aunque los ingenieros estudiarán ya la fórmula para su aplicación a las más extensas, con diversas soluciones en controversia respecto a la manera de subir y apearse de la cadena constantemente en movimiento, y a los métodos de interconectar las cadenas separadas.

Entre las ciudades, el constante decrecimiento del ferrocarril habrá dado lugar a camiones y autobuses de un tamaño y capacidad sorprendentes. Todos llevarán sus remolques y las carreteras deberán soportar tales monstruos. Tendrán sus carriles especiales y sus entradas y salidas de carreteras adecuadas a su tamaño y peso.

En 1990, las carreteras tendrán un tráfico menos denso, aparte del comercial. Aumentará el uso de helicópteros, aunque todavía serán preponderantes los vehículos terrestres. Éstos, corriendo sobre colchones de aire comprimido, y no sobre ruedas, no necesitarán carreteras asfaltadas, puesto que podrán correr igualmente sobre caminos vecinales de tierra o a campo traviesa (cuando el terreno no resulte demasiado desnivelado o tenga obstáculos creados por la mano del hombre), y sobre el agua.

El vehículo terrestre requerirá indudablemente cambios radicales en las regulaciones de tráfico. Uno de los motivos de irritación en 1990 será la altanería de los conductores de los mencionados vehículos de aire comprimido (en particular los adolescentes) hacia los derechos particulares de los ciudadanos. Me imagino que existirá la tendencia entre los irascibles terratenientes a levantar obstáculos en sus propiedades, y si un joven se mata a causa de tales obstáculos, el causante del mal podrá acogerse a alguna nueva ley.

Tal vez el efecto más formidable del crecimiento desmesurado de la población se halle en relación con los alimentos. Estados Unidos no experimentará el hambre que hoy día azota a gran parte del mundo, pero tendrá que mostrarse mucho más consciente de la comida, produciendo más alimentos nutritivos y de menos lujo. Existirá menos especialización en la dieta personal, con tendencia a prescindir de la carne y alimentarse de pescado y cereales.

Platos hoy día considerados poco gratos al paladar formarán parte de la dieta, aunque sólo sea sobre una base experimental (puesto que se necesita la amenaza del hambre para que la gente renuncie a sus prejuicios nutritivos…, y a veces ni con tal amenaza se logra). Las algas marinas son un buen ejemplo de lo que tal vez se sirva en los restaurantes. También habrá gusto por las semillas marinas y las levaduras. Estos alimentos se venderán en los atestados supermercados, y se les dará artificialmente sabor a carne, hígado o queso. En 1990, estos sabores artificiales todavía dejarán mucho que desear.

Otro problema, aparte del crecimiento de población, será el continuo impulso hacia el extremo de la mecanización y el automatismo. Esto será especialmente cierto en Estados Unidos que, naturalmente, continuará viviendo a base de electrodomésticos.

Esto afectará al ama de casa, desde la compra hasta la consumición final. El supermercado de 1990 tendrá los artículos codificados. La compradora marcará los números clave de los productos deseados en unas tarjetas apropiadas, utilizando mostradores larguísimos como guías. Su pedido, debidamente empaquetado, comprobado y valorado, llegará hasta ella a los pocos instantes.

Casi todos los artículos comestibles estarán preparados para ser cocidos con un mínimo de injerencia humana.

La cocina misma podrá parecerse a la cabina de un bombardero a propulsión. En efecto, en 1990 habrá casas de apartamentos que ofrecerán una cocina en comunidad para uso de los inquilinos (igual que existen hoy día lavanderías en común), puesto que se eliminará el despilfarro de cocinas sueltas en cada apartamento. O sea, que se comerá «de restaurante» hasta en casa.

El problema del servicio doméstico continuará sin solucionarse, y la sustitución del mismo por medio del robot casero no aliviará aún la situación. Lo que la aliviará será la constante tendencia a reducir los quehaceres que requieren el empleo de criadas (o de músculos del ama de casa). El creciente uso del aire filtrado disminuirá el problema del polvo. El lavado mediante vibraciones ultrasónicas, además (o en lugar) del jabón, hará que tal tarea sea mucho más rápida y fácil.

El automatismo también cambiará el ambiente fuera del hogar. Cada vez se desvanecerá más el uso de los músculos o las rutinas cerebrales. Habrá, como siempre, ocupaciones creadoras, claro está, y necesidad de jefes, administradores y dependientes. También habrá una gran tendencia a tener que tratar, de un modo u otro, con los ordenadores y sus consecuencias.

Por este motivo, la educación incluirá en su mayor parte las Matemáticas y las Ciencias. En la escuela primaria comenzará ya a enseñarse la Aritmética binaria y el lenguaje de los ordenadores. La educación personalizada y detallada del contacto entre maestro y estudiante quedará reducida sólo a dos clases de niños: los retrasados mentales y los muy inteligentes.

El mayor problema personal creado por el automatismo será el tiempo libre. La mayoría trabajará solamente treinta horas semanales, por lo que todo el mundo estará sujeto al aburrimiento en gran escala. Existirán más diversiones y recreos, y jamás en la historia de la Humanidad habrá sido tan bien remunerada la profesión de actor o cantante.

La televisión será, naturalmente, el centro del hogar, y el teléfono también será motivo de esparcimiento. En 1990, los aparatos irán provistos de un artilugio que permitirá ver, además de oír, al interlocutor. Un ama de casa podrá divertirse mucho más que ahora, siempre que se halle en condiciones de ser vista adecuadamente. Será quizá cosa corriente acudir apresuradamente a la peluquería por la necesidad de tener que llamar más tarde a una amiga. (Y entre las amigas se producirán indudablemente grandes tensiones cuando una esté muy elegante y retocada, y la otra vaya mal vestida y con el pelo alborotado.)

Esta clase de teléfonos puede también revolucionar la labor bibliotecaria. En 1990, las grandes bibliotecas poseerán libros corrientes y populares en microfilmes. Todas las escuelas y muchos hogares tendrán microfilmes visuales.

Las grandes bibliotecas estarán organizadas de forma que el cliente pueda visualizar los microfilmes por teléfono. De esta forma será posible buscar una referencia y obtener una información sin tener que salir de casa o de la oficina.

El negociante o el industrial visionará documentos y recibirá informes por «videoteléfono». Incluso podrán concertarse conferencias mediante teléfonos con pantallas divididas, y el dinero empleado se economizará de los viajes que no serán necesarios (salvo en el gran número de casos en que el viaje de negocios es sólo un pretexto para estar lejos del hogar o divertirse a cuenta de la oficina).

En el mundo de 1990 también se hará una intensa propaganda del deporte como una diversión sana y consumidora del tiempo libre. Supongo que la gran novedad en deporte será el vuelo. Pequeños motores, montados a la espalda, elevarán al hombre a cierta altura. En 1990, este deporte no será barato ni común aún para constituir un medio de transporte, aunque sí será adecuado como deporte y emoción. (¿Será un niño actual el que organizará la primera partida de «aire-polo» utilizando una esfera hinchada con helio, de material muy delgado de plástico como pelota?)

Los cambios que tendrán lugar entre la hora actual y 1990 convencerán a la gente de que la Humanidad no puede continuar reproduciéndose a ciegas, sino que debe de ser debidamente canalizada. Hoy día hay muchos que están ya convencidos de lo esencial que resulta un control de natalidad eficaz si hemos de salvar a la civilización. Los tales aún se hallan en minoría, mas no será así en 1990.

En efecto, en aquellos años, se adoptarán medidas gubernamentales para el control de natalidad en casi todo el mundo. Los adelantos en la efectividad de este control no lograrán impedir el 60 por ciento de crecimiento mundial de 1990, mas habrá llegado a un punto en que el promedio de individuos menores de veintiún años será mucho menor que en la actualidad.

Esto traerá consigo un cambio en la actitud social hacia los niños y la familia, aunque quizá no se trate de un cambio de carácter uniforme. En algunas zonas y algunos sectores de la sociedad, el número relativamente pequeño de niños aumentará su valor, haciendo que la sociedad promueva hacia ellos más consideraciones todavía. En otras zonas y sectores, el reconocimiento de la explosión demográfica como principal peligro para el hombre tornará a los niños impopulares y a la paternidad vagamente antisocial. Los lazos familiares tenderán a disolverse y el matrimonio perderá terreno en favor de otros tipos de unión personal más adecuados a las necesidades de la época.

Y si 1990 ve el comienzo de un equilibrio demográfico o incluso un decrecimiento de las poblaciones, el escritor de aquel día preverá el mundo de 2090 con consistencia evidente y considerable optimismo.

26. La Exposición Universal de2014

La Exposición Universal de Nueva York en 1964-65 estuvo dedicada a «La paz por medio de la Comprensión». Sus atisbos del mundo del mañana eliminaban la guerra nuclear. ¿Por qué no? Si tiene lugar una guerra termonuclear, no vale la pena hablar del futuro. De modo que es preferible que dejemos que los cohetes duerman eternamente en sus moradas y que observemos cómo puede ser el mundo no atomizado del futuro.

Lo que sucederá, al menos visto con los ojos de las exposiciones, es maravilloso. La dirección hacia la que se dirige el hombre debe ser considerada con gran esperanza, y más que en ninguna parte en el pabellón de la «General Electric», de la Exposición Universal de 1964. Allí, los espectadores asistieron a cuatro escenas, cada una animada por unos muñecos de vida aparente (incluyendo un perro que es el alma del espectáculo),

Las escenas, referidas a 1900, 1920, 1940 y 1960, mostraban los adelantos de las aplicaciones eléctricas y los cambios que han introducido en la forma de vida. Y de haber presentado los años 1980, 2000 y sucesivamente, ¿qué habríamos visto? Naturalmente, no lo sé, aunque me lo imagino.

Si consideramos algunos de los cambios mencionados en el Capítulo 25, y otros no mencionados allí, ¿cómo será la Exposición Universal del 2014-15?

Un desarrollo altamente probable es que los hombres continuarán apartándose de la Naturaleza, a fin de crear un ambiente que les siente mejor. En 2014, serán de uso común las paredes electroluminosas. Los techos y los muros resplandecerán suavemente, con una gran variedad de colores que cambiarán apretando un botón.

Las ventanas ya no serán más que un recuerdo arcaico, mas cuando estén presentes estarán polarizadas para impedir la entrada excesiva de la luz solar. Incluso podrá alterarse el grado de opacidad del cristal de manera automática, de acuerdo con la intensidad de la luz que incida sobre el mismo. Éste será todavía un lujo, y los seres mortales más corrientes aún no gozarán de tales aparatos en sus domicilios. La Exposición de 2014 será, no obstante, una sinfonía de electroluminosidad, con poquísimas ventanas en sus estructuras.

Había en la Exposición de 1964 una casa subterránea que a mí me pareció un símbolo del futuro. Si sus ventanas no estaban polarizadas, podían en cambio alterar la «escena» mediante cambios de luz. Existen ciertas ventajas en la vida subterránea (ver Capítulo 25), y en la Exposición de 2014, el «Futurama» de la «General Motors» presentará vistas de ciudades subterráneas completadas con jardines y huertos de luz artificial.

Los aparatos continuarán aliviando a la Humanidad de las tareas más pesadas, y en el tercio final del siglo XXI, que acaba de empezar, veremos la llegada de la criada robot. En 2014, los robots no serán ni muy buenos ni muy abundantes todavía, pero ya existirán.

El pabellón de «IBM» de la Exposición Universal de 2014 puede exhibir, en lugar preferente, una criada robot, grande, torpona, de movimientos lentos, pero capaz de coger, disponer, limpiar y manipular diversos utensilios.

Indudablemente, será muy divertido ver cómo los espectadores esparcen colillas y otros residuos por el suelo y contemplar al robot agacharse para recogerlo todo y clasificarlo en dos apartados: «lo que hay que tirar» y «lo que hay que guardar aparte». (También aparecerán los jardineros-robot.)

La «General Electric» de la Exposición Universal de 2014 presentará películas en 3-D de su «Robot del Futuro», bellos, aerodinámicos, con los aparatos de la limpieza como aplicaciones de su cuerpo, y realizando todas las tareas rápidamente. (Naturalmente, se formarán colas larguísimas para presenciar la película, porque algunas cosas jamás cambiarán, y las colas es una de ellas.)

A principios del siglo XXI, las necesidades de energía de la Humanidad se compensarán mediante las fuentes nucleares, incluso en cantidades pequeñas. Los aparatos de 2014 no tendrán cables eléctricos, por ejemplo, sino que funcionarán mediante baterías de larga vida a base de radioisótopos. Estos no resultarán caros, puesto que serán los subproductos de las plantas de energía de fisión, hacia 2014, y lograrán satisfacer más de la mitad de las necesidades de energía de la Humanidad.

En 2014 ya existirán ciertamente una o dos plantas de energía de fusión, y la Exposición de aquel año estará orientada en este sentido. Ya la de 1964 pudo demostrar una genuina explosión de fusión, mas en la Exposición de 2014 habrá modelos adelantados de plantas de fusión y aparatos que permitirán la producción de la suficiente energía eléctrica para mantener el funcionamiento de tales modelos de manera constante. («Una electricidad producida gracias a la fusión».)

En 2014 operarán unas estaciones de energía solar en algunas zonas desiertas o semidesiertas, como Arizona, Negev, Kazakstan…, donde la luz solar es invariable y fija. En las zonas neblinosas de las grandes ciudades la energía del sol será menos práctica, y estará muy adelantado el intento de derivar la recolecta de tal energía al espacio.

Un pabellón de la Exposición de 2014 presentará modelos de estaciones de energía en el espacio, donde se captará la luz solar por medio de enormes proyectores parabólicos, radiando después la energía conseguida hacia la Tierra.

Dentro de cuarenta y cinco años el mundo se habrá encogido todavía más. En la Exposición de 1964, la «General Motors» exhibía, entre otras cosas, «factorías para construcción de carreteras y calles» en los trópicos, y los visitantes de la Exposición podrán viajar en una «acuabalsa», que se levantará sobre cuatro zancos y se deslizará sobre el agua con un mínimo de fricción. En realidad, la mecánica del transporte avanza ya a pasos agigantados y continuará avanzando.

En 2014, a mi entender, se habrán inventado los vehículos con «cerebro-robot» que podrán ser orientados hacia una meta dada, a la que se dirigirán sin interferencias, sin estar condicionados a los lentos reflejos de un conductor humano. Supongo que uno de los mayores atractivos de la Exposición de 2014 serán los recorridos por la misma en coches robotizados, que funcionarán por entre la muchedumbre a un nivel de medio metro (sostenidos por chorros de aire comprimido), evitándose unos a otros limpia y automáticamente.

Para los viajes cortos, las aceras móviles (con bancos a los lados, y espacio en el centro para estar de pie) harán su aparición en los sectores pobres de las ciudades; y ciertamente, las aceras de la Exposición Mundial de 2014 estarán todas mecanizadas.

Las comunicaciones también estarán harto adelantadas, y los satélites sincronizados posibilitarán que los habitantes de este planeta se llamen entre sí, desde cualquier distancia, con las menores molestias. Esto ya no será ninguna novedad en la Exposición del año 2014. Mas…, ¿y la Luna?

Ciertamente, en aquella fecha habrá ya una estación permanente en la Luna, y las conversaciones entre el satélite natural y la Tierra tendrán lugar a base de rayos láser modulados (ver Capítulo 11), que son más fáciles de manejar en el espacio. Si la colonia lunar puede colaborar, los visitantes de la Exposición de 2014 podrán sostener una conversación real con los colonos de la Luna.

Estas conversaciones, ciertamente, no resultarán muy cómodas, puesto que transcurrirán 2,5 segundos entre la pregunta y la respuesta. (Es el tiempo que tardan la luz y las ondas de radio en efectuar el recorrido de ida y vuelta.) Conversaciones similares con Marte experimentarán una demora de 3,5 minutos, incluso en el perigeo de aquel planeta con la Tierra. Sin embargo, es difícil que en el año 2014 puedan celebrarse conversaciones entre Marte y la Tierra. Puesto que solamente naves dirigidas, sin tripulación humana, habrán aterrizado allí, aunque estará en progreso una expedición humana a cargo de la «NASA», que en la Exposición Universal ya presentará una elaborada colonia marciana.

En cuanto a la televisión, las pantallas murales remplazarán a los ordinarios televisores, y harán su aparición los cubos transparentes. En los videocubos, con la ayuda del láser se conseguirán hologramas que nos presentarán vistas tridimensionales. En realidad, uno de los modelos de la Exposición de 2014 será una Televisión en tres dimensiones, de tamaño natural, donde se presenciarán funciones de ballet. El cubo girará lentamente para poder ser apreciado desde todas sus caras.

En esta feliz especulación será posible seguir indefinidamente, pero no todo será tan grato.

Mientras yo hacía cola para penetrar en el pabellón de la «General Electric», en la Expo-1964, me detuve a contemplar el signo de la Vida Equitativa, que parpadeaba hacia la población de Estados Unidos, con el número (que entonces superaba a 191.000.000), incrementándose en una mitad cada 11 segundos. Durante el tiempo que yo estuve on el pabellón de la «General Electric», la población americana aumentó casi en trescientas personas, y la población mundial en seis mil.

En 2014, es probable que la población mundial haya alcanzado los 6.500.000.000 de habitantes, y que la de Estados Unidos sea al menos de 350.000.000. La agricultura ordinaria soportará los requerimientos de comida con grandes dificultades, si no sufre un completo colapso, y muchas «granjas» cultivarán los eficaces microorganismos.

La Exposición Universal de 2014 presentará la prueba de esto, con un modelo al menos de Bar de Levadura, donde servirán «pavo-falso» y «pseudo-bistec». No serán muy malos (si uno puede pagar el precio), pero habrá una gran resistencia psicológica a tales innovaciones.

Resulta sumamente claro que en 2014 (ver Capítulo 27), habrá que controlar eficazmente la explosión demográfica. En todo el mundo se habrán adoptado severas medidas de control, y se intentará por todos los medios hacerlas aún más populares.

Uno de los pabellones más importantes de la Exposición de 2014, por consiguiente, se dedicará a conferencias, películas y material documental del Centro de Control de la Población Mundial (sólo para adultos, con exhibiciones especiales para adolescentes).

Y en el éxito de tales conferencias y películas descansarán las posibilidades de que haya en la Tierra una Exposición Universal en 2064…, o tal vez en cualquier mundo civilizado.

27. Fecundidad limitada

En los dos capítulos anteriores he apuntado los desastres que aguardan a la Humanidad si continúa de manera indefinida el crecimiento de la población al promedio actual. Algunos expertos, en cambio, presumen que la «ciencia» ya hallará el medio de paliar este peligro; que por muy poblado que esté el mundo, los científicos ya hallarán el medio de alimentar, alojar y divertir a todos sus habitantes.

¿Es cierto esto?

Preguntemos antes: ¿hasta qué punto puede crecer la Humanidad en la Tierra y cuánto tardaremos en alcanzar el límite máximo?

Bien, tratemos de ser optimistas.

Supongamos que la energía no es ningún problema y que la fusión del hidrógeno y la energía solar satisfarán nuestras necesidades. Supongamos, asimismo, que hemos resuelto la fotosíntesis artificial y que conseguimos los alimentos precisos extrayéndolos del agua y del aire, tal como hacen los vegetales. Supongamos que solucionamos todos los problemas de organización en un planeta superpoblado (desde disponer de los vastos residuos hasta atemperar las tensiones raciales). Supongamos todavía que logramos aniquilar toda vida competitiva a fin de disponer del máximo espacio posible.

Suponiendo todo esto, ¿cuál es el límite al crecimiento de la población humana? Bien, hay algo que no podemos evitar si nos atenemos solamente a nuestro planeta. Más pronto o más tarde, comenzará a faltar uno de los constituyentes químicos del cuerpo humano, hasta el punto de que no quedará en toda la Tierra ni siquiera lo suficiente para formar otro ser.

Actualmente, el elemento menos abundante, que probablemente será el primero en agotarse si la Humanidad crece sin límites, es el fósforo. Sin embargo, démosle un respiro a la Humanidad a este respecto, y consideremos el carbono, un componente más abundante de la vida desde el punto de vista de su disponibilidad en masa, y veamos qué conclusiones extraemos de tal consideración.

Naturalmente, no todo el carbono de la Tierra se encuentra en un estado fácilmente disponible para las formas vitales. Pero empecemos por estudiar solamente el «carbono disponible».

El noventa por ciento del carbono disponible se halla en los océanos en forma de iones bicarbonatados. Una pequeña cantidad se halla en el aire como dióxido de carbono, y el resto se contiene en los seres humanos o en los restos corrompidos de criaturas vivas en otros tiempos.

A esto puede añadirse el ingente contenido de carbono del petróleo y el carbón terrestre, puesto que por combustión tales elementos se convierten rápidamente en dióxido de carbono, que se mezcla con el aire o se disuelve en el mar y resulta disponible para la vida.

La cantidad total de carbono presente en la Tierra en esas formas es de 56.000.000.000.000.000.000 gramos (equivalentes a cincuenta y seis billones de toneladas).

Esta cantidad es verdaderamente inmensa, mas no nos frotemos aún las manos, ya que el 90 por 100 de este carbono ha de reservarse para las provisiones alimenticias del hombre (suponiendo que no se haya visto ya reducido al canibalismo). Al fin y al cabo, el hombre ha de comer, y ha de ingerir alimentos que contengan carbono, ya crezca en el suelo como en depósitos químicos, ya sea en forma de carne, de trigo, de levaduras o en una mezcla de componentes químicos nutritivos. Y para conceder un margen de salvación, así como para la producción de sustancias no comestibles, como textiles, plásticos y otros artículos se necesita una provisión de alimentos orgánicos superior en diez veces a la masa de la Humanidad. Esto aún nos deja con más de cinco billones de toneladas de carbono que pueden ser incorporadas eficazmente a los seres humanos.

Supongamos nuevamente que el ser humano normal de la Tierra (incluyendo los niños) pesa cincuenta kilos. Cada uno contendrá una cantidad de carbono equivalente al 18 por 100 de su peso total. Lo cual son unos 8.000 gramos.

La cantidad de seres humanos necesarios para agotar el diez por ciento de todo el carbono disponible de la Tierra sería de 630.000.000.000.000.

Esta cifra, seiscientos treinta billones, ciertamente empequeñece en alto grado nuestra población actual de tres mil millones, dando, al parecer, un amplio margen de expansión, causando la impresión de que el punto del máximo potencial se halla aún muy lejos en el futuro. ¿Es así?

La Tierra duplica normalmente su población cada medio siglo, mas seamos conservadores y pongamos cada ochenta años. Si continúa este promedio duplicativo, en unos 1.500 años, o sea hacia el año 3500, habremos llegado a dicho máximo. A la sazón, la materia viva de la Tierra estará formada exclusivamente de seres humanos junto con sus provisiones necesarias de comida y subproductos orgánicos.

Si la población terrestre se diseminase regularmente por su superficie, cada persona poseería exclusivamente medio metro cuadrado en el año 3500, incluyendo Groenlandia, la Antártida, la cuenca del Amazonas y el desierto del Sahara. A lo que yo denomino apretujamiento.

Todo el mundo estará de acuerdo en que ningún aumento de la capacidad científica logrará jamás convertir en tolerable tal densidad, ni siquiera algo menos. Por consiguiente, si la explosión demográfica continúa sin trabas, se abatirá sobre la Humanidad una crisis intolerable en menos de 1.500 años, por mucho que haga la ciencia.

Bien, supongamos, y sigan las suposiciones, que gracias a un adelanto científico inimaginable, incluso será posible vivir con tanta densidad humana. ¿Qué más después?

Como dije antes, en la Tierra hay más carbono del que se dispone ordinariamente. Lo hay unido a la caliza y otros materiales que forman la corteza terrestre. Este carbono no se halla generalmente al alcance de los seres vivos hasta que los lentos procesos geológicos lo conducen al mar o al aire. Pero seamos optimistas. Supongamos que la Humanidad consigue cavar en la corteza terrestre hasta poder aprovecharse de todo el carbono.

La cantidad de carbono de la corteza terrestre es unas quinientas veces superior a la del aire y el mar, por lo que la Humanidad podría multiplicarse quinientas veces más que en el año 3500.

Esto daria a la Tierra una población total de 300.000.000.000.000.000, o sea, trescientos mil billones de habitantes. Si los mismos se diseminasen regularmente sobre la superficie del Globo (y esta vez podemos incluso suponer que los océanos están cubiertos de tablas de extremo a extremo para sostener a las multitudes), cada individuo tendría derecho solamente a un octavo de palmo cuadrado donde estar. En realidad, la Humanidad estaría apretujada como sardinas en lata.

¿Cuánto tardaría la Humanidad en incorporar todo el carbono de la Tierra, disponible y no disponible, en sus cuerpos y sus alimentos? Sólo siete siglos después del 3500. O sea, que en 4200 habríase llegado al final absoluto del carbono.

Mas, ¿por qué limitamos a nuestro pobre y pequeño planeta? La era espacial ya ha comenzado. La ciencia da tremendas zancadas al frente. El infinito espacio nos llama. En él hay sitio para cualquier número de seres humanos, por lo cual no necesitamos preocupamos por la explosión demográfica.

¿Estamos de acuerdo?

En nuestra galaxia hay unos 135.000.000.000 de estrellas, y tal vez unos 100.000.000.000 de galaxias en el universo conocido. Supongamos ahora que todas las estrellas del universo conocido están rodeadas por diez planetas, capaces de soportar la vida como en la Tierra.

Sigamos suponiendo que no existe el menor problema respecto al traslado de la Humanidad a cualquier otro planeta del universo en un momento dado. Con sólo chascar los dedos…, ¡lista la ocupación universal!

Entonces, ¿cuándo podremos disponer del universo, en la misma medida que la Tierra, en el año 4200? ¿En qué año, los terrícolas se hallarían apretujados como sardinas en lata en toda la superficie de cada uno del par de trillón de trillones de planetas?

Hablando en términos generales, hacia el año 11000.

En resumen, al presente promedio de aumento de población, el Homo sapiens puede llenar el universo hasta el límite tolerable de permanencia en cuestión de nueve mil años.

Por tanto, no existe sitio y la ciencia no puede remediar nada. El promedio de aumento de población debe decrecer, y esto puede lograrse de dos maneras: o aumentando el promedio de muertes o rebajando el promedio de nacimientos.

A elegir.

28. El precio de la vida

El incremento sin tasa de la población no es el único peligro con que se enfrenta la Humanidad. Otro mucho más sutil es el impulso de alargar la existencia y hasta la inmortalidad. ¿Qué ocurrirá si la población se estabiliza en cantidad y el individuo vive eternamente? Actualmente existen organizaciones que tienden a propagar la idea de la congelación o hibernación de los recién fallecidos o de los moribundos. Se trata de resucitar los cuerpos helados cuando la ciencia haya aprendido cómo curar las enfermedades que padecían aquéllos, reconstruir sus organismos, rejuvenecerlos y restaurarles la vida. Es decir, convertirnos todos en nuevos Lázaros.

¿Por qué no? ¿Tenemos algo que perder? Si la ciencia nunca aprende a recuperar la vida, la salud y la juventud, no estaremos más muertos de lo que significará la congelación, y al menos moriremos con una esperanza. Si la ciencia lo aprende, seremos esencialmente inmortales.

¿Quién puede quejarse de un juego en que la posible ganancia es infinita y la posible pérdida nada? Bien, lo más gracioso es que yo sí puedo quejarme. Porque la ganancia es cero y la pérdida infinita.

Y al afirmar tal cosa, no pienso en el individuo, aunque incluso en su caso la inmortalidad no sea lo que parece.

Tal vez resultase bonito llevar una túnica blanca, con un halo y volar sobre calles doradas todo el día, cantando hosannas y aleluyas en un coro perfecto.

Hay una extraña alquimia en la eternidad. Es capaz de transformar lo más bello y mejor en un fastidio. Nada puede escapar a esto. Es el cansancio de todas las cosas, buenas y malas.

Si tratamos sólo del plano individual, este problema podría tal vez solucionarse. Después de todo, no necesitamos vivir eternamente de manera absoluta. No es posible obligar a nadie a estar vivo.

Si uno desea abandonar el mundo de los vivos en una sociedad de inmortales en potencia, puede hacerlo. En esta sociedad, el verdadero clima de la vida sería la muerte civilizada. Incluso podría haber centros especiales donde se celebrase el equivalente de un banquete antes de morir, una última celebración, un último beso a los seres amados que aún no se habrían marchado de este mundo, un adiós y un último apretón de manos a los fieles amigos.

Después, con el acompañamiento de una música suave, y entre los últimos apretones de manos y los besos finales, un compartimiento se cerraría detrás de nosotros, un gas penetraría en el mismo… y fin.

En otras palabras, la inmortalidad no significa «eternidad», sino «el tiempo que uno quiera». ¿Cuánto tiempo es éste? Naturalmente, varía de individuo a individuo. Somerset Maugham, el famoso escritor que falleció en 1965 a los noventa y un años de edad, ansiaba la muerte, pero era viejo, y estaba doliente y ciego. En una sociedad inmortal, es fácil que mostrásemos el vigor y la fortaleza de la juventud durante toda la existencia. ¿Cuánto tiempo esperaría un joven sensible e inteligente en desear la muerte como término al cansancio?

Si era afortunado o bastante listo para enfrentarse ventajosamente con la vida y sus problemas, si dirigiese los negocios de la Humanidad o guiase el asalto del conocimiento sobre lo ignorado, o destilase la belleza del universo, seguramente no se aburriría rápidamente y duraría mucho tiempo antes de llegar al último apretón de manos.

¿Seguimos suponiendo? ¿Quinientos años por término medio? Los estadistas del mundo, los científicos, los artistas, los sabios, serían unos vigorosos multicentenarios y en esto, sólo en esto, estriba el verdadero peligro para la Humanidad.

El cerebro de un individuo es de primordial importancia para la Humanidad sólo cuando tiene menos de treinta y cinco años. Si por entonces no ha dado señales claras de gran talento, es difícil que lo haga más adelante. Si por entonces, en cambio, ya ha dado tales pruebas, probablemente se pasará el resto de su existencia aprovechando los grandes conceptos de su juventud. De morir a los treinta y cinco años, otros hombres menos dotados podrían aprovecharse de los mismos conceptos sin grandes dificultades.

Isaac Newton tenía veinticinco años cuando meditaba ya sobre sus grandes descubrimientos de física. Albert Einstein contaba veintiséis cuando elaboró la teoría de la relatividad. Charles Darwin apenas había cumplido los veintidós cuando emprendió el famoso viaje en el Beagle y realizó las observaciones de las que más tarde extrajo las teorías sobre la evolución por selección natural. Y así una y otra y otra vez.

Esto no significa que los ancianos no hayan jamás realizado grandes obras (el mayor logro de Winston Churchill lo realizó a los sesenta y cinco años), o que no haya existido algún ocasional «florecimiento tardío» (Joseph Conrad empezó a escribir a los treinta y siete años). Sin embargo, casi todos los grandes adelantos de importancia para la Humanidad, los grandes cambios de rumbo, los han efectuado los jóvenes.

Lo cual es muy natural. La mente humana se endurece rápidamente. Esto no tiene nada que ver con el deterioro físico del cerebro o su capacidad limitada, y el problema no se desvanecería si imaginásemos una sociedad de inmortales con unos cerebros que permaneciesen físicamente jóvenes. Una vez el cerebro ha desarrollado una forma de pensamiento, ésta se abre una ruta rápida por entre las circunvoluciones, y se necesita un esfuerzo máximo para desarraigarla de allí.

El gran físico Max Planck dijo que el único medio de formular una nueva y asombrosa teoría aceptable para la ciencia era fabricarla, demostrar que era útil y válida, y aguardar a que dejasen de existir todos los científicos viejos.

Sólo la mente joven, sin huellas, esencialmente en blanco, que todavía no ha chapoteado por el barrizal de los pensamientos arraigados, puede ver una solución realmente revolucionaria. Y, naturalmente, en el curso de unos diez años, el joven revolucionario se convierte en un nuevo ortodoxo. Esto ha sucedido siempre en ciencia, arte, sabiduría y política.

Bien, ¿sería agradable poseer un mundo en que esos aspectos clave de la vida estuviesen dominados por mentes multicentenarias, sin deseos de morir? La muerte es el precio que pagamos por una existencia plena de significado. La muerte abre los caminos. La muerte obliga al viejo y agotado a ceder el terreno a lo nuevo e inteligente. La muerte limpia y prepara la tierra para un nuevo adelanto.

Mas, ¿puede el individuo estar contento con la muerte para sí en favor de la Humanidad en abstracto? ¿Por qué no? Se supone que un hombre debe morir en defensa de su familia o su país. ¿Por qué no en aras de la Humanidad? Ningún individuo vive una existencia que en sus menores detalles constituyen el conglomerado de los logros de otros hombres que viven hoy día y vivieron antaño. La vida que la especie le facilita al individuo, éste se la debe a la especie.

Naturalmente, es posible ahondar más en la fantasía y creer que una ciencia omnipotente posibilitaría que, en lugar de suicidarse, una persona cansada de la vida podría realizar un lavado de cerebro para desprenderse de todas las huellas acumuladas. Entonces, volvería a enfrentarse con el universo, con un cerebro fresco, para empezar de nuevo, como la hurí del paraíso musulmán con su virginidad constantemente renovada.

Mas al empezar de nuevo, ¿no habría ya muerto el individuo? Si no se recuerda una vida pasada, el individuo que dicha vida representa está muerto.

Bien, no vayamos tan lejos. El lavado de cerebro podría ser parcial. Podríamos dejar los recuerdos personales básicos, que permitirían la continuidad de la personalidad.

Tal vez podríamos dejar la educación básica, lo cual ahorraría la necesidad de unos nuevos estudios. O sea, que nos limitaríamos a borrar el orín acumulado.

Por desgracia, la educación básica señala ya el camino; la existencia de una personalidad dada ya indica las tendencias. El nuevo individuo, por muy lavado que estuviera cerebralmente, no significaría un progreso sobre el antiguo, por lo que se repetiría siempre en todo lo esencial.

Ni siquiera un blanqueado completo y la aceptación de una muerte mental, pese al apego a una inmortalidad física, serían suficientes. Existe una diferencia básica entre un individuo viejo de cerebro lavado o renovado, y un individuo completamente nuevo. Éste es el producto del viejo, pero tiene dos padres.

Cada niño nace con la mitad de sus genes de un padre y la otra mitad del otro. Su química básica es distinta de la de ambos padres (exceptuando el caso de un parto múltiple) y diferente de cualquier otra persona viva o que haya vivido. El cerebro del recién nacido no es sólo un cerebro lavado, sino un cerebro diferente.

Morimos solos, pero nacemos de una pareja. El sexo no es sólo esparcimiento, sino un método elaborado a través de millones de años como el medio más efectivo para mantener la flexibilidad de la vida ante un ambiente en cambio constante. Lo que necesitamos son individuos nuevos y distintos, no sólo los viejos, lavados y planchados.

Mas, aun concediendo que la inmortalidad del individuo sea la muerte por corrupción y aburrimiento para la especie, ¿no es posible argüir de un modo fatal que las especies también acaban por extinguirse, o sea, que no hace falta sacrificar la inmortalidad personal, ya que es mortal cuanto hacemos? Sí, miles de especies han perecido a pesar de todo cuanto hayan podido hacer sus sexos y sus muertes individuales.

Y sin embargo, si una especie se extingue debido a un alto en su evolución, por medio de la inmortalidad individual, se produce su muerte absoluta. Si, por otra parte, se permite que el sexo y la muerte individual de una especie sigan su curso evolutivo, es posible que, en el caso del Homo sapiens, el hombre sólo se extinga después de haber dado nacimiento a una especie diferente y (es de esperar) mejor que él.

Si la especie ha de extinguirse, es preferible que lo haga dejando tras de sí una especie superior que pueda emprender con más eficacia la eterna lucha contra las tinieblas y conseguir la clase de victoria que hoy día todavía somos incapaces de entrever. Debidamente visualizada, esta muerte de la especie no es una muerte en absoluto, sino otro paso hacia la única inmortalidad posible: la de la vida y la inteligencia en abstracto.

29. La Luna y el futuro

La Humanidad acepta ya como algo inminente la verdadera conquista de la Luna. Siempre ha estado ahí arriba, como un segundo violín del Sol. Cambia de fases, de nueva a llena y vuelve a ser nueva, define el «mes», y ayudó a los hombres a componer los primeros calendarios.

Su efecto físico más notable sobre la Tierra es su capacidad de levantar las aguas del océano hacia sí. Esto produce las mareas, que durante muchos siglos los hombres lo achacaron a todo menos a la Luna.

Cuando se inventó el telescopio, el primer objeto celeste hacia el cual se dirigió fue la Luna. Entonces resultó ser algo más que un objeto brillante, convirtiéndose en un mundo de montañas, cráteres y amplias regiones llanas a las que se llamó «mares».

Pero los subsiguientes estudios telescópicos pusieron en claro que no sólo no se trataba de mares sino que ni tan sólo había una gota de agua en la Luna. Ni de aire.

La Luna, de acuerdo con los astrónomos, era un mundo muerto, un mundo sin cambios. Carecía de aire y, por tanto, de sonidos y de clima. No tenía agua y, por consiguiente, carecía de vida. Siempre había sido así y siempre será lo mismo.

Al menos, esto se decía en los libros de texto de astronomía.

Ahora la era espacial ha llegado ya y el hombre ha abandonado la Tierra para subir hasta la Luna. Sí, como es natural, la Luna ha sido también el primer sueño de conquista espacial del hombre.

Y la Luna ha dado la respuesta que se esperaba: es un mundo muerto, estéril, inhóspito para la Humanidad a no ser en condiciones muy especiales… y muy caras.

¿Debemos mostrarnos desalentados? ¿Hemos de estar amargados ante el hecho de haber gastado miles de millones, haber sacrificado varias vidas, haber realizado esfuerzos increíbles… sólo para esto? Para alunizar en una superficie de polvo y rocas, en un desierto, en el cadáver blanqueado de un mundo muerto.

No, no debemos mostramos desalentados en absoluto. Por el contrario, debemos darle gracias al destino que dispuso el sistema solar de acuerdo con lo único que podía hacer la felicidad de los astronautas.

Considerémoslo.

Si dejamos aparte a la Luna, los cuerpos celestes más próximos a la Tierra son dos planetas. Venus y Marte. El primero jamás se acerca a menos de 39 millones de kilómetros, y el último nunca se aproxima a menos de 55 millones de kilómetros. Intentar, por primera vez, colocar hombres en esos mundos tan alejados sería una empresa formidable, que la Humanidad nunca se habría atrevido a abordar.

Por fortuna, otro cuerpo celeste, la Luna, está mucho más cerca de nosotros. La Luna se halla, por término medio, a sólo 380.000 kilómetros de distancia. O sea, un poco menos del 1/100 de la distancia a Venus en su perigeo, y algo menos de 1/140 de la distancia a Marte, también en su perigeo. Esta distancia representa algo menos de diez veces la vuelta a la Tierra por el ecuador. Más aún, Venus y Marte sólo están en sus perigeos respectivos con la Tierra a intervalos breves, mientras que la Luna nunca se aparta de nosotros.

Astronómicamente hablando, la Luna es nuestra vecina, colocada idóneamente incluso para el más torpón y primitivo de los disparos. Así, en menos de quince años, la Luna ha sido abordada, fotografiada en sus dos caras, estudiada automáticamente y conquistada por los astronautas.

Alcanzar la Luna era exactamente el ejercicio que necesitábamos para desarrollar nuestros músculos espaciales, para aprender las técnicas apropiadas a fin de saber vivir en el espacio y en mundos extraños. Con la experiencia obtenida ya estamos en condiciones de llegar a otros planetas con menos dificultades de las que habríamos encon trado de haber pretendido aterrizar antes en ellos.

Éste es el principal motivo de haber llegado ya a la Luna. Probablemente fuese éste el único camino para aprender a tomar otros y entrar de lleno en la era espacial.

Pero aunque reconozcamos el gran valor de tener la Luna tan cerca, ¿hemos de asombrarnos por ello? Al fin y al cabo, la Luna existe y está ahí. ¿Por qué no aceptarlo? La respuesta a esta pregunta es que, tras estudiar el resto del sistema solar, hemos de llegar a la conclusión de que la Luna, por derecho propio, no debería de estar ahí. Y el hecho de que sí esté es un caso de suerte casi demasiado hermoso para aceptarlo.

En el sistema solar hay treinta y un satélites conocidos, de los cuales veintiocho en torno a cuatro planetas: Júpiter, Saturno, Urano y Neptuno. Se trata de planetas gigantes, todos mucho mayores que la Tierra. Sus campos de gravitación son inmensos y era de esperar que tuviesen satélites. Júpiter, el mayor, posee doce satélites conocidos, y Saturno, que le sigue en tamaño, diez.

Los planetas menores, como la Tierra, con campos de gravedad mucho más débiles, podrían carecer de satélites. Plutón no tiene ninguno, que sepamos; tampoco Mercurio ni Venus. (Venus es un caso realmente interesante porque, con el mismo tamaño de la Tierra, carece de satélites. Si la Humanidad se hubiese desarrollado allí, y no en la Tierra, los viajes espaciales habrían sido prácticamente imposibles). En cambio, la Tierra, cosa sorprendente, posee un satélite: la Luna.

No avancemos tanto. No he mencionado a Marte. Éste, aunque sólo posee un volumen aproximadamente como 1/10 de la Tierra, tiene dos satélites. ¿Y qué más? No basta tener satélites si no tienen el tamaño debido.

Por ejemplo, echemos un vistazo a los doce satélites de Júpiter. Siete de ellos son muy pequeños, de 30 a 50 kilómetros de diámetro cada uno. Probablemente, se trata de diminutos meteoritos captados por el gigantesco Júpiter de entre los asteroides que existen entre él y Marte.

Un octavo satélite tiene 240 kilómetros de diámetro. Los cuatro restantes, son mundos mayores, con diámetro de tres mil a cinco mil kilómetros.

Todos los satélites de Júpiter, puestos agrupados, no obstante, no llegan a 1/5.000 de la masa del planeta. De modo semejante. Marte tiene dos satélites, repito, pero ambos son diminutos, de 8 y 16 kilómetros de diámetro respectivamente. Juntos componen 1/500.000.000 de la masa de Marte.

En general, pues, cuando un planeta tiene satélites, éstos son mucho más pequeños que su planeta. Por tanto, de haber un satélite en la Tierra, lo normal sería suponer (si no lo supiéramos de otro modo), que a lo sumo tendría un diámetro de 50 kilómetros.

Mas no es así. No sólo la Tierra tiene un satélite, sino que se trata de un satélite gigante, de 3.473 kilómetros de diámetro.

En el sistema solar sólo hay siete satélites gigantes. Júpiter posee cuatro; Saturno y Urano uno cada uno. Entonces, ¿por qué la Tierra posee uno? ¡Asombroso! La Luna tiene 1/81 la masa de la Tierra. Ningún otro satélite es tan grande en comparación con su planeta respectivo, como la Luna respecto a la Tierra. Además, la Luna y la Tierra forman un sistema planetario «doble», cosa única en el sistema solar.

Ésta es la suerte increíble que tenemos. No sólo la Tierra posee una Luna que nos sirve para dar los primeros pasos por el espacio, sino que se trata de un satélite gigante, infinitamente más interesante y útil que los pequeños satélites del mundo marciano.

La superficie de la Luna tiene un área de 38.000.000 de kilómetros cuadrados, o sea la superficie de Europa y África juntas. La exploración no puede ser muy rápida.

Naturalmente, ya al principio de la era espacial, dicha superficie se fotografió por delante y por detrás, de cerca y de lejos. Fue posible alunizar aparatos registradores física y químicamente. Y entonces cabe preguntar: ¿por qué el peligro y el gasto de enviar hombres? Aparte del hecho de que los hombres insisten en ir (y tal ha sido el caso de los últimos astronautas, pidiendo que no sea el del Apolo XVII el último viaje a la Luna de este siglo); la curiosidad y el desafío de lo desconocido, y todavía queda en pie el hecho de que ningún instrumento, por muy delicado que fuese, puede sustituir al cerebro humano.

Ignorábamos qué sorpresas nos reservaba la superficie lunar; no sabíamos qué podía haber bajo la sombra de sus cráteres. Y sólo el cerebro humano podía efectuar una exploración adecuada.

Además, las fotografías aéreas no podían revelar hasta el último detalle de nuestro satélite. Incluso, tras haber alunizado diversas veces con pleno éxito, pasarán muchos años antes de que hayan descifrado por completo todos sus secretos, con el estudio de las muestras traídas a la Tierra.

Por tanto, ¿fueron prácticas tales exploraciones? ¿No estuvimos, o estaremos, jugando con la vida de los astronautas? La exploración lunar es práctica. Peligrosa, sí, pero no tanto, como ha quedado demostrado, como la exploración de la Tierra. Los exploradores lunares no han tenido que enfrentarse con tribus hostiles, con animales feroces, ni con bacterias mortales. Sólo han hallado un ambiente inanimado, aunque arriesgado por unos peligros ya calculados de antemano.

En primer lugar, la Luna no tiene atmósfera ni agua, lo cual es general en el espacio exterior. Los astronautas han tenido que ir bien preparados para hacer frente a estas eventualidades, que no eran tales. Sino certezas. Sus trajes fueron diseñados de acuerdo con estos cálculos.

Pasemos a otra pregunta: ¿por qué explorar la Luna? ¿Qué vamos a encontrar? No hay indicios de que existan piedras preciosas, ya que su constitución es similar a la de la Tierra, por cuanto se ha visto. Y aunque se hubiese encontrado, o se descubriera más adelante una mina de uranio en nuestro satélite, su envío a la Tierra resultaría excesivamente caro para intentar su explotación.

Sin embargo, la Humanidad busca algo más que riquezas materiales. Ante todo, obtener conocimientos. Y solamente alunizando allí era posible conseguirlos de primera mano. Y estos conocimientos ya obtenidos, y los que se consigan más adelante, servirán para llegar a conocer mejor nuestra Tierra, para saber mucho más de nosotros.

La Tierra y la Luna se formaron, según los astrónomos, hace más de cuatro mil millones de años, por procesos naturales. Y los científicos desean enterarse exactamente de tales procesos. Podrían encontrar datos en las estructuras internas de la Tierra, mas las mismas han sido ya demasiado cambiadas o destruidas por la acción conjunta del agua, el viento y los seres vivos.

Por ejemplo, la Tierra pudo estar sujeta a la caída de grandes meteoritos a través de toda su historia, pero solamente existen dos o tres huellas de tales caídas, unas depresiones notables en Arizona y Siberia. El cráter de Arizona, por ejemplo, de unos miles de años de antigüedad solamente, está en una región desértica, relativamente a salvo de toda erosión, por cuyo motivo ha sobrevivido tanto tiempo. ¿Y los cráteres más antiguos? Existen algunos restos, mas ninguno puede ser estudiado claramente.

En la Luna, sin embargo, donde los procesos de erosión son mucho más lentos y menos drásticos que en la Tierra todas las señales de su creación han de estar presentes. Gracias a las muestras traídas de la Luna podremos leer su pasado y el nuestro. Descubriremos quizá, por primera vez, cómo se crearon los planetas (y quizá por qué la Luna es un satélite tan grande).

La Luna, además, podría ser el paraíso de los astrónomos. En la Tierra, en la latitud de sus principales urbes, la noche sólo tiene dieciocho horas a lo sumo. La atmósfera empaña las estrellas, y las variaciones de temperatura hacen que sus luces titilen y tiemblen. Las luces de las ciudades apagan el brillo de los luceros celestes; la niebla y las nubes los oscurecen; los humos de las fábricas y la bruma los borran. Nuestros telescopios han de estar situados en regiones aisladas, en las cimas de las montañas, y aún se ven vejados por las viviendas humanas.

Pero la Luna, donde las noches duran dos semanas y no hay humo ni fábricas, es un sitio ideal para estudiar el firmamento. Las estrellas se contemplan fijamente y con gran brillo. También se divisan con suma claridad los restantes planetas. Un pequeño telescopio instalado en la Luna daría cuenta de detalles sobre la superficie de Marte con más exactitud que con el mayor telescopio de la Tierra. Veríamos Marte mejor que las condiciones de una sonda como el Mariner IV.

Desde la Luna también podríamos estudiar el Sol con más detalle. Sus radiaciones no quedan allí obstruidas, y su corona es visible constantemente.

¿No sería posible instalar una estación espacial, o un satélite con instrumentos automáticos? Tal vez, pero la Luna soportaría mucho mejor un gran laboratorio astronómico, y ofrecería más comodidades que una estación espacial, por muy bien acondicionada que ésta estuviese.

Además, la Luna no tiene sustitución para los radioastrónomos. Hace sólo treinta años que los astrónomos comenzaron a interpretar las ondas de radio que llegan a la Tierra desde el espacio, y gracias a ellas han deducido muchos datos interesantes (ver Capítulo 19). Los radioastrónomos ya están inquietos por el creciente uso que los seres humanos hacen de las ondas de radio, ya que puede empañar las débiles señales procedentes del espacio.

Una estación espacial no daría buenos resultados a este respecto, debido a los «parásitos» en torno al planeta. En la Luna, en cambio, el observatorio astronómico podría instalarse en su cara oculta. Con los casi cuatro mil kilómetros de rocas entre el observatorio y la ruidosa Tierra, los astrónomos podrían escuchar en silencio la música de las esferas, totalmente complacidos.

Diez años en la Luna nos enseñarían mucho más sobre el universo que mil en la Tierra.

Está muy bien que los exploradores y los científicos se diviertan en la Luna, pero también nos gustaría a ti y a mí, querido lector, que en la Luna hubiese algo para el hombre ordinario.

Supongamos que los viajes a la Luna se convierten en una rutina. ¿Existe algún motivo para que el hombre de la calle subiese hasta allí?

Claro que sí. Experimentaría la excitación de los lugares extraños, la emoción de un ambiente totalmente nuevo, y la maravilla de países nunca vistos.

El Sol es allí (visto a través de aparatos protectores, o por medios indirectos como la televisión) un objeto terrible, y el firmamento lunar es increíblemente hermoso con sus miríadas de grandes y refulgentes estrellas. Nada, sin embargo, puede allí igualarse a la magnificencia de la vista de la Tierra, muy rutilante en el cielo lunar.

La Tierra vista desde la Luna (que pasa por las mismas fases lunares), es casi cuatro veces más ancha que la Luna, tal como la vemos desde la Tierra. Tiene unas trece veces su área y refleja mucha más luz que nuestro satélite en las noches claras, por lo que la Tierra resulta desde allí setenta veces más esplendente que la Luna vista desde aquí!

Como la Luna siempre presenta una sola cara hacia la Tierra, ésta parece colgar inmóvil en el cielo lunar. (Desde algunos puntos de la Luna, siempre parece estar directamente encima. Desde otros, está baja en el cielo, en una dirección particular. Y naturalmente, nunca se ve la Tierra desde la otra mitad del satélite.)

De vez en cuando, el Sol, en su paso por el cielo de la Luna, pasa por detrás de la Tierra. (Lo que aquí es un eclipse lunar, allí es un eclipse terrestre.) El Sol permanece detrás de la Tierra una hora aproximadamente, y la superficie de la Luna se oscurece, aunque no por completo.

La luz del Sol resplandece en tomo a la Tierra gracias a la atmósfera de ésta, formando un círculo anaranjado a su alrededor, resplandor que se refleja en la Luna. Más allá del círculo anaranjado se percibe la corona solar. Naturalmente, quien viese esto una sola vez no lo olvidaría mientras viviera.

Por encima de todo esto, hay la experiencia de la leve gravedad. La sensación de una mayor ligereza, de poder saltar muy alto (como se ha visto por televisión), constituiría una inmensa novedad. El control del organismo humano en estas condiciones de gravedad no es muy sencillo, por lo que resulta fácil empezar a dar tumbos. El hombre que algún día sepa sostenerse bien en el paisaje lunar podrá reírse del otro que no esté acostumbrado a su gravedad.

En realidad es posible que algunos individuos consideren a la Luna no solamente como un lugar de turismo, sino como un domicilio permanente.

Efectuada la conquista de la Luna, algún día será posible la estancia permanente allí. La Luna podría utilizarse como fuente de materiales y energía, de modo que una colonia lunar podría ser verdaderamente independiente de la Tierra. Las estaciones nucleares basadas en el uranio lunar podrían emplearse para obtener energía, lo mismo que podría usarse la energía solar, nunca allí empañada por las nubes. La agricultura hidropónica, gracias a tal energía, podría proporcionar diversos alimentos.

La Luna, en realidad, no es un mundo muerto, o tanto como se creía. Últimamente se han detectado rastros de actividades volcánicas, por lo que su calor interno podría emplearse como fuente de energía.

Por tanto, si no hay agua ni aire en la Luna, ¿qué pasa en las regiones subterráneas? No es totalmente imposible que haya rastros de aire y agua en las grietas existentes bajo la superficie y, en tal caso, podrían servir para las necesidades de una colonia lunar.

No sería extraño (y esto no se sabrá con toda seguridad hasta haberse examinado minuciosamente todas las muestras lunares, y aún quizá ni entonces), que exista una vida primitiva y microscópica en esas bolsas subterráneas de aire y agua (ver Capítulo 20).

Aunque no haya aire ni agua bajo el suelo lunar, podría obtenerse el hidrógeno y oxígeno (y otras sustancias) necesarios de las mismas rocas, siempre que éstas fuesen la única fuente de energía disponible.

Tal vez llegue un día en que sea posible vaciar las cavernas subterráneas de la Luna y convertirlas en compartimientos estancos. Podrían construirse lentamente ciudades lunares, en las que los hombres y las mujeres vivirían en una comodidad absoluta, sin necesidad de trajes espaciales. Donde nacerían los hijos y pasarían las generaciones.

Los colonos del satélite se adaptarían a la gravedad de la Luna hasta no poder soportar la mayor atracción terrestre. En tal caso, los colonos quedarían ya aislados del planeta paterno. Con este temor, es probable que los colonos tratasen de realizar ejercicios. Por ejemplo, grandes centros centrífugos imitarían la gravedad de la Tierra, y las regulares estancias en tales centros centrífugos mantendrían en forma a los colonos.

La posibilidad de la colonización de la Luna es un aspecto excitante del futuro. En la fortaleza y la creatividad que se enfrentarán con los peligros de una larga emigración a una nueva tierra. Las colonias, estimuladas por la dureza de una frontera, suelen superar a sus naciones de origen. Los antiguos griegos de Asia Menor y Sicilia fueron más ricos que los de Grecia. Los europeos que construyeron Estados Unidos, Canadá y Australia superaron al Viejo Continente.

¿No podría una sociedad establecida en la Luna superamos a nosotros, formar una civilización nueva e inteligente, solucionar sus problemas contra los que nosotros luchamos en vano, y eventualmente regresar para enseñarnos unos caminos nuevos y mejores, como Norteamérica, más de una vez y por métodos diferentes, ha acudido en auxilio de Europa? Precisamente es éste el tema que será objeto de discusión en el Capítulo 31.

30. El sistema solar y el futuro

En menos de quince años, desde que se colocó el primer satélite en torno a la Tierra, los hombres han llegado a la Luna, conquistándola en nombre de la Humanidad y han repetido la hazaña en diversas ocasiones. El hombre ha llegado al extremo de abandonar la cápsula espacial y pasearse por el espacio. Han llegado satélites-sonda a Marte, Venus y Júpiter.

¿Qué nos espera ahora? Si la Humanidad ha avanzado tanto en quince años, ¿adónde iremos en los próximos diez? ¿Dentro de veinte? ¿De un siglo? Por ejemplo, ¿habrá algo todavía que no hayamos hecho ya en el espacio en el año 2100? Empecemos por las exploraciones no tripuladas. La mayor barrera se superó en 1959, cuando, por primera vez, se envió un cohete mandado por el hombre a una velocidad de más de 11,2 kilómetros por segundo. A esta velocidad, un cohete no queda prisionero de la gravedad terrestre. «Escapa» y entra en órbita en torno al Sol. Cuanto mayor velocidad arrastre al cohete, mayor es su órbita en torno al Sol. Mediante un reajuste minucioso de la velocidad de un cohete, por tanto, podemos acercarlo a Marte o Venus, a pesar de que dichos planetas, incluso en sus perigeos, se hallan siempre a muchos millones de kilómetros de nosotros. El Mariner II efectuó un paso a 6.000 kilómetros de Venus en 1962, y el Mariner IV pasó a 9.600 kilómetros de Marte, en 1965.

Como hemos visto recientemente, se ha llegado a determinar la velocidad de un cohete a fin de que pase junto a Júpiter, y lo mismo sucederá con Saturno, Urano, Neptuno y Plutón. Si los científicos no estuviesen preocupados por otras tareas, todo esto ya sería una auténtica realidad.

Sin embargo, no basta con enviar una simple pieza de metal a Júpiter. Si una sonda planetaria ha de ser útil debe de enviar señales. Esperamos que todo funcione bien a bordo y que la sonda que actualmente está en el trayecto cumpla con el cometido asignado. Sus señales nos van dando su posición y pronto transmitirán más información. ¿Desde qué punto en el espacio seguiremos recibiendo sus señales o las de otra sonda? Los científicos hace ya algún tiempo que enviaron a Júpiter ondas de radar y han detectado su reflejo. La distancia de este viaje de ida y vuelta a Júpiter es de unos 300.000.000 de kilómetros. Se trata de un enorme progreso conseguido desde la Segunda Guerra Mundial, cuando se consideró una gran hazaña hacer rebotar las ondas de radar en la superficie lunar, un viaje de ida y vuelta de menos de 380.000 kilómetros. Es posible que próximamente hayamos desarrollado nuestras técnicas hasta el punto de producir un rayo de radar que rebote en un cuerpo situado a 6.500.000.000 de kilómetros de distancia, que es precisamente la de Plutón, el planeta más remoto conocido del sistema solar.

Por consiguiente, pronto nos hallaremos en disposición de explorar todo el sistema solar mediante sondas. Hacia el año 2000 habrán aterrizado tales sondas en todos los planetas del sistema solar. Por entonces, todavía no conoceremos los resultados de tales sondeos, puesto que los viajes a los espacios más lejanos del sistema solar son muy largos. El Mariner IV tardó más de ocho meses en llegar cerca de Marte. Para llegar a Plutón se requerirán varios años.

¿Podemos explorar más allá del sistema solar? Al fin y al cabo, si propulsamos un cohete a más de 42 kilómetros por segundo (velocidad de escape del Sol a nuestra distancia del mismo), abandonará su órbita en torno al Sol. Dejará el sistema solar para siempre (técnica aplicada a la sonda enviada a Júpiter), y con una puntería acertada se aproximará a Alfa del Centauro, la estrella más próxima a nuestro sistema, o a cualquier otro objeto hacia el que vaya destinado.

Por desgracia, incluso la estrella más cercana se halla unas siete mil veces más lejos que Plutón. El vuelo de una sonda no tripulada tardaría muchos siglos en llegar a Alfa del Centauro. Y es casi imposible llegar a inventar unos haces de rayos de comunicación que puedan seguir el rastro de las sondas hasta las estrellas. Ciertamente, ello no será posible en los próximos siglos (ver Capítulo 22).

¿Y los vuelos tripulados? Una sonda lunar no es lo mismo que el alunizaje de un hombre en el satélite. ¿Llegaremos a poner el pie en Marte y Venus, en lugar de enviar sondas? ¿Dónde podremos trazar la línea y exclamar: No es probable que el hombre llegue hasta aquí en los próximos ciento cincuenta años? El hombre puede explorar el espacio en cuatro fases: en viajes de unos días, de unos meses, de unos años, de unos siglos. La primera fase, el viaje de unos días, llevó al hombre a la Luna.

Afortunadamente, ya se ha comprobado que la ingravidez apenas ejerce ningún efecto sobre la salud de los seres humanos. En segundo lugar, el cinturón de Van Allen no ha significado nunca el menor peligro para las astronaves tripuladas ni para sus guías.

Entre 1980 y 1985, si continúan los vuelos a la Luna será posible instalar allí una base. Ya se han examinado los progresos que resultarían del establecimiento de una estación astronómica en la cara oculta de la Luna. Y desde ésta, debido a su menor gravedad, podrían enviarse otras astronaves a los demás planetas con más facilidad que desde la Tierra.

La segunda fase de la exploración espacial, vuelos de unos meses, colocarán al sistema solar a nuestro alcance. Esto incluye a los planetas Marte, Venus y Mercurio. De ellos, Marte es el menos difícil. A pesar de su atmósfera extremadamente tenue y árida. Marte puede albergar formas de vida en su superficie (ver Capítulo 20).

La principal dificultad para llegar a Marte estriba en la duración del viaje. Los astronautas tendrán que pasar seis meses o más en el espacio. ¿Lograrán permanecer aislados tanto tiempo? ¿Podrán transportar consigo alimentos en cantidad suficiente? ¿Podrán resistir la ingravidez tantos meses? Consideremos estos problemas. La soledad no producirá necesariamente efectos demasiado nefastos. Hace cuatro o cinco siglos, los hombres se aventuraban varios meses por los tenebrosos océanos, en condiciones casi tan peligrosas como las de los vuelos espaciales. Se hallaban todavía más aislados que un viajero espacial hoy día. Estaban completamente separados de su patria, mientras que un astronauta se halla en comunicación con la base espacial constantemente, con el aliento de toda la Humanidad siempre en sus oídos.

Hay que solucionar todavía el problema de los víveres. Ante todo, no será necesario embarcar en la nave con destino a Marte varias toneladas de agua y oxígeno. Es preferible que la nave tenga instalada una planta química en miniatura que destile y purifique el agua y descomponga el dióxido de carbono para recobrar el oxígeno de la respiración. Sin embargo, nada se ha previsto respecto a la comida. Tal vez podría llevarse en forma congelada por completo.

¿Y la ingravidez? Es probable que un hombre en estado de ingravidez durante seis o más meses sufra daños físicos, pero si una nave espacial particularmente diseñada pudiera girar constantemente (o al menos una parte de ella), se produciría un efecto centrífugo, empujando al astronauta hacia las paredes. Esto ejercería sobre él el mismo efecto que un campo gravitacional. No se necesitaría energía para mantener la nave en movimiento giratorio una vez dado el impulso inicial, y sus efectos mantedrían sanos y cómodos a los astronautas Solucionados estos problemas, los hombres podrían aterrizar en Marte hacia 1985, y mantener allí una estación permanente hacia 1995. Las estaciones podrían también situarse en los dos pequeños satélites del planeta rojo, Deimos y Fobos, que carecen de atmósfera y prácticamente de gravedad.

¿Y el peligro de la radiación en esos viajes de varios meses? El peligro principal procede de las partículas cargadas de alta energía y emitidas a intervalos impredecibles por el Sol. Aunque las naves que volasen a Marte irían alejándose de la fuerte radiación solar, habría que proteger contra ésta a los astronautas durante los períodos de intensa actividad solar. Marte no posee cinturones de radiación detectables, por lo que a este respecto no habría que inquietarse al acercarse al planeta.

Los viajes a Venus y Mercurio no serían más largos que el vuelo a Marte, pero los de Mercurio gastarían mucha más energía debido a las mecánicas orbitales precisas (es muy difícil maniobrar una órbita en presencia del gigantesco campo gravitatorio del Sol).

Ni Venus ni Mercurio poseen al parecer cinturones de radiación. Ambos, no obstante, están en dirección al Sol, cuya radiación aumenta peligrosamente a medida que se reduce la distancia hasta él. Si se supera el riesgo de esta radiación y, según todas probabilidades así será, se llegará a Venus y Mercurio antes del año 2000.

Establecer allí bases permanentes ya será otra cuestión. La temperatura superficial de Venus, según datos del Mariner II, es de unos 500° C. Ésta es la temperatura que reina en todo el planeta, bajo sus densas nubes, de día y de noche por lo que al menos bajo la superficie también habrá el mismo calor, cuando menos. O sea, que no existirán posibilidades de refrescarse abriendo surcos en la tierra. Las sondas teledirigidas llegarán a Venus, y una expedición tripulada podría efectuar un vuelo temporal bajo las nubes, pero es sumamente improbable que se establezca una base permanente en Venus en un futuro previsible.

Mercurio ofrece mejores perspectivas, puesto que carece de atmósfera que conserve el calor y lo propague por toda su superficie. Hasta hace poco tiempo, los astrónomos creían que este planeta sólo presentaba una cara al Sol, de forma que ésta estaría constante e increíblemente caliente, mientras que la opuesta se hallaba casi rozando el cero absoluto. De ser así, habríamos podido aterrizar en dicha cara. Es muy sencillo establecer una base calentada artificialmente, sea cual sea el frío. Sin embargo, hoy día sabemos que Mercurio gira lentamente respecto al Sol, de modo que todas sus caras tienen día y noche de unos cincuenta y nueve días terrestres de duración.

Durante la noche todos los puntos de su superficie desprovista de aire tienen amplia oportunidad de enfriarse. Esto significa que las expediciones que eventualmente aterrizaran en Mercurio tendrían que efectuarlo en un punto bastante adentrado de su noche, para que la superficie se hubiese enfriado lo suficiente. Entonces, podría excavarse una base subterránea, antes de que el punto de aterrizaje volviera a surgir a pleno Sol.

Mercurio se halla a una distancia media de 58.000.000 de kilómetros del Sol. ¿Podrán jamás los hombres aproximarse tanto al astro rey? Existe una posibilidad. Hay un pequeño asteroide llamado Ícaro, que a veces pasa a unos cuantos millones de kilómetros de la Tierra. Su órbita es muy ovoidal. A un extremo de la misma, llega a medio camino de la órbita de Júpiter, y al otro cae hacia el Sol, yendo hacia él hasta llegar a una distancia de sólo 30.000.000 de kilómetros.

Si una expedición lograse llegar a Ícaro mientras pasase cerca de la Tierra e implantar en él los instrumentos necesarios, podrían obtenerse maravillosas observaciones del vecino Sol, de las partículas cargadas que emite, y del campo magnético que produce.

Todo abordamiento más cercano al Sol efectuado por el hombre que no sea sobre Ícaro parece muy improbable. Las naves espaciales, tripuladas o no, podrían pasar más cerca del Sol, pero el intenso calor y la radiación serían probablemente fatales, no sólo para los hombres, sino para los instrumentos, a menos que estuviesen particularmente bien protegidos. Es dudoso, por tanto, que en los próximos ciento cincuenta años, los hombres consigan inventar algo mejor que Ícaro.

La tercera fase, o sea los vuelos que durarán varios años, nos trasladarán del espacio exterior al sistema solar. Esto podrá lograrse mediante pasos graduales. Entre las órbitas de Marte y Júpiter giran millares de asteroides. Algunos tienen diámetros de casi doscientos kilómetros. Ceres, el mayor, posee 687 kilómetros de diámetro. Una vez en Marte, podremos alcanzar los asteroides sin grandes dificultades.

Tal vez a principios de los años 2000, el hombre habrá ya puesto el pie en Ceres. Paso a paso, podrán ser abordados otros asteroides. Uno de los más interesantes es Hidalgo, que tiene una órbita muy alargada. Por un extremo se aproxima a 38.000.000 kilómetros de la órbita de Marte, y por el otro, retrocede del Sol tanto como Saturno. La órbita de Hidalgo está completamente inclinada en comparación con las de varios planetas, por lo que no se acerca ni a Júpiter ni a Saturno. Sin embargo, si una expedición pudiera llegar a Hidalgo, cuando está próximo a Marte, los hombres podrían permanecer varios años en el espacio, estudiando las condiciones del sistema solar exterior a su placer, sabiendo que ocasionalmente regresarían a la vecindad de la órbita marciana.

Los astronautas podrían conquistar uno a uno los planetas exteriores, estableciéndose firmemente en uno para pasar al siguiente. Sin embargo, para emprender tales vuelos, aun bajo las condiciones más idóneas, los astronautas tendrían que pasar muchos años en el espacio, si las astronaves estaban equipadas con los cohetes químicos utilizados hoy día. A menos que se invente una nueva clase de cohetes, es posible que el ser humano no trasponga jamás la frontera de los asteroides.

Una posibilidad consiste en el empleo de cohetes nucleares. Éstos podrían ser impulsados por una serie de explosiones atómicas o por los gases de escape expedidos por el calor de un reactor nuclear. En cualquier caso, las naves con cohetes podrían mantenerse bajo aceleración durante largos períodos alcanzando mayores velocidades.

Los científicos también pueden construir un cohete de iones. Los ordinarios obtienen el impulso expulsando hacia atrás ingentes cantidades de gases supercalentados. Esta fuerza bruta es necesaria para elevar la nave por encima de la atmósfera y ponerla en órbita en torno a la Tierra. Una vez en órbita, y rodeada por el vacío, una nave podría utilizar átomos cargados eléctricamente (iones). Éstos serían enviados hacia atrás por la acción de un campo eléctrico. El impulso de los iones es muy débil, por lo que la velocidad del cohete aumentaría muy lentamente. No obstante, el cohete de iones es mucho más eficaz a la larga que uno ordinario. La aceleración puede proseguir por períodos indefinidos, y la velocidad aproximarse a la de la luz (300.000 kilómetros por segundo), al menos en teoría. Hacia el año 2000, cuando el hombre haya llegado a Ceres, tanto los cohetes nucleares como los de iones estarán ya en funcionamiento. En cuyo caso, podremos explorar el sistema solar exterior.

Una generación más tarde, hacia el año 2025, habremos aterrizado seguramente en uno de los satélites de Júpiter. Dentro de un siglo, nos hallaremos en el sistema de satélites de Saturno, con planes para llegar a las lunas de Urano y Neptuno. Y en el año 2100 los hombres pondrán el pie en Plutón, en los límites extremos del sistema solar.

Menciono los satélites de Júpiter, Saturno, Urano y Neptuno solamente. Mas, ¿y sus planetas? Estos cuatro planetas son gigantescos, y sus condiciones se hallan muy lejos de parecerse a las de la Tierra. Son sumamente, fríos y poseen atmósferas densas y venenosas, con increíbles tormentas y vendavales de inimaginada violencia. Las presiones al fondo de las atmósferas respectivas son millares de veces mayores que la nuestra. Tampoco estamos seguros de que sus superficies sean realmente sólidas.

Si los astronautas alcanzan algún día la superficie sólida de los gigantes exteriores (con el uso de una nave espacial que posea las propiedades de los batiscafos que nos han servido para las exploraciones de los fondos abismales oceánicos), se verán sujetos a enormes atracciones de gravedad, mucho más poderosas que las terrestres. Estas atracciones inmovilizarían a los astronautas y casi imposibilitarían el despegue desde el planeta. Las dificultades para enviar naves tripuladas a esos planetas gigantescos son tan enormes que durante largo tiempo los científicos tendrán que conformarse con enviar sondas en espiral hacia Júpiter, Saturno, Urano y Neptuno. Las exploraciones tripuladas de esos planetas no pueden tener lugar en un futuro previsible. No obstante, sí es posible aterrizar en el pequeño Plutón.

La cuarta fase de la exploración espacial, los vuelos de varios siglos de duración, nos llevará desde los planetas a las estrellas más próximas. Como mencioné anteriormente, la estrella más cercana está casi siete mil veces más lejos que Plutón.

Lo cierto es que en ninguna parte del sistema solar hay un planeta que ofrezca comodidades y seguridades para el hombre. En ellos tendría que vivir bajo tierra o bajo unas cúpulas (lo que, pese a todo, resultaría un excitante paso al frente en el progreso del ser humano (ver Capítulo 31). En ninguna parte del sistema solar, sin contar la Tierra, puede haber algo más que formas de vida muy primitiva. Entre las estrellas, no obstante, existen otros planetas con toda seguridad semejantes a la Tierra, que pueden soportar vida con las máximas probabilidades en tal sentido (ver Capítulo 22). Algunos pueden incluso tener vida inteligente. Por desgracia, no sabremos cuál es o cuáles son los planetas que soportan esta vida hasta que las astronaves lleguen a las estrellas a cuyo alrededor giran dichos planetas, de modo que si lo que buscamos es otras formas de vida, lo haremos a ciegas.

¿Y qué otros sistemas estelares pueden alcanzarse?

Ciertamente, la tarea de llegar a las estrellas más cercanas es muchas veces más difícil que alcanzar el planeta más lejano del sistema solar. Un gran problema de estos viajes sería el de proteger a los astronautas contra las mortales partículas cargadas de alta energía que chocarían con la nave, poniendo en peligro a sus pasajeros e instrumentos. Aún no se ha encontrado ninguna solución a tal problema. Además, ni siquiera los cohetes más avanzados que podamos imaginar podrán volar a mayor velocidad que la luz, e incluso a la velocidad de ésta duraría nueve años el viaje de ida y vuelta al cuerpo celeste más próximo, fuera del sistema solar. A las estrellas más distantes tardaríamos cientos de miles de años en llegar.

Hasta en el año 2100, cuando la Humanidad ya habrá conquistado Plutón con toda seguridad, es difícil que se intente efectuar alguna expedición hacia las estrellas. ¿Significa esto que el hombre ha de renunciar a ellas? Renunciar es un verbo muy pesimista. Los científicos ya han especulado sobre varios medios para llegar hasta el verdadero espacio exterior. La primera necesidad, claro está es la capacidad de alcanzar velocidades que se aproximen a la de la luz. Pueden lograrse mediante cohetes de iones o por algún otro invento técnico no surgido todavía.

La teoría de la relatividad de Einstein explica que todos los movimientos internos se retrasan en los objetos que se mueven a grandes velocidades. Los astronautas, por tanto, experimentarían sólo el paso de unos cuantos años en el transcurso de viajes que a los individuos situados en la Tierra les parecería de cientos de miles de años de duración (ver Capítulo 18). Por consiguiente, el hombre llegaría a una estrella distante dentro de su propia existencia, aun cuando tuviese que despedirse para siempre de la Tierra y todo cuanto dejase atrás.

Si resultase que las velocidades casi como la de la luz no son prácticas, sería posible, pese a todo, vivir lo suficiente para llegar hasta las estrellas. Para ello, podría congelarse a los astronautas y dejarlos en una especie de animación suspendida para varias décadas o generaciones hasta tener la meta a la vista. Sin embargo, tampoco sabemos si la animación suspendida por hibernación de baja temperatura sería práctica.

Existe un tercer medio. En lugar de emplear las pequeñas naves de la exploración y colonización de nuestro sistema solar, podría construirse una nave gigantesca para viajar hacia los planetas de las estrellas. En realidad, sería como un planeta minúsculo. En esta «nave-estrella» podrían ir centenares o millares de hombres, dejando aún espacio para la agricultura y rebaños de animales. Una especie de «arca de Noé» espacial. Allí nacerían y crecerían generaciones enteras de hombres y mujeres, envejecerían y morirían, mientras la astronave iría de una a otra estrella. Las condiciones en que podría ser más práctica tal exploración se discutirán en el próximo Capítulo.

Al enviar expediciones a las estrellas, por el sistema que fuese, no podríamos esperar su regreso. Ni siquiera una expedición a las estrellas más próximas podría volver a la Tierra dentro del mismo siglo, según nuestra cuenta de tiempo. Tampoco sería posible comunicarnos con las colonias humanas establecidas en los planetas de otras estrellas en la forma ordinaria. Aunque inventásemos el modo de transmitir rayos de comunicación bastante intensos para llegar a las otras estrellas, transcurrirían docenas de años, hasta siglos, antes de que tales rayos llegasen a una colonia y otro período de tiempo igual para que los colonos contestasen (ver Capítulo 22).

Resumamos lo dicho.

Puede suponerse razonablemente que hacia el año 2100 la Humanidad habrá explorado todo el sistema solar y habrá aterrizado en todos los planetas, satélites o asteroides, a su elección, con excepción de Júpiter, Saturno, Urano, Neptuno y Venus.

Habrá estudiado el Sol desde corta distancia, aunque no a menos de una distancia de 30.000.000 de kilómetros. La Humanidad todavía no habrá efectuado ningún intento para llegar o colonizar los planetas situados fuera de nuestro sistema solar.

Después del año 2100, la Humanidad se verá obligada a efectuar una larga pausa. Probablemente habrá llegado lo más lejos posible sin inventar capacidades técnicas excesivamente diferentes a las que posea entonces. Las proezas espaciales que la Humanidad no haya realizado en el año 2100 (aterrizajes en los planetas gigantes, una aproximación más cercana al Sol, un viaje a las estrellas), no parecerán imposibles, pero existirán todavía tantas dificultades, tantos obstáculos, que la Humanidad tardará varios siglos en intentarlas después del mencionado año 2100.

31. El universo y el futuro

Permítaseme inventar una palabra muy poco eufónica: espomo, y definirla.

Un espomo es cualquier sistema, sustancialmente cerrado respecto a la materia, capaz de soportar vida humana por un período de tiempo indefinido.

La Tierra es un espomo y, hasta el presente, el único conocido que existe. Sus calificaciones para serlo son obvias. Lleva varios millones de años soportando la vida humana, contando a todos los homínidos en general, y continuará soportándola durante un futuro previsible, sin tener en cuenta los efectos colaterales de la locura humana.

Además, está sustancialmente cerrado respecto a la materia. Ésta se añade en forma de lluvia de meteoritos o se pierde en forma de filtración atmosférica, mas todo esto de una manera insignificante, que no afecta a las características espómicas del planeta, ni es probable que las afecte en un futuro previsible.

Mas un espomo no puede estar cerrado respecto a la energía.

La vida es un proceso por el que se organizan mejor los componentes desorganizados del ambiente. Esto significa que la vida comporta un decrecimiento continuo de la entropía, y sólo puede existir a expensas de un continuo y aún mayor aumento de entropía en el ambiente.

Si la Tierra estuviese cerrada con respecto a la energía, la Humanidad y la vida en general, veríamos cómo en un tiempo relativamente breve, gran parte del oxígeno y la materia orgánica se convertiría en dióxido de carbono y otros residuos, hasta tornar la Tierra inhabitable.

La energía del Sol significa toda la diferencia. Entra en el sistema terrestre, mantiene la atmósfera agitada y los océanos líquidos; provoca las lluvias y, mucho más importante, las plantas verdes utilizan la energía solar para reconvertir el dióxido de carbono y el agua en sustancias orgánicas y oxígeno libre.

La entropía del ambiente, elevada por las actividades de la vida, vuelve a descender por la energía solar. Durante millones de años se ha mantenido un equilibrio a expensas de la creciente entropía del Sol, que tiene espacio para un aumento adicional de entropía para otros millones de años.

No necesitamos ir más allá del Sol. Por lo que sabemos, hay procesos que invierten el aumento entrópico del Sol, y de las estrellas en general, y mantienen al universo eternamente estable, como afirman algunos astrónomos (ver Capítulo 19), pero esto no nos atañe a nosotros. El Sol durará, sustancialmente en su forma actual, unos diez mil millones de años, y este tiempo, a escala de la Humanidad, es indefinido. Por tanto, podemos considerar ciertamente a la Tierra como un espomo.

Si la Tierra fuese el único espomo existente, el tema de la espomología sería trivial. Comprendería solamente ciencias como la Geografía y la Geología. Mas es posible que la Tierra no sea el único espomo existente en realidad, habiendo otros muchos en concepción o potencia. En cuyo caso, el tema cobra más interés.

Es posible, mejor dicho, es seguro, que entre las estrellas (si bien no en nuestro sistema solar) puede haber otros espomos. Es decir, planetas suficientemente semejantes a la Tierra en sus características generales, con un sol bastante parecido al nuestro, que sirvan de planetas habituales, o sea, de espomos. La cifra que ya he mencionado en otro lugar de esta obra (ver Capítulo 22) es de 640.000.000 de planetas-espomos posibles sólo en nuestra galaxia.

Sin embargo, todos estos planetas juntos no son suficientes para tornar interesante el tema de la espomología, ya que sólo son Tierras. Desde el punto de vista del espomólogo, al conocer un planeta como la Tierra, ya se han conocido todos los demás iguales. Y puesto que éste es nuestro caso, podemos olvidarnos de los demás.

Lo que deseamos, si queremos que la espomología sea una ciencia interesante, es que haya espomos radicalmente distintos de la Tierra. Y si el tema resulta interesante, veremos que también es valioso.

Preguntémonos, para empezar, por qué la Tierra es un espomo y no lo son Júpiter o Mercurio, por ejemplo. Si queremos expresar la diferencia de manera más sucinta, diremos que es una cuestión de masa. Júpiter, por ejemplo, tiene demasiada; Mercurio, excesivamente poca. La diferencia de masa entraña, sea como sea, casi todas las cualidades que convierten, o no, a un planeta en un espomo.

Si un planeta posee poca masa no puede soportar una atmósfera ni un océano formado por un líquido volátil. Si tiene demasiada, atraerá el hidrógeno y el helio, produciendo una atmósfera venenosa y, a lo sumo, un océano de amoníaco. En ninguno de ambos casos, podrá ser un espomo.

Si su masa es excesiva, se deberá probablemente a hallarse muy lejos de su primaria fuente de calor, pudiendo acumular materia sin apenas competencia por parte de su sol y a una temperatura suficientemente baja, las moléculas de hidrógeno (el elemento primordial de la materia) se tornan demasiado indolentes para ser capturadas. En tales condiciones, el planeta es demasiado frío para ser un espomo.

Si el planeta es poco sólido, debido a estar demasiado cerca del Sol, toda la materia acumulante se pierde en dirección a aquél, y la mayoría de los elementos más comunes son, a tan corta distancia de la fuente de calor y energía, demasiado ligeros y esquivos para ser capturados. Alternativamente, el cuerpo del planeta se forma demasiado cerca de un planeta mayor que le roba la materia de modo que el cuerpo en sí no es más que un satélite.

En el primer caso, el cuerpo celeste está excesivamente caliente para ser un espomo, y en el segundo, demasiado frío.

Naturalmente, hay excepciones a estas reglas; excepciones conocidas dentro de nuestro sistema solar. Nuestra Luna es demasiado grande para el lugar que ocupa en el sistema, mientras que Plutón es demasiado pequeño. Este escape a la regularidad conduce a la teoría de que la Luna es un planeta capturado, y Plutón un satélite liberado.

Por otra parte, suponiendo un sol del tipo apropiado, es razonable esperar que se forme un planeta a la distancia debida de dicho sol, con la composición química más conveniente para convertirse en un buen espomo.

Por consiguiente, podríamos afirmar que la búsqueda de un espomo es la búsqueda de un cuerpo celeste con la masa apropiada.

Mas todo esto se halla dentro del curso de la Naturaleza. Estos espomos son «naturales», fabricados por sí mismos. Añadamos ahora el factor inteligencia. Sólo Dios puede hacer un árbol, según el poeta Joyce Kilmer, pero quizás unos tontos como los seres humanos pudieran fabricar un espomo.

El problema es el siguiente: ¿podemos construir un espomo artificial? ¿Podemos utilizar un cuerpo sin la debida masa y convertirlo en un espomo? En cierto sentido, no podríamos ni intentarlo siquiera. Los cuerpos celestes excesivamente sólidos para ser espomos son muy raros (sólo hay cinco en el sistema solar, contando al mismo Sol, en comparación con los millares de cuerpos poco sólidos para servir de espomos naturales). Los astros con una masa excesiva son, además, demasiado peligrosos para jugar con ellos, debido a sus intensos campos gravitatorios y sus atmósferas inevitablemente enormes.

Si buscamos un astro con poca masa para ser un buen espomo, hallamos al momento que el más cercano a nosotros, la Luna, es un excelente ejemplo de esta clase.

El problema consiste, pues, en la conversión de los astros pequeños en espomos, por lo que la versión específica de dicho problema es: ¿podemos convertir la Luna en un espomo? Ciertamente, la Luna no lo es en la actualidad. A causa de su escasa masa, no posee atmósfera ni agua. Mas, consideremos los elementos esenciales y no los accidentales. Es posible impedir que una atmósfera se disemine por el espacio mediante la fuerza de un campo gravitatorio bastante intenso, pero, a una escala menor, también puede ser conservada junto al astro por medio de obstáculos físicos.

Dicho de otro modo, podemos distinguir dos variedades generales de espomos: externos e internos. Un espomo externo es aquel que tiene una atmósfera y un océano, retenidos junto a la superficie por mediación de un campo de gravedad, de modo que los hombres pueden vivir en dicha superficie. Un espomo interno es aquel en que el aire y el agua pueden retenerse dentro de una cavidad herméticamente cerrada, viviendo los hombres debajo de la superficie.

De manera inevitable, los espomos naturales son externos, mientras que los artificiales deben ser internos.

Supongamos, por tanto, que excavamos un enorme foso bajo la superficie de la Luna, suministrándole aire, agua y los demás elementos necesarios para la vida. Podríamos empezar con materias terrestres, aunque es posible que el agua pudiese conseguirse de los hidratos de sílice de la propia Luna. El oxígeno se obtendría de tal agua.

Con la suficiente energía, y una masa de composición química variada como la de la Luna (o de un astro mucho menor), podrían obtenerse los requerimientos químicos básicos.

La energía es la clave, y nosotros nos hemos acostumbrado a considerar al Sol como la fuente natural de toda energía. En la Naturaleza, la única fuente de energía en cantidades suficientes para soportar un espomo natural, es una estrella como nuestro Sol; mas una estrella, cualquier estrella, es un verdadero desgaste de energía. Casi toda su radiación es absorbida por uno de sus planetas, y sólo una pequeña fracción de ésa se emplea. Mucha menos cantidad, utilizada con mayor eficacia, serviría mejor para este propósito.

Una hoguera, cuya producción de energía es una fracción sumamente ridicula de la solar, nos calienta en invierno, cuando el Sol resulta insuficiente para este propósito. Sin embargo, a la escala de un espomo, una hoguera ordinaria no es bastante. Por fortuna, hay a la vista algo mucho mejor.

A la inmensa escala de un espomo, solamente la fusión del hidrógeno podría considerarse como una fuente de energía a través de un futuro indeterminado. La fusión del hidrógeno a gran escala es la que le proporciona al Sol su energía, y puede ser aquélla la que dé a la Tierra su energía.

Por consiguiente, preveo, aunque no en un futuro inmediato, la posibilidad de que la Luna sea excavada bajo su superficie en forma de cavernas, que estarán aprovisionadas por la misma Luna de todos los materiales básicos, y cuya energía procederá de plantas de fusión de hidrógeno.

Este sistema de cavernas estará poblado por vida animal y vegetal (e inevitablemente de vida microscópica), y habitado por hombres, mujeres y niños, es decir, por familias que no conocerán otra clase de vida…, ni querrán conocerla.

Las ventajas son obvias. La Luna gozará de un ambiente controlado y diseñado especialmente para el hombre; éste tendrá lo que desee y necesite (en muchos aspectos vitales), y no sólo lo que pueda conseguir. Asimismo, gozará de las ventajas de un nuevo principio. De igual modo que Estados Unidos ha conseguido prosperar y florecer debido, en parte, a haber prescindido de las tradiciones de la amargada Europa de la Edad Media, en la Luna, es de esperar, sus colonos se hallarán libres de los tabúes y errores pasados de la Tierra.

También tendrá sus desventajas. Aunque podamos confiar ciertamente en los progresos científicos y tecnológicos, es casi seguro que la ciencia y la técnica jamás conseguirán alterar la gravedad lunar. Los habitantes de nuestro satélite se hallarán constantemente, y para siempre, bajo una atracción de gravedad igual a un sexto de la de la Tierra.

Indudablemente, los colonos lograrán acostumbrarse a ello, y los que nazcan ya en la Luna, los verdaderos selenitas, como no conocerán otra gravedad, la considerarán natural. Sin embargo, ¿sufrirán los hombres, especialmente en el período de transición en que habrán de efectuar diversos viajes entre la Tierra y la Luna? ¿Se debilitarán sus músculos? ¿Se tornarán más débiles sus huesos bajo la influencia de una gravedad mucho menor? ¿Podrán en tales circunstancias resistir un regreso a la Tierra? Tal vez este problema no se presente en toda su intensidad. Los colonos de la Luna podrán ser conservados en buenas condiciones, respecto a los hábitos terrestres, mediante el ejercicio o en cámaras centrífugas. Quizá solamente algunos especialistas necesiten regresar a la Tierra, mientras que el resto de la colonia lunar soporte bastante bien una estancia permanente en el satélite.

Otra desventaja es que un espomo interno puede sufrir catástrofes accidentales que los externos jamás padecen. Una atmósfera y un océano retenidos en la superficie por medio de la gravedad son muy seguros. Aparte de un desastre a escala universal, nada puede cambiar la fuerza de la gravedad ni hacer que se pierdan la atmósfera o el océano de un espomo externo.

En un espomo interno, por otra parte, una caverna horadada por un meteorito grande, o dañada por un alud o un corrimiento de tierras, pierde el aire al momento, y con más lentitud el agua. Es de esperar, no obstante, que los hombres posean el ingenio suficiente para reducir las oportunidades de tales catástrofes. Además, la caverna de un espomo interno estará indudablemente compartimentada, de modo que una catástrofe local no afecte a las demás excavaciones.

Tampoco una catástrofe ha de significar la renuncia a un espomo. También la Tierra las sufre. Periódicamente, nos vemos afectados por los huracanes, las celliscas, los tornados, las inundaciones y las sequías, a ninguno de cuyos males se halla sujeta la Luna. Un selenita patriótico argüirá, ciertamente, en tiempos futuros, que es la Tierra y no la Luna el espomo menos ideal para su habitabilidad a causa de las catástrofes.

Mas, ¿y las dificultades psicológicas? ¿Podrán realmente vivir los hombres durante largos períodos en lo que esencialmente, no será más que una caverna? ¿Podrá soportar el nacer y morir allí? En mi opinión, la respuesta es afirmativa, si la caverna es amplia y cómoda.

Es un error subestimar la flexibilidad de la Humanidad. El hombre ya ha demostrado su capacidad para realizar tremendos reajustes. Una ciudad como Nueva York representa, en cierto modo, casi un espomo artificial, enormemente diferente del ambiente primitivo del hombre en la Tierra, o en la Luna. Sin embargo, el hombre ha efectuado la transición desde la caverna al rascacielos en un período de tiempo insignificante. Incluso un campesino llega a adaptarse a Nueva York en unos cuantos años.

¿Por qué hemos de imaginarnos que un selenita se horrorizaría ante la idea de vivir «emparedado»? Seguramente, más se horrorizaría ante la idea de un mundo como la Tierra, donde los hombres viven penosamente sobre la superficie, expuestos a todos los cambios del clima y a otros desastres. Ningún selenita desearía vivir en la Tierra, lo mismo que ningún neoyorquino quiere vivir en una caverna.

Naturalmente, al pensar en un espomo interno debemos desprendernos de todos nuestros prejuicios. Es fácil caer en la trampa de pensar, vagamente, que un espomo externo es «natural», y uno interno «artificial», y que lo natural es bueno y lo artificial malo.

Podría llegarse más lejos, alegando que un espomo «verdadero» es aquel en que la vida se desarrolla espontáneamente, a partir de materia no viva, como ocurrió en la Tierra (ver Capítulo 9). Un mundo al que hubiera que aplicar técnicas científicas y poblar con especies que tienen ya dos o tres mil millones de años de evolución detrás, no sería un verdadero espomo, sino un imitador.

Mas, frente a este argumento, ¿dónde se halla el Homo sapiens? La vida no se desarrolló en la tierra seca. La única porción de Tierra que es espomo «natural», en el sentido de que la vida se inició allí espontáneamente a través de productos químicos simples, es el océano. Lentamente, ciertos tipos de seres vivos emergieron a la tierra seca, sitio tan hostil a los hijos del mar como la Luna nos parece hoy a nosotros.

Un pez filósofo, si podemos imaginar uno, seguramente movería la cabeza ante la tontería de que unos seres prefiriesen salir del agua y habitar la tierra seca. Le parecería un cambio pésimo pasar del ambiente siempre igual del océano a las extremosidades violentas del aire libre; de la plenitud de agua a la amenaza perenne de la desecación; de un mundo libre de la gravedad tridimensional, a otro con gran atracción gravitacional y dimensional.

Estos peligros no están faltos de realismo, ni son imaginarias las desventajas de la tierra seca. La vida invadió ya la Tierra hace más de 425.000.000 de años, mas incluso hoy día, el océano es mucho más rico en vida que la tierra firme, palmo a palmo. Los animales terrestres tuvieron que evolucionar durante millones de años para desarrollar unas extremidades bastante resistentes que les permitieran separarse de la tierra, posibilitando los movimientos independientes. Transcurrieron unos doscientos millones de años antes de que los seres vivos desarrollasen sus termostatos internos y su aislamiento externo, con el fin de lograr restaurar imperfectamente la temperatura oceánica. El hombre se irguió sobre sus dos pies hace un millón y medio de años, aproximadamente, y todavía paga su impuesto a la gravedad con sus pies planos, sus vértebras deslizadas, sus molestias craneanas, sus panzas, y los demás achaques. En la actualidad, el hombre aún vive con el temor de caerse, temor del que apenas se da cuenta por estar ya tan acostumbrado a él.

No, no; si vamos a torcer el gesto ante la Luna como un lugar poco habitable, lo mismo podemos hacer con los continentes de la Tierra. Vivimos en una parte de ésta que se pobló de manera artificial, desde la parte verdaderamente espómica. Y a pesar de todo, la vida terrestre es menos rica que la oceánica, y en algunos aspectos, menos cómoda que aquélla.

Sin embargo, ¿hemos de lamentar que nuestros antepasados surgieran del agua para afincarse en la tierra? Pese a todos los peligros y desventajas, ello abrió el camino a unos progresos imposibles de realizar en el mar. Debemos considerar, si tendemos la vista hacia atrás, que el océano era un fin muerto, un callejón sin salida, en tanto que la tierra ofreció nuevos y más amplios horizontes.

Tampoco quisiera ser contradictorio, pero el aire es mucho menos viscoso que el agua. En ésta, un ser ha de moverse lentamente o poseer líneas aerodinámicas. Los seres más desarrollados del océano, las ballenas, los tiburones, los peces, son extremadamente aerodinámicos. Los seres terrestres, que regresan al mar, lo son hasta el punto en que necesitan serlo, como ocurre con las nutrias, los pingüinos, las focas, las vacas marinas.

Un cuerpo aerodinámico implica unos apéndices cortos y rechonchos, cuando existen, a excepción de los tentáculos altamente especializados del pulpo. En el aire, falto de viscosidad, por otra parte, es posible moverse con rapidez y poseer una forma irregular al mismo tiempo, de modo que los animales terrestres han podido desarrollar buenas extremidades. A esto le debe el hombre sus inapreciables manos.

Consideremos hasta qué punto, de ser la marsopa tan inteligente como el hombre, le impediría exhibir su talento la falta de manos. De poder llegar a comunicarnos con las marsopas, tal vez nos hallaríamos con unos filósofos muy pesimistas: grandes pensadores, pero nulos como ejecutores.

Asimismo, sólo es posible encender fuego al aire libre y jamás en el agua. Sólo una criatura terrestre, por tanto, pudo desarrollar la técnica que se inició con el descubrimiento del fuego. Es posible argüir que los progresos técnicos de la Humanidad todavía no son perfectos, pero dudo que nadie quisiera regresar a los tiempos en que el fuego todavía no había sido descubierto.

Empleando una analogía química, el paso del mar a la tierra significó una «fase modificada» en el progreso de la vida, lo cual todo el mundo, o casi todo el mundo, ha de considerarlo muy deseable.

¿Es posible, entonces, que el paso desde un espomo «natural» y externo, a otro interno y «artificial» pudiese significar un cambio deseable? Nunca me ha gustado profetizar, ya que en tales asuntos resulta extremadamente difícil, mas lo intentaré.

Creo, por ejemplo, que por muy difícil que fuese el paso inicial de un espomo externo a otro interno, al final sería una cancelación parcial de las dificultades presentadas por la gran aventura anterior de la vida. En un espomo interno, el hombre volvería a recobrar el ambiente inmutable y la menor gravedad del mar, sin abandonar el ambiente menos viscoso del aire. Un espomo interno gozaría, al poco tiempo, de las ventajas inherentes a la tierra y al mar, y no sufriría ninguna de sus desventajas.

Si empezamos con un espomo interno en la Luna, la victoria, el triunfo sólo podría inspirar intentos de expansión, llegando a la formación de espomos de tamaño mediano, como Marte y los grandes satélites de Júpiter. Especialmente, podría producirse una especie de éxodo hacia espomos cada vez más pequeños, o sea, los asteroides que existen por millares en el espacio, entre las órbitas de Marte y Júpiter.

¿Por qué los asteroides? Consideremos la eficiencia. Can la mejor voluntad del mundo, y con todos los progresos técnicos previsibles para el futuro, la Humanidad jamás logrará ahondar mucho en la corteza terrestre, ni en la de la Luna o Marte. Será posible, eso sí, excavar algunos focos o cavernas, pero si nos referimos a espomos internos, con cavernas amplias y cómodas, a lo sumo podremos ahondar un par de kilómetros. Precisamente, el calor interno de la Tierra, como el de Marte o la Luna, tornaría sumamente incómodas unas cavernas más hondas.

Esto significa que virtualmente todo el volumen de un planeta resulta inútil y sólo les sirve a los habitantes del espomo para proporcionarles el adecuado campo de gravedad.

Los asteroides, no obstante, pueden ser espomificados completamente. Pueden ser excavados por entero. No poseen calor interno, más que en un grado muy pequeño, y apenas gravedad. Las cavernas no necesitarían grandes vigas. Si exceptuamos los muy grandes, podríamos utilizar todo el asteroide. (Quizá fuese difícil excavar un asteroide compuesto de níquel, y su composición química tal vez no resultase adecuada como fuente de materias primas, salvo las ferruginosas, mas, al juzgar por el promedio de meteoritos de hierro contra los de piedra, es de esperar que menos del diez por ciento de los asteroides sean metálicos.) Cualquier asteroide pequeño podría constituir un espomo suficientemente grande. Hace unos años, escribí un relato referente a un espomo asteroidal, en que un terráqueo, al visitar el asteroide, expresaba su sorpresa al ver que sus habitantes tenían sitio bastante para el cultivo de tabaco. El guía replicaba:

–Nuestro mundo no es pequeño, doctor Lamorak. Usted nos juzga de acuerdo con las normas bidimensionales. El área superficial de Elsevere (el asteroide), tiene solamente unas tres cuartas partes de la del Estado de Nueva York, pero esto no importa. Recuerde que podemos ocupar, a nuestro antojo, todo el interior de Elsevere. Una esfera de un radio de 80 kilómetros posee un volumen superior a dos millones de kilómetros cúbicos. Si todo Elsevere estuviera ocupado por niveles con veinte metros de separación entre sí, la superficie total del interior del planetoide sería de unos noventa millones de kilómetros cuadrados, igual al área terrestre de la Tierra. Y ninguno de esos kilómetros cuadrados, doctor, sería improductivo.

En el relato, descarté deliberadamente un grave problema que inevitablemente se presentaría en un espomo asteroidal, a fin de poder concentrarme en el aspecto sociológico al que yo apuntaba. Evité toda consideración sobre el hecho de que el campo gravitatorio de un asteroide es pequeñísimo, dándole a mi espomo ficticio una gravedad artificial.

En la vida real, distinta a la ciencia-ficción, no es posible instalar una gravedad artificial por la sola voluntad de un escritor. Sin embargo, y en teoría, sería posible lograr que un espomo asteroidal girase sobre su eje con gran rapidez. El efecto centrífugo sería análogo a un campo gravitatorio dirigido hacia fuera del eje de rotación, en todas direcciones, con algunos efectos secundarios de importancia. El campo gravitatorio así instalado variaría notablemente con la distancia desde el eje, y el. efecto Coriolis sería muy marcado. Cuanto menor el espomo, mayor la velocidad angular requerida para obtener un efecto centrífugo máximo, y más pronunciadas las variaciones en el efecto y en la intromisión del efecto Coriolis.

Supongo que la rotación artificial del espomo no valdría la energía desperdiciada y los problemas secundarios. ¿Por qué no, en cambio, aceptar la falta de gravedad como una condición vital? La vida, en el pasado, pasó de la falta de gravedad del océano a la esclavitud de la gravedad terrestre, y sobrevivió. ¿Por qué no volver hacia atrás? Naturalmente, para cambiar de g a cero-g se requerirían millones de años, y los cuerpos y organismos de los seres que sufrieran tal cambio pasarían por unas modificaciones lentas, penosas y glaciales, mediante la fuerza de la selección natural. A la Humanidad, como es obvio, le falta tiempo para tales cambios.

Pero la Humanidad no sólo está efectuando enormes progresos en la ciencia espacial y la tecnología. La Biología también avanza a pasos agigantados. Es razonable esperar que cuando el hombre pueda llegar a los asteroides con la energía suficiente para transformarlos en espomos, habrá aprendido bastante respecto a la genética, a fin de poder fabricar los tejidos necesarios (ver Capítulo 9). ¿Por qué no suponer que los cambios necesarios para capacitar un cuerpo humano a la falta de gravedad no han de ir dirigidos por la inteligencia, en vez de quedar a merced de la colosal ceguera de la Naturaleza, que sólo conoce el cambio al azar? Un cuerpo destinado a la gravedad cero podría ser bastante distinto de nuestro actual, mas no de manera radical. Los huesos y los músculos podrían ser más pequeños y las piernas más cortas, aunque supongo que esto no llegaría a grandes extremos. A pesar de la falta de peso, el cuerpo aún tendría que luchar con la masa de inercia, igual en un asteroide que en la Tierra.

Un cuerpo para la gravedad cero sería, a mi entender, altamente gracioso en sus movimientos, llegando a conseguir algunas de las habilidades tridimensionales de los peces y las aves. Constituiríamos una especie humana capaz de volar sin tener que sacrificar la utilísima mano en favor de un ala.

Los animales terrestres necesitarían adaptaciones similares, mas, con excepción de los domésticos, los colonos asteroidales podrían seguramente vivir sin ellos. Las plantas crecerían sin gravedad sin grandes dificultades. También podrían criarse peces. El cultivo de algas y la industria química podría combinarse para producir alimentos con el gusto y el sabor de la carne, si fuera necesario.

De acuerdo. Un hombre acostumbrado a gravedad cero, jamás podría volver a la Tierra, ni siquiera visitar un mundo tan grande (desde su punto de vista) como la Luna, pero no le sería más dificultoso que lo es para nosotros no poder respirar bajo el agua (salvo al ahogarnos).

Si nos concentramos en esto, habría dos especies de hombres: los de gravedad g y los de gravedad cero. Nosotros somos g, claro está, lo mismo que lo serían los colonos de los espomos grandes como Marte, la Luna, los satélites mayores de Júpiter, etc. Y los habitantes de los espomos asteroidales serían de gravedad cero.

La segunda fase de la evolución no sería el paso de un espomo externo a otro interno, sino de la gravedad g a la gravedad cero. ¿No pertenecerá el futuro a esta última gravedad? ¿No es posible que nosotros, los seres de gravedad g hayamos llegado a un final, a un callejón sin salida, mientras que los de gravedad cero tengan ante sí un nuevo y más amplio horizonte? Ellos podrían progresar, descartando todos los peligros y desventajas inherentes a la Tierra, mientras que nosotros, incapaces de seguirles, igual que los peces no pueden ya seguirnos a nosotros, nos quedaríamos rezagados, reflexionando sobre nuestras grandezas pasadas, como actualmente hacen los peces con respecto a nuestros antepasados.

Considerémoslo…

Primero, las especies de gravedad cero podrían superarnos a medida que pasara el tiempo. Los asteroides excavados llegarían a soportar una gran población, en conjunto, mucho mayor que la soportada por los espomos externos habitados por las especies acostumbradas a la gravedad. El hecho de que los seres de gravedad cero tuviesen un cuerpo menor (aunque no un cerebro más pequeño), serviría para acrecentar su número.

Segundo, la naturaleza del ambiente de gravedad cero haría que sus habitantes nos superasen también en variabilidad y versatilidad. Los seres con gravedad existirían como un núcleo central (la población terrestre), con pequeñas ramificaciones en Marte, la Luna y otros planetas o planetoides, mientras que la especie sin gravedad estaría repartida entre un millar o más de mundos.

La situación semejaría a la existente entre las civilizaciones griega y romana de la Antigüedad. Los romanos forjaron unas leyes maravillosas, unas severas bases gubernamentales, fueron grandes arquitectos y mejores técnicos, tanto en la guerra ofensiva como en la defensiva. Sin embargo, la civilización romana siempre resultó falta de flexibilidad; Roma nunca dejó de ser Roma.

Los griegos, en cambio, a pesar de alcanzar menos altura material, gozaron de una vida y un verbo en su cultura que aún hoy día nos cautivan, después de transcurridos más de dos mil quinientos años. Ninguna otra cultura ha brillado como la griega, y uno de los motivos de tal milagro es que no se trataba en realidad de una Grecia, sino de un millar de ciudades-Estado griegas, cada cual con su propio gobierno, sus costumbres, su forma de vivir, de amar, de adorar, de morir. Si consideramos la antigua Grecia, el esplendor de Atenas parece empañar el brillo del resto de la nación, y sin embargo cada ciudad contribuyó a su cultura con algo propio. La infinita variedad a que esto dio resultado le otorgó a Grecia una gloria imperecedera e inigualable, muy superior a nuestra actual civilización de una Humanidad-masa, lo que Ionesco denominaría una Humanidad-rinoceronte.

Los seres que viviesen en mundos sin gravedad serían los griegos modernos. Un millar de mundos, todos con su propia forma de desarrollar y expresar su historia y sus antecedentes. La riqueza vital representada por esos diferentes mundos sin gravedad superaría con creces a todo lo desarrollado en el mismo tiempo en una Tierra, muy disminuida y más uniforme a causa de los progresos técnicos.

Una tercera diferencia, crucial en mi opinión, puede explicarse y comprenderse mejor volviendo al tema de las naves espaciales.

Ante lo ya expuesto, sabemos que una nave espacial no es un espomo auténtico, ya que un espomo ha de ser capaz de soportar indefinidamente la vida humana. Una nave espacial es más bien un «espomoide», que puede servir de espomo temporalmente.

Los espomoides ya han funcionado muy bien en diversas ocasiones, particularmente en los viajes a la Luna.

La intención de la raza humana es explorar el sistema solar mediante espomoides, aun antes de haber establecido ningún espomo definitivo en algún lugar del espacio; en realidad, el establecimiento de espomos extraterrestres es algo imposible. Sólo por medio de fases escalonadas podríamos llegar a Plutón (ver Capítulo 30).

Mas allí nos tendremos que parar. Pasado Plutón se hallan ya las estrellas, y las distancias implicadas son tan enormes que las técnicas suficientes para nuestro sistema solar resultarían inútiles ante la nueva situación.

Llegar a las estrellas más próximas entrañaría una de estas tres alternativas:

1. Un vuelo directo de ida y vuelta a la estrella más cercana, siendo el tiempo requerido de una generación a un siglo.

2. Volar a velocidades próximas a la de la luz, introduciendo el efecto de la dilatación del tiempo (ver Capítulo 18), de modo que la duración del viaje para los astronautas sería sólo de unos cuantos meses o años. En este caso, no obstante, al regresar a la Tierra, hallarían que el tiempo aquí transcurrido era de un siglo o más.

3. Efectuar el vuelo con los astronautas congelados en animación suspendida, cuyo efecto sería el mismo que el del Caso 2.

Ninguna de estas alternativas es agradable. Los astronautas estarían expuestos a los peligros y las incertidumbres de la congelación por períodos cada vez mayores, o tendrían que consumir las ingentes cantidades de energía necesarias para alcanzar velocidades extremas. Es posible que una congelación durante varias décadas resultase imposible, y que la energía exigida para lograr el efecto de dilatación del tiempo sea prohibitiva. Si la primera alternativa es la escogida como más simple, los astronautas no sólo pasarán toda su existencia en la aeronave estelar sino que también han de estar preparados a tener hijos y hasta nietos dentro de la nave…, hijos y nietos que, a su vez, tendrán que disponerse a pasar toda su vida en la astronave.

En cuanto a los que esperen en la Tierra, no existen alternativas. Una astronave que despegue con destino a una estrella próxima, tardará siempre cientos de años en regresar…, si regresa. Los astronautas conseguirán acortar el tiempo mediante el efecto de dilatación temporal, o por la congelación, mas tales medios no afectarán a quienes les aguarden en la Tierra. La nave estelar no regresará en ningún caso antes de un siglo, y entre la multitud que la vitoree a la llegada no habrá ya ni un solo miembro de los que la despidieron fervorosamente cuando se produjo la partida.

En tales circunstancias, la exploración estelar no resultará nunca una proeza popular para nadie, ni entre los astronautas ni entre los terrestres. Algunas expediciones podrían funcionar en calidad de tours de force, pero los terráqueos, que no podrán seguirlas, que no podrán ver el resultado en toda su vida, perderán interés en tales hazañas.

Consideremos ahora bajo qué condiciones podrían ser populares tales expediciones.

Cuanto más prolongada la travesía de exploración dentro del sistema solar, más elaborado tendrá que ser el espomoide. Cuando lleguemos a los planetas exteriores, los viajes espaciales necesitarán años de duración, y un espomoide capaz de soportar una dotación por varios años, precisando un mecanismo de repetición de ciclos, cosa excesivamente sofisticada si ha de servir a una tripulación por tiempo indefinido.

La tendencia, por tanto, en las exploraciones espaciales, será ir desde un espomoide a un espomo y, ciertamente, con respecto a la exploración estelar, se necesitará un espomo sumamente elaborado.

No sólo una nave espacial con destino a las estrellas es un espomo, sino un espomo interno, de un tipo extremado.

Al reunir la tripulación de una nave estelar, les pediremos en realidad a los hombres y mujeres que la compongan que se trasladen de un espomo externo a otro extremadamente interno…, lo cual tal vez sea pedir demasiado.

Naturalmente, durante todo este capítulo me estoy refiriendo al establecimiento de espomos…, ¡pero por etapas! El cambio del espomo externo que es la Tierra a otro interno como la Luna, es, en muchos aspectos, sencillo.

Todavía existirá la oportunidad de comunicarse con la Tierra, ésta estará a la vista, aunque sólo sea dentro de la pantalla de la televisión instalada en la caverna, y finalmente habrá la posibilidad de regresar algún día al planeta-madre.

Por tanto, serán los colonos selenitas, acostumbrados ya a un espomo interno de carácter plácido, los que irán a espomificar Marte y Ganimedes. Y serán los colonos más alejados, los que estarán ya mucho más divorciados de la Tierra por el mero hecho de que ésta no colgará de su cielo como un balón enorme, los que darán los pasos siguientes hacia los asteroides y la fase de gravedad cero.

Poco a poco, los habitantes de los espomos superarán sus ansias de cielos azules, de aire libre, de océanos, de montes, ríos y animales.

Mas, ni siquiera un colono de la Luna o Marte se sentirá a gusto en una nave estelar, que tendrá una gravedad cero, a menos que la misma gire rápidamente…, con todos los problemas inherentes a tal giro.

No, los astronautas más adecuados para una nave estelar deberían de ser seres acostumbrados a la gravedad cero, con lo cual no habría ya necesidad de recluirlos puesto que un espomo asteroidal ya será una nave estelar en sí mismo. Ascendiendo desde una nave espacial primitiva, y descendiendo desde la Tierra, nos enfrentamos en el centro con la ecuación: espomo asteroide = nave estelar.

En tales condiciones, podría realizarse un viaje a las estrellas sin grandes molestias. Si se acoplase a un asteroide un cohete con motores, obligándole a desviarse de su rumbo y a alejarse del Sol (la velocidad de escape del Sol es considerablemente menor en el cinturón de asteroides que cerca de la Tierra), ¿qué les importaría a los habitantes de tal asteroide la falta de gravedad? Serían seres que siempre habrían vivido en un espomo interno de gravedad cero, y continuarían viviendo en un espomo de iguales condiciones. No abandonarían su hogar, sino que se lo llevarían consigo. ¿Qué importancia tendría la duración de su viaje a las estrellas? ¿Cuántas generaciones podrían vivir y morir durante tal viaje? Nada de esto cambiaría su forma de vivir.

Sí, cierto, dejarían al Sol…, ¿y qué? Los moradores de un asteroide jamás dependerán para nada del Sol. Sus habitantes, debidamente ataviados, podrían emerger del asteroide y observar al Sol como una diminuta bolita en el cielo, pero nada más. Quizás acabasen por echar de menos esta imagen e idealizaran «el sol de su hogar». Mas esto sólo provocaría una sensación de nostalgia, como el moderno morador de una gran urbe experimenta respecto a su «viejo poblado ancestral».

La nave estelar, el asteroide estelar, al quedar desviado de su antigua órbita, daría el tercer y final paso del destete de la vida. Antaño, las formas de vida fueron destetadas del océano. Con el establecimiento de los espomos extraterrestres, las formas de vida quedarían destetadas de la Tierra. Con las naves estelares, quedarían destetadas del sistema solar.

Mas, ¿por qué se han de convertir los asteroides en naves estelares? ¿Qué ganarán con ello? Bastantes cosas: Primero, satisfacer la curiosidad, el deseo básico, punzante de saber. ¿Por qué no saber cómo es el universo? ¿Qué hay ahí fuera? Segundo, el deseo de libertad… ¿Por qué dar inútilmente vueltas en torno al Sol, cuando es posible ser una parte independiente del universo, sin estar sujeto a ninguna estrella? Tercero, la utilidad de saber: puesto que un viaje estelar nos proporcionaría nuevos conocimientos a los que ya poseemos, conocimientos que nos permitirían darle más seguridad al espomo, al mismo tiempo que mayor comodidad.

La travesía a las estrellas no tendría por qué ser monótona ni falta de aventuras. Cierto, tardaríamos cientos o miles de años en llegar a una estrella, y transcurrirían generaciones sin que los astronautas divisasen una de cerca, pero, ¿significa esto que no habría nada que ver? No puedo adivinar qué fenómenos ocurrirían al paso de la astronave, ni qué bellezas de la Naturaleza admirarían los navegantes celestes. Sin embargo, una cosa parece cierta: el universo ha de estar mejor poblado de lo que parece.

Vemos actualmente las estrellas debido a su intenso brillo; pero las estrellas pequeñas son más numerosas que las grandes, y las borrosas mucho más aún que las refulgentes. Con toda seguridad, hay cuerpos celestes muy borrosos y pequeños, que no pueden distinguirse, excepto desde muy cerca, y que se hallan en mayoría.

Quizá no pasaría una generación sin ver uno de dichos cuerpos celestes más pequeños, algún astro material en el que la nave podría detenerse a investigar. Si el astro fuera grande, la nave estelar no podría aterrizar en ella, pero sí rodearla, adoptar una órbita temporal, observarla y partir de nuevo más allá. Si el astro fuese lo bastante pequeño para carecer casi de gravedad, podría excavarse y servir como depósito de minerales para sustituir las inevitables pérdidas sufridas por cualquier espomo, por muy eficiente que sea su ciclo.

Al llegar cerca de una estrella, con sus planetas iluminados, las observaciones serían especialmente intensas e interesantes. El sistema podría contener espomos externos, planetas semejantes a la Tierra, conteniendo vida…, quizás incluso vida inteligente.

¡Qué fenómeno sería éste, de acuerdo con la existencia humana! ¡Qué afortunada la generación que pudiera contemplar tal vista! Los astronautas observarían silenciosamente, vigilarían, se alejarían, atraídos por el espacio infinito…, y en el planeta habitado, los seres podrían charlar excitadamente del platillo volante… ¡No! No pretendo dar aquí una explicación de los platillos volantes supuestamente vistos en la Tierra (ver Capítulo 24).

La vecindad de una estrella podría ofrecer la oportunidad de repostar. Concibo que las provisiones de deuterio, necesarias para los reactores de fusión podrían ser recogidas en el espacio, al paso de la nave, pero el deuterio, se halla increíblemente diseminado. Estaría, como es natural, más concentrado en un sistema estelar. Tal vez la cercanía de una estrella no sólo brindaría la ocasión de contemplar un espectáculo inusitado, sino el medio de repostar deuterio…, el suficiente para la duración del viaje por otro millón de años.

Si se encontraban los astronautas con un cínturón de asteroides en torno a una estrella, podría efectuarse un aterrizaje. Para ello, la nave estelar podría adoptar una órbita apropiada. Entonces, otros asteroides se convertirían en espomos. La colonia se dividiría y establecerían otras. Eventualmente, una o más, o todas, se transformarían en naves estelares. Quizás una astronave vieja, usada, sin valor ya para que fuese reparada, podría quedar abandonada en tales ocasiones…, indudablemente con mucho más pesar de lo que lo serían jamás el Sol y la Tierra.

En realidad, podría producirse una «alternancia de generaciones», a través de los millones de años, con respecto a las naves estelares. Habría una generación en que las naves estelares se moverían por la vastedad del espacio, en que el incremento de población debería de estar severamente controlado. Y habría otra generación, tras encontrar un cinturón de asteroides, en que la población no se movería, proliferando entonces en abundancia.

Al término de cada una de estas generaciones inmóviles, se produciría también una proliferación de naves estelares. Al transcurrir los años, y convertirse los siglos en milenios, las astronaves comenzarían a pulular por el universo…, todo éste convertido en su hogar.

Y de vez en cuando, tal vez se encontrarían dos espomos, mediante cita convenida.

Me imagino que esto comportaría un ritual de importancia sin igual. No se trataría de una cita breve de saludo y despedida, sino que los dos espomos se dispondrían a estar largo tiempo lado a lado.

Cada cual transmitiría al otro sus archivos, con descripciones mutuas de los sectores por ambos visitados. Se expondrían nuevas teorías y modernas interpretaciones. Se intercambiarían obras de literatura y de arte, y se darían conferencias respecto a las distintas costumbres.

Además, habría la oportunidad de intercambiar los genes. Ya que el resultado inevitable de tal cita sería un intercambio de población.

Sin embargo, es posible que tal intercambio genético no pudiera tener lugar en buen número de casos. Una soledad prolongada podría permitir el desarrollo de variedades de imposible cruce. Entonces, habría que verificar si las dos poblaciones eran compatibles entre sí. En caso contrario, podría llevarse a cabo, de todos modos, un intercambio intelectual.

Eventualmente, el espacio contendría, pues, innumerables variedades de inteligencias de gravedad cero, con el universo por hogar, donde todos los seres serían inteligentes, descendientes quizá de un planeta que sólo existiría en su memoria como una leyenda, de la que se habría desvanecido totalmente la primitiva Humanidad terráquea.

Tal vez el Homo sapiens no sea la única especie que efectúe la transición a una cultura estelar. Tal vez exista un punto crucial, alcanzado por todas las inteligencias, desde el cual se bifurquen dos rutas, una rumbo a la verdadera conquista del espacio, y la otra hacia una lenta desintegración en la vida planetaria.

Quizás ahí fuera haya seres inteligentes esperando al hombre. Y cuando nos unamos a ellos, estaremos unidos a dichos seres, no en términos de semejanza material y orgánica, sino en la vida que vivimos y en el intelecto que cultivamos.

¿Es ésta la consecuencia de la nueva fase del cambio, que hará plenamente posible la exploración espacial? ¿O sólo voy dando tumbos en un vano intento de ver lo invisible, de prever lo imprevisible? Tal vez el punto esencial de esta fase del cambio se halla lejos de mi alcance, como el aroma de una rosa lo está del pez, o una sinfonía de Beethoven no puede ser captada por un chimpancé.

¡Pero lo he intentado!

Tercera parte

RELATIVA A LA

CIENCIA-FICCIÓN

32. Escape a la realidad

De todas las ramas de la literatura, la ciencia-ficción es la más moderna. En la respuesta literaria a los problemas peculiares a nuestra era y a nada más.

La literatura normal es, a lo sumo, virtualmente intemporal. Trata de las tensiones de la mente y el alma humanas, y de las relaciones mutuas entre los seres humanos. Presumiblemente, mientras la bioquímica y la psicología permanecen esencialmente inalterables, los estudios penetrantes de esta naturaleza mantendrán su valor a través de las generaciones. Ciertamente, Homero y Shakespeare no muestran síntomas de decadencia. Por tanto, no se trata de esta clase de literatura a la que me refiero.

En el reino más temporal de las literaturas especializadas, el escritor halla su inspiración en un mundo del presente o el pasado, más o menos estilizado. El misterio, el relato deportivo, la narración de aventuras, la novela rosa o gótica, se escriben contra un fondo contemporáneo familiar al lector. La novela histórica y la del «Oeste» contienen fórmulas del pasado, menos familiares pero también aceptadas.

En cada caso, el fondo es «real». Podría desecharse por poseer por este motivo un valor intrínseco escaso, por ser sólo de importancia como escenario en el que se representa el particular drama humano. Posee la falta de importancia de cualquier decorado teatral, o del sillón adecuado colocado en su debido lugar para que el protagonista pueda alcanzarlo con un número determinado de pasos, de acuerdo con la acción de la comedia.

En una clase completamente diferente se hallan esos ejemplos de literatura especializada, en que el fondo o la puesta en escena tiene tan poca relación con la realidad como los mismos protagonistas. Menos, a veces. En esta literatura, tan acostumbrados estamos a la mansedumbre y buena conducta del fondo y a la única ficción de los protagonistas, que existe actualmente la tendencia a dejar que el escenario asuma la verdadera importancia. Esto proporciona a esa literatura esa «sensación» completamente diferente de la usual.

Existen tres tipos de «literatura de falso fondo» que, por orden decreciente de edades, son:

1. Fantasía.

2. Sátira social.

3. Ciencia-ficción.

La fantasía probablemente es sumamente antigua, tanto como el lenguaje. En un mundo primitivo, donde la mayor parte de los aspectos de la Naturaleza y la vida consciente eran desconocidos y aparentemente de conocimiento imposible, salvo por revelación directa, los intentos de explicación por parte del hombre conducían directamente a la fantasía.

Soñar con una persona muerta daba lugar a historias de fantasmas. Los efectos ruinosos de la tormenta y la sequía servían de inspiración para los cuentos de espíritus malévolos. Los hechos poco conocidos se distorsionaban en maravillas, de modo que los rinocerontes se convertían en unicornios, las vacas marinas en sirenas, y los cráneos de los elefantes sicilianos de la Prehistoria en caníbales gigantes de un solo ojo.

En realidad, ¿fue la fantasía realmente fantasía hasta el alborear de nuestra sofisticada edad? ¿Es una historia de fantasmas una fantasía para la persona que cree firmemente en los fantasmas? El fondo, que a nosotros no nos parece guardar relación con la verdad, era el verdadero fondo para nuestros antepasados. En este aspecto la fantasía anterior a nuestra época fue simplemente otro aspecto de la literatura contra un fondo familiar.

Las fantasías modernas se escriben y son leídas por el público que sabe que lo son. Mas la neofantasía todavía halla su inspiración en las deducciones del pasado. Los cuentos todavía tratan de fantasmas y vampiros, de brujas y demonios, con el uso de encantamientos, y los peligros de la maldad. Tales relatos, en la actualidad, tienen más éxito si se escriben exclusivamente con el propósito de entretener. Ya no asustan a nadie.

La sátira social es, por entero, más sofisticada que la fantasía. Si ésta es un tipo universal de literatura regional, la sátira social es la obra de un intelecto avanzado y atrapado en una sociedad que no recibe bien las críticas. (Casi debería decir «atrapado en una sociedad», sin la frase calificadora, ya que no hay ninguna sociedad que guste de las críticas.)

En su forma primitiva, la sátira social encontró su forma en las fábulas de animales, como las del famoso Esopo. En las mismas, los animales hablaban, y la sociedad humana se imponía sobre sus características animales, comportándose de modo que ridiculizaban las tonterías y los crímenes humanos. El lector reía y asentía vehementemente, sin enfadarse ante las tonterías y los crímenes de los animales, o sea, en realidad, gozando de su superioridad.

El satírico depende de la reflexión posterior, de la meditación sobre la moraleja de la fábula. Y como el lector ha quedado seducido y ha aceptado dicha moraleja, puesto que desaprueba la conducta de un animal y no de la raza humana, es menos probable que se enoje.

Las parábolas de la Biblia y las divertidas historietas contadas por Lincoln estaban destinadas a dar unas moralejas indirectamente, con lentitud, profundas.

La sátira social se gradúa desde la anécdota al tratado, y el ejemplo más célebre es la Utopía de Tomás Moro. Este libro trata de la sociedad de una isla ficticia. (La palabra «utopía» significa en griego «ningún lugar», lo mismo que el Erewhon de Samuel Butler, que significa «nowhere»[12] deletreado al revés.) Tomás Moro utilizó su sociedad ficticia como un látigo que azotó a su propia sociedad. Utopía es alabada como justa y virtuosa por aquellos aspectos que de modo más claro le faltaban a la sociedad del tiempo del autor. Y el lector tenía que estar de acuerdo con aquél respecto a los fallos graves de la sociedad. Luego, lentamente, el lector comenzaba a sentirse poco satisfecho con su mundo, que no era realmente ninguna utopía.

Los Viajes de Gulliver, de Jonathan Swift, son un ejemplo de sátira en ambos estilos. Los liliputienses de la primera parte y los laputanos de la tercera quedan ridiculizados por sus necedades (trazadas hasta el exceso) comunes a la sociedad contemporánea de Swift. Los brobdingnagianos de la segunda parte y los houyhnhnmos de la cuarta son exaltados por aquellas virtudes que no poseía claramente la propia sociedad de Swift.

Es posible confundir la sátira social con la ciencia-ficción porque en esta última se describe siempre una sociedad diferente de la real. Hacerlo es particularmente fácil porque, de forma ocasional, en las descripciones de sociedades ficticias, los autores incluyen detalles de una ciencia o una técnica mucho más avanzada que la suya propia. Por ejemplo, en Utopía, Moro describe el uso de unas incubadoras para criar polluelos; en la tercera parte de Viajes de Gulliver, Swift describe un descubrimiento ficticio de dos lunas en Marte (todo lo cual resultó más adelante ser completamente correcto en todos sus datos).

Sin embargo, es importante comprender que los satíricos sociales no estaban primordialmente interesados en sus sociedades ficticias. El autor satírico mantiene sus ojos firmemente clavados en su propia sociedad y emplea las creaciones de su imaginación para dar lecciones de moral. Sus sociedades ficticias no son lo que podrían ser, sino solamente lo que deberían o no deberían ser.

En el siglo pasado, los autores dedicados a la sátira social emplearon deliberadamente los progresos científicos como una de sus armas preferidas. Así nacieron El año 2000, de Edward Bellamy, Un mundo feliz, de Aldous Huxley, y 1984, de George Orwell, entre las obras más conocidas de este estilo.

Es casi inevitable considerar tales obras como pertenecientes a la ciencia-ficción, aunque no lo sean en realidad. El intento del autor es completamente moralizador. Bellamy alaba a su sociedad, y Huxley y Orwell la denuncian, cada uno con el deseo de hacerla cambiar, de hacerle perder las malas costumbres que las afligen. Es sátira social, a pesar de toda su ciencia.

Entonces, ¿qué es la ciencia-ficción?

La ciencia-ficción, como la fantasía y la sátira social, se refiere a un «fondo» que no es real. Al revés que la fantasía, su fondo no está completamente divorciado de la realidad, sino que representa de manera más o menos plausible una extrapolación de la realidad. Al revés que la sátira social, el fondo irreal existe por sí mismo, no para aplicaciones morales.

La ciencia-ficción puede definirse como la rama de la literatura que trata de la respuesta de los seres humanos a los progresos de la ciencia y la tecnología.

Los cambios en ciencia y en tecnología, al ocurrir con brusquedad y afectar profundamente al ser humano dentro de su existencia normal, representan un fenómeno peculiar del mundo solamente desde la revolución industrial (con algunas excepciones locales y temporales). Es un fenómeno que ha existido en Inglaterra y los Países Bajos desde 1750; y en Estados Unidos y Europa Occidental desde 1850; y, en general, en todo el mundo a partir de 1920.

El primer escritor conocido que respondió a este nuevo factor en los temas humanos, tratando regularmente con la ciencia-ficción, estudiando el efecto del progreso científico sobre la Humanidad, sin acentuar las enseñanzas morales, fue Julio Verne. En lengua inglesa, el primer maestro fue H. G. Wells. Entre ambos, sentaron los cimientos de todos los temas que los escritores de ciencia-ficción han venido tratando desde entonces.

Hasta 1926 no se instaló un mercado especial exclusivamente para los productos de la ciencia-ficción. Fue aquel año cuando Hugo Gernsback publicó por primera vez las Amazing Stories[13]. Hacia 1930, se hallaban en las librerías y quioscos otras tres revistas de ciencia-ficción.

Fue posible, lentamente (y económicamente) que un joven decidiese vivir de la ciencia-ficción, pero transcurrieron diez años antes de que los escritores estuvieran suficientemente desarrollados para alcanzar la madurez en este campo literario.

El período de madurez suele darse frecuentemente como el momento en que John W. Campbell, junior, pasó a ser editor de Astounding Stories[14] (que rápidamente tituló Astounding Science-Fiction)[15]. Era el 6 de octubre de 1937.

Para Campbell, la ciencia-ficción era esencialmente como la definida anteriormente. Acentuó las aventuras de ciencia-ficción con nuevos inventos o avatares en otros mundos (una especie de super-western, donde las naves espaciales reemplazaban a los caballos, y las pistolas de rayos a los revólveres), y los argumentos daban entrada a meditaciones respecto a las posibles sociedades del futuro.

Tras el lanzamiento de la bomba atómica, la ciencia-ficción se cubrió de respetabilidad. Muchos que habían considerado las historias relativas a una guerra atómica (impresas con todo detalle a principios de 1941) sumamente ridiculas, e incluso patológicas, se apresuraron a rectificar sus criterios. La masa de lectores aumentó. Las revistas populares comenzaron a publicar ocasionalmente relatos de ciencia-ficción. Algunos editores (particularmente «Doubleday y Compañía», de Nueva York), publicaron novelas de ciencia-ficción. Y se editaron nuevas revistas especializadas.

Hacia 1950, aparecieron La revista de la Fantasía y la Ciencia-Ficción y Ciencia-Ficción de la Galaxia, que, juntamente con Astounding (actualmente bautizada como Analog Science Fact… Science Fiction) se consideraron como las «tres grandes» de estos temas.

La política editorial de las «tres grandes» ofrece un contraste interesante. Todas se dedican a la ciencia-ficción, pero Analog se adhiere más rígidamente a la ciencia-ficción en el sentido más puro. Tal como implica su nombre. Fantasía y Ciencia-Ficción añade una generosa dosis de fantasía moderna, mientras que Galaxia contiene bastante sátira social. De esta forma, están representadas las tres ramas principales de la «literatura del fondo falso».

Muchas personas (incluyendo algunos lectores de ciencia-ficción) no conceden ninguna importancia a dicha literatura…, salvo, quizá, como medio de obtener cierta diversión. Lo cual significa una subestimación muy grave de la importancia del tema.

Esta subestimación se debe en parte a que las formas de «ciencia-ficción» más familiares al público en general son las aventuras de dibujos de personajes tales como Flash Gordon y Supermán, y la galería de «monstruos» creada por Hollywood[16].

Ni los dibujos ni las cintas de Hollywood suelen ser auténtica ciencia-ficción. Ahí reside la confusión. Ambos son el resultado de añadir cierto aspecto nebuloso de ciencia a un tipo muy viejo de literatura: la de fantasía y aventuras. Sustituyamos el dragón que mata Sigfrido por el monstruo igualmente fabuloso matado por Flash Gordon, y apenas habrá que efectuar otros cambios. La Quimera que devasta el país y ha de ser exterminada por Belerofonte montado en su caballo volador. Pegaso es semejante al monstruo que surge de veinte mil brazas de profundidad en la laguna negra, y debe ser aniquilado por el protagonista cinematográfico desde su aeroplano.

Para hallar ciencia-ficción madura, verdadera ciencia-ficción, hay que leer las revistas y los libros especializados. Y aún no todas las historias son «buenas». (Si bien, realmente, ¿por qué hay que esperar que toda, o casi toda, la ciencia-ficción sea buena? Uno de los mejores autores de ciencia-ficción afirmó ante un auditorio de fanáticos seguidores de dichos temas: «Unas nueve décimas de la ciencia-ficción son malas.» Los oyentes se quedaron atónitos, y el escritor añadió solemnemente: «Las nueve décimas partes de todo son malas.»)

Sin embargo, entre lo malo hay algunos relatos entretenidos, bien escritos y excitantes, incluso distintos del resto de la literatura. En ellos se hallan sociedades nuevas y extrañas, algunas orientadas primordialmente hacia la publicidad y su psicología; otras escondidas en ciudades subterráneas; unas enfrentadas con el descubrimiento de nuevas formas de vida inteligente; y las demás encaradas con la falta de recursos o el aumento de población; también son comunes la telepatía y sus implicaciones.

¿Es esto importante? Claro que sí. La buena ciencia-ficción es divertida, mas al propio tiempo cumple algo que no se encuentra en las demás formas de literatura: considera el futuro de manera consistente.

Vivimos en una sociedad que, por primera vez, ha de considerar el futuro. Hasta 1750, el hombre de la calle estaba seguro de que, hasta el día del Juicio Final, la vida, en sus aspectos más esenciales, siempre sería igual, aparte de algunos cambios en el reparto de personajes que interpretaban el drama humano.

A partir de 1750, los hombres comprendieron cada vez más que la sociedad iba cambiando hacia direcciones imprevisibles y extrañas, y que seguiría en este rumbo; que lo que había sido bueno para el padre no lo sería para el hijo; que las cosas ya no continuarían siendo siempre igual.

Después de 1945, los hombres comprendieron que incluso el mero hecho de la continuada existencia de la sociedad humana en cualquier forma ya había periclitado. La posibilidad de una nueva clase de día del Juicio Final comenzó a imponerse.

La ciencia-ficción se basa en los cambios sociales. Acepta el cambio. En cierto sentido, ensaya distintos cambios; trata de intuir las consecuencias de los mismos y, en forma de narración, presenta los resultados al público, a un público que cada vez más necesita advertir las posibilidades del cambio antes de que el desastre lo arrastre consigo.

Por todo esto, resulta irónico que se tilde a la ciencia-ficción de «literatura de escape». No se trata de «escape» en el sentido del «es imposible» de la literatura normal de ficción, ni del «nunca fue» de la fantasía, sino que se trata del «podría ser». Es una forma extraña de literatura de escape que inquieta a sus lectores con bombas atómicas, superpoblación, guerra bacteriológica, viajes a la Luna, y otros fenómenos por el estilo, antes de que el resto del mundo se enfrente con tales problemas. (¡El resto del mundo habría tenido que escuchar nuestros avisos mucho antes!)

No, no, si la ciencia-ficción es un escape, lo es hacia la realidad.

Los escritores de ciencia-ficción no siempre saben lo que hacen. Muchos jurarían verazmente que sólo les interesa escribir un relato más o menos plausible y ganarse un dólar honradamente. Sin embargo, para mí representan los ojos de la Humanidad vueltos, por primera vez, hacia la contemplación ciega y agónica del excitante y peligroso futuro, no de los individuos, sino de la raza humana en general.

33. El culto de la ignorancia

El 25 de junio de 1956, contemplé el Programa del Productor por televisión, y asistí, en forma sorprendente, al conflicto entre la Necesidad de la Educación y el Culto de la Ignorancia. La Necesidad de la Educación llegó a casa con el primer anuncio. La firma comercial necesitaba, al parecer, ingenieros de cohetes, y deseaba atraerlos hacia una factoría de Florida. El anuncio destacaba el clima y las playas de la localidad, las buenas condiciones de trabajo, el alojamiento barato y cómodo, la magnífica paga, el progreso rápido, la sólida seguridad. Ni siquiera se necesitaba experiencia. El efecto era tal que experimenté el impulso de echar a correr, no a andar, hasta el aeropuerto más próximo y subir al avión de Florida.

Tras haber superado tal impulso, y habiendo meditado medio segundo sobre la falta de ingenieros y técnicos, falta debida al carácter intensificado de nuestra técnica, me dispuse a gozar de la comedia presentada, que era una adaptación de Feliz cumpleaños, de Anita Loos, protagonizada por Betty Field y Barry Nelson. Me gustó; la comedia era excelente…, mas la empresa apadrinadora del programa, que antes estaba de rodillas pidiendo técnicos e ingenieros, estaba ahora pagando para presentar ante un auditorio de millones de personas el siguiente argumento:

Barry Nelson es un funcionario de Banco que pasa gran parte de su tiempo libre en un bar, porque en él encuentra mujeres (según explica). La escena representa el bar y los personajes constituyen un grupo picaresco de chicas de corazón de oro. Barry Nelson, en la comedia, explica que no lee libros (dialoga con una bibliotecaria) aunque, admite confundido, los leía antaño. Dice que su padre le entregaba algún dinero si aprendía a recitar los libros de la Biblia por orden, y para demostrar que aún sabe hacerlo, los recita, explicando que de niño lo hacía con mucha más rapidez. Así, se le presenta al televidente un ejemplo de cómo se aprende en los libros, dejando bien sentado que es algo ridículo e inútil, y que Barry hizo muy bien de arrinconarlos y dedicarse a los bares.

Betty Field, por otra parte, es una bibliotecaria, o sea, una joven educada, que de vez en cuando sí lee libros. Es tímida, corroída por la desgracia y, naturalmente, los chicos no la miran siquiera. En la comedia, viola los hábitos de la abstención y toma un trago, luego otro… y otros más. Lentamente, se va despojando de sus inhibiciones. Queda en ella aniquilado el estigma de la inteligencia, capa a capa, en tanto ella desciende hacia los estratos del alcoholismo. El resultado es que los asistentes al bar, que antes la miraban con grandes suspicacias, acaban por convertirla en su heroína; su alcohólico padre, que antes le pegaba, la adora de corazón; y, más importante aún, el empleado de Banco, que jamás se había fijado en ella, la ama con violenta explosión.

Repito que la comedia me encantó. Y sin embargo, considerada a la sobria luz de la mañana siguiente, la comedia me pareció que presentaba un estereotipo de lo que podríamos llamar el Culto de la Ignorancia. Según el mismo, sólo se halla la felicidad en la ignorancia; y la educación y la sabiduría conducen a todas las desdichas de la existencia.

¿Existe una relación entre esto y el hecho de que la empresa apadrinadora del programa tuviese falta de técnicos e ingenieros?

Sí, necesitamos técnicos. La sociedad los necesita en conjunto, o quedará aniquilada bajo el peso de sus propias máquinas. Mas, ¿cómo intentamos obtenerlos?

¿Es suficiente que una empresa intente seducirlos? Lo que con esto se consigue es que los ingenieros abandonen una especialidad para dedicarse a otra, faltando así en otra rama del saber. Si una comunidad es bastante rica para poder apoderarse de lo que lavó otra, esto dará buen resultado, de lo contrario, no.

Se han sugerido soluciones al problema de la mano de obra especializada. Algunos aconsejan que se pague mejor a los profesores científicos, que se otorguen becas a los estudiantes inteligentes, que los químicos y físicos dediquen parte de su tiempo a la enseñanza… Todo esto es válido, pero, ¿permitirá ir muy lejos? Si hubiera bastantes profesores expertos en ciencia, ¿a quiénes enseñarían? A un grupo de estudiantes, la mayoría de los cuales habrían aprendido ya en la niñez las limitaciones de la gente educada y el valor de la ignorancia natural.

Pensemos en los modelos literarios del «chico malo», los mejores de los cuales fueron Tom Sawyer y Penrod Schofield (con ejemplos más modernos dados por la Radio y la Televisión). La escuela es su enemiga; los maestros son odiosos; aprender a leer, un fastidio, una desilusión. ¿Cuáles son los malvados de la historia? Sid Sawyers y Georgie Bassets, víboras que visten ropas limpias, hablan un inglés correcto y les gusta la escuela (criaturas abominables).

Jamás robé la manzana del huerto del vecino ni quité un melón de su pila (claro que en Brooklyn hay pocas oportunidades de tales travesuras), pero me vi bastante seducido por la hipócrita habilidad del autor y aprendí a detestar a los favoritos del maestro que no se dedicaban a tales jugarretas, que no mentían jamás y que estudiaban, sin querer participar en esos deliciosos juegos de la delincuencia juvenil.

Tal vez fuesen nuestros antecedentes pioneros, cuando la escuela sólo parecía un medio de apartar a un chico de sus deberes y hacerle aprender las declinaciones latinas, ante la desesperación de su abrumado padre. Fuese como fuese, muchos de nosotros recordamos aún la rechifla exhibida por los periódicos ante los «profesores» de los primeros días del New Deal. Se da también por descontado que Adlai Stevenson se vio ayudado en sus derrotas a la presidencia en 1952 y 1956 por su persistente revelación de inteligencia.

¿Habéis observado alguna vez el papel representado por los lentes en los cines y la televisión? Las gafas, en el arte más popular de la actualidad, son el símbolo del intelecto bien desarrollado (seguramente debido a la errónea creencia de que la gente educada arruina su vista mediante el pernicioso vicio de leer). Ordinariamente, los protagonistas de una película no llevan gafas. Ocasionalmente, el protagonista es un arquitecto o un químico, y sí las lleva a fin de demostrar que ha asistido a la Universidad. En este caso, se las quita a cada momento, puesto que no es posible ser viril y llevar gafas al mismo tiempo. Cierto, se las pone para leer, y se las vuelve a quitar, para asumir el papel de «macho» asignado en la cinta.

Otro ejemplo mejor lo dio Hollywood por medio de una situación que en la actualidad el propio Hollywood ha reducido a polvo (cosa casi increíble). La situación a que me refiero es aquella en que se supone que una bellísima actriz, a la que llamaré Laura Hermosa, es fea, puesto que lleva gafas.

Esto ha ocurrido en innumerables ocasiones. Laura Hermosa es bibliotecaria o profesora (las dos ocupaciones femeninas que, de acuerdo con los convencionalismos de Hollywood, garantizan la soltería y la desdicha) y, naturalmente, lleva unas gafas de concha de carey (el tipo más intelectual) para indicar tal cosa.

Cualquier hombre que asista a la proyección de la película experimentará una reacción a la vista de Laura Hermosa con sus gafas exactamente igual que si no las llevara. Mas ante la vista distorsionada del protagonista de la película, Laura Hermosa con sus gafas es muy fea. En un momento dado de la cinta, una amiga de Laura, que conoce bien la vida, le quita las gafas. De repente, resulta que ella puede ver muy bien sin lentes, y nuestro protagonista cae rendidamente enamorado a los pies de la ya bellísima Laura, con lo que se logra un final perfecto.

¿Existe una persona tan obtusa que no vea que: 1), la presencia de las gafas no arruina en modo alguno la perfección física de Laura, y que el protagonista debe saberlo, y 2), que si Laura lleva gafas por algún motivo de peso, el hecho de quitárselas dará lugar a que bese a otro hombre, puesto que probablemente es cegata de nacimiento?

No, las gafas no lo son en el sentido literario. Sólo son un símbolo, un símbolo de inteligencia. Y al auditorio se le enseñan dos cosas: a) La evidencia de una educación extensiva es un mal social y provoca la infelicidad; b) La educación formal es innecesaria, puede reducirse a voluntad, y el desarrollo intelectual limitado conduce a la felicidad.

Tenemos que combatir este modelo de ignorancia humana hacia una educación incompleta y marchita, si queremos poseer suficiente materia prima, es decir, niños que crezcan en el respeto y admiren la inteligencia, niños a los que habrá que añadir más adelante los atractivos antes enumerados (dinero, seguridad, prestigio), a fin de aumentar nuestras reservas de científicos y técnicos.

Lo que parece esperanzador a este respecto es que exista una rama de la literatura popular dedicada a la afirmación de que el cerebro es muy necesario. A esta rama se la conoce como ciencia-ficción (ver Capítulo 32).

Naturalmente, un relato de ciencia-ficción puede ser totalmente frívolo, cual lo sería el caso de una narración que tratase de un hombre que inventase un aparato gracias al cual pudiera ver sin obstáculos a través de las paredes y los vestidos. Está claro que, bien llevado, el resultado sería muy cómico, pero muy poco común. Una historia de ciencia-ficción puede incluso ser anticientífica, como una escrita hace muchos años atrás, que describía la Tierra destruida por las bombas atómicas, con escasos y diseminados supervivientes, todos convencidos de que tal desastre no habría sucedido de haber evitado la Humanidad meter las narices en la ciencia, y haberse aferrado exclusivamente a las cosas más simples de la existencia.

Mas una parte significativa de los relatos de ciencia-ficción posee como motivo principal un problema técnico, y sus protagonistas suelen ser individuos sabios.

Podría citar muchos ejemplos entre mis propios argumentos. Uno trata de un grupo de científicos que viajan hasta un planeta muy lejano para encontrar la razón de la muerte en masa de un grupo anterior de colonos, a pesar de la naturaleza ideal del planeta para la vida humana en él. La respuesta es que la corteza del planeta posee un alto porcentaje de componentes de berilio, siendo la muerte la consecuencia del envenenamiento por tal producto.

Otra narración se refiere a los esfuerzos de un historiador para obtener permiso del Gobierno para utilizar la máquina «del tiempo», con el fin de conseguir datos sobre la antigua Cartago.

Ante la negativa del Gobierno, el historiador contrata los servicios de un físico para que construya una máquina del tiempo…, con unos resultados trágicos, totalmente inesperados.

En el primer relato, se produce una consideración del problema de la creciente cantidad de datos científicos y la comprensión de la incapacidad de la mente humana para contender con una fracción de los mismos. En el segundo, hay la descripción de lo que podría suceder en una sociedad cuyas concesiones gubernamentales fuesen la única contribución a la investigación. Todo esto se halla muy por encima de las películas de monstruos que suele realizar Hollywood, bajo el nombre de «ciencia-ficción».

Mas, tanto los relatos como su fondo sociológico, son menos importantes que el hecho de que, aunque el científico de marras sea el héroe o el villano (según sea inteligente y simpático al lector, o inteligente y antipático), la ciencia y la inteligencia en sí, como fuerzas abstractas, están representadas simpáticamente. La investigación científica se presenta, casi invariablemente, como un proceso excitante, emocionante; usualmente, sus fines son buenos en sí mismos y para la Humanidad, y sus protagonistas son personas inteligentes, dignas de admiración y respeto.

Naturalmente, los escritores de ciencia-ficción no hacen esto deliberadamente. En este caso, sus narraciones resultarían impublicables, o tan aburridas que harían más daño que bien.

Estos resultados se obtienen impensadamente. Pese a que un autor de ciencia-ficción piense siempre en escribir con dignidad, a fin exclusivamente de ganarse el sustento, jamás puede escapar al atractivo de narrar una historia inteligente, educadora, científica. Éste es el subproducto secundario e inevitable de la ciencia-ficción.

NOTA ESPECIAL. – Cuando redacté este capítulo y fue leído por primera vez, la gente lo recibió con una gran falta de interés. Un año más tarde, la Unión Soviética lanzó el Sputnik I, el primer satélite, y de repente nos vimos inmersos en una carrera tecnológica con nuestro rival, al que hasta entonces habíamos subestimado.

De repente, todo el mundo empezó a atacar al culto de la ignorancia, y quizá ya el tema de la educación no será nunca como antaño.

Sin embargo, encuentro justo señalar que es siempre deseable ver el borde del precipicio antes de caer en él. Gritar después, es muy fácil.

34. La espada de Aquiles

Hacia 1200 a. de C. (dice la Historia) las fuerzas griegas se disponían a atacar la ciudad de Troya. Un oráculo profetizó que el ataque sería vano a menos que el joven Aquiles se uniese al ejército griego. Pero la madre de Aquiles, la ninfa Tetis, había ataviado a su hijo con ropas de mujer, escondiéndolo entre las damas de la corte de la isla egea de Scyros. Sabía que si su hijo iba a Troya moriría y, maternalmente, hallaba la perspectiva poco grata.

Llegó a Scyros una delegación de griegos mandados por el voluntarioso Odiseo.

No habría sido muy político registrar a todas las damas, mas Odiseo estaba especializado en medios indirectos. De esta manera, exhibió una serie de finos vestidos y joyas, y les rogó a las damas que cogiesen lo que más les gustase, a lo que ellas accedieron encantadas.

Entre las ropas se hallaba escondida una magnífica espada. Una de las doncellas avanzó, la cogió y la manejó con sorprendente agilidad y destreza. La doncella era, claro está, Aquiles, que se marchó a Troya, donde halló la muerte.

En aquella época las guerras eran diferentes. Tanto en las campañas contra los enemigos humanos como en las libradas en contra de las fuerzas de la Naturaleza, los buenos guerreros son actualmente nuestros científicos e inventores.

Los científicos nacen y se hacen. La chispa existe, sí, pero puede extinguirse con suma facilidad. Por tanto, los educadores se enfrentan hoy día con una grave tarea: la de inventar métodos de enseñanza que estimulen la creatividad en los jóvenes.

Mas, enseñar creatividad, es en sí una tarea consumidora de creatividad. Requiere unos maestros superlativamente buenos y unas técnicas altamente imaginativas. Esparcir tal educación, aunque pudiera hacerse, sería perder el tiempo. Aunque todos los seres humanos posean cierto grado de creatividad (¿quién lo duda, al presenciar los innumerables descubrimientos que hacen los niños cuando crecen?), este don se halla en mayor grado en unos seres que en otros, y no siempre se inclina hacia la ciencia. Si nuestra sociedad ha de desarrollar la creatividad en las ciencias con un máximo de eficacia, hemos de buscar la veta más rica; hemos de hallar a los niños que posean el más alto potencial y enfocar nuestros esfuerzos sobre ellos.

Mas, ¿cómo se detecta a un científico creador en potencia? Naturalmente, hay niños prodigio. No hay duda de que el joven Arrenio y el joven Gauss estaban destinados a grandes cosas en caso de vivir, aunque hubiesen carecido de educación. Por otra parte, Isaac Newton no prometía gran cosa hasta los dieciséis años. A simple vista, es posible incluso confundir la creatividad en flor con la mentalidad retrasada o la delincuencia juvenil, ambas de cuyas cualidades sospecharon sus contemporáneos en Thomas Alva Edison.

Los hombres han tratado de imaginar ensayos y análisis para la creatividad, y han querido llegar a unos criterios de selección empíricos, anotando las cualidades que los individuos de conocida creatividad tenían en común.

Mas todos estos ensayos y criterios son inseguros, y extremadamente discutidos.

Necesitamos una prueba sencilla, algo tan simple como la espada de Aquiles. Una medida que sirve, rápidamente y sin ambigüedades, para seleccionar la creatividad en potencia, de entre las filas populares. No podemos esperar que con tal prueba quede aparte el joven verdaderamente creador. Satisfechos estaríamos, a mi entender, con encontrar un subgrupo en que la incidencia de creatividad en potencia fuese superior, sustancialmente, a la de la población en general.

Me gustaría sugerir tal espada de Aquiles. Podría ser ésta simplemente el interés por la buena ciencia-ficción. Esta sugerencia no es una simple adivinanza de mi parte. Está basada en el cálculo (creo yo) de una razonable validez. Veamos.

Yo, entre otras cosas, soy escritor de ciencia-ficción, Y sé que mis obras se venden bien. Una de ellas, en las ediciones americanas, incluyendo las de bolsillo, alcanzó la venta de 400.000 ejemplares. Una parte fue a las bibliotecas donde tal vez una docena de personas leyeron cada ejemplar. Luego, muchos individuos adquirieron un ejemplar de bolsillo y solamente le echaron una breve ojeada, sin interés. Supongamos de forma razonable, que estas dos cantidades se anulan entre sí; entonces, podemos calcular que en Estados Unidos existen unos 400.000 individuos interesados en la ciencia-ficción.

Es éste un cálculo muy generoso, porque me han dicho que la ciencia-ficción salida de mi pluma se vende mejor que la de otros, y yo he elegido uno de mis libros, en realidad, el que obtuvo más venta. Mas, gracias a este generoso cálculo, podemos afirmar que de un total de población de 180.000.000 de norteamericanos, uno de cada cuatrocientos cincuenta está interesado en ciencia-ficción.

Consideremos que durante un cuarto de siglo he vivido y trabajado en el mundo académico, y en círculos donde he conocido a muchos científicos. La mitad (y no me refiero a todos los científicos que he conocido sino exclusivamente a los que juzgo creadores) han leído relatos de ciencia-ficción en algún momento de su existencia.

En una reciente conferencia sobre los métodos para enseñar ciencia creadora, a la que asistí, y en la que hablé, sugerí este cálculo en una conversación privada, y mi interlocutor sostuvo con vehemencia que, no el 50%, sino el 95 % de los presentes estaba interesado en ciencia-ficción. Mas rebajemos un poco tanto entusiasmo y quedémonos en el 50 %. Uno de cada dos individuos.

Podría argüirse que el interés de un científico por la ciencia-ficción es un mero reflejo de su preocupación profesional. No creo que tal sea el caso, puesto que muy raras veces se empieza a leer ciencia-ficción de mayor. Esta costumbre empieza en la adolescencia, por regla general, y el interés por la ciencia viene estimulado por la lectura y no a la inversa.

Comparemos este cálculo algo conservador de uno entre dos individuos interesados en ciencia-ficción entre los científicos creadores, con el generoso cálculo de uno entre cuatrocientos cincuenta de la población en general. Sólo cabe concluir que, mediante el simple proceso de escoger a todos los lectores de ciencia-ficción entre los jóvenes de diez a quince años, podemos concentrar el índice de creatividad científica en potencia en gran parte.

Si este razonamiento tiene alguna validez, y estoy seguro de que sí, es una vergüenza que a veces actúen las fuerzas para impedir que un joven goce con los relatos de ciencia-ficción. Los profesores ingleses a menudo ponen la ciencia-ficción en el grupo de material de lecturas prohibidas para los estudiantes, y no aceptan, por ejemplo, una novela de ciencia-ficción criticada como un razonable ofrecimiento en respuesta a un deber realizado en casa. (He recibido innumerables cartas de lectores jóvenes quejándose de este abuso.)

Muchos profesores ingleses no están interesados en ciencia ni en ciencia-ficción. Molestos ante los relatos de un mundo extraño al suyo, que les parece fantástico, siguen el camino más fácil y prohiben tales lecturas. Afortunadamente, esta tendencia va disminuyendo, pero me gustaría que desapareciese lo antes posible.

La ciencia-ficción tiene sus buenos ejemplos, como las demás ramas de la literatura, y si los maestros ingleses, por falta de experiencia, tienen dificultad en distinguir la buena de la mala ciencia-ficción, sólo tienen que solicitar ayuda, y lo digo con toda sinceridad, a cualquier rapaz de doce años de su clase.

Si la ciencia-ficción estuviese en las bibliotecas, y a los estudiantes no se les prohibiese leer tales libros, me atrevo a pronosticar que la espada de Aquiles sería excelente.

Como es natural, no serviría ello para descubrir a todos los científicos creadores en potencia, y el porcentaje de tales chicos es tan bajo que ni siquiera un subgrupo contendría una gran mayoría. Sin embargo, seriamos más afortunados por tener tales grupos, que sin ninguno, como estamos ahora.

Y desafío a todo el mundo a que encuentre una espada de Aquiles mejor.

NOTA ESPECIAL. – Si alguien supone que mis cálculos, respecto a la importancia de la ciencia-ficción, están dictados por mi afición a la misma, según los he descrito en los tres capítulos precedentes, que siga, por favor.

Los tres capítulos finales de esta obra representan una gentil sátira de dicho género.

En cierto modo, son la medida de mi creencia en el valor del tema. Considero que vale lo bastante para soportar y resistir, sin daño, un poco de diversión a sus expensas.

35. Cómo no construir un robot

Jamás me han pedido que actúe de consejero técnico en un programa de televisión. Una lástima para la televisión, claro, pero yo soy una persona muy atareada y no tengo tiempo de lamentarlo por la TV. Que paguen las consecuencias.

Hablo, claro está, en condiciones ordinarias. En la temporada 1964-65, no obstante, observé la tendencia a ir demasiado lejos. Me refiero al programa My living dolly[17].

Esta serie trata del Robot AF-709, construido en secreto por un científico en un centro espacial. El robotista es destinado al Pakistán, y deja el robot a su mejor amigo, un psiquiatra llamado doctor McDonald, para que lo guarde. Es preciso que nadie sospeche que el robot es un robot, ¿entendido? Esto presenta ciertas dificultades, ya que el científico ha fabricado el robot en forma de humanoide. En realidad, el robot se llama Rhoda y semeja una mujer de físico encantador.

¿Verdad que es un desastre? Yo debí preverlo. Mí tremenda modestia me impide confesar que yo soy una gran autoridad en robots…, por lo que sólo brevemente me refiero a ello. Yo soy una autoridad en robots.

De habérmelo preguntado, yo habría contestado:

–No es posible construir un robot con la forma de una mujer de físico espectacular. En efecto, se trata de una ingeniería muy mala en robots.

El mejor robot es el construido de metal, con un cuerpo cilindrico y suave; una cabeza cónica, y unos miembros tubulares y resistentes. En la geometría de un robot hay una majestad sombría y retumbante, que muy pocos cambiarían por la lastimosa silueta irregular de Rhoda el Robot. Y al decir pocos, me refiero a pocos expertos en robots.

La serie pretendía demostrar que el robot estaba destinado a comprobar los efectos del ambiente espacial en los astronautas. Por tanto, era deseable una cubierta de plástico, con propiedades semejantes a las de la piel humana, imitando las curvas y los planos del cuerpo humano…, femenino. Pero…, y aquí es donde lo ilógico me abruma, ¿por qué imitar el cuerpo femenino cuando los astronautas son masculinos? Sí, cierto, las interioridades de un robot son relativamente abultadas, de modo que un robot ha de ser ancho.

Pero de construirse con forma masculina, el tamaño necesario no sería tan notable. En la forma femenina resulta poco grato, y atrae una atención poco deseable.

Incluso con una chica más alta de lo normal, hay espacio suficiente para todo el equipo interno. Por lo tanto, tiene que haber bultos e irregularidades suficientes para esconder y disimular los controles, ya que no pueden ocultarse en el torso propiamente dicho. Naturalmente, tales bultos atraen las miradas… de desaprobación.

Todo esto ha dado como resultado que los productores sufrieran enormes molestias para encontrar una joven de físico espectacular, cuando hubieran podido elegir un hombre ligeramente mayor de lo normal. En realidad, se tomaron muchas molestias para nada.

¿Podrían decirme, o a otro cualquiera, qué ganaron construyendo un robot desequilibrado, con la forma femenina? Mientras estaba en mi salita contemplando a Rhoda la Robot en la pantalla, me vi impulsado a examinarla atentamente, a fin de calcular hasta qué punto estaba desequilibrada. Cualquier experto en robots hubiese experimentado el mismo impulso.

Además, había la cuestión de los controles. Aquellos que no vieron ese programa no lo creerán, pero les doy mi palabra de que había exactamente cuatro botones de control, semejantes a otros tantos lunares, colocados en la parte superior de la espalda.

No estaban señalados de ninguna forma, y el doctor McDonald, que no era muy inteligente (cosa rara, ya que los psiquiatras son notables por su elevada inteligencia y su rápida comprensión, según me contó uno de ellos), nunca recordaba cuál era el control debido. Además, como estaban tan expuestos, en particular el de abrir y cerrar, podían estropearse por accidente.

Es risible suponer que cuatro controles bastarían para hacer funcionar un robot de la complejidad de Rhoda. Había que contemplarla con la mirada de un experto para ver que respondía a más de cuatro clases de estímulos.

Cualquier niño se daría cuenta de que serían necesarias varias seres de botones, aparte de varios numeradores e interruptores, y un par de tornillos de ajuste. Para todos estos controles existe también un sitio lógico, que es el abdomen.

Consideremos las ventajas de tal lugar.

Primero: el abdomen del robot está de cara a su manipulador, el cual puede así manejarlo siempre apropiadamente. No necesita, como le ocurría constantemente al doctor McDonald, levantar o girar a Rhoda a fin de llegar a su espalda.

Segundo, si bien la espalda, gracias a las modas femeninas, se halla expuesta a un toque accidental, el abdomen está cubierto por varias capas de tela. Los controles abdominales estarían así mejor protegidos contra contactos intempestivos o casuales.

El abdomen, además, asegura un sitio perfecto para la protección del botón de abrir y cerrar. Puede colocarse, como medida de seguridad, dentro del ombligo.

Al fin y al cabo, considero altamente significativo que el abdomen del robot jamás estuviera al descubierto durante el espectáculo. En la primera parte y hasta más de la mitad de la segunda, el robot sólo llevaba una tela que le cubría desde las axilas a media pierna. Para una mente adiestrada, esto es muy significativo.

¿Qué escondía el robot, sino su abdomen? ¡Protegía sus controles! En dos ocasiones, una en la primera parte y otra en la segunda, el robot fingió quitarse la tela y el doctor McDonald lo impidió, muy agitado.

Esto me desalentó muchísimo, ya que de haber podido ver aquellos controles, habría quedado demostrado que los productores habían recibido buenos consejos practicos, al fin y al cabo, y que los «controles» de la espalda sólo servían para ocultar los verdaderos por razones de seguridad.

Pensando esto, y lleno de pura curiosidad científica, me puse de pie en ambas ocasiones en que el robot iba a qmtarse la tela, y hasta llegué a gritar:

–¡No se lo impidas, idiota!

Mas, ¿qué puede esperarse de un hombre como el doctor McDonald? Se lo impidió, y supongo que el motivo de que el espectáculo no tuviese una continuación se debió a que aquel robot tan caro e intrincado quedó arruinado por falta de una apropiada manipulación.

Ahí tenemos otro fallo. El doctor McDonald era un mal guardián del robot. Un experto en robots como yo habría tomado a Rhoda por lo que era…, por un robot mal diseñado, que necesitaba un trato muy cuidadoso y un manejo muy sensible para medrar debidamente.

Yo considero que un psiquiatra ha de estar sentado todo el día y estar ensimismado en los problemas freudianos de sus pacientes femeninas. ¿Es extraño, por tanto, que se sienta tímido ante las mujeres? Como solterón que, por necesidad, podía tener poca experiencia con las jóvenes, el doctor McDonald hubiera tenido que ser extremadamente tímido y modesto. ¿Cómo cabía esperar que manejase a Rhoda con la necesaria destreza y autoridad?

Asimismo, el doctor McDonaId me pareció incapaz de comprender los aspectos más simples de la ingeniería de los robots. Por ejemplo, el robot declaró en varias ocasiones:

–Hago lo que me ordenan.

¡Naturalmente! Y sin embargo, siempre que ella efectuaba tal declaración, el doctor parecía demudado, trastornado.

¿Por qué? Un robot ha de efectuar lo que le ordenen, mientras ello esté de acuerdo con los circuitos impresos en su cerebro y en consonancia con los propósitos para los que está destinado. Esto lo sabe cualquier niño.

El robot Rhoda lo hacía todo para ayudar a un experto en robots a imaginarse los efectos del ambiente espacial en el organismo humano. Ésta era su gran contribución a la ciencia y la Humanidad.

Entonces, ¿qué había en esta situación lógica que trastornase al doctor McDonald? Cuando el pobre robot, mal diseñado para semejar una mujer de físico espectacular, se ofreció a ejecutar sus deberes de acuerdo con lo ordenado, qué pensamientos debieron pasar por la mente del doctor McDonald…

Supongo que nadie los conocerá jamás.[18]

36. El insidioso tío Martin

¿Un marciano en la televisión?

Cuando, hace unos años, llegó hasta mí este rumor, apenas di crédito a mis oídos. Nada podía ser más excitante, más emocionante, más científicamente útil, que tener un marciano en la televisión. Por consiguiente, esperé ansioso la aparición del programa titulado My favorite martian[19].

Para mí, esto era altamente significativo. Durante más de un cuarto de siglo me he dedicado a escribir relatos de ciencia-ficción, de modo que estoy sumamente familiarizado con el aspecto de los marcianos, según los han descrito las mejores mentalidades americanas (incluyéndome yo mismo). (Ver Capítulo 23.)

Mientras iba contando las horas que faltaban para el pase del programa, repasé varias descripciones de marcianos. Por ejemplo, había marcianos humanoides que, aunque eran altos y de forma espiral, con miembros delgados, sus pechos eran bulbosos. Había marcianos con rostro de crisantemo; con tentáculos como pulpos; otros que se parecían a las avestruces, y finalmente, marcianos como gusanos con plumas.

Naturalmente, en algunas ocasiones las marcianas eran descritas como mujeres bellísimas, provistas de un máximo de encanto y un mínimo de tela, mas nunca me las he tomado en serio. Hay que pensar en el caso razonablemente. En Marte hace mucho frío, y las bellas princesas marcianas tendrían que llevar pieles costosas…, ¿y dónde se las procurarían en Marte? La consideración de tales extremos distingue al escritor de ciencia-ficción de talento y reflexión del vulgar aficionado.

Bien, no importa. Lo real, el verdadero marciano no tardaría en salir por televisión. Todas las dudas quedarían disipadas.

Con el corazón palpitante y el aliento en suspenso…, contemplé por primera vez al ser que todo el mundo conoció muy pronto como tío Martin.

¡Salté de mi butaca, lleno de asombro! ¿Era posible? Si parecía un terráqueo…

Claro que yo no me dejo engañar fácilmente. Mi adiestramiento como escritor de ciencia-ficción me ha dotado de una gran percepción respecto a los detalles extraterrestres. Busqué, pues, las pequeñas desviaciones de la forma que un americano normal no habría observado ni en mil años.

Busqué, por ejemplo, seis o siete dedos en cada mano (o al menos en una), o un pulgar de más. Escruté a tío Martin atentamente, en busca de una segunda cabeza que podía esconder en un bolsillo; o una cola que podía asomar ocasionalmente por debajo del pantalón. Algún detalle como éstos, que nada significaría para un espectador vulgar, para mí habría sido muy significativo.

Mas no vi nada en absoluto, hasta que mi hijo observó que tío Martin poseía un par de antenas que aparecían de vez en cuando. No me había fijado en ellas.

Antenas, ¿eh? ¿Era algo decisivo? Comencé a observar a los terráqueos por la calle y, al cabo de varios días, estuve convencido. Los terráqueos no tenían antenas; al menos, no en la zona de Boston. ¡Buena cosa! Un hombre menos listo se habría quedado convencido al momento. Sólo por las antenas, habría llegado a la conclusión de que tío Martin, a pesar de su aspecto terrestre, era un marciano. Sin embargo, la gente de la televisión no es bastante lista para engañar a un experto como yo.

Necesitaba más pruebas. Y pasé semanas sumido en mis pensamientos, dejando incluso de trabajar hasta solucionar el problema. Tío Martin parecía muy a gusto en la Tierra, y no obstante debía hallar muy raras las condiciones de nuestro ambiente.

Por ejemplo. Marte es muy frío y seco. ¿No encontraba tío Martin la Tierra extraordinariamente húmeda y caliente? En tal caso, no lo daba a entender.

Claro está, en un programa quedó bien demostrado que tío Martin poseía una temperatura orgánica muy superior a la de los terráqueos. ¿Significa esto que no quedaba afectado ni por el frío de Marte ni por el calor terrestre? Unas eficaces glándulas sudoríparas podían contrarrestar la humedad. Por consiguiente, este dato tampoco era concluyente. Incansable, sin remordimientos, pasé a otras consideraciones.

¿Y la atmósfera? La de Marte no es ni una décima tan densa como la terrestre y no contiene oxígeno. Éste es un producto químico muy activo, que indudablemente envenenaría a un marciano no acostumbrado a respirarlo. La cuestión estribaba en saber cómo seguía vivo tío Martin, respirando nuestra atmósfera.

Mas, ¿respiraba nuestra atmósfera? No quise saltar a ninguna conclusión. Vigilé programa tras programa, tratando de detectar el movimiento rítmico del pecho. Por desgracia, no acerté en ningún momento a detectar tal movimiento.

Como en asuntos científicos es muy importante comprobarlo todo, elegí otro personaje de la trama para ver. si el pecho de un ser terráqueo subía y bajaba al respirar. Al azar, escogí a la atractiva portera, y observé su prominente busto durante cinco o seis programas. Sí, su pecho subía y bajaba, mas no quedé convencido en el caso de tío Martin.

La evidencia no era concluyente.

Entonces, se me ocurrió la solución. ¡La gravedad! La gravedad de la superficie de Marte es solamente dos quintos de la nuestra. Cualquier individuo adaptado a Marte pesaría mucho más en la Tierra. Andaría con grandes dificultades, y se levantaría mediante un enorme esfuerzo. La vida terrestre sería una terrible y constante tortura para él.

Mas tío Martin no parecía tener dificultades para moverse. Más bien caminaba con ligereza y gracejo. Comprobé de nuevo con la portera, y la reacción a la gravedad terrestre pareció ser la misma en ambos casos.

Al fin, tenía ya un dato concluyente. Mi cuidadoso análisis de la situación había requerido varias temporadas, mas valía la pena. Estoy seguro de que la conclusión a que llegué conmovió a toda la nación.

La conclusión era sencilla. ¡Tío Martín no era un marciano! Era un terráqueo, ni más ni menos.

Aunque tampoco era un simple terráqueo. Tenía antenas; yo mismo las había visto. Poseía asimismo todos los poderes de los marcianos. Podía tornarse invisible y mover los objetos sólo señalándolos con el dedo.

Naturalmente, supuse que tales poderes eran falsos. Sospeché también que todo era un truco, ya que es muy difícil engañar a un escritor de ciencia-ficción como yo. Conozco todas las tretas.

Por ejemplo, podía tratarse de un intento de distracción. Veíamos cómo el dedo de tío Martin se movía en dirección a una silla, en tanto otra persona, rápidamente, la movía. O tal vez tío Martin llevase un tubo largo unido al dedo, un tubo de color gris, para que resultase invisible. Cuando parecía que tío Martín, por ejemplo, desaparecía, era que alguien había colocado una pantalla ante él, exactamente igual que el fondo del escenario.

Pensé una docena de trucos sutiles, pero al final me convencí de que no empleaban ninguno.

Por tanto, llegué a la conclusión de que tío Martín era un terráqueo, si bien poseía poderes marcianos.

Entonces, llegué a la única solución posible: los marcianos estaban detrás del programa. E indudablemente, habría otros programas similares. ¿Por qué? Si los marcianos deseaban demostrar sus poderes, ¿por qué no utilizar a un marciano? ¿Por qué servirse de un terráqueo? Mi hijo me dio inadvertidamente la clave, que mi gran cerebro captó al instante.

–¡Caramba, me gusta tío Martin! – exclamó mi hijo.

¡Naturalmente! ¿Le habría gustado de haber sido tío Martin un gusano con plumas o un pulpo? ¡Jamás! Por tanto, los marcianos presentaban deliberadamente una imagen falsa al mundo. ¡Estaban subvertiendo a nuestra pobre juventud! ¡Estaban ganando nuestros corazones astutamente! Nos presentaban a un marciano exactamente igual a nosotros; con poderes especiales, si bien sólo los utilizaba para ayudar al jovencito con quien vivía, y para mantener a la portera libre de líos. Incluso el detective, que constantemente sospechaba que tío Martín era un ser raro, era tratado con gentileza.

La conclusión inevitable es que los marcianos desean ayudarnos y que hasta aman a sus enemigos. Al menos, ésta es la conclusión que ellos desean que creamos.

Mas, ¿es válida? De ser así, ¿por qué tanto trabajo para ocultar lo que mis años de estudios y perseverancia me hizo comprender? ¿No sería posible que después de habernos lavado el cerebro, tornándonos amables hacia los marcianos, éstos, solapadamente, se presentasen tal como son, y nos destruyesen? ¡Sería tonto y necio creer otra cosa! ¡Alerta, terráqueos! ¡No os fiéis de los marcianos! ¡De prisa, despertad antes de que sea tarde! ¡Abrid vuestros ojos ante la conspiración marciana que nos rodea! ¡No os dejéis engañar por el insidioso tío Martín! Si actuamos a tiempo, salvaremos a la Tierra, mas el tiempo apremia.

¡Actuemos ahora!

37. Los encantadores y perdidospaisajes de la Luna

Hace algún tiempo, una sonda planetaria, el Mariner TV, pasó muy cerca de Marte y violó la castidad de nuestro hermano del sistema solar con una serie de veintiuna fotografías. El velo de la distancia fue desgarrado y las cicatrices de Marte quedaron al descubierto.

¡Nada de canales! Sólo hoyos como en la Luna. Muchos cráteres. Uno medía más de cien kilómetros de diámetro. La última visión de un mundo exótico pasó al limbo y el sistema solar resultó estar menos poblado que nunca.

Actualmente soy escritor de ciencia-ficción, mas en los años treinta no era más que un lector asiduo de tales temas. En aquella época, el sistema solar estaba poblado por razas misteriosas, bellas princesas, bestias y monstruos terribles, y hasta plantas inteligentes, de carácter mortal.

Era un sistema solar como jamás veremos…, al que la ciencia arruinó.

Hasta los tiempos modernos, los hombres creían que sólo la Tierra estaba habitada. Me refiero a la gente vulgar. Las personas más cultas, incluso en los tiempos antiguos, creían que el Sol y la Luna eran otros mundos, y que también podían serlo los planetas. Hasta la actualidad un mundo deshabitado era una contradicción. ¿De qué servía un mundo, a menos que estuviese habitado por seres como nosotros? Un mundo deshabitado era perdido, y esto era una mácula para Dios (si uno era religioso), o para la maquinaria lógica del universo (si no lo era).

Así, el autor satírico griego, Luciano de Samosata, que vivió en el siglo II d. de C. escribió sobre un terráqueo que visitaba la Luna, y la encontró habitada por unos seres que estaban en guerra con los habitantes del Sol por el derecho a colonizar Venus.

En 1800, el gran astrónomo anglogermano, William Herschel, pobló el sistema solar. Creyó que las manchas del Sol eran agujeros de su atmósfera por lo que era dable divisar la superficie interna del Sol…, un Sol que podía ser frío y estar habitado.

En 1901, H. G. Wells, en El primer hombre en la Luna, aún pobló el satélite de plantas. Y describió a unos selenitas inteligentes, que vivían bajo el suelo.

¿Por qué bajo el suelo? La realidad se iba abriendo paso.

Tan pronto como fue posible la observación telescópica de la Luna en el siglo XVII, resultó claro que la Luna carecía de aire y agua. En su superficie, había unas depresiones muy extensas, denominadas «mares», que se bautizaron con nombres muy sonoros: «Mar de la Tranquilidad», «Mar de la Serenidad», «Mar de los Sueños»…

Mas, ¡ay!, eran plácidos, tranquilos y serenos debido a la ausencia del aire. Y si estaban marcados por los sueños, eran los tristes sueños de un mundo habitado que no existía, la visión de un mundo más pequeño y delicado que el nuestro. El sueño quedó remplazado por la pesadilla de los mares de polvo, de los cráteres siempre silenciosos, siempre iguales, del Sol moviéndose lentamente, de largas y frígidas noches. La ciencia moderna añadió aún la pesadilla de la radiación mortal.

Los escritores de ciencia-fícción aún hablan de una Luna poblada, a pesar de todo, mas sin convicción. Volar directamente delante de la ciencia le dio mala fama a la ciencia-fícción, y como los escritores son ya conscientes de sus obras, hoy día la ciencia-ficción goza de mejor renombre.

Naturalmente, había la otra cara de la Luna, la que jamás vemos desde la Tierra. ¿Y si el satélite tenía forma de huevo, abultado hacia nosotros? La gravedad terrestre sería la responsable de esta forma, deteniendo la rotación de la Luna. Lo que veíamos podía ser, en efecto, una enorme montaña sin atmósfera. En la otra cara habría aire y agua y tierras llanas, y habitantes. Era una idea bellísima, sin medios de comprobarla o refutarla, puesto que la otra cara de la Luna siempre fue invisible desde la Tierra.

De pronto, en 1959, los rusos enviaron al Lunik III en torno a la Luna, a fin de atisbar al otro lado. Y allí se desvanecieron los mares, el aire y las nubes; allí se perdió el encantador paisaje lunar. Su otra cara era peor que la que vemos, más montañosa y con más cráteres.

Bien, ¿el subsuelo? ¿Como los selenitas de H. G. Wells?

No. Los científicos han considerado el asunto y han adelantado toda clase de razones para suponer que, a lo sumo, puede haber una vida bacteriana, u otra igualmente sencilla, en el interior de la Luna. Nada más.

Claro está, no menciones siquiera al Sol. Su temperatura exterior es de 10.000° C, y sus manchas, a pesar de Herschel, son más negras por comparación, puesto que al menos tienen una temperatura de 7.000° C. Tampoco su interior está frío. Al contrario, el calor se acentúa hacia el centro, hasta llegar a los 25.000.000° C.

Mas en los años treinta no era la Luna (ni el Sol) donde situábamos la vida. Todos suponíamos lo peor del Sol…, y también nuestro satélite.

¡Pero teníamos a Marte! ¡En el caso de Marte, la ciencia estaba de nuestra parte!

Al fin y al cabo, un astrónomo italiano, Giovanni V. Schiaparelli, descubrió los famosos canales en 1877. Y otros astrónomos, como Camilo Flammarion y Percival Lowell, insistieron en que tales canales sólo podían haber sido construidos por seres inteligentes, lo cual indicaba que Marte estaba habitado.

¡Cuántas historias de ciencia-ficción se centraron en Marte! ¡Cuántas princesas encantadoras, apenas vestidas muchas de ellas, sentadas sobre caballos de seis patas, esperaban a ser libertadas por el terráqueo que luchaba con espadas gigantes!

Naturalmente, se razonaba. Marte era un mundo más pequeño que la Tierra, y se había enfriado antes. Su civilización estaba más adelantada que la nuestra y era más decadente. El agua desaparecía lentamente, y habían construido los canales en un intento desesperado de evitar el inevitable fin. Los viejos marcianos se enfrentaban con dicho destino con ecuanimidad filosófica, ofreciendo sus enseñanzas a la raza más juvenil de la Tierra. O bien, acuciados por la necesidad, planeaban invadir nuestro planeta, el siguiente hacia el Sol, matando o esclavizando a los terráqueos.

¡Cuántas veces me emocioné ante las maquinaciones de los malvados marcianos y la inevitable victoria de los terráqueos!

Tal vez la civilización marciana había ya desaparecido, y los terráqueos iban a reconstruirla de entre sus ruinas. El minúsculo sol de Marte brillaba en un cielo sin nubes, purpúreo, sobre el último vestigio de los canales, mientras los arqueólogos humanos escudriñaban incansablemente los restos de los misteriosos marcianos muertos.

Naturalmente, de los observatorios surgían noticias inquietantes. La atmósfera de Marte era tan tenue como en la cima del monte Everest. Prácticamente, dicho aire carecía de oxígeno. Muchos astrónomos no creían en los canales, que no veían; y el planeta rojo apenas contenía agua.

Todos luchamos contra esto. Todos nos aferramos a Marte. Era nuestra mejor esperanza. No podían hacerla desaparecer.

Mas lo lograron. Oh, sí, los astrónomos cedían en algunos detalles. Decididamente, había agua en Marte; los casquetes polares claramente visibles eran de agua helada (y no de dióxido de carbono sólido, ni otra paparrucha semejante), mas no había mucha. Y las zonas verdosas de Marte podían indicar una vida vegetal…, mas no selvas, ni árboles o hierba. A lo máximo, una vegetación primitiva de líquenes.

Después, llegó el Mariner IV, y los canales de Marte saltaron por la borda. Ni rastro de ellos. Aquellos astrónomos que creían en los famosos canales habían visto líneas de cráteres de trazado irregular en los límites de la visibilidad, y las trazadas en línea recta no existían.

Además, la existencia de cráteres no sólo demostraba la casi total ausencia de aire y agua, sino que ambos elementos faltaban de allí desde hacía millones de años.

¿Y las princesas? Lo mismo que los líquenes.

¿Y Venus? Está más cerca del Sol, tardó más en enfriarse (según los autores de ciencia-ficción de los años treinta), y es más joven que la Tierra. En realidad, se trataba de un mundo más joven, porque su atmósfera estaba llena de nubes. Era un mundo lleno de selvas de gran exotismo.

Se escribieron relatos sobre el ambiente de Venus, en que el moho lo amenazaba todo, donde las plantas rapaces libraban una guerra civil sin tregua ni cuartel. Se creía, entonces, que Venus siempre mantenía una sola cara hacía el Sol, y que la capa de nubes impedía que su temperatura fuese indebidamente elevada. El lado oscuro de Venus, con su eterna luminosidad, tenía un ambiente totalmente distinto, misterioso, con aire cálido procedente del lado diurno, que se helaba en montañas de oxígeno y nitrógeno sólidos.

¿O las nubes significaban que Venus contenía un enorme océano en su superficie? Tan enamorado estuve de esta posibilidad, que en 1954 escribí una novela respecto a ese planeta, describiéndolo como un gran océano que se extendía por toda su superficie. Poblé aquel océano de seres fantásticos, incluyendo un pulpo de dos kilómetros de longitud.

Sí, nadie podía refutamos. Era imposible distinguir nada bajo aquella capa de nubes. Teníamos un mundo a nuestro gusto, sin que la ciencia pudiera destruirlo.

Los astrónomos, no obstante, jugaban al gato y al ratón con las nubes. Uno aseguraba que eran de formaldehído. De gasolina, afirmaba un segundo. De polvo, dijo un tercero. Todos estábamos expectantes, hasta que se averiguó que eran nubes de agua.

Por fin, los astrónomos decidieron que en la atmósfera de Venus no había oxígeno (los astrónomos jamás hallan oxígeno en ninguna atmósfera, ver Capítulo 13). Los escritores de ciencia-ficción replicaron que esto no podía tomarse en serio. Al fin y al cabo, los astrónomos sólo veían el aire situado encima de las nubes. Mas, ¿y debajo?

Entonces, los astrónomos captaron señales de radio desde algunos planetas, y entre éstos Venus. Las señales enviadas por este planeta sólo podían ser radiadas desde un objeto muy caliente, de 300° C por lo menos. En 1962, la sonda de Venus, el Mariner II, descubrió y confirmó que Venus era un planeta muy caliente.

Sí, Venus estaba cubierto por un océano, como yo había predicho en 1954. Lo malo era que aquel océano era una inmensa corriente. Las nubes que cubrían Venus no indicaban la presencia de agua, sino que eran toda la provisión de agua del planeta.

Incluso resultó que Venus gira lentamente respecto al Sol. No existe una noche perpetua en una cara, y ningún refugio contra el calor. Venus es un planeta totalmente caliente.

Final de Venus. Final de la selva más bella del sistema solar; final del enorme océano.

Tampoco había grandes esperanzas cifradas en Mercurio. Estaba demasiado cerca del Sol, presentándole eternamente una cara. Mas, ¿y la zona «intermedia»? ¿Podía discurrir el aire desde las montañas de oxígeno al lado nocturno?

¡Imposible! Los astrónomos lo explicaron con todo detalle. La órbita de Mercurio es muy elíptica. A cada revolución, se aproxima mucho al Sol, acelera su marcha, y después se aparta de aquél, rezagándose. Como resultado de este movimiento, su superficie se balancea como un péndulo, de forma que cada lado de la zona intermedia tiene 44 días de sol y 44 noches. No hay zona intermedia.

En 1965, la cosa aún empeoró. Resultó, según los haces de radar rebotados en Mercurio, que este planeta gira lentamente. Tampoco hay una cara de noche eterna. Todas las partes del astro gozan de largos períodos de luz solar.

Tampoco hay oxígeno helado.

Pasado Marte hallamos los mundos gigantescos del sistema solar, Júpiter, Saturno, Urano y Neptuno, contando entre todos veintinueve satélites, cinco de ellos grandes.

En los felices treinta, poblamos a todos esos planetas y satélites. Se escribrieron muchas historias sobre Júpiter y Saturno. Unos representaban a este último como un mundo de praderas, un gigantesco Oeste, con grandes manadas de ganado. Lo cual era estupendo, ya que la superficie de Saturno es ochenta veces mayor que la de la Tierra, si lo que de allí vemos es realmente su superficie.

En cuanto a los satélites… En mis novelas de aquella época, mis protagonistas se vieron amenazados en Ganimedes y Calisto, dos lunas de Júpiter. Titán, la mayor de Saturno, era otra de mis favoritas.

Tampoco significaba ningún inconveniente la enorme distancia desde esos planetas al Sol. Una de las grandes novelas de aquel tiempo describía un sistema solar destinado a un fin prematuro como resultado de las maquinaciones de los malvados habitantes de Neptuno.

Sin embargo, era una batalla perdida. Los planetas exteriores son demasiado fríos, y sus atmósferas excesivamente densas; además, dichas atmósferas son irrevocablemente venenosas. En cuanto a los satélites, sólo en uno se ha localizado una atmósfera: Titán. Este satélite posee una tenue capa de aire…, pero naturalmente es ponzoñoso.

Algunos astrónomos han especulado con la idea de que la temperatura de Júpiter podría ser más elevada de lo que pensamos, hasta resultar adecuada para nosotros…, si pudiésemos respirar gases venenosos. Además, hoy día se cree que los planetas exteriores están compuestos casi exclusivamente de hidrógeno, particularmente en estado gaseoso en la atmósfera, en estado líquido más abajo, y en estado sólido en el centro¿Qué más queda? ¿Los cometas?

En una de sus novelas. Julio Verne hace chocar un cometa con la Tierra, llevándose consigo una parte de la misma con habitantes, los cuales empiezan a vivir en el cometa, más o menos cómodamente, durante largo tiempo. Incluso hay un océano en el cometa.

Pero los cometas, como sabemos hoy día, no son más que volúmenes inmensos de gases y polvo, rodeando a un objeto del tamaño de un asteroide, o un conjunto de guijarros pegados entre sí por el gas helado.

La ciencia no ha dejado libre ni siquiera a la Tierra.

Poco a poco, por medio de las exploraciones, se han desvanecido las tribus perdidas y las civilizaciones ocultas. La Atlántida se esfumó para siempre; el África negra no ha producido ninguna «She», y el Tibet no contiene ningún «Shangri-La».

En las inmensidades del Amazonas sólo existen unas tribus miserables, y el gran continente del Sur es sólo una Australia desierta, poblada por los aborígenes. La Antártida es un conjunto de hielos.

La novelería ha cedido el sitio a la realidad.

¿Y el interior de la Tierra? ¿Qué hay allí? El interés por las cuevas sin fondo del interior del planeta se remonta a La Ilíada. Ningún héroe griego era bueno si no invadía el mundo interior. Teseo lo hizo. Y Hércules, también Odiseo. Los romanos lo copiaron y Eneas descendió al centro de la Tierra.

Los escritores modernos también poseen su subsuelo, y se han forjado muchas historias respecto a otros mundos dentro del planeta, respecto a enormes fosas con un «sol» radiactivo en el centro de la Tierra. Con océanos y continentes internos, hombres y monstruos.

Mas hasta este juego inocente se ha desvanecido. Midiendo la densidad de la Tierra, estudiando las ondas de los terremotos, y mediante otra docena de métodos distintos, los geólogos se han convencido de que la Tierra es completamente sólida. No es hueca, ni puede haber cuevas que desciendan a más de tres kilómetros de profundidad.

¿Dónde nos deja todo esto? En ninguna parte del sistema solar, salvo en la superficie de la Tierra, único lugar seguro para la Humanidad. En ninguna parte encontraremos primos, sabios mentores, ni peligrosos enemigos. ¡Estamos solos!

No del todo, claro. Existen otras estrellas, con otras familias de planetas (ver Capítulo 22). Pero están lejos, muy lejos…, y son muy difíciles de alcanzar; preservadas, aparentemente, por la distancia del tiempo (ver Capítulo 31).

No, no, las estrellas no nos interesan. Queremos sólo el sistema solar, el sistema solar que nos arrebataron hace treinta años.

El sistema solar que jamás volveremos a tener.

[1] En inglés: What is mind? No matter! What is matter? Never mind!

Se trata, en realidad de un juego de palabras, ya que los vocablos matter y mind significan en dicho idioma, respectivamente: «materia» y «mente», pero antecedidas de una negación, no o never, hay que traducir ambas expresiones por «¡No importa!», por lo que toda la expresión adquiere un sentido distinto del literal. (N. del T.)

[2] ¿No es posible curar una mente enferma, arrancar de la memoria un pesar arraigado, borrar los conflictos grabados en el cerebro, y con algún suave antídoto, como el olvido, purificar el oprimido pecho de la emoción peligrosa que pesa sobre el corazón?

Doctor: En este caso, la paciente debe curarse a sí misma.

[3] En el texto pone billones, lo que es una mala traducción, que se repite varias veces en este capítulo. (Dom)

[4] Otro error de traducción que se repite varias veces en este capítulo: en el libro pone “gene” cuando el singular de genes es “gen” para el Diccionario (Dom)

[5] Día de la conmemoración de la Independencia de Norteamérica (N. Del T)

[6] Esto se refiere exclusivamente a la moneda norteamericana (N. Del T.)

[7] La frase en inglés es: Microwave Amplification by Stimulated Emission of Radiation, y las iniciales en mayúsculas son las que componen el nombre maser. (N. del T.)

[8] Como se ha visto en la práctica, a raíz de los viajes lunares, este peligro no ha representado ninguna dificultad para las travesías hacia la Luna. (N. del T.)

[9] En efecto, en la actualidad, las sondas enviadas a Marte han procedido a recoger esta clase de datos, si bien sus resultados todavía se hallan bajo estudio, sin que se conozcan exactamente sus resultados. (N. del T.)

[10] Siéntate, Jessica. Mira cómo el suelo del cielo / se halla esmaltado de objetos de oro brillante. / No existe un solo cuerpo celeste que contemples / que no cante como un ángel, / rivalizando con los querubines de bellos ojos; / esta armonía se halla también en las almas inmortales, / pero mientras esta envoltura de podredumbre carnal / esté pegada a nuestro espíritu no podemos oírla.

[11] Estadísticas referidas a 1967. (N. del T.)

[12] Nowhere, en inglés, significa precisamente «en ninguna parte». (N. del T.)

[13] Historias Asombrosas.

[14] Historias Pasmosas.

[15] Pasmosa Ciencia-Ficción.

[16] En la actualidad, el cine y la Televisión han presentado algunas obras valiosas. Por ejemplo, Viaje Fantástico, que representa un viaje imaginario a través de la sangre humana, con una nave y una tripulación de tamaño microscópico…, película que no ha regateado esfuerzo ni imaginación para que todo resulte razonable y perfecto. De modo similar, la Television presentó en 1966 Star Trek, programa en que se trataba seriamente la ciencia-ficción. (N. del A.)

[17] Mi muñeca viviente. (N. del T.)

[18] Supongo que el lector habrá comprendido que estoy bromeando. En realidad, me encantó My living Doll, y lamento que ya no se proyecte en televisión esta serie. Además, afirmo aquí mi enorme admiración para la señorita Julie Newmar, que interpretaba a la perfección el papel del robot Rhoda.

[19] Mi marciano favorito.

13/06/2008
