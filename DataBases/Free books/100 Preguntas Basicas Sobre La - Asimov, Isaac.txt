100 Preguntas Basicas Sobre La Ciencia

ISAAC ASIMOV

100 PREGUNTAS BÁSICASSOBRE LA CIENCIA

(Publicado en inglés por la Editorial Houghton Mifflin Company)

© 1973 by Isaac Asimov

© Ed. cast.: Alianza Editorial, S. A., Madrid, 1977, 1978, 1979

Calle Milán, 38. Teléfono 200 00 45

Ronda de Toledo, 24 – Madrid-5

Printed in Spain

A Richard F. Dempewolff

INTRODUCCIÓN

Por el año 1965, esa respetable revista que es Science Digest inició una nueva sección titulada «Please Explain» [«Por favor, explique»]. El propósito de esta sección era seleccionar algunas de las preguntas formuladas por los lectores y contestarlas en unas 500 palabras.

La revista me preguntó si estaba dispuesto a abordar de vez en cuando una pregunta a cambio de una cantidad razonable de dinero. «Bien», contesté con ciertas reservas, «pero siempre que sea de vez en cuando».

Tuve que suponerlo. Mí colaboración, que en principio iba a ser esporádica, adquirió carácter mensual, y la sección «Please Explain» pasó a ser «Isaac Asimov Explains» [«I. A. explica»]. (Para evitar la posible trampa de mi archiconocida modestia diré que el cambio se hizo sin consultarme.) Cuando quise darme cuenta, llevaba ya colaborando más de ocho años y había acumulado un centenar de preguntas y respuestas.

¿Y quién podría resistir la tentación de reunir esos ensayos y hacer con ellos un libro? ¡Yo, desde luego, no! ¡Ni tampoco Houghton Mifflin!

Como las respuestas que tengo que dar dependen de las preguntas que formulan los lectores, los ensayos no se reparten uniformemente por todo el campo de la ciencia. Por alguna razón, los lectores se hallan profundamente interesados por la física teórica, siendo especialmente numerosas las preguntas acerca de la velocidad de la luz y de las partículas subatómicas.

De ahí que exista cierta duplicidad entre las respuestas, así como algunas omisiones flagrantes.

Ambas cosas tienen sus ventajas. Las duplicidades provienen, en parte, de que he intentado hacer las respuestas lo más completas posible. El lector puede consultar por tanto cualquier cuestión que se le venga a la imaginación y leer el libro en el orden que le plazca.

Y en cuanto a las omisiones… ¿qué hay de malo en dejar que susciten una sana curiosidad? Si es suficientemente sana, envíe su propia pregunta a Science Digest. Si tengo ocasión (y sé lo suficiente) la contestaré, y de aquí a ocho años podría haber material bastante para publicar un libro titulado «Isaac Asimov sigue explicando».

CIEN PREGUNTAS BÁSICAS SOBRELA CIENCIA

1. ¿Qué es el método científico?

Evidentemente, el método científico es el método que utilizan los científicos para hacer descubrimientos científicos. Pero esta definición no parece muy útil. ¿Podemos dar más detalles?

Pues bien, cabría dar la siguiente versión ideal de dicho método:

1. Detectar la existencia de un problema, como puede ser, por ejemplo, la cuestión de por qué los objetos se mueven como lo hacen, acelerando en ciertas condiciones y decelerando en otras.

2. Separar luego y desechar los aspectos no esenciales del problema. El olor de un objeto, por ejemplo, no juega ningún papel en su movimiento.

3. Reunir todos los datos posibles que incidan en el problema. En los tiempos antiguos y medievales equivalía simplemente a la observación sagaz de la naturaleza, tal como existía. A principios de los tiempos modernos empezó a entreverse la posibilidad de ayudar a la naturaleza en ese sentido. Cabía planear deliberadamente una situación en la cual los objetos se comportaran de una manera determinada y suministraran datos relevantes para el problema. Uno podía, por ejemplo, hacer rodar una serie de esferas a lo largo de un plano inclinado, variando el tamaño de las esferas, la naturaleza de su superficie, la inclinación del plano, etc. Tales situaciones deliberadamente planeadas son experimentos, y el papel del experimento es tan capital para la ciencia moderna, que a veces se habla de «ciencia experimental» para distinguirla de la ciencia de los antiguos griegos.

4. Reunidos todos los datos elabórese una generalización provisional que los describa a todos ellos de la manera más simple posible: un enunciado breve o una relación matemática. Esto es una hipótesis.

5. Con la hipótesis en la mano se pueden predecir los resultados de experimentos que no se nos habían ocurrido hasta entonces. Intentar hacerlos y mirar si la hipótesis es válida.

6. Si los experimentos funcionan tal como se esperaba, la hipótesis sale reforzada y puede adquirir el status de una teoría o incluso de un «ley natural».

Está claro que ninguna teoría ni ley natural tiene carácter definitivo. El proceso se repite una y otra vez. Continuamente se hacen y obtienen nuevos datos, nuevas observaciones, nuevos experimentos. Las viejas leyes naturales se ven constantemente superadas por otras más generales que explican todo cuanto explicaban las antiguas y un poco más.

Todo esto, como digo, es una versión ideal del método científico. En la práctica no es necesario que el científico pase por los distintos puntos como si fuese una serie de ejercicios caligráficos, y normalmente no lo hace.

Más que nada son factores como la intuición, la sagacidad y la suerte, a secas, los que juegan un papel. La historia de la ciencia está llena de casos en los que un científico da de pronto con una idea brillante basada en datos insuficientes y en poca o ninguna experimentación, llegando así a una verdad útil cuyo descubrimiento quizá hubiese requerido años mediante la aplicación directa y estricta del método científico.

F. A. Kekulé dio con la estructura del benceno mientras descabezaba un sueño en el autobús. Otto Loewi despertó en medio de la noche con la solución del problema de la conducción sináptica. Donald Glaser concibió la idea de la cámara de burbujas mientras miraba ociosamente su vaso de cerveza.

¿Quiere decir esto que a fin de cuentas todo es cuestión de suerte y no de cabeza? No, no y mil veces no. Esta clase de «suerte» sólo se da en los mejores cerebros; sólo en aquellos cuya «intuición» es la recompensa de una larga experiencia, una comprensión profunda y un pensamiento disciplinado.

2. ¿Quién fue, en su opinión, elcientífico más grande que jamás

existió?

Si la pregunta fuese «¿Quién fue el segundo científico más grande?» sería imposible de contestar. Hay por lo menos una docena de hombres que, en mi opinión, podrían aspirar a esa segunda plaza. Entre ellos figurarían, por ejemplo, Albert Einstein, Ernest Rutherford, Niels Bohr, Louis Pasteur, Charles Darwin, Galileo Galilei, Clerk Maxwell, Arquímedes y otros.

Incluso es muy probable que ni siquiera exista eso que hemos llamado el segundo científico más grande. Las credenciales de tantos y tantos son tan buenas y la dificultad de distinguir niveles de mérito es tan grande, que al final quizá tendríamos que declarar un empate entre diez o doce.

Pero como la pregunta es «¿Quién es el más grande?», no hay problema alguno. En mi opinión, la mayoría de los historiadores de la ciencia no dudarían en afirmar que Isaac Newton fue el talento científico más grande que jamás haya visto el mundo. Tenía sus faltas, viva el cielo: era un mal conferenciante, tenía algo de cobarde moral y de llorón autocompasivo y de vez en cuando era víctima de serias depresiones. Pero como científico no tenía igual.

Fundó las matemáticas superiores después de elaborar el cálculo. Fundó la óptica moderna mediante sus experimentos de descomponer la luz blanca en los colores del espectro. Fundó la física moderna al establecer las leyes del movimiento y deducir sus consecuencias. Fundó la astronomía moderna estableciendo la ley de la gravitación universal.

Cualquiera de estas cuatro hazañas habría bastado por sí sola para distinguirle como científico de importancia capital. Las cuatro juntas le colocan en primer lugar de modo incuestionable.

Pero no son sólo sus descubrimientos lo que hay que destacar en la figura de Newton. Más importante aún fue su manera de presentarlos.

Los antiguos griegos habían reunido una cantidad ingente de pensamiento científico y filosófico. Los nombres de Platón, Aristóteles, Euclides, Arquímedes y Ptolomeo habían descollado durante dos mil años como gigantes sobre las generaciones siguientes. Los grandes pensadores árabes y europeos echaron mano de los griegos y apenas osaron exponer una idea propia sin refrendarla con alguna referencia a los antiguos. Aristóteles, en particular, fue el «maestro de aquellos que saben».

Durante los siglos XVI y XVII, una serie de experimentadores, como Galileo y Robert Boyle, demostraron que los antiguos griegos no siempre dieron con la respuesta correcta. Galileo, por ejemplo, tiró abajo las ideas de Aristóteles acerca de la física, efectuando el trabajo que Newton resumió más tarde en sus tres leyes del movimiento. No obstante, los intelectuales europeos siguieron sin atreverse a romper con los durante tanto tiempo idolatrados griegos.

Luego, en 1687 publicó Newton sus Principia Mathematica, en latín (el libro científico más grande jamás escrito, según la mayoría de los científicos). Allí presentó sus leyes del movimiento, su teoría de la gravitación y muchas otras cosas, utilizando las matemáticas en el estilo estrictamente griego y organizando todo de manera impecablemente elegante. Quienes leyeron el libro tuvieron que admitir que al fin se hallaban ante una mente igual o superior a cualquiera de las de la Antigüedad, y que la visión del mundo que presentaba era hermosa, completa e infinitamente superior en racionalidad e inevitabilidad a todo lo que contenían los libros griegos.

Ese hombre y ese libro destruyeron la influencia paralizante de los antiguos y rompieron para siempre el complejo de inferioridad intelectual del hombre moderno.

Tras la muerte de Newton, Alexander Pope lo resumió todo en dos líneas:

«La Naturaleza y sus leyes permanecían ocultas en la noche. Dijo Dios: ¡Sea Newton! Y todo fue luz.»

3. ¿Por qué dos o más científicos,ignorantes del trabajo de los otros,

dan a menudo simultáneamente con la

misma teoría?

La manera más simple de contestar a esto es decir que los científicos no trabajan en el vacío. Están inmersos, por así decirlo, en la estructura y progreso evolutivo de la ciencia, y todos ellos encaran los mismos problemas en cada momento.

Así, en la primera mitad del siglo XIX el problema de la evolución de las especies estaba «en el candelero». Algunos biólogos se oponían acaloradamente a la idea misma, mientras que otros especulaban ávidamente con sus consecuencias y trataban de encontrar pruebas que la apoyaran. Pero lo cierto es que, cada uno a su manera, casi todos los biólogos pensaban sobre la misma cuestión. La clave del problema era ésta:

Si la evolución es un hecho, ¿qué es lo que la motiva?

En Gran Bretaña, Charles Darwin pensaba sobre ello. En las Indias Orientales, Alfred Wallace, inglés también, pensaba sobre el mismo problema. Ambos habían viajado por todo el mundo; ambos habían hecho observaciones similares; y sucedió que ambos, en un punto crucial de su pensamiento, leyeron un libro de Thomas Malthus que describía los efectos de la presión demográfica sobre los seres humanos. Tanto Darwin como Wallace empezaron a pensar sobre la presión demográfica en todas las especies. ¿Qué individuos sobrevivirían y cuáles no? Ambos llegaron a la teoría de la evolución por selección natural.

Lo cual no tiene en realidad nada de sorprendente. Dos hombres que trabajan sobre el mismo problema y con los mismos métodos, encarados con los mismos hechos a observar y disponiendo de los mismos libros de consulta, es muy probable que lleguen a las mismas soluciones. Lo que ya me sorprende más es que el segundo nombre de Darwin, Wallace y Malthus empezase en los tres casos por R.

A finales del siglo xix eran muchos los biólogos que trataban de poner en claro la mecánica de la genética. Tres hombres, trabajando los tres en el mismo problema, al mismo tiempo y de la misma manera, pero en diferentes países, llegaron a las mismas conclusiones. Pero entonces los tres, repasando la literatura, descubrieron que otro, Gregor Mendel, había obtenido treinta y cuatro años antes las leyes de la herencia y habían pasado inadvertido.

Una de las aspiraciones más ambiciosas de los años 1880-1889 era la producción barata de aluminio. Se conocían los usos y la naturaleza del metal, pero resultaba difícil prepararlo a partir de sus minerales. Millones de dólares dependían literalmente de la obtención de una técnica sencilla. Es difícil precisar el número de químicos que se hallaban trabajando en el mismo problema, apoyándose en las mismas experiencias de otros científicos. Dos de ellos: Charles Hall en los Estados Unidos y Paul Héroult en Francia, obtuvieron la misma respuesta en el mismo año de 1886. Nada más natural. Pero ¿y esto?: los apellidos de ambos empezaban por H, ambos nacieron en 1863 y ambos murieron en 1914.

Hoy día son muchos los que tratan de idear teorías que expliquen el comportamiento de las partículas subatómicas. Murray Gell-Man y Yuval Ne'emen, uno en América y otro en Israel, llegaron simultáneamente a teorías parecidas. El principio del máser se obtuvo simultáneamente en Estados Unidos y en la Unión Soviética. Y estoy casi seguro de que el proceso clave para el aprovechamiento futuro de la potencia de la fusión nuclear será obtenido independiente y simultáneamente por dos o más personas.

Naturalmente, hay veces en que el rayo brilla una sola vez. Gregor Mendel no tuvo competidores, ni tampoco Newton ni Einstein. Sus grandes ideas sólo se les ocurrieron a ellos y el resto del mundo les siguió.

4. ¿Qué dice el teorema de Gödel?¿Demuestra que la verdad es

inalcanzable?

Desde los tiempos de Euclides, hace ya dos mil doscientos años, los matemáticos han intentado partir de ciertos enunciados llamados «axiomas» y deducir luego de ellos toda clase de conclusiones útiles.

En ciertos aspectos es casi como un juego, con dos reglas. En primer lugar, los axiomas tienen que ser los menos posibles. En segundo lugar, los axiomas tienen que ser consistentes. Tiene que ser imposible deducir dos conclusiones que se contradigan mutuamente.

Cualquier libro de geometría de bachillerato comienza con un conjunto de axiomas: por dos puntos cualesquiera sólo se puede trazar una recta; el total es la suma de las partes, etc. Durante mucho tiempo se supuso que los axiomas de Euclides eran los únicos que podían constituir una geometría consistente y que por eso eran «verdaderos».

Pero en el siglo xix se demostró que modificando de cierta manera los axiomas de Euclides se podían construir geometrías diferentes, «no euclidianas». Cada una de estas geometrías difería de las otras, pero todas ellas eran consistentes. A partir de entonces no tenía ya sentido preguntar cuál de ellas era «verdadera». En lugar, de ello había que preguntar cuál era útil.

De hecho, son muchos los conjuntos de axiomas a partir de los cuales se podría construir un sistema matemático consistente: todos ellos distintos y todos ellos consistentes.

En ninguno de esos sistemas matemáticos tendría que ser posible deducir, a partir de sus axiomas, que algo es a la vez así y no así, porque entonces las matemáticas no serían consistentes, habría que desecharlas. ¿Pero qué ocurre si establecemos un enunciado y comprobamos que no podemos demostrar que es o así o no así?

Supongamos que digo: «El enunciado que estoy haciendo es falso.»

¿Es falso? Si es falso, entonces es falso que esté diciendo algo falso y tengo que estar diciendo algo verdadero. Pero si estoy diciendo algo verdadero, entonces es cierto que estoy diciendo algo falso y sería verdad que estoy diciendo algo falso. Podría estar yendo de un lado para otro indefinidamente. Es imposible demostrar que lo que he dicho es o así o no así.

Supongamos que ajustamos los axiomas de la lógica a fin de eliminar la posibilidad de hacer enunciados de ese tipo. ¿Podríamos encontrar otro modo de hacer enunciados del tipo «ni así ni no así»?

En 1931 el matemático austriaco Kurt Gödel presentó una demostración válida de que para cualquier conjunto de axiomas siempre es posible hacer enunciados que, a partir de esos axiomas, no puede demostrarse ni que son así ni que no son así. En ese sentido, es imposible elaborar jamás un conjunto de axiomas a partir de los cuales se pueda deducir un sistema matemático completo.

¿Quiere decir esto que nunca podremos encontrar la «verdad»? ¡Ni hablar!

Primero: el que un sistema matemático no sea completo no quiere decir que lo que contiene sea «falso». El sistema puede seguir siendo muy útil, siempre que no intentemos utilizarlo más allá de sus límites.

Segundo: el teorema de Gödel sólo se aplica a sistemas deductivos del tipo que se utiliza en matemáticas. Pero la deducción no es el único modo de descubrir la «verdad». No hay axiomas que nos permitan deducir las dimensiones del sistema solar. Estas últimas fueron obtenidas mediante observaciones y medidas -otro camino hada la «verdad».

5. ¿Qué diferencia hay entre losnúmeros ordinarios y los números

binarios y cuáles son las ventajas de

cada uno?

Los números ordinarios que utilizamos normalmente están escritos «en base 10». Es decir, están escritos como potencias de diez. Lo que escribimos como 7.291 es en realidad 7 ´ 103 más 2 ´ 102 más 9 ´ 101 más 1 ´ 100. Recuérdese que 103 = 10 ´ 10 ´ 10 = 1.000; que 102 =10 ´ 10 = 100; que 101 = 10, y que 100 = 1. Por tanto, 7.291 es 7 ´ 1.000 más 2 ´ 100 más 9 ´ 10 más 1, que es lo que decimos cuando leemos el número en voz alta: «siete mil doscientos noventa (nueve decenas) y uno».

Estamos ya tan familiarizados con el uso de las potencias de diez que sólo escribimos las cifras por las que van multiplicadas (7.291 en este caso) e ignoramos el resto.

Pero las potencias de diez no tienen nada de especial. Igual servirían las potencias de cualquier otro número mayor que uno. Supongamos, por ejemplo, que queremos escribir el número 7.291 en potencias de ocho. Recordemos que 80 = 1; 81 =8; 82 = 8 ´ 8 =64; 83= 8 ´ 8 ´ 8 = 512; y 84 = 8 ´ 8 ´ 8 ´ 8 = 4.096. El número 7.291 se podría escribir entonces como 1 ´ 84 más 6 ´ 83 más 1 ´ 82 más 7 ´ 81 más 3 ´ 80. (El lector lo puede comprobar haciendo los cálculos pertinentes.) Si escribimos sólo las cifras tenemos 16.173. Podemos decir entonces que 16.173 (en base 8) = 7.291 (en base 10).

La ventaja del sistema en base 8 es que sólo hay que recordar siete dígitos aparte del 0. Si intentásemos utilizar el dígito 8, llegaríamos a obtener alguna vez 8 x 83, que es igual a 1 x 84, con lo cual siempre podemos utilizar un 1 en lugar de un 8. Así, 8 (en base 10) = 10 (en base 8); 89 (en base 10) = 131 (en base 8); etc. Por otro lado, los números tienen más dígitos en el sistema de base 8 que en el de base 10. Cuanto más pequeña es la base, tantos menos dígitos diferentes se manejan, pero tantos más entran en la composición de los números.

Si utilizamos el sistema de base 20, el número 7.291 se convierte en 18 ´ 202 más 4 ´ 201 más 11 ´ 200. Si escribimos el 18 como # y el 11 como %, podemos decir que # 4 % (en base 20) = 7.291 (en base 10). En un sistema de base 20 tendríamos que tener 19 dígitos diferentes, pero a cambio tendríamos menos dígitos por número.

El 10 es una base conveniente. No exige recordar demasiados dígitos diferentes y tampoco da demasiados dígitos en un número dado.

¿Y los números basados en potencias de dos, es decir los números en base 2? Esos son los «números binarios», de la palabra latina que significa «dos de cada vez».

El número 7.291 es igual a 1 ´ 212 más 1 ´ 211 más 1 ´ 210 más 0 ´ 29 más 0 ´ 28 más 0 ´ 27 más 1 ´ 26 más 1 ´ 25 más 1 ´ 24 más 1 ´ 23 más 0 ´ 22 más 1 ´ 21 más 1 ´ 20. (El lector puede comprobarlo, recordando que 29, por ejemplo, es 2 multiplicado por sí mismo nueve veces: 2 ´ 2 ´ 2 ´ 2 ´ 2 ´ 2 ´ 2 ´ 2 ´ 2 = 512.) Si nos limitamos a escribir los dígitos tenemos 1110001111011 (en base 2) = 7.291 (en base 10).

Los números binarios contienen sólo unos y ceros, de modo que la adición y la multiplicación son fantásticamente simples. Sin embargo, hay tantos dígitos en números incluso pequeños, como el 7.291, que es muy fácil que la mente humana se confunda.

Los computadores, por su parte, pueden utilizar conmutadores de dos posiciones. En una dirección, cuando pasa la corriente, puede simbolizar un 1; en la otra dirección, cuando no pasa corriente, un 0. Disponiendo los circuitos de manera que los conmutadores se abran y cierren de acuerdo con las reglas binarias de la adición y de la multiplicación, el computador puede realizar cálculos aritméticos a gran velocidad. Y puede hacerlo mucho más rápido que si tuviese que trabajar con ruedas dentadas marcadas del 0 al 9 como en las calculadoras de mesa ordinarias basadas en el sistema de base 10 o decimal.

6. ¿Qué son los números imaginarios?

Hay dos clases de números con las que la mayoría de nosotros está familiarizado: los números positivos (+5, +17,5) y los números negativos (-5, – 17,5). Los números negativos fueron introducidos en la Edad Media para poder resolver problemas como 3 – 5. A los antiguos les parecía imposible restar cinco manzanas de tres manzanas. Pero los banqueros medievales tenían una idea muy clara de la deuda. «Dame cinco manzanas. Sólo tengo dinero para tres, de modo que te dejo a deber dos», que es como decir (+3) – (+5)= (-2).

Los números positivos y negativos se pueden multiplicar según reglas bien definidas. Un número positivo multiplicado por otro positivo da un producto positivo. Un número positivo multiplicado por otro negativo da un producto negativo. Y lo que es más importante, un número negativo multiplicado por otro negativo da un producto positivo.

Así: (+1) ´ (+1) = (+1); (+1) ´ (-1) = (-1); y (-1) ´ (-1) = (+1).

Supongamos ahora que nos preguntamos: ¿Qué número multiplicado por sí mismo da +1? O expresándolo de manera más matemática: ¿Cuál es la raíz cuadrada de +1?

Hay dos soluciones. Una es +1, puesto que (+1) ´ (+1) = (+ 1). La otra es -1, puesto que (-1) ´ (-1) = (+1). Los matemáticos lo expresan en su jerga escribiendo = ± 1

Sigamos ahora preguntando: ¿Cuál es la raíz cuadrada de -1?

Aquí nos ponen en un brete. No es + 1, porque multiplicado por sí mismo da +1. Tampoco es -1, porque multiplicado por sí mismo da también +1. Cierto que (+1) ´ (-1) = (-1), pero esto es la multiplicación de dos números diferentes y no la de un número por sí mismo.

Podemos entonces inventar un número y darle un signo especial, por ejemplo # 1, definiéndolo como sigue: # 1 es un número tal que (# 1) ´ (# 1) = (-1). Cuando se introdujo por vez primera esta noción, los matemáticos se referían a ella como un «número imaginario» debido simplemente a que no existía en el sistema de números a que estaban acostumbrados. De hecho no es más imaginario que los «números reales» ordinarios. Los llamados números imaginarios tienen propiedades perfectamente definidas y se manejan con tanta facilidad como los números que ya existían antes.

Y, sin embargo, como se pensaba que los nuevos números eran «imaginarios», se utilizó el símbolo «i». Podemos hablar de números imaginarios positivos (+i) y números imaginarios negativos (-i), mientras que (+1) es un número real positivo y (-1) un número real negativo. Así pues, podemos decir = +i.

El sistema de los números reales tiene una contrapartida similar en el sistema de los números imaginarios. Si tenemos +5, – 17,32, +3/10, también podemos tener +5i, – 17,32i, +3i/10.

Incluso podemos representar gráficamente el sistema de números imaginarios.

Supóngase que representamos el sistema de los números reales sobre una recta, con el 0 (cero) en el centro. Los números positivos se hallan a un lado del cero y los negativos al otro.

Podemos entonces representar el sistema imaginario de números a lo largo de otra recta que corte a la primera en ángulo recto en el punto cero, con los imaginarios positivos a un lado y los negativos al otro. Utilizando ambos tipos al mismo tiempo se pueden localizar números en cualquier lugar del plano: (+2) + (+3i) ó (+3) + (-2i). Éstos son «números complejos».

Para los matemáticos y los físicos resulta utilísimo poder asociar todos los puntos de un plano con un sistema de números. No podrían pasarse sin los llamados números imaginarios.

7. ¿Qué son los números primos y porqué les interesan a los matemáticos?

Un número primo es un número que no puede expresarse como producto de dos números distintos de sí mismo y uno. El 15 = 3 ´ 5, con lo cual 15 no es un número primo; 12 = 6 ´ 2 = 4 ´ 3, con lo cual 12 tampoco es un número primo. En cambio 13 = 13 ´ 1 y no es el producto de ningún otro par de números, por lo cual 13 es un número primo.

Hay números de los que no hay manera de decir a simple vista si son primos o no. Hay ciertos tipos, en cambio, de los cuales se puede decir inmediatamente que no son primos. Cualquier número, por largo que sea, que termine en 2, 4, 5, 6, 8 ó 0 o cuyos dígitos sumen un número divisible por 3, no es primo. Sin embargo, un número que acabe en 1, 3, 7 ó 9 y cuyos dígitos sumen un número no divisible por 3, puede que sea primo -pero puede que no-. No hay ninguna fórmula que nos lo diga. Hay que ensayar y ver si se puede escribir como producto de dos números más pequeños.

Una manera de encontrar números primos consiste en escribir todos los números del 2 al más alto posible, por ejemplo el 10.000. El primero es 2, que es primo. Lo dejamos donde está y recorremos toda la lista tachando uno de cada dos números, con lo cual eliminamos todos los números divisibles por dos, que no son primos. De los que quedan, el número más pequeño después del 2 es el 3. Este es el siguiente primo. Dejándolo donde está, tachamos a partir de él uno de cada tres números, deshaciéndonos así de todos los divisibles por 3. El siguiente número sin tachar es el 5, por lo cual tachamos uno de cada cinco números a partir de él. El siguiente es el 7, uno de cada siete; luego el 11, uno de cada once; luego el 13…, etc.

Podría pensarse que después de tachar y tachar números llegará un momento en que todos los números mayores que uno dado estarán tachados y que por tanto no quedará ningún número primo superior a un cierto número primo máximo. En realidad no es así. Por mucho que subamos en los millones y billones, siempre quedan números primos que han escapado a todas las tachaduras.

Ya en el año 300 a. C. demostró el matemático griego Euclides que por mucho que subamos siempre tiene que haber números primos superiores a esos. Tomemos los seis primeros números primos y multipliquémoslos: 2 ´ 3 ´ 5 ´ 7 ´ 11 ´ 13 = 30.030. Sumando 1 obtenemos 30.031. Este número no es divisible por 2, 3, 5, 7, 11 ni 13, puesto que al dividir siempre dará un resto de 1. Si 30.031 no se puede dividir por ningún número excepto él mismo, es que es primo. Si se puede, entonces los números de los cuales es producto tienen que ser superiores a 13. De hecho 30.031 = 59 ´ 509.

Esto mismo lo podemos hacer para el primer centenar de números primos, para el primer billón o para cualquier número. Si calculamos el producto y sumamos 1, el número final o bien es un número primo o bien es el producto de números primos mayores que los que hemos incluido en la lista. Por mucho que subamos siempre habrá números primos aún mayores, con lo cual el número de números primos es infinito.

De cuando en cuando aparecen parejas de números impares consecutivos, ambos primos: 5, 7; 11, 13; 17, 19; 29, 31; 41, 43. Tales parejas de primos aparecen por doquier hasta donde los matemáticos han podido comprobar. ¿Es infinito el número de tales parejas de primos? Nadie lo sabe. Los matemáticos, creen que sí, pero nunca lo han podido probar. Por eso están interesados en los números primos. Los números primos presentan problemas aparentemente inocentes pero que son muy difíciles de resolver, y los matemáticos no pueden resistir el desafío.

¿Qué utilidad tiene eso? Ninguna; pero eso precisamente parece aumentar el interés.

8. ¿Qué ocurriría si una fuerzairresistible se enfrentase con un

cuerpo inamovible?

He aquí un rompecabezas clásico sobre el que han debido verter su palabrería millones y millones de argumentos.

Pero antes de dar mi solución pongamos algunas cosas en claro. El juego de explorar el universo mediante técnicas racionales hay que jugarlo, como todos los juegos, de acuerdo con ciertas reglas. Si dos personas quieren conversar inteligentemente tienen que ponerse de acuerdo acerca del significado de los símbolos que utilizan (palabras o cualesquiera otros) y sus comentarios han de tener sentido en función de ese significado.

Todas las preguntas que no tengan sentido en función de las definiciones convenidas se las echa fuera de casa. No hay respuesta porque la pregunta no ha debido ser formulada.

Supongamos por ejemplo que pregunto: «¿Cuánto pesa la justicia?» (quizá esté pensando en la estatua de la justicia con la balanza en la mano).

Pero el peso es una propiedad de la masa, y sólo tienen masa las cosas materiales. (De hecho, la definición más simple de materia es «aquello que tiene masa».)

La justicia no es una cosa material, sino una abstracción. Por definición, la masa no es una de sus propiedades, y preguntar por el peso de la justicia es formular una pregunta sin sentido. No existe respuesta.

Por otro lado, mediante una serie de manipulaciones algebraicas muy simples es posible demostrar que 1 = 2. Lo malo es que en el curso de la demostración hay que dividir por cero. A fin de evitar una igualdad tan inconveniente (por no hablar de otras muchas demostraciones que destruirían la utilidad de las matemáticas), los matemáticos han decidido excluir la división por cero en cualquier operación matemática. Así pues, la pregunta «¿cuánto vale la fracción 2/0?» viola las reglas del juego y carece de sentido. No precisa de respuesta.

Ahora ya estamos listos para vérnoslas con esa fuerza irresistible y ese cuerpo inamovible.

Una «fuerza irresistible» es, por definición (si queremos que las palabras tengan significado), una fuerza que no puede ser resistida; una fuerza que moverá o destruirá cualquier cuerpo que encuentre, por grande que sea, sin debilitarse ni desviarse perceptiblemente. En un universo que contiene una fuerza irresistible no puede haber ningún cuerpo inamovible, pues acabamos de definir esa fuerza irresistible como una fuerza capaz de mover cualquier cosa.

Un «cuerpo inamovible» es, por definición (si queremos que las palabras tengan algún significado), un cuerpo que no puede ser movido; un cuerpo que absorberá cualquier fuerza que encuentre, por muy grande que sea, sin cambiar ni sufrir daños perceptibles en el encuentro. En un universo que contiene un cuerpo inamovible no puede haber ninguna fuerza irresistible porque acabamos de definir ese cuerpo inamovible como un cuerpo capaz de resistir cualquier fuerza.

Si formulamos una pregunta que implique la existencia simultánea de una fuerza irresistible y de un cuerpo inamovible, estamos violando las definiciones implicadas por las frases mismas. Las reglas del juego de la razón no lo permiten. Así pues, la pregunta «¿Qué ocurriría si una fuerza irresistible se enfrentase con un cuerpo inamovible?» carece de sentido y no precisa de respuesta.

El lector quizá se pregunte si es posible construir las definiciones de modo que no quepa formular preguntas incontestables. La respuesta es que no, como ya expliqué en la cuestión 4.

9. ¿Cuántas partículas hay en eluniverso?

En realidad no hay una respuesta concreta a esta pregunta, porque de entrada no sabemos cómo es de grande el universo. Sin embargo hagamos algunas hipótesis.

Uno de los cálculos es que hay unas 100.000.000.000 (ó 1011, un 1 seguido de 11 ceros) de galaxias en el universo. Cada una de estas galaxias tiene por término medio una masa 100.000.000.000 (ó 1011) mayor que la del Sol.

Quiere decirse que la cantidad total de materia en el universo es igual a 1011 ´ 1011 ó 1022 veces la masa del Sol. Dicho con otras palabras, en el universo hay materia suficiente para hacer 10.000.000.000.000.000.000.000 (diez mil trillones) de soles como el nuestro.

La masa del Sol es de 2 ´ 1033 gramos. Esto significa que la cantidad total de materia en el universo tiene una masa de 1022 ´ 2 ´ 1033 gramos. Lo cual puede escribirse como 20.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000. Dicho con palabras, veinte nonillones.

Procedamos ahora desde el otro extremo. La masa del universo está concentrada casi por entero en los nucleones que contiene. (Los nucleones son las partículas que constituyen los componentes principales del núcleo atómico.) Los nucleones son cosas diminutas y hacen falta 6 ´ 1023 de ellos para juntar una masa de 1 gramo.

Pues bien, si 6 ´ 1023 nucleones hacen 1 gramo y si hay 2 ´ 1055 gramos en el universo, entonces el número total de nucleones en el universo es 6 ´ 1023 ´ 2 ´ 1055 ó 12 ´ 1078, que de manera más convencional se escribiría 1,2 ´ 1079.

Los astrónomos opinan que el 90 por 100 de los átomos del universo son hidrógeno, el 9 por 100 helio y el 1 por 100 elementos más complicados. Una muestra típica de 100 átomos consistiría entonces en 90 átomos de hidrógeno, 9 átomos de helio y 1 átomo de oxígeno (por ejemplo). Los núcleos de los átomos de hidrógeno contendrían 1 nucleón cada uno: 1 protón. Los núcleos de los átomos de helio contendrían 4 nucleones cada uno: 2 protones y 2 neutrones. El núcleo del átomo de oxígeno contendría 16 nucleones: 8 protones y 8 neutrones.

Los cien átomos juntos contendrían, por tanto, 142 nucleones: 116 protones y 26 neutrones

Existe una diferencia entre estos dos tipos de nucleones. El neutrón no tiene carga eléctrica y no es preciso considerar ninguna partícula que lo acompañe. Pero el protón tiene una carga eléctrica positiva y como el universo es, según se cree, eléctricamente neutro en su conjunto, tiene que existir un electrón (con una carga eléctrica negativa) por cada protón.

Así pues, por cada 142 nucleones hay 116 electrones (para compensar los 116 protones). Para mantener la proporción, los 1,2 ´ 1079 nucleones del universo tienen que ir acompañados de 1 x 1079 electrones. Sumando los nucleones y electrones, tenemos un número total de 2,2 x 1079 partículas de materia en el universo. Lo cual se puede escribir como 22.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.– 000.000.000.000.000.000.000.000 (ó 22 tredecillones).

Si el universo es mitad materia y mitad antimateria, entonces la mitad de esas partículas son antinucleones y antielectrones. Pero esto no afectaría al número total.

De las demás partículas, las únicas que existen en cantidades importantes en el universo son los fotones, los neutrinos y posiblemente los gravitones. Pero como son partículas sin masa no las contaré. Veintidós tredecíllones es después de todo suficiente y constituye un universo apreciable.

10. ¿De dónde vino la sustancia deluniverso? ¿Qué hay más allá del borde

del universo?

La respuesta a la primera pregunta es simplemente que nadie lo sabe.

La ciencia no garantiza una respuesta a todo. Lo único que ofrece es un sistema para obtener respuestas una vez que se tiene suficiente información. Hasta ahora no disponemos de la clase de información que nos podría decir de dónde vino la sustancia del universo.

Pero especulemos un poco. A mí, por mi parte, se me ha ocurrido que podría haber algo llamado «energía negativa» que fuese igual que la «energía positiva» ordinaria pero con la particularidad de que cantidades iguales de ambas se unirían para dar nada como resultado (igual que + 1 y – 1 sumados dan 0).

Y al revés: lo que antes era nada podría cambiar de pronto y convertirse en una pompa de «energía positiva» y otra pompa igual de «energía negativa». De ser así, la pompa de «energía positiva» quizá se convirtiese en el universo que conocemos, mientras que en algún otro lado existiría el correspondiente «universo negativo».

¿Pero por qué lo que antes era nada se convirtió de pronto en dos pompas de energía opuesta?

¿Y por qué no? Sí 0 = (+ 1) + (- 1), entonces algo que es cero podría convertirse igual de bien en + 1 y – 1. Acaso sea que en un mar infinito de nada se estén formando constantemente pompas de energía positiva y negativa de igual tamaño, para luego, después de sufrir una serie de cambios evolutivos, recombinarse y desaparecer. Estamos en una de esas pompas, en el período de tiempo entre la nada y la nada, y pensando sobre ello.

Pero todo esto no es más que especulación. Los científicos no han descubierto hasta ahora nada que se parezca a esa «energía negativa» ni tienen ninguna razón para suponer que exista; hasta entonces mí idea carecerá de valor.

¿Y qué hay más allí del universo? Supongamos que contesto: no-universo.

El lector dirá que eso no significa nada, y quizá esté en lo cierto. Por otro lado, hay muchas preguntas que no tienen respuesta sensata (por ejemplo, «¿qué altura tiene arriba?»), y estas preguntas son «preguntas sin sentido».

Pero pensemos de todos modos sobre ello.

Imagínese el lector convertido en una hormiga muy inteligente que viviese en medio del continente norteamericano. A lo largo de una vida entera de viaje habría cubierto kilómetros y kilómetros cuadrados de superficie terrestre y con ayuda de unos prismáticos inventados por él mismo vería miles y miles de kilómetros más. Naturalmente, supondría que la tierra continuaba sin fin.

Pero la hormiga podría también preguntarse si la tierra se acaba en algún lugar. Y entonces se plantearía una pregunta embarazosa: «Sí la tierra se acaba, ¿qué habrá más allá?»

Recuérdese bien: la única experiencia está relacionada con la tierra. La hormiga nunca ha visto el océano, ni tiene la noción de océano, ni puede imaginarse más que tierra. ¿No tendría que decir: «Si la tierra de hecho se acaba, al otro lado tiene que haber no-tierra, sea lo que fuese eso», y no estaría en lo cierto?

Pues bien, si el universo se define como la suma total de la materia y energía y todo el espacio que llenan, entonces, en el supuesto de que el universo tenga un fin, tiene que haber no-materia y no-energía inmersas en el no-espacio al otro lado. Dicho brevemente, no-universo sea lo que fuere eso.

Y si el universo nació como una pompa de energía positiva formada, junto con otra de energía negativa, a partir de nada, entonces más allá del universo hay nada, o lo que quizá sea lo mismo, no-universo.

11. ¿Por qué se habla de la «bajatemperatura del espacio»? ¿Cómo puede

tener el espacio vacío una

temperatura?

Ni debería hablarse de «baja temperatura del espacio» ni puede el espacio vacío tener una temperatura. La temperatura es el contenido térmico medio por átomo de una cantidad de materia, y sólo la materia puede tener temperatura.

Supongamos que un cuerpo como la Luna flotase en el espacio, a años luz de la estrella más cercana. Si al principio la superficie está a 25º C, perdería continuamente calor por radiación, pero también lo ganaría de la radiación de las estrellas lejanas. Sin embargo, la radiación que llegaría hasta ella desde las estrellas sería tan pequeña, que no compensaría la pérdida ocasionada por su propia radiación, y la temperatura de la superficie comenzaría a bajar al instante.

A medida que la temperatura de la superficie lunar bajase iría decreciendo el ritmo de pérdida de calor por radiación, hasta que finalmente, cuando la temperatura fuese suficientemente baja, la pérdida por radiación sería lo suficientemente pequeña como para ser compensada por la absorción de la radiación de las lejanas estrellas. En ese momento, la temperatura de la superficie lunar sería realmente baja: ligeramente superior al cero absoluto.

Esta baja temperatura de la superficie lunar, lejos de las estrellas, es un ejemplo de lo que la gente quiere decir cuando habla de la «baja temperatura del espacio».

En realidad, la Luna no está lejos de todas las estrellas. Está bastante cerca -menos de 100 millones de millas- de una de ellas: el Sol. Si la Luna diese al Sol siempre la misma cara, esta cara iría absorbiendo calor solar hasta que su temperatura en el centro de la cara sobrepasara con mucho el punto de ebullición del agua. Sólo a esa temperatura tan alta estarían equilibrados el gran influjo solar y su propia pérdida por radiación.

El calor solar avanzaría muy despacio a través de la sustancia aislante de la Luna, de suerte que la cara opuesta recibiría muy poco calor y este poco lo radiaría al espacio. La cara nocturna se hallaría por tanto a la «baja temperatura del espacio».

Ahora bien, la Luna gira con respecto al Sol, de suerte que cualquier parte de la superficie recibe sólo el equivalente de dos semanas de luz solar de cada vez. Con este período de radiación tan limitado la temperatura superficial, de la Luna apenas alcanza el punto de ebullición del agua en algunos lugares. Durante la larga noche, la temperatura permanece nada menos que a 120º por encima del cero absoluto (más bien frío para nosotros) en todo momento, porque antes de que siga bajando vuelve a salir el Sol.

La Tierra es un caso completamente diferente, debido a que tiene una atmósfera y océanos. El océano se traga el calor de manera mucho más eficaz que la roca desnuda y lo suelta más despacio. Actúa como un colchón térmico: su temperatura no sube tanto en presencia del Sol ni baja tampoco tanto, comparado con la Tierra, en ausencia suya. La Tierra gira además tan rápido, que en la mayor parte de su superficie el día y la noche sólo duran horas. Por otro lado, los vientos atmosféricos transportan el calor de la cara diurna a la nocturna y de los trópicos a los polos.

Todo esto hace que la Tierra esté sometida a una gama de temperaturas mucho más pequeñas que la Luna, pese a que ambos cuerpos distan lo mismo del Sol.

¿Qué le pasaría a una persona que se viera expuesta a las temperaturas subantárticas de la cara nocturna de la Luna? No tanto como uno diría. Aquí, en la Tierra, aun yendo abrigados con vestidos aislantes, el cuerpo humano pierde rápidamente calor, que se disipa en la atmósfera y sus vientos, que a su vez se encargan de llevárselo lejos. La situación en la Luna es muy diferente. Un hombre, enfundado en su traje y botas espaciales, experimentaría una pérdida muy escasa, ya fuese por conducción a la superficie o por convección al espacio vacío en ausencia de viento. Es como si se hallase dentro de un termo en el vacío y radiando sólo pequeñas cantidades de infrarrojos. El proceso de enfriamiento sería muy lento. Su cuerpo estaría produciendo naturalmente calor todo el tiempo, y es más probable que sintiese calor que no frío.

12. ¿Qué es el polvo cósmico y dedónde viene?

Según las teorías astronómicas actuales, las galaxias fueron en origen grandes conglomerados de gas y polvo que giraban lentamente, fragmentándose en vórtices turbulentos y condensándose en estrellas. En algunas regiones donde la formación de estrellas fue muy activa, casi todo el polvo y el gas fue a parar a una estrella u otra. Poco o nada fue lo que quedó en el espacio intermedio. Esto es cierto para los cúmulos globulares, las galaxias elípticas y el núcleo central de las galaxias espirales.

Dicho proceso fue mucho menos eficaz en las afueras de las galaxias espirales. Las estrellas se formaron en números mucho menores y sobró mucho polvo y mucho gas. Nosotros, los habitantes de la Tierra, nos encontramos en los brazos espirales de nuestra galaxia y vemos las manchas oscuras que proyectan las nubes de polvo contra el resplandor de la Vía Láctea. El centro de nuestra propia galaxia queda completamente oscurecido por tales nubes.

El material de que está formado el universo consiste en su mayor parte en hidrógeno y helio. Los átomos de helio no tienen ninguna tendencia a juntarse unos con otros. Los de hidrógeno sí, pero sólo en parejas, formando moléculas de hidrógeno (H2). Quiere decirse que la mayor parte del material que flota entre las estrellas consiste en pequeños átomos de helio o en pequeños átomos y moléculas de hidrógeno. Todo ello constituye el gas interestelar, que forma la mayor parte de la materia entre las estrellas.

El polvo interestelar (o polvo cósmico) que se halla presente en cantidades mucho más pequeñas, se compone de partículas diminutas, pero mucho más grandes que átomos o moléculas, y por tanto deben contener átomos que no son ni de hidrógeno ni de helio.

El tipo de átomo más común en el universo, después del hidrógeno y del helio, es el oxígeno. El oxígeno puede combinarse con hidrógeno para formar grupos oxhidrilo (OH) y moléculas de agua (H2O), que tienen una marcada tendencia a unirse a otros grupos y moléculas del mismo tipo que encuentren en el camino, de forma que poco a poco se van constituyendo pequeñísimas partículas compuestas por millones y millones de tales moléculas. Los grupos oxhidrilo y las moléculas de agua pueden llegar a constituir una parte importante del polvo cósmico. Fue en 1965 cuando se detectó por primera vez grupos oxhidrilo en el espacio y se comenzó a estudiar su distribución. Desde entonces se ha informado también de la existencia de moléculas más complejas, que contienen átomos de carbono así como de hidrógeno y oxígeno.

El polvo cósmico tiene que contener también agrupaciones atómicas formadas por átomos aún menos comunes que los de hidrógeno, oxígeno y carbono. En el espacio interestelar se han detectado átomos de calcio, sodio, potasio y hierro, observando la luz que esos átomos absorben.

Dentro de nuestro sistema solar hay un material parecido, aportado quizás por los cometas. Es posible que fuera de los límites visibles del sistema solar exista una capa con gran número de cometas, y que algunos de ellos se precipiten hacia el Sol (acaso por los efectos gravitatorios de las estrellas cercanas). Los cometas son conglomerados sueltos de diminutos fragmentos sólidos de metal y roca, unidos por una mezcla de hielo, metano y amoníaco congelados y otros materiales parecidos. Cada vez que un cometa se aproxima al Sol, se evapora parte de su materia, liberando diminutas partículas sólidas que se esparcen por el espacio en forma de larga cola. En última instancia el cometa se desintegra por completo.

A lo largo de la historia del sistema solar se han desintegrado innumerables cometas y han llenado de polvo el espacio interior del sistema. La Tierra recoge cada día miles de millones de estas partículas de polvo («micrometeoroides»). Los científicos espaciales se interesan por ellas por diversas razones; una de ellas es que los micrometeoroides de mayor tamaño podrían suponer un peligro para los futuros astronautas y colonizadores de la Luna.

13. ¿Qué son los pulsares?

En el verano de 1967 Anthony Hewish y sus colaboradores de la Universidad de Cambridge detectaron, por accidente, emisiones de radio en los cielos que en nada se parecían a las que se habían detectado hasta entonces. Llegaban en impulsos muy regulares a intervalos de sólo 1 1/3 segundos. Para ser exactos, a intervalos de 1,33730109 segundos. La fuente emisora recibió el nombre de «estrella pulsante» o «pulsar» en abreviatura (pulsating star en inglés).

Durante los dos años siguientes se descubrieron un número bastante grande de tales pulsares, y el lector seguramente se preguntará por qué no se descubrieron antes. El caso es que un pulsar radia mucha energía en cada impulso, pero estos impulsos son tan breves que por término medio la intensidad de radioondas es muy baja, pasando inadvertida. Es más, los astrónomos suponían que las fuentes de radio emitían energía a un nivel constante y no prestaban atención a los impulsos intermitentes.

Uno de los pulsares más rápidos fue el que se encontró en la nebulosa del Cangrejo, comprobándose que radiaba en la zona visible del espectro electromagnético.

Se apagaba y se encendía en perfecta sincronización con los impulsos de radio. Aunque había sido observado muchas veces, había pasado hasta entonces por una estrella ordinaria. Nadie pensó jamás en observarlo con un aparato de detección lo bastante delicado como para demostrar que guiñaba treinta veces por segundo. Con pulsaciones tan rápidas, la luz parecía constante, tanto para el ojo humano como para los instrumentos ordinarios.

¿Pero qué es un pulsar? Si un objeto emite energía a intervalos periódicos es que está experimentando algún fenómeno de carácter físico en dichos intervalos. Puede ser, por ejemplo, un cuerpo que se está expandiendo y contrayendo y que emite un impulso de energía en cada contracción. O podría girar alrededor de su eje o alrededor de otro cuerpo y emitir un impulso de energía en cada rotación o revolución.

La dificultad estribaba en que la cadencia de impulsos era rapidísima, desde un impulso cada cuatro segundos a uno cada 1/30 de segundo. El pulsar tenía que ser un cuerpo muy caliente, pues si no podría emitir tanta energía; y tenía que ser un cuerpo muy pequeño, porque si no, no podría hacer nada con esa rapidez.

Los cuerpos calientes más pequeños que habían observado los científicos eran las estrellas enanas blancas. Pueden llegar a tener la masa de nuestro sol, son tanto o más calientes que él y sin embargo no son mayores que la Tierra. ¿Podría ser que esas enanas blancas produjesen impulsos al expandirse y contraerse o al rotar? ¿O se trataba de dos enanas blancas girando una alrededor de la otra? Pero por muchas vueltas que le dieron los astrónomos al problema no conseguían que las enanas blancas se movieran con suficiente rapidez.

En cuanto a objetos aún más pequeños, los astrónomos habían previsto teóricamente la posibilidad de que una estrella se contrajera brutalmente bajo la atracción de la gravedad, estrujando los núcleos atómicos unos contra otros. Los electrones y protones interaccionarían y formarían neutrones, y la estrella se convertiría en una especie de gelatina de neutrones. Una «estrella de neutrones» como ésta podría tener la misma masa que el Sol y medir sin embargo sólo diez millas de diámetro.

Ahora bien, jamás se había observado una estrella de neutrones, y siendo tan pequeñas se temía que aunque existiesen no fueran detectables.

Con todo, un cuerpo tan pequeño sí podría girar suficientemente rápido para producir los impulsos. En ciertas condiciones los electrones sólo podrían escapar en ciertos puntos de la superficie. Al girar la estrella de neutrones, los electrones saldrían despedidos como el agua de un aspersor; en cada vuelta habría un momento en que el chorro apuntase en dirección a la Tierra, haciéndonos llegar ondas de radio y luz visible.

Thomas Gold, de la Universidad Cornell, pensó que, en ese supuesto, la estrella de neutrones perdería energía y las pulsaciones se irían espaciando cada vez más, cosa que resultó ser cierta. Hoy día parece muy probable que los pulsares sean esas estrellas de neutrones que los astrónomos creían indetectables.

14. Se dice que un centímetro cúbicode una estrella de neutrones pesa

miles de millones de toneladas. ¿Cómo

es posible?

Un átomo tiene aproximadamente 10-8 centímetros de diámetro. En los sólidos y líquidos ordinarios los átomos están muy juntos, casi en contacto mutuo. La densidad de los sólidos y líquidos ordinarios depende por tanto del tamaño exacto de los átomos, del grado de empaquetamiento y del peso de los distintos átomos.

De los sólidos ordinarios, el menos denso es el hidrógeno solidificado, con una densidad de 0,076 gramos por centímetro cúbico. El más denso es un metal raro, el osmio, con una densidad de 22,48 gramos por centímetro cúbico.

Si los átomos fuesen bolas macizas e incomprensibles, el osmio sería el material más denso posible y un centímetro cúbico de materia jamás podría pesar ni un kilogramo, y mucho menos toneladas.

Pero los átomos no son macizos. El físico neozelandés Ernest Rutherford demostró ya en 1909 que los átomos eran en su mayor parte espacio vacío. La corteza exterior de los átomos contiene sólo electrones ligerísimos, mientras que el 99,9 por 100 de la masa del átomo está concentrada en una estructura diminuta situada en el centro: el núcleo atómico.

El núcleo atómico tiene un diámetro de unos 10-13 centímetros (aproximadamente 1/100.000 del propio átomo). Si los átomos de una esfera de materia se pudieran estrujar hasta el punto de desplazar todos los electrones y dejar a los núcleos atómicos en contacto mutuo, el diámetro de la esfera disminuiría hasta 1/100.000 de su tamaño anterior.

De modo análogo, sí se pudiera comprimir la Tierra hasta dejarla reducida a un balón de núcleos atómicos, toda su materia quedaría reducida a una esfera de unos 130 metros de diámetro. En esas mismas condiciones, el Sol mediría 13,7 kilómetros de diámetro. Y si pudiéramos convertir toda la materia conocida del universo en núcleos atómicos en contacto, obtendríamos una esfera de sólo algunos cientos de millones de kilómetros de diámetro, que cabría cómodamente dentro del cinturón de asteroides del sistema solar.

El calor y la presión que reinan en el centro de las estrellas rompen la estructura atómica y permiten que los núcleos atómicos empiecen a empaquetarse unos junto a otros. Las densidades en el centro del Sol son mucho más altas que la del osmio, pero como los núcleos atómicos se mueven de un lado a otro sin impedimento alguno, el material sigue siendo un gas. Hay estrellas que se componen casi por entero de tales átomos destrozados. La compañera de la estrella Sirio es una «enana blanca» no mayor que el planeta Urano, y sin embargo tiene una masa parecida a la del Sol.

Los núcleos atómicos se componen de protones y neutrones. Todos los protones tienen cargas eléctricas positivas y se repelen entre sí, de modo que en un lugar dado no se pueden reunir más de un centenar de ellos. Los neutrones, por el contrario, no tienen carga y en condiciones adecuadas es posible empaquetar un sinfín de ellos para formar una «estrella de neutrones». Los pulsares, según se cree, son estrellas de neutrones.

Si el Sol se convirtiera en una estrella de neutrones, toda su masa quedaría concentrada en una pelota cuyo diámetro sería 1/100.000 del actual y su volumen (1/100.000)3 ó 1/1.000.000.000.000.000 (una milbillónésima) del actual. Su densidad sería por tanto 1.000.000.000.000.000 (mil billones) de veces superior a la que tiene ahora.

La densidad global del Sol hoy día es de 1,4 gramos por centímetro cúbico. Si fuese una estrella de neutrones, su densidad sería de 1.400.000.000.000.000 gramos por centímetro cúbico. Es decir, un centímetro cúbico de una estrella de neutrones puede llegar a pesar 1.400.000.000 (mil cuatrocientos millones) de toneladas.

15. ¿Qué es un agujero negro?

Para entender lo que es un agujero negro empecemos por una estrella como el Sol. El Sol tiene un diámetro de 1.390.000 kilómetros y una masa 330.000 veces superior a la de la Tierra. Teniendo en cuenta esa masa y la distancia de la superficie al centro se demuestra que cualquier objeto colocado sobre la superficie del Sol estaría sometido a una atracción gravitatoria 28 veces superior a la gravedad terrestre en la superficie.

Una estrella corriente conserva su tamaño normal gracias al equilibrio entre una altísima temperatura central, que tiende a expandir la sustancia estelar, y la gigantesca atracción gravitatoria, que tiende a contraerla y estrujarla.

Si en un momento dado la temperatura interna desciende, la gravitación se hará dueña de la situación. La estrella comienza a contraerse y a lo largo de ese proceso la estructura atómica del interior se desintegra. En lugar de átomos habrá ahora electrones, protones y neutrones sueltos. La estrella sigue contrayéndose hasta el momento en que la repulsión mutua de los electrones contrarresta cualquier contracción ulterior.

La estrella es ahora una «enana blanca». Si una estrella como el Sol sufriera este colapso que conduce al estado de enana blanca, toda su masa quedaría reducida a una esfera de unos 16.000 kilómetros de diámetro, y su gravedad superficial (con la misma masa pero a una distancia mucho menor del centro) sería 210.000 veces superior a la de la Tierra.

En determinadas condiciones la atracción gravitatoria se hace demasiado fuerte para ser contrarrestada por la repulsión electrónica. La estrella se contrae de nuevo, obligando a los electrones y protones a combinarse para formar neutrones y forzando también a estos últimos a apelotonarse en estrecho contacto. La estructura neutrónica contrarresta entonces cualquier ulterior contracción y lo que tenemos es una «estrella de neutrones», que podría albergar toda la masa de nuestro sol en una esfera de sólo 16 kilómetros de diámetro. La gravedad superficial sería 210.000.000.000 veces superior a la de la Tierra.

En ciertas condiciones, la gravitación puede superar incluso la resistencia de la estructura neutrónica. En ese caso ya no hay nada que pueda oponerse al colapso. La estrella puede contraerse hasta un volumen cero y la gravedad superficial aumentar hacia el infinito.

Según la teoría de la relatividad, la luz emitida por una estrella pierde algo de su energía al avanzar contra el campo gravitatorio de la estrella. Cuanto más intenso es el campo, tanto mayor es la pérdida de energía, lo cual ha sido comprobado experimentalmente en el espacio y en el laboratorio.

La luz emitida por una estrella ordinaria como el Sol pierde muy poca energía. La emitida por una enana blanca, algo más; y la emitida por una estrella de neutrones aún más. A lo largo del proceso de colapso de la estrella de neutrones llega un momento en que la luz que emana de la superficie pierde toda su energía y no puede escapar.

Un objeto sometido a una compresión mayor que la de las estrellas de neutrones tendría un campo gravitatorio tan intenso, que cualquier cosa que se aproximara a él quedaría atrapada y no podría volver a salir. Es como si el objeto atrapado hubiera caído en un agujero infinitamente hondo y no cesase nunca de caer. Y como ni siquiera la luz puede escapar, el objeto comprimido será negro. Literalmente, un «agujero negro».

Hoy día los astrónomos están buscando pruebas de la existencia de agujeros negros en distintos lugares del universo.

16. ¿Qué temperatura puede alcanzaruna estrella?

Depende de la estrella y de qué parte de la estrella consideremos.

Más del 99 por 100 de las estrellas que podemos detectar pertenecen -como nuestro Sol- a una clasificación llamada «secuencia principal», y al hablar de la temperatura de una estrella queremos decir, por lo general, la temperatura de su superficie. Empecemos por aquí.

Toda estrella tiene una tendencia a «colapsar» (derrumbarse hacia el interior) bajo su propia atracción gravitatoria, pero a medida que lo hace aumenta la temperatura en su interior. Y al calentarse el interior, la estrella tiende a expandirse. Al final se establece el equilibrio y la estrella alcanza un cierto tamaño fijo. Cuanto mayor es la masa de la estrella, mayor tiene que ser la temperatura interna para contrarrestar esa tendencia al colapso; y mayor también, por consiguiente, la temperatura superficial.

El Sol, que es una estrella de tamaño medio, tiene una temperatura superficial de 6.000º C. Las estrellas de masa inferior tienen temperaturas superficiales más bajas, algunas de sólo 2.500º C.

Las estrellas de masa superior tienen temperaturas más altas: 10.000º C, 20.000º C y más. Las estrellas de mayor masa, y por tanto las más calientes y más brillantes, tienen una temperatura superficial constante de 50.000º C como mínimo, y quizá más. Nos atreveríamos a decir que la temperatura superficial constante más alta posible de una estrella de la secuencia principal es 80.000º C.

¿Por qué no más? ¿Y si consideramos estrellas de masa cada vez mayor? Aquí hay que parar el carro. Si una estrella ordinaria adquiere una masa tal que su temperatura superficial supera los 80.000º C, las altísimas temperaturas del interior producirán una explosión. En momentos determinados es posible que se alcancen temperaturas superiores, pero una vez pasada la explosión quedará atrás una estrella más pequeña y más fría que antes.

La superficie, sin embargo, no es la parte más caliente de una estrella. El calor de la superficie se transmite hacia afuera, a la delgada atmósfera (o «corona») que rodea a la estrella. La cantidad total de calor no es mucha, pero como los átomos son muy escasos en la corona (comparados con los que hay en la estrella misma), cada uno de ellos recibe una cuantiosa ración. Lo que mide la temperatura es la energía térmica por átomo, y por esa razón la corona solar tiene una temperatura de 1.000.000º C aproximadamente.

También el interior de una estrella es mucho más caliente que la superficie. Y tiene que ser así porque sino no podría aguantar las capas exteriores de la estrella contra la enorme atracción centrípeta de la gravedad. La temperatura del núcleo interior del Sol viene a ser de unos 15.000.000º C.

Una estrella de masa mayor que la del Sol tendrá naturalmente una temperatura nuclear y una temperatura superficial más altas. Por otro lado, para una masa dada las estrellas tienden a hacerse más calientes en su núcleo interior a medida que envejecen. Algunos astrónomos han intentado calcular la temperatura que puede alcanzar el núcleo interior antes de que la estrella se desintegre. Una de las estimaciones que yo conozco da una temperatura máxima de 6.000.000.000º C.

¿Y qué ocurre con los objetos que no se hallan en la secuencia principal? En particular, ¿qué decir acerca de los objetos descubiertos recientemente, en los años sesenta? Tenemos los pulsares, que según se cree son «estrellas de neutrones» increíblemente densas, con toda la masa de una estrella ordinaria empaquetada en una esfera de un par de decenas de kilómetros de diámetro. La temperatura de su interior ¿no podría sobrepasar ese «máximo» de los seis mil millones de grados? Y también están los quasares, que según algunos son un millón de estrellas ordinarias, o más, colapsadas todas en una ¿Qué decir de la temperatura de su núcleo interior?

Hasta ahora nadie lo sabe.

17. ¿Hasta dónde puede llegar elproceso de fusión dentro de una

estrella?

Cuando un número determinado de protones y neutrones se juntan para formar un núcleo atómico, la combinación resultante es más estable y contiene menos masa que esos mismos protones y neutrones por separado. Al formarse la combinación, el exceso de masa se convierte en energía y se dispersa por radiación.

Mil toneladas de hidrógeno, cuyos núcleos están constituidos por un solo protón, se convierten en 993 toneladas de helio, cuyos núcleos constan de dos protones y dos neutrones. Las siete toneladas restantes de masa se emiten en forma de energía.

Las estrellas como nuestro Sol radian energía formada de esta manera. El Sol convierte unas 654.600.000 toneladas de hidrógeno en algo menos de 650.000.000 toneladas de helio por segundo. Pierde por tanto 4.600.000 toneladas de masa cada segundo. Pero incluso a este ritmo tan tremendo, el Sol contiene suficiente hidrógeno para mantenerse todavía activo durante miles de millones de años.

Ahora bien, llegará el día en que las reservas de hidrógeno del Sol lleguen a agotarse. ¿Significa eso que el proceso de fusión se parará y que el Sol se enfriará?

No del todo. Los núcleos de helio no representan el empaquetamiento más económico de los protones y neutrones. Los núcleos de helio se pueden fusionar en núcleos aún más complicados, tan complicados como los del hierro. De este modo se seguirá emitiendo energía.

Pero tampoco mucha más. Las 1.000 toneladas de hidrógeno que, según hemos dicho, se fusionan en 993 toneladas de helio se pueden fusionar luego en 991,5 toneladas de hierro. Al pasar de hidrógeno a helio se convierten en energía siete toneladas de masa, pero sólo una y media al pasar de helio a hierro.

Y al llegar al hierro entramos en una vía muerta. Los protones y neutrones del núcleo de hierro están empaquetados con una estabilidad máxima. Cualquier cambio que se produzca en el hierro, ya sea en la dirección de átomos más simples o de átomos más complejos, no emite energía sino que la absorbe.

Podemos decir por tanto que cuando la estrella alcanza la fase del helio ha emitido ya unas cuatro quintas partes de toda la energía de fusión disponible; al pasar al hierro emite la quinta parte restante y allí se acaba la historia.

Pero ¿qué sucede después?

Al pasar a la etapa de fusión posterior al helio el núcleo de la estrella se torna mucho más caliente. Según una teoría, al llegar a la etapa del hierro se vuelve lo bastante caliente como para iniciar reacciones nucleares que producen cantidades enormes de neutrinos. El material estelar no absorbe los neutrinos: tan pronto como se forman salen disparados a la velocidad de la luz, llevándose energía consigo. El núcleo de la estrella pierde energía, se enfría de forma bastante brusca y la estrella se convierte por colapso en una enana blanca.

En el curso de este colapso, las capas exteriores, que aún poseen átomos menos complicados que los de hierro, se fusionan todos a un tiempo, explotando en una «nova». La energía resultante forma átomos más complicados que los de hierro, incluso de uranio y más complejos aún.

Los restos de tales novas, que contienen átomos pesados, se mezclan con el gas interestelar. Las estrellas formadas a partir de ese gas, llamadas «estrellas de la segunda generación», contienen pequeñas cantidades de átomos pesados que jamás podrían haber conseguido a través del proceso de fusión ordinario. El Sol es una estrella de la segunda generación. Y por eso, hay oro y uranio en la Tierra.

18. ¿Qué ocurre con toda la energíaemitida por las estrellas?

Las estrellas emiten energía de diferentes maneras:

1. En forma de fotones de radiación electromagnética carentes de masa, desde los rayos gamma más energéticos a las ondas radioeléctricas menos energéticas (incluso la materia fría radia fotones; cuanto más fría es la materia, tanto más débiles son los fotones). La luz visible es parte de esta clase de radiación.

2. En forma de otras partículas sin masa, como son los neutrinos y los gravitones.

3. En forma de partículas cargadas de alta energía, principalmente protones, pero también cantidades menores de diversos núcleos atómicos y otras clases de partículas. Son los rayos cósmicos.

Todas estas partículas emitidas -fotones, neutrinos, gravitones, protones, etc.– son estables mientras se hallen aisladas en el espacio. Pueden viajar miles de millones de años sin sufrir ningún cambio, al menos por lo que sabemos.

Así pues, todas estas partículas radiadas sobreviven hasta el momento (por muy lejano que sea) en que chocan contra alguna forma de materia que las absorbe. En el caso de los fotones sirve casi cualquier clase de materia. Los protones energéticos son ya más difíciles de parar y absorber, y mucho más difíciles aún los neutrinos. En cuanto a los gravitones, poco es lo que se sabe hasta ahora.

Supongamos ahora que el universo sólo consistiese en estrellas colocadas en una configuración invariable. Cualquier partícula emitida por una estrella viajaría por el espacio hasta chocar contra algo (otra estrella) y ser absorbida. Las partículas viajarían de una estrella a otra y, a fin de cuentas, cada una de ellas recuperaría toda la energía que había radiado. Parece entonces que el universo debería continuar inmutable para siempre.

El hecho de que no sea así es consecuencia de tres cosas:

1. El universo no consta sólo de estrellas sino que contiene una cantidad importante de materia fría, desde grandes planetas hasta polvo interestelar. Cuando esta materia fría frena a una partícula, la absorbe y emite a cambio partículas menos energéticas. Lo cual significa que en definitiva la temperatura de la materia fría aumenta con el tiempo, mientras que el contenido energético de las estrellas disminuye.

2. Algunas de las partículas (neutrinos y gravitones, por ejemplo) emitidas por las estrellas y también por otras formas de materia tienen una tendencia tan pequeña a ser absorbidas por éstas que desde que existe el universo sólo han sido absorbidas un porcentaje diminuto de ellas. Lo cual equivale a decir que la fracción de la energía total de las estrellas que pulula por el espacio es cada vez mayor y que el contenido energético de las estrellas disminuye.

3. El universo está en expansión. Cada año es mayor el espacio entre las galaxias, de modo que incluso partículas absorbibles, como los protones y los fotones, pueden viajar por término medio distancias mayores antes de chocar contra la materia y ser absorbidas. Esta es otra razón de que cada año sea menor la energía absorbida por las estrellas en comparación con la emitida, porque hace falta una cantidad extra de energía para llenar ese espacio adicional, producido por la expansión, con partículas energéticas y hasta entonces no absorbidas. Esta última razón es suficiente por sí misma. Mientras el universo siga en expansión, continuará enfriándose.

Naturalmente, cuando el universo comience a contraerse de nuevo -suponiendo que lo haga- la situación será la inversa y empezará a calentarse otra vez.

19. ¿Qué es el viento solar?

Ya en 1850, el astrónomo inglés Richard C. Carrington, estudiando a la sazón las manchas solares, notó una pequeñísima erupción en la cara del Sol que permaneció visible durante unos cinco minutos. Carrington pensó que había tenido la suerte de observar la caída de un gran meteoro en el Sol.

El uso de instrumentos más refinados para el estudio del Sol mostró hacia los años veinte de este siglo que esas «erupciones solares» eran sucesos comunes, que solían ocurrir en conjunción con las manchas solares. El astrónomo americano George E. Hale había inventado en 1889 el «espectroheliógrafo», que permitía observar el Sol a través de la luz de una longitud de onda determinada y fotografiar el Sol con la luz de hidrógeno incandescente de la atmósfera solar o del calcio incandescente, por ejemplo. Y se comprobó que las erupciones solares no tenían nada que ver con los meteoritos, sino que eran efímeras explosiones de hidrógeno caliente.

Las erupciones de pequeño tamaño son muy comunes pudiéndose detectar cientos de ellas en un día, especialmente donde hay grandes complejos de manchas solares y cuando éstas están creciendo. Las de gran tamaño, como la que vio Carrington, son raras, apareciendo sólo unas cuantas cada año.

Hay veces en que la erupción se produce justo en el centro del disco solar y explotan hacia arriba en dirección a la Tierra. Al cabo de un tiempo empiezan a ocurrir cosas muy curiosas en nuestro planeta. En cuestión de días las auroras boreales se abrillantan, dejándose ver a veces desde las regiones templadas. La aguja magnética se desmanda y se vuelve loca, por lo que a veces se habla de una «tormenta magnética».

Hasta el siglo presente tales sucesos no afectaban gran cosa a la población general. Pero en el siglo xx se comprobó que las tormentas magnéticas también afectaban a la recepción de radio y al comportamiento de los equipos electrónicos en general. La importancia de las tormentas magnéticas aumentó a medida que la humanidad fue dependiendo cada vez más de dichos equipos. Durante una de esas tormentas es muy posible que la transmisión por radio y televisión se interrumpa y que los equipos de radar dejen de funcionar.

Cuando los astrónomos estudiaron las erupciones con más detenimiento se vio que en la explosión salía despedido hacia arriba hidrógeno caliente y que parte de él lograba saltar al espacio a pesar de la gigantesca gravedad del Sol. Como los núcleos de hidrógeno son simples protones, el Sol está rodeado de una nube de protones (y de otros núcleos más complicados en cantidades más pequeñas) dispersos en todas direcciones. En 1958 el físico americano Eugene N. Parker llamó «viento solar» a esta nube de protones que mana hacia fuera.

Aquellos protones que salen despedidos en dirección a la Tierra llegan hasta nosotros, aunque la mayor parte de ellos bordean el planeta, obligados por la fuerza del campo magnético. Algunos, sin embargo, logran entrar en la atmósfera superior, donde dan lugar a las auroras boreales y a una serie de fenómenos eléctricos. Una erupción especialmente grande, que proyecte una nube muy intensa hacia la Tierra, producirá lo que podríamos llamar una «galerna solar» y dará lugar a los efectos de la tormenta magnética.

El viento solar es el agente responsable de las colas de los cometas. Lo que hace es barrer hacia afuera la nube de polvo y gas que rodea al cometa, cuando pasa cerca del Sol. También se ha observado el efecto del viento solar sobre los satélites artificiales. Uno de ellos, el Echo I, grande y ligero de peso, se desvió perceptiblemente de su órbita calculada por la acción del viento solar.

20. ¿Hasta cuándo podrá mantener eISoI la vida en la Tierra?

El Sol podrá mantener la vida terrestre (tal como la conocemos) mientras radie energía como lo hace ahora, y a este período de tiempo podemos ponerle ciertos límites.

La radiación del Sol proviene de la fusión del hidrógeno a helio. Para producir toda la radiación vertida por el Sol hace falta una cantidad ingente de fusión: cada segundo tienen que fusionarse 654.600.000 toneladas de hidrógeno en 650.000.000 toneladas de helio. (Las 4.600.000 toneladas restantes se convierten en energía de radiación y las pierde el Sol para siempre. La ínfima porción de esta energía que incide sobre la Tierra basta para mantener toda la vida de nuestro planeta.)

Nadie diría que con este consumo tan alto de hidrógeno por segundo el Sol pudiera durar mucho tiempo, pero es que ese cálculo no tiene en cuenta el enorme tamaño del Sol. Su masa totaliza 2.200.000.000.000.000.000.000.000.000 (más de dos mil cuatrillones) de toneladas. Un 53 por 100 de esta masa es hidrógeno, lo cual significa que el Sol contiene en la actualidad 1.166.000.000.000. 000.000.000.000.000 de toneladas, aproximadamente, de hidrógeno.

(Para satisfacer la curiosidad del lector, diremos que el resto de la masa del Sol es casi todo helio. Menos del 0,1 por 100 de su masa está constituido por átomos más complicados que el helio. El helio es más compacto que el hidrógeno. En condiciones idénticas, un número dado de átomos de helio tiene una masa cuatro veces mayor que el mismo número de átomos de hidrógeno. O digámoslo así: una masa dada de helio ocupa menos espacio que la misma masa de hidrógeno. En función del volumen -el espacio ocupado-, el Sol es hidrógeno en un 80 por 100.)

Si suponemos que el Sol fue en origen todo hidrógeno, que siempre ha convertido hidrógeno en helio al ritmo de 654 millones de toneladas por segundo y que lo seguirá haciendo hasta el final, se calcula que ha estado radiando desde hace unos cuarenta mil millones de años y que continuará así otros sesenta mil.

Pero las cosas no son en realidad tan simples. El Sol es una «estrella de la segunda generación», constituida a partir del gas y polvo cósmicos desperdigados por estrellas que se habían quemado y explotado miles de millones de años atrás. Así pues, la materia prima del Sol contenía ya mucho helio, desde el principio casi tanto como tiene ahora. Lo cual significa que el Sol ha estado radiando durante un ratito solamente (a escala astronómica), porque sus reservas originales de hidrógeno sólo han disminuido moderadamente. El Sol puede que no tengo más de seis mil millones de años.

Pero además es que el Sol no continuará radiando exactamente al mismo ritmo que ahora. El hidrógeno y el helio no están perfectamente entremezclados. El helio está concentrado en el núcleo central, y la reacción de fusión se produce en la superficie de este núcleo.

A medida que el Sol siga radiando, irá adquiriendo una masa cada vez mayor ese núcleo de helio y la temperatura en el centro aumentará. En última instancia, la temperatura sube lo suficiente como para transformar los átomos de helio en átomos más complicados. Hasta entonces el Sol radiará más o menos como ahora, pero una vez que comience la fusión del helio, empezará a expandirse y a convertirse poco a poco en una gigante roja. El calor se hará insoportable en la Tierra, los océanos se evaporarán y el planeta dejará de albergar la vida en la forma que conocemos.

Los astrónomos estiman que el Sol entrará en esta nueva fase dentro de unos ocho mil millones de años. Y como ocho mil millones de años es un plazo bastante largo, no hay motivo para alarmarse todavía.

21. Si la temperatura de lasuperficie solar es tan alta que está

al blanco, ¿por qué las manchas

solares son negras? Para ser negras

tendrían que ser frías, y ¿cómo puede

haber algo frío en el Sol?

La pregunta, tal corno está formulada, parece una verdadera pega. De hecho, a principios del siglo pasado el gran astrónomo William Herschel concluyó que las manchas solares tenían que ser frías porque eran negras. La única manera de explicarlo era suponer que el Sol no era caliente en su totalidad. Según Herschel, tenía una atmósfera incandescente, pero debajo había un cuerpo sólido frío, que es lo que nosotros veíamos a través de una serie de grietas de la atmósfera solar. Estas grietas eran las manchas solares. Herschel llegó incluso a pensar que el frío interior del Sol podía estar habitado por seres vivientes.

Pero esto es falso. Hoy día estamos completamente seguros de que el Sol es caliente en su totalidad. Es más, la superficie que vemos es la parte más fría del Sol, y aun así es ya demasiado caliente, sin lugar a dudas, para los seres vivos.

Radiación y temperatura están estrechamente relacionadas. En 1894, el físico alemán Wilhelm Wien estudió los distintos tipos de luz radiada a diferentes temperaturas y concluyó que, en condiciones ideales, cualquier objeto, independientemente de su composición química, radiaba una gama determinada de luz para cada temperatura.

A medida que aumenta la temperatura, la longitud de onda del máximo de radiación se hace cada vez más corta, del mismo modo para todos los cuerpos. A unos 600º C se desliza en la porción visible suficiente radiación para conferir al objeto un aspecto rojo mate. A temperaturas aún mayores, el objeto se hace rojo brillante, anaranjado, blanco y blanco azulado. (A temperaturas suficientemente altas, la radiación se hallaría en su mayor parte en el ultravioleta, y más allá aún.)

Midiendo con cuidado la longitud de onda del máximo de radiación solar (que se halla en la región del color amarillo) es posible calcular la temperatura de la superficie solar: resulta ser de unos 6.000º C.

Las manchas solares no se hallan a esta temperatura. Son bastante más frías y su temperatura en el centro hay que situarla en los 4.000º C solamente. Parece ser que las manchas solares representan gigantescas expansiones de gases, y tales expansiones, ya sean en el Sol o en un frigorífico, dan lugar a una importante caída de temperatura. Qué duda cabe que para mantener fría una gigantesca mancha solar durante días y semanas contra el calor que afluye de las zonas circundantes, más calientes, hace falta una enorme bomba térmica, y lo cierto es que los astrónomos no han dado aún con un mecanismo completamente satisfactorio para la formación de esas manchas.

Incluso a 4.000º C, las manchas solares deberían ser muy brillantes: mucho más que un arco voltaico, y un arco voltaico es ya demasiado brillante para mirarlo directamente.

Lo que ocurre es que las manchas solares son, efectivamente, más brillantes que un arco voltaico, y de ello pueden dar fe los instrumentos. El quid está en que el ojo humano no ve la luz de un modo absoluto, sino que juzga el brillo por comparación con el entorno. Las zonas más calientes de la superficie solar, las que podríamos llamar normales, son de cuatro a cinco veces más brillantes que las regiones más frías en el centro de una mancha solar, y comparando éstas con aquéllas, nos parecen negras. Ese negro es una especie de ilusión óptica.

Que esto es así puede demostrarse a veces durante los eclipses. La Luna eclipsante, con su cara oscura vuelta hacia la Tierra, es realmente negra contra el globo brillante del Sol. Cuando el borde de la Luna pasa por encima de una gran mancha solar, de modo que el «negro» de la mancha contrasta con la Luna, entonces se ve que la mancha, en realidad, no es negra.

22. ¿Por qué todos los planetasocupan aproximadamente el mismo plano

orbital?

La mejor conjetura astronómica es que todos se mueven en el mismo plano orbital porque nacieron de un mismo y único disco plano de materia.

Las teorías al uso sugieren que el sistema solar fue en origen una enorme masa de gas y polvo en rotación, que acaso fuese esférica en un principio. Bajo la influencia de su propia atracción gravitatoria fue condensándose, con lo cual tuvo que empezar a girar cada vez más deprisa para conservar el momento angular.

En un cierto momento de este proceso de condensación y rotación cada vez más acentuadas, el efecto centrífugo acabó por desgajar una porción de materia del plano ecuatorial. Esta porción de materia desgajada, que representaba un porcentaje pequeño del total, formó un gran disco plano alrededor de la porción central principal de la nube. De un modo u otro (pues sobre los detalles no hay ni mucho menos un consenso general) se condensaron una serie de planetas a partir de ese disco, mientras que el grueso de la nube se convirtió en el Sol. Los planetas siguieron girando en la región antes ocupada por el disco, y por esa razón giran todos ellos más o menos en el mismo plano del ecuador solar.

Por razones parecidas, los planetas, a medida que se fueron condensando, fueron formando satélites que giran, por lo general, en un único plano, que coincide con el del ecuador del planeta.

Según se cree, las excepciones a esta regla son debidas a sucesos violentos ocurridos mucho después de la formación general del sistema solar. El planeta Plutón gira en un plano que forma un ángulo de 17 grados con el plano de revolución de la Tierra. (Ningún otro planeta tiene una órbita tan inclinada.) Algunos astrónomos han conjeturado que Plutón quizá fuese en otro tiempo un satélite de Neptuno y que logró liberarse gracias a algún cataclismo no determinado. De los satélites actuales de Neptuno, el principal, que es Tritón, no gira en el plano ecuatorial de Neptuno, lo cual constituye otro indicio de algún cataclismo que afectó a ese planeta.

Júpiter posee siete satélites pequeños y distantes que no giran en el plano de su ecuador. El satélite más exterior de Saturno se halla en el mismo caso. Es probable que estos satélites no se formaran en su presente posición, en el momento de nacer el sistema solar, sino que sean asteroides capturados mucho después por esos planetas gigantes.

Muchos de los asteroides que giran entre las órbitas de Marte y Júpiter tienen planos orbitales muy inclinados. Una vez más, todo parece indicar una catástrofe. Es muy posible que en origen los asteroides fuesen un solo planeta pequeño que giraba en el plano general. Mucho después de la formación del sistema solar, una explosión o serie de explosiones puede que fragmentara ese malhadado mundo, colocando los fragmentos en órbitas que, en muchos casos diferían grandemente del plano orbital general.

Los cometas giran en todos los planos posibles. Ahora bien, hay astrónomos que creen que muy en las afueras del sistema solar, como a un año-luz del Sol, existe una nube dispersa de cometas. Estos cometas puede que se hayan condensado a partir de las porciones más exteriores de la nube esférica original, antes de comenzar la contracción general y antes de formarse el disco ecuatorial.

En tales circunstancias, cuando de vez en cuando un cometa abandona esa capa esférica y se precipita en las regiones interiores del sistema solar (quizá como resultado de la influencia gravitatoria de estrellas lejanas), su plano de rotación alrededor del Sol puede ser cualquiera.

23. ¿En qué difiere Plutón de todoslos demás planetas?

Plutón es notable por ser el planeta más alejado del Sol (su distancia media es de 5.790 millones de kilómetros.) Claro está que alguno había de ser el más distante, y ése es precisamente Plutón.

Pero ahí no para la cosa. Plutón posee ciertas características poco usuales, que lo distinguen de los otros ocho planetas y hacen de él un objeto de notable curiosidad para los astrónomos. Por ejemplo:

1. Plutón tiene la órbita más elíptica de entre los planetas principales. Una circunferencia perfecta tiene una excentricidad de cero. La excentricidad de la órbita terrestre es de sólo 0,017, de modo que es casi circular. La de Plutón, en cambio, es de 0,25. Unas veces se halla a sólo 4.340 millones de kilómetros del Sol mientras que otras se aleja hasta 7.240. Es más, cuando Plutón alcanza su punto más próximo al Sol, se halla más cerca de éste que Neptuno, dejando así de ser, durante un rato, el planeta más alejado. En la actualidad se mueve más cerca del Sol que Neptuno y seguirá así durante unos cuarenta años.

2. Plutón tiene una órbita más inclinada que cualquiera de los planetas principales. Si alineásemos a todos los planetas (dentro de sus órbitas) a un mismo lado del Sol, todos quedarían más o menos en fila india -todos excepto Plutón-. La órbita de Plutón está inclinada 17 grados respecto a la nuestra y, por tanto, quedaría muy por encima o muy por debajo de la posición general de los demás planetas. (Por eso Plutón nunca podría chocar con Neptuno al cruzar la órbita de éste, pues la cruzaría muy por debajo.)

3. Los ocho planetas distintos de Plutón se dividen en dos grupos. Primero están los cuatro planetas cercanos al Sol: Mercurio, Venus, Tierra y Marte; todos ellos son pequeños, densos y tienen una atmósfera relativamente escasa. Luego están los cuatro planetas exteriores: Júpiter, Saturno Urano y Neptuno; planetas gigantes, de baja densidad y enormes atmósferas. Lo cual deja fuera a Plutón, que figura entre los «gigantes gaseosos», pero que es un mundo pequeño y denso como los planetas interiores. Indudablemente, está fuera de lugar.

4. Si nos olvidamos de Mercurio y Venus -cuya gran proximidad al Sol ha hecho que los efectos gravitatorios hayan disminuido su velocidad-, podemos decir que todos los planetas giran con rapidez alrededor de sus ejes. Los períodos de rotación oscilan entre diez y veinticinco horas. Plutón, sin embargo, tiene un período de rotación de 153 horas: casi siete días.

¿A qué responden todos estos extremos? ¿Hay alguna razón para que Plutón sea tan diferente?

Una conjetura especialmente interesante es la siguiente. Supongamos que Plutón no fuese antes un planeta, sino un satélite de Neptuno. Y supongamos también que una catástrofe cósmica de algún tipo lo sacara de su órbita y lo colocara en otra, planetaria e independiente.

En ese supuesto, la naturaleza de la explosión (sí es eso lo que fue) muy bien pudo lanzarlo a una órbita inclinada, que, sin embargo, sigue trayendo a Plutón una y otra vez hacia Neptuno, que es de donde había partido.

Como satélite, sería pequeño y quizá denso, en lugar de un gigante gaseoso como los verdaderos planetas exteriores. Y, por otro lado, giraría alrededor de su eje en el mismo tiempo que tardaba en girar alrededor de Neptuno, gracias a la atracción gravitatoria de éste. (Esto es cierto en general para los satélites; y es cierto, en particular, para la Luna.) En ese caso, el período de rotación de Plutón podría muy bien ser de una semana. (El período de rotación de la Luna es de cuatro semanas.) Puede que Plutón, al ser arrancado de Neptuno, conservara su período de rotación, adquiriendo así un período muy raro para un planeta.

Pero, por desgracia, todo esto no son más que especulaciones. No hay ninguna prueba sólida de que Plutón fuese antes un satélite de Neptuno; y aun si lo fuese, no sabemos qué clase de catástrofe pudo haberlo arrancado de allí.

24. ¿Por qué los cometas tienen unacola?

Los cometas han aterrorizado durante siglos a la humanidad. De cuando en cuando, y sin razón aparente, surgía uno en los cielos. Su forma era distinta de la de los demás cuerpos celestes, su contorno no era nítido sino borroso y exhibía una tenue cola que parecía manar de él. Las imaginaciones más calenturientas veían en esa cola el cabello desordenado de una mujer abatida por el dolor (la palabra «cometa» viene de otra latina que significa «cabello») y, según se decía, presagiaban desastres.

En el siglo xviii se averiguó por fin que algunos cometas seguían órbitas regulares alrededor del Sol, pero en general muy alargadas. En el extremo más remoto de su órbita resultaban invisibles, dejándose ver solamente en el más cercano, que frecuentaban una vez cada doce o cien o mil años.

El astrónomo holandés Jan H. Oort sugirió en 1950 la existencia de una gran nube de quizá miles de millones de planetoides que giraban alrededor del Sol a un año luz o más de distancia. Tendrían más de un millar de veces la distancia al Sol de Plutón, el planeta más lejano y, pese a su número, serían completamente invisibles. De cuando en cuando, debido quizá a la atracción gravitatoria de las estrellas más próximas, alguno de ellos vería frenado su movimiento orbital y comenzaría a precipitarse hacia el Sol. Y una de esas veces podría ocurrir que el planetoide penetrara con bastante profundidad en el interior del sistema solar y virase alrededor del Sol a una distancia mínima de unos cuantos millones de kilómetros. De ahí en adelante conservaría la nueva órbita y constituiría la clase de objeto que nosotros llamamos cometa.

Más o menos por entonces, el astrónomo norteamericano Fred L. Whipple conjeturó que los cometas estaban compuestos principalmente por sustancias de bajo punto de ebullición como el amoníaco y el metano, incluyendo en su interior granos de material rocoso. En esa nube de cometas, tan alejada del Sol, el amoníaco, el metano y otras sustancias estarían congelados en duro «hielo».

La estructura gélida de los cometas es estable en ese reducto exterior; pero ¿qué ocurre cuando uno de ellos decelera y se acerca al Sol? Al entrar en las regiones interiores del sistema solar, el calor cada vez mayor que recibe del Sol hace que sus hielos comiencen a evaporarse. Las partículas rocosas atrapadas en la capa de hielo superficial quedan libres. El resultado es que el núcleo del cometa queda rodeado por una nube de polvo y vapor, que se espesa a medida que se acerca al Sol.

Por otro lado tenemos el viento solar, que es una nube de partículas subatómicas que emerge del Sol en todas las direcciones. El viento solar ejerce una fuerza que es superior a la diminuta atracción gravitatoria del cometa. Por tanto, ese viento solar empujará a la nube de polvo y vapor del cometa, alejándola del Sol. A medida que el cometa se aproxima al Sol, el viento solar arrecia y esa nube de polvo y vapor se estira en una larga cola que huye del Sol. La cola es tanto más larga cuanto más próximo se halle el cometa al Sol; pero lo cierto es que siempre está compuesta de materia muy dispersa.

Claro está que los cometas no duran mucho una vez que entran en las entrañas del sistema solar. Cada pasada por las proximidades del Sol ocasiona una pérdida de material, y al cabo de unas cuantas docenas de vueltas el cometa queda reducido a un diminuto núcleo rocoso o se desintegra del todo en una nube de pequeños meteoros. Alrededor del Sol giran, efectivamente, en órbitas regulares, una serie de «corrientes de meteoros», y cuando alguna de ellas intersecta la atmósfera terrestre, se produce un vistoso despliegue de estrellas errantes. Sin duda, los restos de cometas muertos.

25. ¿Por qué Ia Luna muestrasiempre la misma cara hacia la

Tierra?

La atracción gravitatoria de la Luna sobre la Tierra hace subir el nivel del océano a ambos lados de nuestro planeta y crea así dos abultamientos. A medida que la Tierra gira de oeste a este, estos dos bultos -de los cuales uno mira siempre hacia la Luna y el otro en dirección contraria- se desplazan de este a oeste alrededor de la Tierra.

Al efectuar este desplazamiento, los dos bultos rozan contra el fondo de los mares poco profundos como el de Bering o el de Irlanda. Tal rozamiento convierte energía de rotación en calor, y este consumo de la energía de rotación terrestre hace que el movimiento de rotación de la Tierra alrededor de su eje vaya disminuyendo poco a poco. Las marcas actúan como un freno sobre la rotación de la Tierra, y como consecuencia de ello los días terrestres se van alargando un segundo cada mil años.

Pero no es sólo el agua del océano lo que sube de nivel en respuesta a la gravedad lunar. La corteza sólida de la Tierra también acusa el efecto, aunque en medida menos notable. El resultado son dos pequeños abultamientos rocosos que van girando alrededor de la Tierra, el uno mirando hacia la Luna y el otro en la cara opuesta de nuestro planeta. Durante este desplazamiento, el rozamiento de una capa rocosa contra otra va minando también la energía de rotación terrestre. (Los bultos, claro está, no se mueven físicamente alrededor del planeta, sino que, a medida que el planeta gira, remiten en un lugar y se forman en otro, según qué porciones de la superficie pasen por debajo de la Luna.)

La Luna no tiene mares ni mareas en el sentido corriente. Sin embargo, la corteza sólida de la Luna acusa la fuerza gravitatoria de la Tierra, y no hay que olvidar que ésta es ochenta veces más grande que la de la Luna. El abultamiento provocado en la superficie lunar es mucho mayor que el de la superficie terrestre. Por tanto, si la Luna rotase en un período de veinticuatro horas, estaría sometida a un rozamiento muchísimo mayor que la Tierra. Además, como nuestro satélite tiene una masa mucho menor que la Tierra, su energía total de rotación sería ya de entrada, para períodos de rotación iguales, mucho menor.

Así, pues, la Luna, con una reserva inicial de energía muy pequeña, socavada rápidamente por los grandes bultos provocados por la Tierra, tuvo que sufrir una disminución relativamente rápida de su período de rotación. Hace seguramente muchos millones de años debió de decelerarse hasta el punto de que el día lunar se igualó con el mes lunar. De ahí en adelante, la Luna siempre mostraría la misma cara hacia la Tierra.

Esto, a su vez, congela los abultamientos en una posición fija. Uno de ellos mira hacía la Tierra desde el centro mismo de la cara lunar que nosotros vemos, mientras que el otro apunta en la dirección contraria desde el centro mismo de la cara que no vemos. Puesto que las dos caras no cambian de posición a medida que la Luna gira alrededor de la Tierra, los bultos no experimentan ningún nuevo cambio ni tampoco se produce rozamiento alguno que altere el período de rotación del satélite. La Luna continuará mostrándonos la misma cara indefinidamente; lo cual, como veis, no es ninguna coincidencia, sino consecuencia inevitable de la gravitación y del rozamiento.

La Luna es un caso relativamente simple. En ciertas condiciones, el rozamiento debido a las mareas puede dar lugar a condiciones de estabilidad más complicadas. Durante unos ochenta años, por ejemplo, se pensó que Mercurio (el planeta más cercano al Sol y el más afectado por la gravedad solar) ofrecía siempre la misma cara al Sol, por el mismo motivo que la Luna ofrece siempre la misma cara a la Tierra. Pero se ha comprobado que, en el caso de Mercurio, los efectos del rozamiento producen un período estable de rotación de 58 días, que es justamente dos tercios de los 88 días que constituyen el período de revolución de Mercurio alrededor del Sol.

26. ¿Qué son esas concentraciones demasa que se han descubierto en la

Luna?

La ley de Newton de la gravitación universal admite una fórmula muy simple, siempre que se suponga que todos los objetos del universo tienen concentrada la masa en un solo punto. Si los objetos están muy alejados, podemos sentar esa hipótesis; pero cuanto más cerca estén unos de otros, tanto más habremos de tener en cuenta que su masa está, en realidad, distribuida por todo el cuerpo.

Con todo, el tratamiento sigue siendo muy sencillo, siempre que, primero, el objeto sea una esfera perfecta, y, segundo, su densidad sea radialmente simétrica.

Al decir que la densidad es «radialmente simétrica» debemos entender que si el objeto es muy denso en el centro y cada vez menos hacia la superficie, la manera en que la densidad decrece es exactamente la misma cualquiera que sea la dirección en que nos movamos a partir del centro. Da igual que haya cambios bruscos de densidad, siempre que esos cambios sean exactamente iguales en todas las direcciones a partir del centro.

Los objetos astronómicos más o menos grandes cumplen aproximadamente esos requisitos. Por lo general, son de forma casi esférica y su densidad exhibe una simetría casi radial. Claro está que cuando se trata de objetos muy próximos entre sí hay que contar con pequeñas desviaciones. Al estudiar los efectos gravitatorios entre la Luna y la Tierra hay que tener en cuenta que la Tierra no es una esfera perfecta, sino que presenta un abultamiento en el ecuador. El exceso de materia en el abultamiento produce un diminuto efecto gravitatorio que requiere especial atención.

Durante los años sesenta, los Estados Unidos pusieron en órbita alrededor de la Luna varios vehículos espaciales (los «Lunar Orbíters»). Conociendo como conocían el tamaño y la forma de la Luna con todo detalle, los expertos en cohetes estaban seguros de poder calcular con toda exactitud el tiempo que tardarían los vehículos en circundar el satélite. Pero cuál no sería su sorpresa cuando comprobaron que los vehículos se movían un poquitín demasiado aprisa en ciertas partes de la órbita.

Se observaron las órbitas con todo detalle y resultó que los vehículos se aceleraban ligeramente al pasar sobre los grandes mares lunares, que son regiones llanas con pocos cráteres. Esto sólo se podía deber a que la densidad de la Luna no tuviese una simetría perfectamente radial. En dichos mares tenía que haber una concentración adicional de masa que producía efectos gravitatorios no tenidos en cuenta. Los astrónomos empezaron a hablar de «concentraciones de masa» o, en forma abreviada, «mascones».

¿Qué son estas concentraciones de masa?

Dos son las teorías propuestas. Algunos astrónomos piensan que los mares lunares son cráteres supergigantes producidos por la colisión de meteoritos gigantescos con la Luna. Estos meteoritos puede que se enterraran bajo la superficie de los mares y que aún estén allí. Quizá estén compuestos de hierro en su mayor parte y sean, por tanto, mucho más densos que la superficie normal de la Luna. Constituirían, pues, una concentración de masa anormalmente alta.

Otra teoría es que, a lo largo de la historia de la Luna, los mares lunares fuesen realmente mares de agua. Antes de que el agua se evaporase al espacio, se habrían depositado densos sedimentos, explicando así ese exceso de masa.

Las futuras exploraciones de la superficie lunar deberían determinar cuál de esas teorías es la correcta (o si no lo es ninguna de las dos), lo cual podría a su vez revelarnos mucho más acerca de la historia de la Luna (y también de la Tierra).

27. Ahora que ya hemos aIunizadoseis veces en nuestro satélite, ¿qué

hemos averiguado acerca de él?

En cierto modo es injusto esperar demasiado de las exploraciones lunares, si tenemos en cuenta los límites de lo que se ha hecho. Al fin y al cabo, no se ha hecho otra cosa que recoger algún que otro material de la superficie en seis lugares muy separados y dentro de un área total equivalente a América del Norte y América del Sur juntas. Pudiera muy bien ser que en cualquiera de estos alunizajes los astronautas no hayan estado ni a cinco kilómetros de alguna clave para descifrar los enigmas lunares, sin que ellos lo supiesen.

Por otra parte, los astrónomos y los geólogos no han hecho sino comenzar su misión. El estudio de las rocas lunares proseguirá durante años. El proceso puede ser útil, porque algunas de las rocas tienen unos 4.000 millones de años y son, por tanto, reliquias de los primeros mil millones de años de existencia del sistema solar. jamás se ha encontrado en la Tierra nada que se remonte, inmutable, a un período tan remoto.

De entre las cosas que nos ha revelado la investigación de la constitución química de la superficie lunar, la más clara quizá sea que la distribución de elementos es muy diferente de la de la Tierra. Comparadas con la Tierra, en las rocas de la superficie lunar escasean aquellos elementos que tienden a formar compuestos de bajo punto de fusión: hidrógeno, carbono, sodio, plomo, etc. Los elementos que forman compuestos de alto punto de fusión (zirconio, titanio y las tierras raras) se encuentran en mayor porcentaje en la corteza lunar que en la terrestre.

Una explicación lógica sería suponer que la superficie lunar sufrió en otro tiempo un calentamiento lo bastante fuerte y continuado como para evaporar y echar a perder los compuestos de bajo punto de fusión, dejando atrás los de alto punto de fusión. Esta conclusión viene además apoyada por el hecho de que, al parecer, hay una proporción muy alta de materiales vítreos en la Luna, como si gran parte de la superficie se hubiese fundido y solidificado después.

Pero ¿qué es lo que originó ese calor? Pues, por ejemplo, el impacto de grandes meteoritos a lo largo de la historia remota de la Luna, o bien, gigantescas erupciones volcánicas. En ese caso, el efecto aparecería en ciertas zonas y no en otras. Sin embargo, hasta ahora las pruebas parecen indicar que dicho efecto se extiende a toda la Luna.

Acaso viniese ocasionado por un largo período de sobrecalentamiento del Sol. En ese caso, la Tierra también habría estado inmersa en un calor parecido. Aunque la Tierra está protegida por el aire y los océanos, mientras que la Luna no, podrían existir pruebas en nuestro planeta de ese cálido período. No se ha encontrado ninguna, pero quizá sea porque en la Tierra no hay rocas que hayan subsistido, sin modificación alguna, desde aquellos primeros mil millones de años del sistema solar.

Una tercera posibilidad es que la Luna estuviese en otro tiempo mucho más cerca del Sol. Quizá fuese en origen un planeta independiente, con una órbita alargada que, en uno de los extremos, la aproximara al Sol hasta una distancia parecida a la de Mercurio hoy día. En ese supuesto podemos estar seguros de que su superficie estaba completamente cocida por el Sol.

Puede que el otro extremo de la órbita llevara a la Luna bastante cerca de la órbita terrestre y que en un momento dado -quizá no más de mil millones de años atrás- la Tierra lograra capturarla, convirtiendo así en satélite lo que antes fuera planeta.

Sea cual fuere la causa, lo cierto es que esa agostada superficie lunar es descorazonadora en un aspecto, pues alimenta la posibilidad de que no haya agua en toda la superficie, lo cual significa que el trabajo de establecer una colonia en la Luna es mucho más difícil que en otras circunstancias.

28. ¿Hay vida en Marte?

En realidad, no lo sabemos todavía. Y quizá no lo sepamos hasta el día en que amarticen allí los científicos e investiguen.

Pero, a juzgar por lo que sabemos hoy día, parece probable que exista vida en Marte. Cierto es que la sonda Mariner IX, colocada en órbita a unas mil millas sobre la superficie de Marte, no observó ningún signo de vida, pese a que rastreó todo el planeta. Pero la Tierra, vista desde la misma distancia y con los mismos métodos, tampoco revelaría ningún signo de vida.

La atmósfera de Marte está muy enrarecida, es cien veces menos densa que la de la Tierra, y lo poco que hay es casi todo ello anhídrido carbónico. Por otra parte, Marte dista del Sol vez y media más que la Tierra, de modo que de noche la temperatura alcanza cifras antárticas y en las regiones polares hace suficiente frío para congelar el anhídrido carbónico.

El hombre no podría sobrevivir en ese medio sin una protección especial. Ni, para el caso, ningún animal terrestre. Los colonizadores de Marte (colonizadores terrestres, se entiende) tendrían que vivir en cúpulas o en cavernas subterráneas. Pero ¿quiere eso decir que en Marte no puedan existir formas complejas de vida, adaptadas a las condiciones de este planeta? Puede que las posibilidades sean escasas, pero tampoco podemos eliminarlas del todo.

¿Qué decir, por ejemplo, de formas de vida muy simples, plantas del tipo de los líquenes o microorganismos parecidos a las bacterias? Aquí las posibilidades son ya mejores, incluso bastante buenas, diríamos.

Admitimos de entrada que también había esperanzas de que en la Luna existiesen formas simples de vida y que todo ello quedó luego en nada. Pero es que Marte ofrece un medio ambiente mucho más favorable que la Luna. Marte está mucho más lejos del Sol y tiene una atmósfera que ofrece cierta protección, de modo que está mucho menos sometido a la radiación del Sol, que rompería las complejas moléculas necesarias para la vida.

Además, al ser Marte más frío y más grande que la Luna, es también capaz de retener las sustancias volátiles que sirven como puntos de arranque fundamentales para la vida. Marte es rico en anhídrido carbónico y, sin duda alguna, tiene agua. A partir de ahí puede formarse la vida. Si, como se ha comprobado, ciertas formas de vida terrestre sumamente simple son capaces de sobrevivir en condiciones marcianas simuladas, tanto más cierto será esto en el caso de formas de vida adaptadas desde el principio a las condiciones de ese planeta.

Las fotografías tomadas por el Mariner IX demuestran que las condiciones en Marte no tienen por qué ser tan rigurosas como las que imperan hoy día. Hay regiones volcánicas, así como un volcán gigante, el Nix Olympica, que es dos veces más ancho que cualquier volcán de la Tierra. Lo cual significa que Marte es un mundo geológicamente activo, capaz de experimentar cambios.

La faz de Marte muestra además marcas ondulantes que tienen todo el aspecto de cauces fluviales y cuyas características, según conjeturas de algunos astrónomos, demuestran que no hace mucho (geológicamente hablando) llevaban todavía agua. Es más, los casquetes polares de Marte parecen pasar por períodos alternados de crecimiento y recesión.

Es posible que Marte alterne entre una especie de largo invierno, durante el cual se hiela casi toda la atmósfera y el resto está muy rarificada (como ocurre en la actualidad), y una especie de largo verano, durante el cual casi toda la atmósfera se derrite y adquiere una densidad parecida a la de la Tierra.

Así, pues, es posible que en el suelo marciano yazgan latentes ciertas formas de vida y que, cuando llegue el verano y la atmósfera se espese y el, agua corra, la vida florezca en mayor medida de lo que cabría hoy esperar.

29. Supongamos que hay vida enMarte. ¿Merece realmente la pena ir

hasta allí sólo para verla?

Los científicos no dudarían ni un momento en contestar con un fortísimo «¡sí!».

Todas las formas de vida terrestre, sin excepción, están basadas en las grandes moléculas de proteínas y ácidos nucleicos. Todas utilizan la misma clase de reacciones químicas, mediadas por la misma especie de enzimas. Toda la vida terrestre consiste en variaciones sobre el mismo tema.

Si hay vida en Marte, por muy simple que sea, puede que exista como variaciones sobre un tema muy distinto. De golpe y porrazo doblaríamos el número de tipos de vida conocidos y quizá adquiriríamos inmediatamente una compresión más básica de la naturaleza de la vida.

Y aun si la vida en Marte resulta estar basada en el mismo tema que el de la Tierra, puede ser que haya interesantes diferencias de detalle. Por ejemplo, todas las moléculas de proteína de la Tierra están construidas de aminoácidos, los cuales (salvo uno) admiten, o bien una orientación derecha, o bien una orientación izquierda. En cualesquiera condiciones en que no esté involucrada la vida, los dos tipos son igual de estables y existen en cantidades iguales.

En las proteínas terrestres, sin embargo, todos los aminoácidos, con excepciones rarísimas e insignificantes, son de orientación izquierda. Esto permite la construcción de proteínas en pilas perfectas, lo cual sería imposible si unas fuesen derechas y otras izquierdas (aunque las pilas serían igual de perfectas si todas fuesen derechas).

Entonces, ¿por qué izquierda sí y derecha no? ¿Es cuestión de pura casualidad? ¿Será que el primer brote de vida en la Tierra resultó ser izquierdo? ¿O es que hay en la naturaleza alguna asimetría básica que hace inevitable la forma izquierda? La vida marciana podría contestar a esta pregunta y otras parecidas.

Aun si la vida marciana resultara estar basada en el mismo tema que la vida terrestre y fuese idéntica en todos los detalles, valdría la pena saberlo. Pues ese hecho podría ser una interesante prueba de que el tema de la vida, tal como existe en la Tierra, quizá sea el único posible en cualquier planeta, siquiera remotamente parecido a la Tierra.

Además, aunque la vida en Marte fuese un calco de la vida terrestre desde el punto de vista bioquímico, cabría aún la posibilidad de que aquélla estuviese constituida por sistemas moleculares más primitivos que los que se han desarrollado a lo largo de miles de millones de años en el ambiente mucho más prolífico y suave de la Tierra. Marte sería entonces un laboratorio en el que podríamos observar la protovida tal como (quizá) existió antes en la Tierra. Incluso podríamos experimentar con ella -cosa que sólo podríamos hacer aquí si tuviéramos una máquina del tiempo- y buscar ciertas verdades fundamentales que se hallan ocultas en las complejidades de la vida terrestre.

Y aunque no existiese vida alguna en Marte, podrían existir moléculas orgánicas que, sin ser materia viviente, estuvieran en camino hacia la vida, por así decirlo. De este modo podrían indicar la naturaleza del camino antaño seguido en la Tierra durante el período de «evolución química», previo al desarrollo del primer sistema lo bastante complejo para merecer el calificativo de viviente.

En resumen: aprendamos lo que aprendamos en Marte sobre la vida, es muy probable que nos ayude a comprender mejor la vida terrestre (igual que el estudio del latín y del francés nos ayuda a entender mejor el inglés). Y qué duda cabe que el ir a Marte para aprender algo sobre la Tierra que aquí no podemos aprender, es razón más que suficiente para hacerlo, si es que se puede.

30. ¿Cómo y cuándo se formaron losocéanos?

A principios del siglo xx se pensaba que la Tierra y los demás planetas estaban formados de materia arrancada del Sol. Y circulaba la imagen de una Tierra en gradual proceso de enfriamiento, desde la incandescencia hasta el rojo vivo, para pasar luego a un calor moderado y finalmente al punto de ebullición del agua. Una vez enfriada lo bastante para que el agua se condensase, el vapor de agua de la atmósfera caliente de la Tierra pasó a estado líquido y empezó a llover, y llover, y llover. Al cabo de muchos años de esta increíble lluvia de agua hirviendo que saltaba y bramaba al golpear el suelo caliente, las cuencas de la accidentada superficie del planeta acabaron por enfriarse lo bastante como para retener el agua, llenarse y constituir así los océanos.

Muy espectacular…, pero absolutamente falso, podríamos casi asegurar.

Hoy día, los científicos están convencidos de que la Tierra y demás planetas no se formaron a partir del Sol, sino a partir de partículas que se conglomeraron hacia la misma época en que el Sol estaba gestándose. La Tierra nunca estuvo a la temperatura del Sol, pero adquirió bastante calor gracias a la energía de colisión de todas las partículas que la formaron. Tanto, que su masa, relativamente pequeña, no era capaz en un principio de retener una atmósfera ni el vapor de agua.

O lo que es lo mismo, el cuerpo sólido de esta Tierra recién formada no tenía ni atmósfera ni océanos. ¿De dónde vinieron entonces?

Desde luego había agua (y gases) combinada débilmente con las sustancias rocosas que constituían la porción sólida del globo. A medida que esa porción sólida se fue empaquetando de forma cada vez más compacta bajo el tirón de la gravedad, el interior se fue haciendo cada vez más caliente. Los gases y el vapor de agua se vieron expulsados de esa su anterior combinación con la roca y abandonaron la sustancia sólida.

Las pompas gaseosas, al formarse y agruparse, conmocionaron a la joven Tierra con enormes cataclismos, mientras que el calor liberado provocaba violentas erupciones volcánicas. Durante muchísimos años no cayó ni una gota de agua líquida del cielo; era más bien vapor de agua, que salía silbando de la corteza, para luego condensarse. Los océanos se formaron desde arriba, no desde abajo.

En lo que los geólogos no están de acuerdo hoy día es en la velocidad de formación de los océanos. ¿Salió todo el vapor de agua en cosa de mil millones de años, de suerte que el océano tiene el tamaño actual desde que comenzó la vida? ¿O se trata de un proceso lento en el que el océano ha ido creciendo a través de las eras geológicas y sigue creciendo aún?

Quienes mantienen que el océano se formó en los comienzos mismos del juego y que ha conservado un tamaño constante desde entonces, señalan que los continentes parecen ser un rasgo permanente de la Tierra. No parece que fuesen mucho más grandes en tiempos pasados, cuando era el océano supuestamente mucho más pequeño.

Por otra parte, quienes opinan que el océano ha venido creciendo constantemente, señalan que las erupciones volcánicas escupen aún hoy cantidades ingentes de vapor de agua al aire: vapor de agua de rocas profundas, no del océano. Además, en el Pacífico hay montañas submarinas cuyas cimas, planas, quizá estuviesen antes al nivel del mar, pero ahora quedan a cientos de pies por debajo de él.

Acaso sea posible llegar a un compromiso. Se ha sugerido que aunque el océano ha ido efectivamente creciendo continuamente, el peso del agua acumulada hizo que el fondo marino cediera. Es decir, los océanos han crecido constantemente en profundidad, no en anchura. Lo cual explicaría la presencia de esas mesetas marinas sumergidas y también la existencia de los continentes.

31. Los océanos ¿se están haciendomás salados? ¿Se harán algún día tan

salados que maten toda la vida?

En la Tierra existe un ciclo del agua. Cada año se evaporan unos 125.000 kilómetros cúbicos de agua del océano, que luego caen en forma de lluvia y vuelven, de un modo u otro, al océano.

El equilibrio entre las dos ramas del ciclo -evaporación y vuelta al océano- no es perfecto. De todo el contenido del océano, sólo se evapora el agua propiamente dicha, de modo que la lluvia es agua casi pura. Pero, al volver a la Tierra, parte de esa agua cae primero sobre tierra firme, se filtra en el suelo y recoge una serie de productos químicos solubles que transporta consigo hasta el océano. El agua de los ríos, por ejemplo, es sal en un 1/100 de 1 por 100: no lo suficiente para dejar de ser insípida, pero sí para ser importante.

Parece, pues, que el océano está recibiendo constantemente trazas de sales y otros productos químicos de la Tierra, sin perder ni un ápice de ellos durante la evaporación. Hay que pensar, por tanto, que el océano se hace cada vez más salino; muy despacio, claro está, pero al cabo de millones y millones de años de tiempo geológico la sal tendría que alcanzar concentraciones enormes. Hoy día, las aguas del océano contienen un 3,5 por 100 de materiales disueltos, que en su mayor parte son sal común.

El agua de los ríos vierte también sus sales en algunos lagos interiores que no están conectados con el mar, acumulándose allí los materiales disueltos igual que en el océano. Si el lago está situado en una región cálida y su velocidad media de evaporación es mayor que la del océano, los materiales disueltos se acumulan con mayor rapidez y el lago puede llegar a tener una salinidad mucho mayor que la del océano. El mar Muerto, en la raya entre Israel y Jordania, tiene un 25 por 100 de materiales disueltos. Es tan salado, que no hay nada capaz de vivir en sus aguas.

El océano ¿está abocado también a un fin tan lúgubre?

Podría ser, si no fuera porque hay procesos que tienden a reducir el contenido salino del océano. Las tormentas, por ejemplo, arrastran consigo tierra adentro la espuma de las olas y distribuyen sobre el continente las sales disueltas.

Pero hay un factor que opera a una escala mucho más importante, y es que ciertas combinaciones de sustancias disueltas, en concentraciones suficientes, se unen en compuestos insolubles que van a parar al fondo del mar. Y, por otro lado, hay sustancias que son absorbidas por las células de los organismos marinos.

El balance final es que el océano es mucho menos rico en sustancias disueltas de lo que debería ser si calculamos todo el material que han tenido que aportar los ríos a lo largo de los últimos miles de millones de años. Por otra parte, el fondo del océano es muy rico en sustancias que tienen que haber venido de la tierra. Por todo el suelo marino hay grandes cantidades de metales en forma de nódulos.

Andando el tiempo, puede también que una porción poco profunda del océano quede acorralada por tierras que suben de nivel. Estas porciones de océano se van evaporando poco a poco, dejando atrás grandes cantidades de materiales disueltos, que regresan así a la tierra. Las minas de sal, de las que se pueden extraer grandes cantidades de este compuesto y volúmenes menores de otras sustancias, son los restos de esas porciones de océano desecadas.

¿Cuál es entonces el resultado global? A la larga, ¿aumenta ligeramente la salinidad del océano? ¿0 en realidad se está haciendo menos salado? ¿Vira unas veces en una dirección y otras en la contraria, conservando por término medio un equilibrio? Los geólogos en realidad no lo saben.

32. ¿Hay de verdad oro en elocéano?

Sí, claro. ¿Por qué no lo va a haber?

El agua de lluvia corre y se filtra constantemente por las tierras resecas en su camino de vuelta hacia el océano, y al hacerlo disuelve un poco de todos los materiales que empapa y atraviesa. Al final es poca la cantidad disuelta y además hay sustancias que son menos solubles que otras. A lo cual hay que añadir que algunas, después de llegar al océano, se hunden hasta el fondo del mar.

Sin embargo, al cabo de los miles y miles de millones de años que lleva existiendo el océano es tanta la cantidad de materiales disueltos que se han vertido en el agua, que verdaderamente hay grandes cantidades de cada elemento en los compuestos mezclados con las moléculas de agua del mar.

Aproximadamente un 3,25 por 100 del mar es materia sólida disuelta; y en total, contando todo, hay 330.000.000 millas cúbicas (1,4 ´ 1018 metros cúbicos) de agua marina, que pesan aproximadamente 1,5 trillones de toneladas. Si separáramos del agua del mar todas las materias sólidas, obtendríamos un peso total de 50.000 billones (50.000.000.000.000.000) de toneladas. Claro está que más de las tres cuartas partes de la materia sólida es sal ordinaria, pero en el cuarto restante hay un poco de todo.

Por ejemplo, hay suficientes compuestos de magnesio para dar un total de 1.900.000.000.000.000 (1.900 billones) de toneladas de ese metal. Con esta reserva oceánica tendríamos para mucho tiempo, sobre todo porque lo que extrajésemos y usásemos iría a parar de nuevo, en último término, al océano.

Pero ocurre que el magnesio no está repartido de manera discontinua, con ricas bolsas aquí y allá (como sucede con los minerales terrestres). El hecho de que esté repartido uniformemente por todo el océano significa que, aun trabajando con un rendimiento perfecto, tendríamos que extraer magnesio de 950 litros de agua marina para obtener un kilo. Hoy día hay ya métodos para hacerlo económicamente, pudiendo obtenerse magnesio en cantidades cualesquiera de manera rentable.

Otro elemento que se halla presente en el agua marina en cantidades grandes es el bromo (un pariente del cloro, pero menos común). El mar contiene compuestos disueltos que arrojarían un total de 100 billones (100.000.000.000.000) de toneladas de bromo. Equivale aproximadamente a un veinteavo de la reserva de magnesio, con lo cual habría que despojar de su contenido a una cantidad de agua veinte veces mayor -unos 19.000 litros, con rendimiento perfecto- para obtener un kilo de bromo. También en este caso se puede trabajar con rentabilidad, y de hecho el mar es uno de los principales proveedores de bromo del mundo.

Un tercer pariente del cloro y del bromo es el yodo. A escala mundial escasea más que ellos, y en el océano se halla presente en cantidades mil veces menores que el bromo. El total asciende a 86.000 millones de toneladas, lo cual suena a mucho, pero equivale sólo a un kilo por cada 20 millones de litros de agua. Es demasiado poco para que su extracción directa resulte rentable, pero, por suerte, las algas marinas se encargan de extraer el yodo por nosotros y sus cenizas proporcionan cantidades importantes de este elemento.

Lo cual nos lleva al oro. La cantidad total de oro que hay en el agua del mar oscila entre los 6 y los 12 millones de toneladas. Si hubiese dado esta cifra al principio del artículo, habría sonado a muchísimo. ¡Por lo menos 6 millones de toneladas! ¡Qué barbaridad!

Pero a estas alturas veréis que no es mucho. Para extraer un solo kilo de oro habría que escudriñar de 130 a 270 mil millones de litros, lo cual costaría mucho más que un kilo de oro. Así que el oro se deja en el océano.

33. ¿Qué ocurriría si se derritieranlos casquetes glaciares?

La superficie de tierra firme de nuestro planeta soporta una carga de unos 38 millones de kilómetros cúbicos de hielo (de los cuales, un 85 por 100 está en el continente de la Antártida). Como el agua es algo más densa que el hielo, esos 38 millones, al derretirse, se quedarían en unos 33 millones de kilómetros cúbicos de agua.

Está claro que si el hielo se derritiese, toda el agua, o casi toda, iría a parar al océano. El océano tiene una superficie total de 360 millones de kilómetros cuadrados, Si dicha superficie permaneciera constante y los 33 millones de kilómetros cúbicos de hielo fundido se esparcieran uniformemente por toda su extensión alcanzaría una altura de 33/360 ó 0,092 kilómetros. Es decir, la capa de hielo fundido tendría un espesor de 92 metros.

Pero lo cierto es que la extensión superficial del océano no permanecería constante, porque, de subir su nivel, se comería unos cinco millones de kilómetros cuadrados de las tierras bajas que hoy día festonean sus orillas. Lo cual significa que la superficie del océano aumentaría y que la capa de ese nuevo aporte de agua no sería tan gruesa como acabamos de suponer, aparte de que el peso adicional de agua haría ceder un poco el fondo del mar. Aun así, el nivel subiría probablemente unos 60 metros, lo bastante como para alcanzar la vigésima planta del Empire State Building y anegar buena parte de las zonas más pobladas de la Tierra.

La cantidad de hielos terrestres ha variado mucho a lo largo de la historia geológica de la Tierra. En el apogeo de un período glacial avanzan, gigantescos, los glaciares sobre millones de kilómetros cuadrados de tierra, y el nivel del océano baja hasta el punto de dejar al aire libre las plataformas continentales.

En cambio, cuando la carga de hielo es prácticamente nula, como sucedió durante decenas de millones de años, el nivel del océano es alto y pequeña la superficie continental.

Ninguna de las dos situaciones tiene por qué ser catastrófica. En pleno período glacial, los hielos cubren millones de kilómetros cuadrados de tierra, que quedan así inhabilitados para la vida terrestre. Pero, en cambio, salen a la luz millones de kilómetros cuadrados de plataforma continental, con posibilidad de ser habitados.

Si, por el contrario, se derrite el hielo, el agua anegará millones de kilómetros cuadrados, que quedan así inservibles para la vida terrestre. Pero en ausencia de hielo y con áreas terrestres más pequeñas, el clima será ahora más benigno y habrá pocos desiertos, por lo cual será mayor el porcentaje de tierras habitables. Y como la variación en el volumen total del océano es relativamente pequeña (6 ó 7 por 100 como máximo), la vida marina no se verá afectada demasiado.

Si el cambio de nivel durase miles y miles de años, como siempre ha sido en el pasado, no habría dificultad para afrontarlo. Pero el problema es que la tecnología humana está vertiendo polvo y anhídrido carbónico en el aire. El polvo intercepta la radiación solar y enfría la Tierra, mientras que el anhídrido carbónico atrapa el calor y la calienta. Si uno de los efectos llega a predominar en el futuro sobre el otro, la temperatura de la Tierra quizá suba o baje con relativa rapidez. Y en cosa de cien años puede que los hielos se derritan o que se formen glaciares continentales.

Lo catastrófico no será tanto el cambio en sí como la velocidad del cambio.

34. ¿De dónde vino el aire querespiramos?

La opinión de los astrónomos es que los planetas nacieron de torbellinos de gas y polvo, constituidos en general por los diversos elementos presentes, en proporciones correspondientes a su abundancia cósmica. Un 90 por 100 de los átomos eran hidrógeno y otro 9 por 100 helio. El resto incluía todos los demás elementos, principalmente neón, oxígeno, carbono, nitrógeno, carbón, azufre, silicio, magnesio, hierro y aluminio.

El globo sólido de la Tierra en sí nació de una mezcla rocosa de silicatos y sulfuros de magnesio, hierro y aluminio, cuyas moléculas se mantenían firmemente unidas por fuerzas químicas. El exceso de hierro fue hundiéndose lentamente a través de la roca y formó un núcleo metálico incandescente.

Durante este proceso de aglomeración, la materia sólida de la Tierra atrapó una serie de materiales gaseosos y los retuvo en los vanos que quedaban entre las partículas sólidas o bien mediante uniones químicas débiles Estos gases contendrían seguramente átomos de helio, neón y argón, que no se combinaron con nada; y átomos de hidrógeno, que o bien se combinaron entre sí por parejas para formar moléculas de hidrógeno (H2), o bien se combinaron con otros átomos: con oxígeno para formar agua (H2O), con nitrógeno para formar amoníaco (NH3) o con carbono para formar metano (CH4).

A medida que el material de este planeta en ciernes se fue apelotonando, el efecto opresor de la presión y el aún más violento de la acción volcánica fueron expulsando los gases. Las moléculas de hidrógeno y los átomos de helio y neón, al ser demasiado ligeros para ser retenidos, escaparon rápidamente.

La atmósfera de la Tierra quedó constituida por lo que quedaba: vapor de agua, amoníaco, metano y algo de argón. La mayor parte del vapor de agua, pero no todo, se condensó y formó un océano.

Tal es, en la actualidad, la clase de atmósfera que poseen algunos planetas como Júpiter y Saturno, los cuales, sin embargo, son bastante grandes para retener hidrógeno, helio y neón.

Por su parte, la atmósfera de los planetas interiores comenzó a evolucionar químicamente. Los rayos ultravioletas del cercano Sol rompieron las moléculas de vapor de agua en hidrógeno y oxígeno. El hidrógeno escapó, pero el oxígeno fue acumulándose y combinándose con amoníaco y metano. Con el primero formó nitrógeno y agua; con el segundo, anhídrido carbónico y agua. Poco a poco, la atmósfera de los planetas interiores pasó de ser una mezcla de amoníaco y metano a una mezcla de nitrógeno y anhídrido carbónico. Marte y Venus tienen hoy día atmósferas compuestas por nitrógeno y anhídrido carbónico, mientras que la Tierra debió de tener una parecida hace miles de millones de años, cuando empezó a surgir la vida.

Esa atmósfera es además estable. Una vez formada, la ulterior acción de los rayos ultravioletas sobre el vapor de agua hace que se vaya acumulando oxígeno libre (moléculas formadas por dos átomos de oxígeno, O2). Una acción ultravioleta aún más intensa transforma ese oxígeno en ozono (con tres átomos de oxígeno por molécula, O3). El ozono absorbe la radiación ultravioleta y actúa de barrera. La radiación ultravioleta que logra atravesar la capa de ozono en la alta atmósfera y romper las moléculas de agua más abajo es muy escasa, con lo cual se detiene la evolución química de la atmósfera…, al menos hasta que aparezca algo nuevo.

Pues bien, en la Tierra apareció de hecho algo nuevo. Fue el desarrollo de un grupo de formas de vida capaces de utilizar la luz visible para romper las moléculas de agua. Como la capa de ozono no intercepta la luz visible, ese proceso (la fotosíntesis) podía proseguir indefinidamente. A través de la fotosíntesis se consumía anhídrido carbónico y se liberaba oxígeno. Así, pues, hace 500 millones de años, la atmósfera empezó a convertirse en una mezcla de nitrógeno y oxígeno, que es la que existe hoy.

35. ¿Qué es el efecto«invernadero»?

Cuando decimos que un objeto es «transparente» porque podemos ver a través de él, no queremos necesariamente decir que lo puedan atravesar todos los tipos de luz. A través de un cristal rojo, por ejemplo, se puede ver, siendo, por tanto, transparente. Pero, en cambio, la luz azul no lo atraviesa. El vidrio ordinario es transparente para todos los colores de la luz, pero muy poco para la radiación ultravioleta y la infrarroja.

Pensad ahora en una casa de cristal al aire libre y a pleno sol. La luz visible del Sol atraviesa sin más el vidrio y es absorbida por los objetos que se hallen dentro de la casa. Como resultado de ello, dichos objetos se calientan, igual que se calientan los que están fuera, expuestos a la luz directa del Sol.

Los objetos calentados por la luz solar ceden de nuevo ese calor en forma de radiación. Pero como no están a la temperatura del Sol, no emiten luz visible, sino radiación infrarroja, que es mucho menos energética. Al cabo de un tiempo, ceden igual cantidad de energía en forma de infrarrojos que la que absorben en forma de luz solar, por lo cual su temperatura permanece constante (aunque, naturalmente, están más calientes que si no estuviesen expuestos a la acción directa del Sol).

Los objetos al aire libre no tienen dificultad alguna para deshacerse de la radiación infrarroja, pero el caso es muy distinto para los objetos situados al sol dentro de la casa de cristal. Sólo una parte pequeña de la radiación infrarroja que emiten logra traspasar el cristal. El resto se refleja en las paredes y va acumulándose en el interior. La temperatura de los objetos interiores sube mucho más que la de los exteriores. Y la temperatura del interior de la casa va aumentando hasta que la radiación infrarroja que se filtra por el vidrio es suficiente para establecer el equilibrio.

Esa es la razón por la que se pueden cultivar plantas dentro de un invernadero, pese a que la temperatura exterior bastaría para helarlas. El calor adicional que se acumula dentro del invernadero -gracias a que el vidrio es bastante transparente a la luz visible pero muy poco a los infrarrojos- es lo que se denomina «efecto invernadero».

La atmósfera terrestre consiste casi por entero en oxígeno, nitrógeno y argón. Estos gases son bastante transparentes tanto para la luz visible como para la clase de radiación infrarroja que emite la superficie terrestre cuando está caliente. Pero la atmósfera contiene también un 0,03 por 100 de anhídrido carbónico, que es transparente para la luz visible pero no demasiado para los infrarrojos. El anhídrido carbónico de la atmósfera actúa como el vidrio del invernadero.

Como la cantidad de anhídrido carbónico que hay en nuestra atmósfera es muy pequeña, el efecto es relativamente secundario. Aun así, la Tierra es un poco más caliente que en ausencia de anhídrido carbónico. Es más, si el contenido en anhídrido carbónico de la atmósfera fuese el doble, el efecto invernadero, ahora mayor, calentaría la Tierra un par de grados más, lo suficiente para provocar la descongelación gradual de los casquetes polares.

Un ejemplo de efecto invernadero a lo grande lo tenemos en Venus, cuya densa atmósfera parece consistir casi toda ella en anhídrido carbónico. Dada su mayor proximidad al Sol, los astrónomos esperaban que Venus fuese más caliente que la Tierra. Pero, ignorantes de la composición exacta de su atmósfera, no habían contado con el calentamiento adicional del efecto invernadero. Su sorpresa fue grande cuando comprobaron que la temperatura superficial de Venus estaba muy por encima del punto de ebullición del agua, cientos de grados más de lo que se esperaban.

36. ¿Qué ocurre con las sondasplanetarias después de pasar por un

planeta? ¿A dónde van a parar?

La mayoría de los satélites lanzados por los Estados Unidos y la Unión Soviética entran en órbita alrededor de la Tierra.

La órbita de un satélite puede cortar la superficie de la Tierra, de modo que vuelve a nuestro planeta al cabo de una sola vuelta. Los dos primeros vuelos «suborbitales» de las cápsulas Mercurio fueron de este tipo. Hay veces que la órbita del satélite describe un bucle tan grande alrededor de la Tierra, que llega incluso más allá de la Luna, como hizo el Lunik III para tomar fotografías de la «otra cara» de la Luna.

Si se lanza un satélite con una velocidad mayor que 11 kilómetros por segundo, el campo gravitatorio terrestre no le podrá retener y el satélite entrará en una órbita independiente alrededor del Sol, cuyo campo gravitatorio, más intenso que el de la Tierra, le permite retener cuerpos de mayor velocidad. Una órbita alrededor del Sol puede cortar la superficie de algún cuerpo celeste, como fue el caso de los Rangers VII, VIII y IX, que se estrellaron contra la Luna (a propósito, claro está).

Pero también puede ser que un satélite en órbita alrededor del Sol no corte la superficie de ningún cuerpo celeste, y entonces seguirá describiendo su elipse alrededor del Sol indefinidamente. Las diversas «sondas lunares» y «sondas planetarias» son de esta clase.

Las trayectorias de las sondas colocadas en órbita alrededor del Sol pueden calcularse de modo que en su primera revolución se aproximen mucho a la Luna (Pioneer IV), a Venus (Mariner II) o a Marte (Mariner IV). En el transcurso de esta aproximación, la sonda envía información acerca del cuerpo estudiado y del espacio circundante. La sonda rebasará luego el cuerpo celeste y proseguirá su órbita alrededor del Sol.

Si las sondas no se vieran afectadas por el campo gravitatorio del planeta por el que pasan, volverían finalmente al punto del espacio desde el que fueron lanzadas (aunque la Tierra habría proseguido entretanto su órbita y no estaría ahí ya).

Lo cierto, sin embargo, es que la sonda planetaria se desplaza a una nueva órbita como consecuencia de la atracción del planeta por el que pasa. Es más: la órbita cambia un poco cada vez que pasa cerca de un cuerpo pesado, con lo cual es casi imposible predecir con exactitud la posición de una sonda al cabo de una o dos revoluciones alrededor del Sol. Las ecuaciones que representan sus movimientos son demasiado complicadas para que merezca la pena molestarse en resolverlas.

Si las sondas pudiesen radiar continuamente señales, habría la posibilidad de seguirlas, cualquiera que fuese su órbita, sobre todo cerca de la Tierra. Pero es que, una vez que se agotan las baterías, el satélite se pierde. No puede emitir señales y además es demasiado pequeño para divisarlo. Todas las sondas acaban por perderse, y con ello ya se cuenta.

No obstante, continúan describiendo órbitas alrededor del Sol y permanecen en las mismas regiones generales del espacio, sin emprender largos viajes a otros planetas. Como no recibimos ninguna información de ellas, no nos sirven de nada y lo mejor que se puede hacer es considerarlas como «basura interplanetaria». Girarán así para siempre en su órbita, a no ser que en alguna de sus revoluciones alrededor del Sol se estrellen contra la Tierra, la Luna, Marte o Venus.

37. ¿Cuál será el fin de laTierra?

El primero en intentar hacer un estudio detallado de la historia pasada y previsiblemente futura de la Tierra sin recurrir a la intervención divina fue el geólogo escocés James Hutton. En 1785 publicó el primer libro de geología moderna, en el cual admitía que del estudio de la Tierra no veía signo alguno de un comienzo ni perspectivas de fin ninguno.

Desde entonces hemos avanzado algo. Hoy día estamos bastante seguros de que la Tierra adquirió su forma actual hace unos 4.700 millones de años. Fue por entonces cuando, a partir del polvo y gas de la nebulosa originaria que formó el sistema solar, nació la Tierra tal como la conocemos hoy día. Una vez formada, y dejada en paz como colección de metales y rocas cubierta por una delgada película de agua y aire, podría existir para siempre, al menos por lo que sabemos hoy. Pero ¿la dejarán en paz?

El objeto más cercano, de tamaño suficiente y energía bastante para afectar seriamente a la Tierral es el Sol. Mientras el Sol mantenga su actual nivel de actividad (como lleva haciendo durante miles de millones de años), la Tierra seguirá esencialmente inmutable. Ahora bien, ¿puede el Sol mantener para siempre ese nivel? Y, caso de que no, ¿qué cambio se producirá y cómo afectará esto a la Tierra?

Hasta los años treinta parecía evidente que el Sol, como cualquier otro cuerpo caliente, tenía que acabar enfriándose. Vertía y vertía energía al espacio, por lo cual este inmenso torrente tendría que disminuir y reducirse poco a poco a un simple chorrito. El Sol se haría naranja, luego rojo, iría apagándose cada vez más y finalmente se apagaría.

En estas condiciones, también la Tierra se iría enfriando lentamente. El agua se congelaría y las regiones polares serían cada vez más extensas. En último término, ni siquiera las regiones ecuatoriales tendrían suficiente calor para mantener la vida. El océano entero se congelaría en un bloque macizo de hielo e incluso el aire se licuaría primero y luego se congelaría. Durante billones de años, esta Tierra gélida (y los demás planetas) seguiría girando alrededor del difunto Sol.

Pero aun en esas condiciones, la Tierra, como planeta, seguiría existiendo.

Sin embargo, durante la década de los treinta, los científicos nucleares empezaron por primera vez a calcular las reacciones nucleares que tienen lugar en el interior del Sol y otras estrellas. Y hallaron que aunque el Sol tiene que acabar por enfriarse, habrá períodos de fuerte calentamiento antes de ese fin. Una vez consumida la mayor parte del combustible básico, que es el hidrógeno, empezarán a desarrollarse otras reacciones nucleares, que calentarán el Sol y harán que se expanda enormemente. Aunque emitirá una cantidad mayor de calor, cada porción de su ahora vastísima superficie tocará a una fracción mucho más pequeña de ese calor y será, por tanto, más fría. El Sol se convertirá en una gigante roja.

En tales condiciones es probable que la Tierra se convierta en un ascua y luego se vaporice. En ese momento, la Tierra, como cuerpo planetario sólido, acabará sus días. Pero no os preocupéis demasiado. Echadle todavía unos ocho mil millones de años.

38. ¿Qué es un físico teórico y quétipo de trabajo hace?

La ciencia de la física trata principalmente de la energía en sus diversas formas y de la interacción de la energía con la materia. Un físico está interesado en las leyes que gobiernan el movimiento porque cualquier trozo de materia en movimiento posee «energía cinética». Y también le interesan el calor, el sonido, la luz, la electricidad, el magnetismo y la radiactividad, porque todos ellos son formas de energía. Y en nuestro siglo se vio que incluso la masa es una forma de energía.

Al físico también le interesa la manera en que una forma de energía se convierte en otra y las reglas que gobiernan esa conversión.

Ni que decir tiene que los físicos se pueden especializar. El que se centra en la interacción dela energía con las partículas subatómicas es el «físico nuclear». (El núcleo es la estructura principal dentro del átomo.) Si lo que le interesa es la interacción de energía y materia en las estrellas, es un «astrofísico».

Luego están los que estudian los aspectos energéticos de las reacciones químicas, que son los «químicos físicos», y los que se interesan principalmente por la manera en que los tejidos vivos manejan y producen energía, que son los «biofísicos» (la palabra griega «bios» significa «vida»).

Hay físicos que se dedican a hacer medidas cuidadosas bajo diversas condiciones controladas. Uno quizá quiera medir la cantidad exacta de calor producido por determinadas reacciones químicas. Otro, medir de qué manera se desintegra una partícula subatómica en otra serie de partículas más energía. Un tercero, medir de qué manera varían diminutos potenciales eléctricos en el cerebro bajo la influencia de ciertas drogas. En todos estos casos tenemos ante nosotros a un «físico experimental».

Por otra parte, hay físicos a quienes les interesa especialmente estudiar las mediciones hechas por otros e intentar darles un sentido general. Quizá logre hallar una relación matemática que explique por qué todas esas medidas son como son. Y una vez hallada esa relación matemática podrá utilizarla para predecir los valores de otras mediciones aún no efectuadas. Si al efectuar éstas resulta que concuerdan con lo predicho, el físico en cuestión puede que haya dado con lo que a menudo se llama una «ley de la naturaleza».

Los físicos que intentan descubrir de esta manera las leyes de la naturaleza se llaman «físicos teóricos».

Hay físicos experimentales muy brillantes a quienes no les interesa demasiado teorizar. Un ejemplo es Albert A. Michelson, que inventó el interferómetro e hizo medidas muy exactas de la velocidad de la luz. Y también hay físicos teóricos verdaderamente geniales a quienes no les preocupa la experimentación. Albert Einstein, el fundador de la teoría de la relatividad, fue uno de ellos.

Tanto los físicos experimentales como los teóricos son de gran valor para la ciencia, aun cuando los primeros se limiten a medir y los segundos a razonar matemáticamente. Pero no deja de ser fascinante encontrar alguno que sobresalga como experimentador y como teórico, ambas cosas a la vez. Enrico Fermi fue un ejemplo notable de estos físicos de «dos caras». (También era un excelente profesor, lo que quizás le hiciese un físico de «tres caras».)

39. El tiempo, ¿es una ilusión oexiste realmente? ¿Cómo habría que

describirlo?

El tiempo, para empezar, es un asunto psicológico; es una sensación de duración. Uno come, y al cabo de un rato vuelve a tener hambre. Es de día, y al cabo de un rato se hace de noche.

La cuestión de qué es esta sensación de duración, de qué es lo que hace que uno sea consciente de que algo ocurre «al cabo de un rato», forma parte del problema del mecanismo de la mente en general, problema que aún no está resuelto.

Tarde o temprano, todos nos damos cuenta de que esa sensación de duración varía con las circunstancias. Una jornada de trabajo parece mucho más larga que un día con la persona amada; y una hora en una conferencia aburrida, mucho más larga que una hora con los naipes. Lo cual podría significar que lo que llamamos un «día» o una «hora» es más largo unas veces que otras. Pero cuidado con la trampa. Un período que a uno le parece corto quizá se le antoje largo a otro, y ni desmesuradamente corto ni largo a un tercero.

Para que este sentido de la duración resulte útil a un grupo de gente es preciso encontrar un método para medir su longitud que sea universal y no personal. Si un grupo acuerda reunirse «dentro de seis semanas exactamente», sería absurdo dejar que cada cual se presentara en el lugar de la cita cuando, en algún rincón de su interior, sienta que han pasado, seis semanas. Mejor será que se pongan todos de acuerdo en contar cuarenta y dos períodos de luz-oscuridad y presentarse entonces, sin hacer caso de lo que diga el sentido de la duración.

En el momento que elegimos un fenómeno físico objetivo como medio para sustituir el sentido innato de la duración por un sistema de contar, tenemos algo a lo que podemos llamar «tiempo». En ese sentido, no debemos intentar definir el tiempo como esto o aquello, sino sólo como un sistema de medida.

Las primeras medidas del tiempo estaban basadas en fenómenos astronómicos periódicos: la repetición del mediodía (el Sol en la posición más alta) marcaba el día; la repetición de la Luna nueva marcaba el mes; la repetición del equinoccio vernal (el Sol de mediodía sobre el ecuador después de la estación fría) marcaba el año. Dividiendo el día en unidades iguales obtenemos las horas, los minutos y los segundos.

Estas unidades menores de tiempo no podían medirse con exactitud sin utilizar un movimiento periódico más rápido que la repetición del mediodía. El uso de la oscilación regular de un péndulo o de un diapasón introdujo en el siglo xvii los modernos relojes. Fue entonces cuando la medida del tiempo empezó a adquirir una precisión aceptable. Hoy día se utilizan las vibraciones de los átomos para una precisión aún mayor.

Pero ¿quién nos asegura que estos fenómenos periódicos son realmente «regulares»? ¿No serán tan poco de fiar como nuestro sentido de la duración?

Puede que sí, pero es que hay varios métodos independientes de medir el tiempo y los podemos comparar entre sí. Si alguno o varios de ellos son completamente irregulares, dicha comparación lo pondrá de manifiesto. Y aunque todos ellos sean irregulares, es sumamente improbable que lo sean de la misma forma. Si, por el contrario, todos los métodos de medir el tiempo coinciden con gran aproximación, como de hecho ocurre, la única conclusión que cabe es que los distintos fenómenos periódicos que usamos son todos ellos esencialmente regulares. (Aunque no perfectamente regulares. La longitud del día, por ejemplo, varía ligeramente.)

Las medidas físicas miden el «tiempo físico». Hay organismos, entre ellos. el hombre, que tienen métodos de engranarse en fenómenos periódicos (como despertarse y dormirse) aun sin referencia a cambios exteriores (como el día y la noche). Pero este «tiempo biológico» no es, ni con mucho tan regular como el tiempo físico.

Y también está, claro es, el sentido de duración o «tiempo psicológico». Aun teniendo un reloj delante de las narices, una jornada de trabajo sigue pareciéndonos más larga que un día con la persona amada.

40. ¿Cuál es la unidad de tiempomás pequeña posible?

Poco después de 1800 se sugirió que la materia consistía en pequeñas unidades llamadas «átomos». Poco después de 1900 se aceptó que la energía constaba de pequeñas unidades llamadas «cuantos». Pues bien, ¿hay alguna otra magnitud común que venga en pequeñas unidades fijas? ¿El tiempo, por ejemplo?

Hay dos maneras de encontrar una «unidad lo más pequeña posible». Está primero el método directo de dividir una cantidad conocida hasta que no se pueda seguir dividiendo: descomponer una masa conocida en cantidades cada vez más pequeñas hasta quedarnos con un solo átomo, o dividir energías conocidas hasta obtener un solo cuanto. El otro método, indirecto, consiste en observar algún fenómeno que no pueda explicarse a menos que supongamos la existencia de una unidad mínima.

En el caso de la materia, la necesidad de una teoría atómica vino a través de una serie muy nutrida de observaciones químicas, entre las cuales figuraban la «ley de las proporciones definidas» y la «ley de las proporciones múltiples». En el caso de la energía, fue el estudio de la radiación del cuerpo negro y la existencia del efecto fotoeléctrico lo que determinó la necesidad de la teoría cuántica.

En el caso del tiempo, el método indirecto falla… al menos hasta ahora. No se han observado fenómenos que hagan necesario suponer que existe una unidad de tiempo mínima.

¿Y por el método directo? ¿Podemos observar períodos de tiempo cada vez más cortos, hasta llegar a algo que sea lo más corto posible?

Los físicos empezaron a manejar intervalos de tiempo ultracortos a raíz del descubrimiento de la radiactividad. Algunos tipos de átomos tenían una vida media muy breve. El polonio 212, por ejemplo, tiene una vida media inferior a una millonésima (10-6) de segundo. Se desintegra en el tiempo que tarda la Tierra en recorrer una pulgada en su giro alrededor del Sol a 29,8 kilómetros por segundo. Pero por mucho que los físicos estudiaron estos procesos con detalle, no había ningún signo, durante ese intervalo, de que el tiempo fluyese a pequeños saltos y no uniformemente.

Pero podemos ir un poco más lejos. Algunas partículas subatómicas se desintegran en intervalos de tiempo mucho más cortos. En la cámara de burbujas hay partículas que, viajando casi a la velocidad de la luz, logran formar, entre el momento de su nacimiento y el de su desintegración, una traza de unos tres centímetros, que corresponde a una vida de una diezmilmillonésima (10-10) de segundo.

Más ahí tampoco acaba la cosa. Durante los años sesenta se descubrieron partículas de vida especialmente corta. Tan efímeras, que aun moviéndose casi a la velocidad de la luz no podían desplazarse lo bastante para dejar una traza medible. El tiempo que vivían había que medirlo por métodos indirectos y resultó que estas «resonancias» de vida ultracorta vivían sólo diezcuatrillonésimas (10-23) de segundo.

Es casi imposible hacerse una idea de un tiempo tan fugaz. La vida de una resonancia es a una millonésima de segundo lo que una millonésima de segundo a tres mil años.

O mirémoslo de otra manera, La luz se mueve en el vacío a unos 300.000 kilómetros por segundo, que es la velocidad más grande que se conoce. Pues bien, la distancia que recorre la luz entre el nacimiento y la muerte de una resonancia es de 10-13 centímetros. ¡Aproximadamente la anchura de un protón!

Pero tampoco hay que pensar que la vida de una resonancia es la unidad de tiempo más pequeña que puede haber. No hay signos de que exista un límite.

41. ¿Qué es la cuarta dimensión?

La palabra «dimensión» viene de un término latino que significa «medir completamente». Vayamos, pues, con algunas medidas.

Supongamos que tienes una línea recta y que quieres marcar sobre ella un punto fijo X, de manera que cualquier otra persona pueda encontrarlo con sólo leer tu descripción. Para empezar, haces una señal en cualquier lugar de la línea y la llamas «cero». Mides luego y compruebas que X está exactamente a dos pulgadas de la marca del cero. Si está a uno de los lados, convienes en llamar a esa distancia + 2; si está al otro, – 2.

El punto queda así localizado con un solo número, siempre que los demás acepten esas «convenciones»: dónde está la marca del cero, y qué lado es más y cuál menos.

Como para localizar un punto sobre una línea sólo se necesita un número, la línea, o cualquier trozo de ella es «uni-dimensional» («un solo número para medir completamente»).

Pero supón que tienes una gran hoja de papel y que quieres localizar en ella un punto fijo X. Empiezas en la marca del cero y compruebas que está a cinco pulgadas… ¿pero en qué dirección? Lo que puedes hacer es descomponer la distancia en dos direcciones. Tres pulgadas al norte y cuatro al este. Sí llamamos al norte más y al sur menos y al este más y al oeste menos, podrás localizar el punto con dos números: +3, +4.

O también puedes decir que está a cinco pulgadas del cero y a un ángulo de 36,87º de la línea este-oeste. De nuevo dos números: 5 y 36,87º. Hagas lo que hagas, siempre necesitarás dos números para localizar un punto fijo en un plano. Un plano, o cualquier trozo de él, es bidimensional.

Supón ahora que lo que tienes es un espacio como el interior de una habitación. Un punto fijo X lo podrías localizar diciendo que está a cinco pulgadas, por ejemplo, al norte de la marca cero, dos pulgadas al éste de ella y 15 pulgadas por encima de ella. O también dando una distancia y dos ángulos. Hagas lo que hagas, siempre necesitarás tres números para localizar un punto fijo en el interior de una habitación (o en el interior del universo).

La habitación, o el universo, son, por tanto, tridimensionales.

Supongamos que hubiese un espacio de naturaleza tal, que se necesitaran cuatro números, o cinco, o dieciocho, para localizar un punto fijo en él. Sería un espacio cuadridimensional, o de cinco dimensiones, o de dieciocho dimensiones. Tales espacios no existen en el universo ordinario, pero los matemáticos sí pueden concebir estos «hiperespacios» y calcular qué propiedades tendrían las correspondientes figuras matemáticas. E incluso llegan a calcular las propiedades que se cumplirían para cualquier espacio dimensional: lo que se llama «geometría n-dimensional».

Pero, ¿y si lo que estamos manejando son puntos, no fijos, sino variables en el tiempo? Si queremos localizar la posición de un mosquito que está volando en una habitación, tendremos que dar los tres números que ya conocemos: norte-sur, este-oeste y arriba-abajo. Pero luego tendríamos que añadir un cuarto número que representara el tiempo, porque el mosquito habrá ocupado esa posición espacial sólo durante un instante, y ese instante hay que identificarlo.

Lo mismo vale para todo cuanto hay en el universo. Tenemos el espacio, que es tridimensional, y hay que añadir el tiempo para obtener un «espacio-tiempo» cuadridimensional. Pero dándole un tratamiento diferente que a las tres «dimensiones espaciales». En ciertas ecuaciones clave en las que los símbolos de las tres dimensiones espaciales tienen signo positivo, el símbolo del tiempo lo lleva negativo.

Por tanto, no debemos decir que el tiempo es la cuarta dimensión. Es sólo una cuarta dimensión, diferente de las otras tres.

42. ¿Qué quiere decir que el espacioestá curvado?

Al leer, así, de pronto, que la teoría de la relatividad de Einstein habla del «espacio curvado», uno quizá tiene todo derecho a sentirse desconcertado. El espacio vacío ¿cómo puede, ser curvo? ¿Cómo se puede doblar el vacío?

Para verlo, imaginemos que alguien observa, desde una nave espacial, un planeta cercano. El planeta está cubierto todo él por un profundo océano, de modo que es una esfera de superficie tan pulida como la de una bola de billar. Y supongamos también que por este océano planetario navega un velero a lo largo del ecuador, rumbo este.

Imaginemos ahora algo más. El planeta es completamente invisible para el observador. Lo único que ve es el velero. Al estudiar su trayectoria comprueba con sorpresa que el barco sigue un camino circular. Al final, regresará al punto de partida, habiendo descrito entonces una circunferencia completa.

Si el barco cambia de rumbo, ya no será una circunferencia perfecta. Pero por mucho que cambie de rumbo, por mucho que vire y retroceda, la trayectoria se acoplará perfectamente a la superficie de una esfera.

De todo ello el observador deducirá que en el centro de la esfera hay una fuerza gravitatoria que mantiene al barco atado a una superficie esférica invisible. O también podría deducir que el barco está confinado a una sección particular del espacio y que esa sección está curvada en forma de esfera. O digámoslo así: la elección está entre una fuerza y una geometría espacial.

Diréis que la situación es imaginaria, pero en realidad no lo es. La Tierra describe una elipse alrededor del Sol, como si navegara por una superficie curvada e invisible, y para explicar la elipse suponemos que entre el Sol y la Tierra hay una fuerza gravitatoria que mantiene a nuestro planeta en su órbita.

Pero suponed que en lugar de ello consideramos una geometría espacial. Para definirla podríamos mirar, no el espacio en sí, que es invisible, sino la manera en que los objetos se mueven en él. Si el espacio fuese «plano», los objetos se moverían en líneas rectas; si fuese «curvo», en líneas curvas.

Un objeto de masa y velocidad dadas, que se mueva muy alejado de cualquier otra masa, sigue de hecho una trayectoria casi recta. Al acercarse a otra masa, la trayectoria se hace cada vez más curva. La masa, al parecer, curva el espacio; cuanto mayor y más próxima, más acentuada será la curvatura.

Quizá parezca mucho más conveniente y natural hablar de la gravitación corno una fuerza, que no como una geometría espacial… hasta que se considera la luz. La luz no tiene masa, y según las viejas teorías no debería verse afectada por la fuerza gravitatoria. Pero si la luz viaja por el espacio curvado, también debería curvarse su trayectoria. Conociendo la velocidad de la luz se puede calcular la deflexión de su trayectoria al pasar cerca de la ingente masa del Sol.

En 1919 se comprobó esta parte de la teoría de Einstein (anunciada tres años antes) durante un eclipse de Sol. Para ello se comparó la posición de las estrellas próximas al Sol con la posición registrada cuando el Sol no se hallaba en esa parte de los cielos. La teoría de Einstein quedó confirmada y desde entonces es más exacto hablar de la gravedad en función del espacio curvado, que no en función de una fuerza.

Sin embargo, justo es decir que ciertas medidas, muy delicadas, de la forma del Sol, realizadas en 1967, pusieron en duda la teoría de la gravitación de Einstein. Para ver lo que pasará ahora y en el futuro habrá que esperar.

43. En muchas novelas deciencia-ficción se leen cosas sobre

«campos de fuerza» e «hiperespacio».

¿Qué son? ¿Existen realmente?

Toda partícula subatómica da lugar a por lo menos una de cuatro clases distintas de influencias: la gravitatoria, la electromagnética, la nuclear débil y la nuclear fuerte. Cualquiera de ellas se extiende desde su fuente de origen en la forma de un «campo» que, en teoría, permea el universo entero. Los campos de un gran número de partículas juntas pueden sumar sus influencias y crear un campo resultante muy intenso. El campo gravitatorio es, con mucho, el más débil de los cuatro, pero el del Sol (cuerpo compuesto por un número enorme de partículas) es muy fuerte, precisamente por la razón anterior.

Dos partículas colocadas dentro de un campo pueden moverse al encuentro una de otra o alejarse entre sí, según sea la naturaleza de las partículas y del campo; y además lo harán con una aceleración que depende de la distancia entre ambas. La interpretación que se suele dar a estas aceleraciones es que están producidas por «fuerzas», con lo cual se habla de «campos de fuerza». En este sentido, existen realmente.

Ahora bien, los campos de fuerza que conocemos tienen siempre por origen la materia y no existen en ausencia de ella, mientras que en los relatos de ciencia-ficción es a veces muy útil imaginar la construcción de intensos campos de fuerza sin materia. El novelista puede así convertir una sección del vacío en una barrera contra partículas y radiación. ¡Igual que si fuese una lámina de acero de seis pies de espesor. Tendría todas las fuerzas interatómicas, pero ninguno de los átomos que las crean. Esos «campos de fuerza libres de materias» son un recurso muy útil de la ciencia-ficción, pero sin base alguna en la ciencia actual.

El «hiperespacio» es otro recurso útil de la ciencia-ficción: un artificio para burlar la barrera de la velocidad de la luz.

Para ver cómo funciona, pensad en una hoja de papel plana y muy grande, en la que hay dos puntos a seis pies uno de otro. Imaginad ahora un lentísimo caracol que sólo pueda caminar un pie a la hora. Está claro que tardará seis horas en pasar de un punto al otro.

Pero suponed que cogemos ahora esa hoja de papel, que en esencia es bidimensional, y la doblamos por la tercera dimensión, poniendo casi en contacto los dos puntos. Si la distancia es ahora de sólo una décima de pulgada y si el caracol es capaz de cruzar de algún modo el espacio que queda entre los dos trozos de papel así doblados, podrá pasar de un punto a otro en medio minuto exactamente.

Vayamos ahora con la analogía. Si tenemos dos estrellas que distan cincuenta años-luz entre sí, una nave espacial que vuele a la máxima velocidad (la de la luz) tardará cincuenta años en ir de una a otra (referidos a alguien que se encuentre en cualquiera de estos dos sistemas estelares). Todo esto crea numerosas complicaciones, pero los escritores de ciencia-ficción han descubierto un modo de simplificar los argumentos, y es pretender que la estructura del espacio (en esencia tridimensional) puede doblarse por una cuarta dimensión espacial, dejando así entre las dos estrellas un vano cuadridimensional muy pequeño. La nave cruza entonces ese estrecho y se presenta en la estrella en un santiamén.

Los matemáticos acostumbran a hablar de los objetos de cuatro dimensiones como si se tratara de objetos análogos tridimensionales y añadiendo luego el prefijo «hiper», palabra griega que significa «por encima de», «más allá de». Un objeto cuya superficie dista lo mismo del centro en las cuatro dimensiones es una «hiperesfera». Y de la misma manera podemos obtener el «hipertetraedro», el «hipercubo» y el «hiperelipsoide». Con este convenio podemos llamar «hiperespacio» a ese vano cuadridimensional entre las estrellas.

Pero ¡viva el cielo!, que por muy útil que le sea al escritor de ciencia-ficción el hiperespacio, nada hay en la ciencia actual que demuestre la existencia de tal cosa, salvo como abstracción matemática.

44. ¿Qué es la antigravedad? ¿Cómopuede estudiarse?

Hay dos tipos de campos -los electromagnéticos y los gravitatorios- cuya intensidad decrece con el cuadrado de la distancia. Esta disminución de intensidad es suficientemente lenta para permitir que un campo electromagnético o gravitatorio sea detectable a grandes distancias. La Tierra está firmemente sujeta por el campo gravitatorio solar, pese a que el Sol está a 150 millones de kilómetros.

Sin embargo, el campo gravitatorio es, con mucho, el más débil de los dos. El campo electromagnético creado por un electrón es algo así como cuatro septillones más intenso que su campo gravitatorio.

Claro está que, parecer, sí parecen intensos los campos gravitatorios. Cada vez que nos caemos experimentamos dolorosamente la intensidad del campo gravitatorio terrestre. Pero es sólo porque la Tierra tiene un tamaño inmenso. Cada fragmento diminuto contribuye a ese campo, y al final la suma es ingente.

Pero suponed que cogemos 100 millones de electrones (que, juntados en un punto, ni siquiera se verían al microscopio) y los dispersamos por un volumen del tamaño de la Tierra. El campo electromagnético resultante sería igual al campo gravitatorio de toda la Tierra.

¿Por qué no notamos más los campos electromagnéticos que los gravitatorios?

Aquí es donde surge otra diferencia. Hay dos clases de carga eléctrica, llamadas positiva y negativa, de modo que un campo electromagnético puede resultar en una atracción (entre una carga positiva y otra negativa) o en una repulsión (entre dos positivas o entre dos negativas). De hecho, si la Tierra no contuviera otra cosa que esos 100 millones de electrones, éstos se repelerían y se dispersarían en todas direcciones.

Las fuerzas de atracción y repulsión electromagnéticas sirven para mezclar a fondo las cargas positivas y negativas, de modo que el efecto de éstas se anula en definitiva. Aquí y allá es posible que surjan pequeñísimos excesos o defectos de electrones, y los campos electromagnéticos que nosotros estudiamos son precisamente los correspondientes a estos desplazamientos.

El campo gravitatorio, por el contrario, parece ser que sólo produce fuerzas de atracción. Cualquier objeto que posea masa atrae a cualquier otro que también la posea, y a medida que se acumula la masa aumenta también la intensidad del campo gravitatorio, sin cancelación alguna.

Si un objeto con masa repeliera a otro objeto (dotado también de masa) con la misma intensidad y de la misma manera que se atraen dichos objetos en las condiciones gravitatorias normales, lo que tendríamos sería «antigravedad» o «gravedad negativa».

Jamás se ha detectado una repulsión gravitatoria de este tipo, pero quizá sea porque todos los objetos que podemos estudiar con detalle están constituidos por partículas ordinarias.

Pero además de las partículas ordinarias están las «antipartículas», que son iguales que aquéllas, salvo que el campo electromagnético está invertido. Si una partícula dada tiene carga negativa, la correspondiente antipartícula la tiene positiva. Y puede ser que el campo gravitatorio de las antipartículas también esté invertido. Dos antipartículas se atraerían gravitatoriamente igual que dos partículas, pero una antipartícula y una partícula se repelerían.

Lo malo es que los campos gravitatorios son tan débiles, que en partículas o antipartículas sueltas es imposible detectarlos, como no sea en masas grandes. Masas grandes de partículas sí tenemos, pero en cambio nadie ha reunido una masa apreciable de antipartículas. Ni tampoco ha sugerido nadie un modo alternativo, y práctico, de detectar los efectos antigravitatorios.

45. ¿Cuál es la velocidad de lagravitación?

Una manera más larga, pero quizá más clara, de plantear la cuestión es ésta: supongamos que el Sol dejara de pronto de existir y se desvaneciera en la nada. ¿Cuánto tiempo pasaría antes de que la Tierra dejara de verse solicitada por su campo gravitatorio?

Una pregunta parecida podría ser: ¿Cuánto tiempo después de la desaparición del Sol dejaría la Tierra de recibir su luz?

La respuesta a la segunda pregunta la conocemos muy bien. Sabemos que el Sol se halla a poco menos de 150 millones de kilómetros de la Tierra y también que la luz se propaga a 299.793 kilómetros por segundo en el vacío. El último rayo de luz que abandonara el Sol, justo antes de desaparecer, tardaría 8,3 minutos en llegar a la Tierra. O digámoslo así: al Sol lo veríamos desaparecer 8,3 minutos más tarde de haber desaparecido realmente.

El motivo de que sea fácil contestar esta pregunta acerca de la luz es que hay una serie de métodos para medir efectivamente su velocidad de propagación. Tales mediciones son viables gracias a que podemos detectar cambios en la debilísima luz emitida por los cuerpos celestes remotos, y gracias también a que somos capaces de producir haces de luz muy intensos.

Con los campos gravitatorios no tenemos esas ventajas. Es muy difícil estudiar pequeños cambios en campos gravitatorios débiles, y además no sabemos producir, aquí en la Tierra, efectos gravitatorios intensos que se extiendan a grandes distancias.

Así, que hay que recurrir a la teoría. Hay cuatro tipos de interacción en el universo: 1) nucleares fuertes, 2) nucleares débiles, 3) electromagnéticas, y 4) gravitatorias. Las dos primeras son de corto alcance y decrecen muy rápidamente con la distancia. A distancias superiores a la anchura de un núcleo atómico, las interacciones nucleares son tan débiles que pueden ignorarse. Las interacciones electromagnéticas y gravitatorias son, por el contrario, de largo alcance. Decrecen sólo con el cuadrado de la distancia, lo cual quiere decir que se dejan sentir a distancias astronómicas.

Los físicos creen que cualquier interacción entre dos cuerpos tiene lugar por intercambio de partículas sub-atómicas. Cuanto mayor sea la masa de la partícula de intercambio, menor será el alcance de la interacción. La interacción nuclear fuerte, por ejemplo, resulta del intercambio de piones, que tienen una masa 270 veces más grande que la de los electrones. La interacción nuclear débil tiene lugar por intercambio de partículas más pesadas aún: las partículas W (que, por cierto, no han sido detectadas aún).

Si las partículas de intercambio no tienen masa, la interacción tiene un alcance máximo, y esto es lo que ocurre con la interacción electromagnética. La partícula de intercambio es en este caso el fotón, que no tiene masa. Una corriente de estos fotones carentes de masa constituye un haz de luz o de radiaciones afines. La interacción gravitatoria, que tiene un alcance tan grande como la electromagnética, ha de implicar una partícula de intercambio carente también de masa: lo que se llama el gravitón.

Pero los físicos tienen buenas razones para suponer que las partículas sin masa no pueden viajar por el vacío a una velocidad superior a la de la luz; es decir, a 299.793 kilómetros por segundo, ni más ni menos.

Si es así, los gravitones viajan exactamente a la velocidad de los fotones. Lo cual significa que los últimos gravitones que emitiera el Sol al desaparecer llegarían hasta nosotros junto con los últimos fotones. En el momento en que dejásemos de ver el Sol, dejaríamos también de estar bajo su atracción gravitatoria.

En resumen, la gravitación se propaga a la velocidad de la luz.

46. ¿Qué es la teoria del campounificado?

A mediados del siglo xix se conocían cuatro fenómenos que eran capaces de hacerse notar a través del vacío. Eran: 1) gravitación, 2) luz, 3) atracción y repulsión eléctricas, y 4) atracción y repulsión magnéticas.

Al principio parecía que los cuatro fenómenos eran completamente independientes, que no tenían necesariamente ninguna conexión entre sí. Pero entre 1864 y 1873 el físico teórico escocés J. Clerk Maxwell analizó matemáticamente los fenómenos eléctricos y magnéticos, encontrando ciertas relaciones básicas (las «ecuaciones de Maxwell») que describían tanto los fenómenos eléctricos como los magnéticos y que demostraban que los unos dependían de los otros. No había ningún efecto eléctrico que no fuese acompañado de un determinado efecto magnético, y viceversa. En otras palabras, podía hablarse de un «campo electromagnético», que se extendía a través del vacío y que, por contacto, influía sobre los cuerpos de acuerdo con la intensidad del campo en ese punto del espacio.

Maxwell demostró también que haciendo oscilar de manera regular a este campo se originaba una radiación que se alejaba de la fuente de oscilación a la velocidad de la luz en todas direcciones. La luz propiamente dicha era una de esas «radiaciones electromagnéticas» y Maxwell predijo la existencia de formas de luz con longitudes de onda mucho más pequeñas y mucho más grandes que la de la luz ordinaria. Esas otras formas de luz fueron descubiertas a lo largo de los veinte años siguientes, y hoy día se habla de todo un «espectro electromagnético».

Así pues, de los cuatro fenómenos mencionados al principio, tres (electricidad, magnetismo y luz) quedaron fundidos en un único campo. Pero quedaba aún la gravedad por explicar. Estaban 1) el campo electromagnético y 2) el campo gravitatorio, que al parecer seguían siendo dos campos independientes.

Los físicos, sin embargo, pensaban que sería mucho más bonito que hubiese un solo campo (esa es la «teoría del campo unificado»); y así empezaron a buscar la manera de describir los efectos electromagnéticos y los gravitatorios de modo que la existencia de unos pudiera utilizarse para describir la naturaleza de la existencia de los otros.

Pero aunque se descubriesen unas ecuaciones que combinaran los efectos electromagnéticos y los gravitatorios, no lograrían del todo proporcionar -según los criterios actuales- un campo auténticamente unificado. Después de 1935 se descubrieron dos nuevos tipos de campo que sólo afectan a las partículas subatómicas y, además, sólo a distancias inferiores al diámetro de un núcleo atómico. Son la «interacción nuclear fuerte» y la «interacción nuclear débil».

Un auténtico campo unificado tendría que dar cuenta de los cuatro campos que hoy se conocen.

47. ¿Qué es, en pocas palabras, lateoría de la relatividad de Einstein?

Según las leyes del movimiento establecidas por primera vez con detalle por Isaac Newton hacia 1680-89, dos o más movimientos se suman de acuerdo con las reglas de la aritmética elemental. Supongamos que un tren pasa a nuestro lado a 20 kilómetros por hora y que un niño tira desde el tren una pelota a 20 kilómetros por hora en la dirección del movimiento del tren. Para el niño, que se mueve junto con el tren, la pelota se mueve a 20 kilómetros por hora. Pero para nosotros, el movimiento del tren y el de la pelota se suman, de modo que la pelota se moverá a la velocidad de 40 kilómetros por hora.

Como veis, no se puede hablar de la velocidad de la pelota a secas. Lo que cuenta es su velocidad con respecto a un observador particular. Cualquier teoría del movimiento que intente explicar la manera en que las velocidades (y fenómenos afines) parecen variar de un observador a otro sería una «teoría de la relatividad».

La teoría de la relatividad de Einstein nació del siguiente hecho: lo que funciona para pelotas tiradas desde un tren no funciona para la luz. En principio podría hacerse que la luz se propagara, o bien a favor del movimiento terrestre, o bien en contra de él. En el primer caso parecería viajar más rápido que en el segundo (de la misma manera que un avión viaja más aprisa, en relación con el suelo, cuando lleva viento de cola que cuando lo lleva de cara). Sin embargo, medidas muy cuidadosas demostraron que la velocidad de la luz nunca variaba, fuese cual fuese la naturaleza del movimiento de la fuente que emitía la luz.

Einstein dijo entonces: supongamos que cuando se mide la velocidad de la luz en el vacío, siempre resulta el mismo valor (unos 299.793 kilómetros por segundo), en cualesquiera circunstancias. ¿Cómo podemos disponer las leyes del universo para explicar esto?

Einstein encontró que para explicar la constancia de la velocidad de la luz había que aceptar una serie de fenómenos inesperados.

Halló que los objetos tenían que acortarse en la dirección del movimiento, tanto más cuanto mayor fuese su velocidad, hasta llegar finalmente a una longitud nula en el límite de la velocidad de la luz; que la masa de los objetos en movimiento tenía que aumentar con la velocidad, hasta hacerse infinita en el límite de la velocidad de la luz; que el paso del tiempo en un objeto en movimiento era cada vez más lento a medida que aumentaba la velocidad, hasta llegar a pararse en dicho límite; que la masa era equivalente a una cierta cantidad de energía y viceversa.

Todo esto lo elaboró en 1905 en la forma de la «teoría especial de la relatividad», que se ocupaba de cuerpos con velocidad constante. En 1915 extrajo consecuencias aún más sutiles para objetos con velocidad variable, incluyendo una descripción del comportamiento de los efectos gravitatorios. Era la «teoría general de la relatividad».

Los cambios predichos por Einstein sólo son notables a grandes velocidades. Tales velocidades han sido observadas entre las partículas subatómicas, viéndose que los cambios predichos por Einstein se daban realmente, y con gran exactitud. Es más, sí la teoría de la relatividad de Einstein fuese incorrecta, los aceleradores de partículas no podrían funcionar, las bombas atómicas no explotarían y habría ciertas observaciones astronómicas imposibles de hacer.

Pero a las velocidades corrientes, los cambios predichos son tan pequeños que pueden ignorarse. En estas circunstancias rige la aritmética elemental de las leyes de Newton; y como estamos acostumbrados al funcionamiento de estas leyes, nos parecen ya de «sentido común», mientras que la ley de Einstein se nos antoja «extraña».

48. ¿Por qué la materia no puedemoverse más deprisa que la velocidad

de la luz? (Parte 1)

La energía suministrada a un cuerpo puede influir sobre él de distintas maneras. Si un martillo golpea a un clavo en medio del aire, el clavo sale despedido y gana energía cinética o, dicho de otro modo, energía de movimiento. Si el martillo golpea sobre un clavo incrustado en madera dura e incapaz por tanto de moverse, el clavo seguirá ganando energía, pero en forma de calor.

Albert Einstein demostró en su teoría de la relatividad que la masa cabía contemplarla como una forma de energía (y el invento de la bomba atómica probó que estaba en lo cierto). Al añadir energía a un cuerpo, esa energía puede aparecer por tanto en la forma de masa, o bien en otra serie de formas.

En condiciones ordinarias, la ganancia de energía en forma de masa es tan increíblemente pequeña, que sería imposible medirla. Fue en el siglo xx -con la observación de partículas subatómicas que se movían a velocidades de decenas de miles de kilómetros por segundo- cuando se empezaron a encontrar aumentos de masa que eran suficientemente grandes para poder detectarlos. Un cuerpo que se moviera a unos 260.000 kilómetros por segundo respecto a nosotros mostraría una masa dos veces mayor que en reposo (siempre respecto a nosotros).

La energía que se comunica a un cuerpo libre puede integrarse en él de dos maneras distintas: 1) en forma de velocidad, con lo cual aumenta la rapidez del movimiento, y 2) en forma de masa, con lo cual se hace «más pesado». La división entre estas dos formas de ganancia de energía, tal como la medimos nosotros, depende en primer lugar de la velocidad del cuerpo (medida, una vez más, por nosotros).

Si el cuerpo se mueve a velocidades normales, prácticamente toda la energía se incorpora en forma de velocidad: el cuerpo se mueve más aprisa sin sufrir apenas ningún cambio de masa.

A medida que aumenta la velocidad del cuerpo (y suponiendo que se sigue inyectando constantemente energía) es cada vez menos la energía que se convierte en velocidad y más la que se transforma en masa. Observamos que aunque el cuerpo siga moviéndose cada vez más rápido, el ritmo de aumento de velocidad decrece. Como contrapartida notamos que gana masa a un ritmo ligeramente mayor.

Al aumentar aún más la velocidad y acercarse a los 299.793 kilómetros por segundo, que es la velocidad de la luz en el vacío, casi toda la energía añadida entra en forma de masa. Es decir, la velocidad del cuerpo aumenta muy lentamente, pero ahora es la masa la que sube a pasos agigantados. En el momento en que se alcanza la velocidad de la luz, toda la energía añadida aparece en forma de masa adicional.

El cuerpo no puede sobrepasar la velocidad de la luz, porque para conseguirlo hay que comunicarle energía adicional, y a la velocidad de la luz toda esa energía, por mucha que sea, se convertirá en nueva masa, con lo cual la velocidad no aumentará ni un ápice.

Todo esto no es «pura teoría». Los científicos han observado con todo cuidado durante años las partículas subatómicas. En los rayos cósmicos hay partículas de energía increíblemente alta, pero por mucho que aumenta su masa, la velocidad nunca llega a la de la luz en el vacío. La masa y la velocidad de las partículas subatómicas son exactamente como predice la teoría de la relatividad, y la velocidad de la luz es una velocidad máxima como una cuestión de hecho, no en virtud de simples especulaciones.

49. ¿Por qué la materia no puedemoverse más deprisa que la velocidad

de la luz? (Parte 2)

Las explicaciones anteriores no dejaron sentada del todo la cuestión, sino que plantearon dudas e incitaron a muchos a formular por carta nuevas preguntas. Algunos preguntaban: «¿Por qué se convierte la energía en masa y no en velocidad?» o «¿Por qué se propaga la luz a 299.793 kilómetros por segundo y no a otra velocidad?»

Hoy por hoy, la única respuesta posible a esas preguntas es: «Porque así es el universo».

Otros preguntaban: «¿Cómo aumenta la masa?» Esto ya es más fácil. No es que aumente el número de átomos, que sigue siendo el mismo, sino que es la masa de cada átomo (en realidad de cada partícula dentro del átomo) la que aumenta.

Hubo quienes preguntaron si no sería posible aumentar los recursos terrestres a base de mover la materia muy deprisa, doblando así su masa. De ese modo tendríamos justamente el doble.

No es cierto. El aumento de masa no es «real». Es una cuestión de medida. La velocidad sólo adquiere significado como medida relativa a algo: a la persona que efectúa la medida, pongamos por caso. Lo único que cuenta es la medición. Ni tú ni yo podemos medir materia que se mueve más deprisa que la luz.

Pero supón que te agarras a esa materia que acabas de comprobar que tiene el doble de su masa normal y que la quieres utilizar para un fin determinado. Al moverte junto con ella, su velocidad con respecto a ti es cero y de pronto su masa es otra vez la normal.

Si pasas como un relámpago al lado de tu amigo a una velocidad próxima a la de la luz, verías que su masa es enorme y él vería igual de enorme la tuya. Tanto tú corno él pensaríais que vuestra propia masa era normal.

Preguntaréis: «¿Pero cuál de los dos ha aumentado realmente de masa?» La respuesta es: «Depende de quién haga la medida». No hay «realmente» que valga; las cosas son tal como son medidas con respecto a algo y por alguien. De ahí el nombre de teoría de la «relatividad».

Nosotros pensamos que estamos cabeza arriba y que los australianos están cabeza abajo. Los australianos piensan lo mismo pero al revés. ¿Cuál de las dos visiones es «realmente» la correcta? Ninguna de las dos. No hay «realmente» que valga. Depende de en qué punto de la Tierra nos encontremos. Todo es relativo.

Hubo también lectores que preguntaron: «Si la masa aumenta con la velocidad, ¿no se haría cero cuando el objeto estuviera absolutamente quieto?» Pero es que no hay el «absolutamente quieto». Sólo hay «reposo relativo». Una cosa puede estar en reposo en relación con otra. Cuando un objeto está en reposo en relación con la persona que efectúa la medida, posee una cierta masa mínima denominada «masa en reposo». La masa no puede ser menor que eso.

A velocidades relativas grandes no sólo aumenta la masa de un objeto, sino que disminuye también la longitud del mismo en la dirección del movimiento y se retrasa el paso del tiempo por dicho objeto.

Y si preguntamos que por qué, la respuesta es: «Porque si no fuese así, la velocidad de la luz no sería la velocidad máxima para la materia.»

50. Las partículas que se mueven másdeprisa que la luz emiten radiación

luminosa. ¿Cómo es posible, si no

hay nada que se propague más deprisa

que la luz?

A menudo se oye decir que las partículas no pueden moverse «más deprisa que la luz» y que la «velocidad de la luz» es el límite último de velocidad.

Pero decir esto es decir las cosas a medias, porque la luz viaja a velocidades diferentes según el medio en que se mueve. Donde más deprisa se mueve la luz es en el vacío: allí lo hace a 299.793 kilómetros por segundo. Éste es el límite último de velocidades.

Por consiguiente, para ser precisos habría que decir que las partículas no pueden moverse «más deprisa que la velocidad de la luz en el vacío».

Cuando la luz se mueve a través de un medio transparente, siempre lo hace más despacio que en el vacío, y en algunos casos mucho más despacio. Cuanto más despacio se mueva en un medio dado, tanto mayor es el ángulo con que se dobla (refracta) al entrar en ese medio desde el vacío y con un ángulo oblicuo. La magnitud de ese doblamiento viene definida por lo que se denomina el «índice de refracción».

Si dividimos la velocidad de la luz en el vacío por el índice de refracción de un medio dado, lo que obtenemos es la velocidad de la luz en dicho medio. El índice de refracción del aire, a la presión y temperatura normales, es aproximadamente 1,003, de modo que la velocidad de la luz en el aire es 299.793 dividido por 1,0003 ó 299.703 kilómetros por segundo. Es decir, 90 kilómetros por segundo menos que la velocidad de la luz en el vacío.

El índice de refracción del agua es 1,33, del vidrio corriente 1,7 y del diamante 2,42. Esto significa que la luz se mueve a 225.408 kilómetros por segundo por el agua, a 176.349 kilómetros por segundo por el vidrio y a sólo 123.881 kilómetros por segundo por el diamante.

Las partículas no pueden moverse a más de 299.793 kilómetros por segundo, pero desde luego sí a 257.500 kilómetros por segundo, pongamos por caso, incluso en el agua. En ese momento están moviéndose por el agua a una velocidad mayor que la de la luz en el agua. Es más, las partículas pueden moverse más deprisa que la luz en cualquier medio excepto el vacío.

Las partículas que se mueven más deprisa que la luz en un determinado medio distinto del vacío emiten una luz azul que van dejando tras de sí como si fuese una cola. El ángulo que forman los lados de esta cola con la dirección de la partícula depende de la diferencia entre la velocidad de la partícula y la de la luz en ese medio.

El primero que observó esta luz azul emitida por las partículas más veloces que la luz fue un físico ruso llamado Pavel A. Cerenkov, que anunció el fenómeno en 1934. Esa luz se denomina, por tanto, «radiación de Cerenkov». En 1937, otros dos físicos rusos, Eya M. Frank e Igor Y. Tamm, explicaron la existencia de esta luz, relacionándola con las velocidades relativas de la partícula y de la luz en el medio de que se tratara. Como resultado de ello, los tres recibieron en 1958 el Premio Nóbel de Física.

Para detectar dicha radiación y medir su intensidad y la dirección con que se emite se han diseñado instrumentos especiales, llamados «contadores de Cerenkov».

Los contadores de Cerenkov son muy útiles porque sólo son activados por partículas muy rápidas y porque el ángulo de emisión de la luz permite calcular fácilmente su velocidad. Los rayos cósmicos muy energéticos se mueven a una velocidad tan próxima a la de la luz en el vacío, que producen radiación de Cerenkov incluso en el aire.

Los taquiones, partículas hipotéticas que sólo se pueden mover más de prisa que la luz en el vacío, dejarían un brevísimo relámpago de radiación de Cerenkov incluso en el vacío. Las esperanzas que tienen los físicos de probar la existencia real de los taquiones se cifran en detectar precisamente esa radiación de Cerenkov (suponiendo que existan, claro está).

51. Si no hay nada más rápido quela luz, ¿qué son los taquiones, que

al parecer se mueven más deprisa que

ella?

La teoría especial de la relatividad de Einstein dice que es imposible hacer que ningún objeto de nuestro universo se mueva a una velocidad mayor que la de la luz en el vacío. Haría falta una cantidad infinita de energía para comunicarle una velocidad igual a la de luz, y la cantidad «plus quam infinita» necesaria para pasar de ese punto sería impensable.

Pero supongamos que un objeto estuviese moviéndose ya más deprisa que la luz.

La luz se propaga a 299.793 kilómetros por segundo. Pero, ¿qué ocurriría si un objeto de un kilogramo de peso y de un centímetro de longitud se estuviera moviendo a 423.971 kilómetros por segundo? Utilizando las ecuaciones de Einstein comprobamos que el objeto tendría entonces una masa de – kilogramos y una longitud de + centímetros.

O dicho con otras palabras: cualquier objeto que se mueva más deprisa que la luz tendría que tener una masa y una longitud expresadas en lo que los matemáticos llaman «números imaginarios» (véase pregunta 6). Y como no conocemos ninguna manera de visualizar masas ni longitudes expresadas en números imaginarios, lo inmediato es suponer que tales cosas, al ser impensables, no existen.

Pero en el año 1967, Gerald Feinberg, de la Universidad Columbia, se preguntó si era justo proceder así. (Feinberg no fue el primero que sugirió la partícula; el mérito es de O. M. Bilaniuk y E. C. G. Sudarshan. Pero fue Feinberg quien divulgó la idea.) Pudiera ser, se dijo, que una masa y una longitud «imaginarias» fuesen simplemente un modo de describir un objeto con gravedad negativa (pongamos por caso): un objeto que, dentro de nuestro universo, repele a la materia en lugar de atraerla gravitatoriamente.

Feinberg llamó «taquiones» a estas partículas más rápidas que la luz y de masa y longitud imaginarias; la palabra viene de otra que en griego significa «rápido». Si concedemos la existencia de estos taquiones, ¿podrán cumplir los requisitos de las ecuaciones de Einstein?

Aparentemente, sí. No hay inconveniente alguno en imaginar un universo entero de taquiones que se muevan más deprisa que la luz pero que sigan cumpliendo los requisitos de la relatividad. Sin embargo, en lo que toca a la energía y a la velocidad, la situación es opuesta a lo que estamos acostumbrados.

En nuestro universo, el «universo lento», un cuerpo inmóvil tiene energía nula; a medida que adquiere energía va moviéndose cada vez más deprisa, y cuando la energía se hace infinita el cuerpo va a la velocidad de la luz. En el «universo rápido», un taquión de energía nula se mueve a velocidad infinita, y cuanta más energía adquiere más despacio va; cuando la energía se hace infinita, la velocidad se reduce a la de la luz.

En nuestro universo lento ningún cuerpo puede moverse más deprisa que la luz bajo ninguna circunstancia. En el universo rápido, un taquión no puede moverse más despacio que la luz en ninguna circunstancia. La velocidad de la luz es la frontera entre ambos universos y no puede ser cruzada.

Pero los taquiones ¿realmente existen? Nada nos impide decidir que es posible que exista un universo rápido que no viole la teoría de Einstein, pero el que sea posible no quiere decir que sea.

Una posible manera de detectar el universo rápido se basa en la consideración de que un taquión, al atravesar un vacío con velocidad superior a la de la luz, tiene que dejar tras sí un rastro de luz potencialmente detectable. Naturalmente, la mayoría de los taquiones irían muy, muy deprisa, millones de veces más deprisa que la luz (igual que los objetos corrientes se mueven muy despacio, a una millonésima de la velocidad de la luz).

Los taquiones ordinarios y sus relámpagos de luz pasarían a nuestro lado mucho antes de que nos pudiésemos percatar de su presencia. Tan sólo aquellos pocos de energía muy alta pasarían con velocidades próximas a la de la luz. Y aún así, recorrerían un kilómetro en algo así como 1/300.000 de segundo, de modo que detectarlos exigiría una operación harto delicada.

52. Los taquiones de energía cero semueven con velocidad Infinita. ¿Es de

verdad posible una velocidad infinita?

La idea de una partícula que se mueve a velocidad infinita tiene sus paradojas. Iría de A a B en un tiempo nulo, lo cual significa que estaría en A y B al mismo tiempo, y también en todos los lugares intermedios. Y seguiría hasta los puntos C, D, E, etc., a través de una distancia infinita, todo ello en un tiempo nulo. Una partícula que se moviera a velocidad infinita tendría por tanto las propiedades de una barra sólida de longitud infinita.

Si el espacio es curvo, como sugiere la teoría de la relatividad de Einstein, esa barra sólida sería en realidad un gran círculo, o espiral, o una curva sinuosa de forma aún más complicada.

Pero supongamos que hay un universo de taquiones, es decir de partículas que poseen todas ellas una velocidad mayor que la de la luz. A medida que van adquiriendo más y más energía, se mueven cada vez más lentamente, hasta que, al llegar a una energía infinita, su velocidad se reduce a la de la luz. Al perder energía, van cada vez más deprisa, yendo a velocidad infinita cuando la energía es nula.

Es de imaginar que en un universo así habría partículas de muy distintas energías. Unas muy energéticas, otras muy poco y otras intermedias (como ocurre con las partículas de nuestro universo).

La transferencia de energía de una partícula a otra en ese universo (igual que en el nuestro) tendría que ser a través de una interacción, como puede ser, por ejemplo, un choque. Si la partícula A, de poca energía, choca con la partícula B, de alta energía, lo más probable es que A gane energía a expensas de B, con lo cual habría una tendencia general a la formación de partículas de energía intermedia.

Claro está que habría excepciones. Al interaccionar dos partículas de igual energía, puede que una gane energía a expensas de la otra, ampliando así la gama. Incluso es posible (aunque improbable) que una partícula de alta energía gane aún más al chocar con otra de baja energía, dejando a ésta con menos energía que al comienzo.

Teniendo en cuenta la naturaleza fortuita de tales colisiones y de la transferencia de energía, se llega a la conclusión de que habrá toda una gama de energías y de que la mayoría de las partículas serán de energía intermedia; habrá algunas de alta (o baja) energía; pocas de muy alta (o muy baja) energía; un número pequeñísimo de muy, muy alta (o muy, muy baja) energía; y sólo trazas de energía muy, muy, muy alta (o muy, muy, muy baja).

Esta distribución de energías a lo largo de una determinada gama podríamos expresarla matemáticamente, y entonces veríamos que ninguna de las partículas tendría en realidad una energía infinita o una energía nula; habría partículas que se acercarían mucho a estos valores, pero sin alcanzarlos nunca. Algunos taquiones se moverían a veces a una velocidad ligeramente superior a la de la luz, pero sin llegar a ella; y habría otros que quizá se moverían a velocidades verdaderamente gigantescas, un millón (o un billón o un trillón) de veces más deprisa que la luz, pero nunca a velocidades realmente infinitas.

Supongamos que dos taquiones de la misma categoría chocan exactamente de frente. ¿No se anularían entonces las energías cinéticas de ambas partículas, abandonando éstas el lugar de la colisión a velocidad infinita? He aquí de nuevo una situación a la que nos podemos aproximar cuanto queramos, pero sin llegar nunca a ella. La probabilidad de que los dos taquiones tengan exactamente la misma energía y choquen exactamente de frente es despreciable.

Dicho con otras palabras, en el universo de taquiones las velocidades podrían acercarse al infinito pero nunca alcanzarlo; y en ese caso no hay que preocuparse por las paradojas que el infinito siempre parece plantear.

53. ¿Qué es el principio deincertidumbre de Heisenberg?

Antes de explicar la cuestión de la incertidumbre, empecemos por preguntar: ¿qué es la certidumbre? Cuando uno sabe algo de fijo y exactamente acerca de un objeto, tiene certidumbre sobre ese dato, sea cual fuere.

¿Y cómo llega uno a saber una cosa? De un modo o de otro, no hay más remedio que interaccionar con el objeto. Hay que pesarlo para averiguar su peso, golpearlo para ver cómo es de duro, o quizá simplemente mirarlo para ver dónde está. Pero grande o pequeña, tiene que haber interacción.

Pues bien, esta interacción introduce siempre algún cambio en la propiedad que estamos tratando de determinar. O digámoslo así: el aprender algo modifica ese algo por el mismo hecho de aprenderlo, de modo que, a fin de cuentas, no lo hemos aprendido exactamente.

Supongamos, por ejemplo, que queremos medir la temperatura del agua caliente de un baño. Metemos un termómetro y medimos la temperatura del agua. Pero el termómetro está frío, y su presencia en el agua la enfría una chispa. Lo que obtenemos sigue siendo una buena aproximación de la temperatura, pero no exactamente hasta la billonésima de grado. El termómetro ha modificado de manera casi imperceptible la temperatura que estaba midiendo.

O supongamos que queremos medir la presión de un neumático. Para ello utilizamos una especie de barrita que es empujada hacia afuera por una cierta cantidad del aire que antes estaba en el neumático. Pero el hecho de que se escape este poco de aire significa que la presión ha disminuido un poco por el mismo acto de medirla.

¿Es posible inventar aparatos de medida tan diminutos, sensibles e indirectos que no introduzcan ningún cambio en la propiedad medida?

El físico alemán Werner Heisenberg llegó, en 1927, a la conclusión de que no. La pequeñez de un dispositivo de medida tiene un límite. Podría ser tan pequeño como una partícula subatómica, pero no más. Podría utilizar tan sólo un cuanto de energía, pero no menos. Una sola partícula y un solo cuanto de energía son suficientes para introducir ciertos cambios. Y aunque nos limitemos a mirar una cosa para verla, la percibimos gracias a los fotones de luz que rebotan en el objeto, y eso introduce ya un cambio.

Tales cambios son harto diminutos, y en la vida corriente de hecho los ignoramos; pero los cambios siguen estando ahí. E imaginemos lo que ocurre cuando los objetos que estarnos manejando son diminutos y cualquier cambio, por diminuto que sea, adquiere su importancia.

Si lo que queremos, por ejemplo, es determinar la posición de un electrón, tendríamos que hacer rebotar un cuanto de luz en él -o mejor un fotón de rayos gamma- para «verlo». Y ese fotón, al chocar, desplazaría por completo al electrón.

Heisenberg logró demostrar que es imposible idear ningún método para determinar exacta y simultáneamente la posición y el momento de un objeto. Cuanto mayor es la precisión con que determinamos la posición, menor es la del momento, y viceversa. Heisenberg calculó la magnitud de esa inexactitud o «incertidumbre» de dichas propiedades, y ese es su «principio de incertidumbre».

El principio implica una cierta «granulación» del universo. Si ampliamos una fotografía de un periódico, llega un momento en que lo único que vemos son pequeños granos o puntos y perdemos todo detalle. Lo mismo ocurre si miramos el universo demasiado cerca.

Hay quienes se sienten decepcionados por esta circunstancia y lo toman como una confesión de eterna ignorancia. Ni mucho menos. Lo que nos interesa saber es cómo funciona el universo, y el principio de incertidumbre es un factor clave de su funcionamiento. La granulación está ahí, y eso es todo. Heisenberg nos lo ha mostrado y los físicos se lo agradecen.

54. ¿Qué es la paridad?

Supongamos que damos a cada partícula subatómica una de dos etiquetas, la A o la B. Supongamos además que siempre que una partícula A se desintegra en otras dos partículas, éstas son o ambas A o ambas B. Cabría entonces escribir A = A + A o A = B + B. Al desintegrarse una partícula B en otras dos, una de ellas sería siempre A y la otra B, de modo que podríamos escribir B = A + B.

Quizá descubriríamos también otras situaciones. Al chocar dos partículas y desintegrarse en otras tres, podríamos encontrar que A + A = A + B + B, o que A +B – B + B + B.

Pero habría situaciones que nunca observaríamos. No encontraríamos, por ejemplo, que A + B = A + A, ni que A + B + A = B + A + B.

¿Qué significa todo esto? Pues bien, imaginemos que A representa un número entero par (2 ó 4 ó 6) y B cualquier entero impar, como 3, 5 ó 7. La suma de dos enteros pares siempre es un entero par (6 = 2 + 4), de modo que A = A + A. La suma de dos enteros impares siempre es par (8 = 3 + 5), de modo que A = B + B. Sin embargo, la suma de un entero par y otro impar es siempre impar (7 = 3 + 4), de modo que B = A + B.

Dicho con otras palabras, hay ciertas partículas subatómicas que cabría llamar «impares» y otras que cabría llamar «pares», porque, sólo forman aquellas combinaciones y desintegraciones que se cumplen en el caso de sumar enteros pares e impares.

Cuando dos enteros son ambos pares o ambos impares, los matemáticos dicen que «tienen la misma paridad». Si uno de ellos es par y el otro impar, son de «paridad diferente». Por consiguiente, cuando las partículas subatómicas se comportan como si algunas de ellas fuesen pares y otras impares, sin quebrantar nunca las reglas de adición de números pares e impares, se considera que hay «conservación de la paridad».

En 1927, el físico Eugene Wigner demostró que había conservación de la paridad entre las partículas subatómicas, porque podía decirse que dichas partículas poseían una «simetría izquierda-derecha». Los objetos que poseen tal simetría son idénticos a sus imágenes especulares (la imagen reflejada en un espejo). Los numerales 8 y 0 y las letras H y X tienen esa simetría. Si volvemos el 8, el 0, la H o la X de manera que lo que estaba antes a la izquierda esté ahora a la derecha y viceversa, lo que obtendríamos seguiría siendo 8, 0, H y X. Las letras b y p no tienen esa simetría izquierda-derecha. Si les damos la vuelta, la b se convierte en d y la p en q, letras completamente distintas.

En 1956, los físicos Tsung Dao Lee y Chen Ning Yang demostraron que la paridad no se debería conservar en ciertos tipos de sucesos subatómicos, y los experimentos demostraron en seguida que estaban en lo cierto. Según esto, había partículas subatómicas que se comportaban como si no fuesen simétricas bajo ciertas condiciones.

Por esta razón se elaboró una ley de conservación más general. Allí donde una partícula no era simétrica, su antipartícula (de carga eléctrica o campo magnético opuesto) tampoco lo era, pero de manera contraria. Si una partícula era como p, su antipartícula era como q.

Juntando la carga eléctrica (C) y la paridad (P), se puede establecer una regla elemental que nos dice qué sucesos subatómicos pueden tener lugar y cuáles no. Esto es lo que se denomina la «conservación CP».

Más tarde se vio que para que la regla estuviese a salvo de todo riesgo había que considerar también la dirección del tiempo (T); pues hay que señalar que los sucesos subatómicos cabe contemplarlos como si se desarrollaran para adelante o para atrás en el tiempo. Esto es lo que se denomina «conservación CPT».

Hace poco se ha puesto también en tela de juicio la conservación CPT pero hasta ahora no se ha llegado a una decisión final.

55. ¿Por qué se habla de la vidamedia de un isótopo y no de su vida

entera?

Hay átomos que son inestables. Abandonados a su suerte, tarde o temprano experimentan espontáneamente un cambio. De su núcleo saldrá una partícula energética o un fotón de rayos gamma y el átomo se convertirá en otro diferente. (Los isótopos son tipos particulares de átomos.) Una serie de átomos inestables agrupados en un lugar radiarán partículas o rayos gamma en todas direcciones, por lo cual se dice que son radiactivos.

No hay ningún modo de predecir cuándo un átomo radiactivo va a experimentar un cambio. Puede que sea al cabo de un segundo o de un año o de billones de años. Por tanto, es imposible medir la «vida entera» de un átomo radiactivo, es decir el tiempo que permanecerá inalterado. La «vida entera» puede tener cualquier valor, y por consiguiente no tiene sentido hablar de ella.

Pero supongamos que lo que tenemos es una multitud de átomos de un determinado isótopo radiactivo concentrados en un lugar. En cualquier momento dado habrá algunos que estén experimentando un cambio. En esas condiciones se comprueba que aunque es imposible saber cuándo va a cambiar un átomo concreto, sí que se puede predecir que al cabo de tantos segundos cambiarán tantos y tantos átomos de un total de un cuatrillón, pongamos por caso.

Todo es cuestión de estadística. Es imposible saber si Fulanito de tal va a morir o no en un accidente de coche en tal y tal año, pero sí se puede predecir con bastante precisión cuántos habitantes del país van a morir en carretera ese año.

Dado un número grande de átomos de un isótopo determinado, es posible medir la cantidad de radiación en un momento dado y predecir la radiación (el número de átomos que cambian) en cualquier tiempo futuro. Y se comprueba que, en virtud de cómo se producen esos cambios, siempre hace falta el mismo tiempo para que cambien 1/10 de todos los átomos, independientemente de cuántos hubiese al principio. Es más, siempre hace falta el mismo tiempo para que cambien 2/10 de ellos, ó 4/17, ó 19/573, o cualquier otra fracción, independientemente del número inicial de átomos.

Así pues, en lugar de hablar de la «vida entera» de los átomos de un isótopo particular -que carecería de sentido-, se suele hablar del tiempo que tarda en cambiar una fracción determinada de los átomos, lo cual es muy fácil de medir. La fracción más simple es 1/2, y por eso se suele hablar del tiempo que tiene que pasar para que la mitad de los átomos de un isótopo experimenten un cambio. Esa es la «vida media» del isótopo.

Cuanto más estable es un isótopo, menos probable es que sus átomos experimenten un cambio y que un número dado de átomos experimenten un cambio al cabo de una hora, por ejemplo, después de iniciar las observaciones. Esto significa que hace falta más tiempo para que la mitad de los átomos cambien.

Con otras palabras: cuanto más larga es la vida media de un isótopo, tanto más estable; cuanto más corta la vida media, menos estable.

Algunas vidas medias son verdaderamente grandes. El isótopo torio-232 tiene una vida media de catorce mil millones de años. Haría falta todo ese tiempo para que la mitad de cualquier cantidad de torio-232 se desintegrara. Por eso queda todavía tanto torio-232 en la corteza terrestre, pese a que lleva allí (desintegrándose continuamente) casi cinco mil millones de años.

Pero también hay vidas medias que son muy cortas. La del isótopo helio-5 es aproximadamente igual a una cienmillonésima parte de una billonésima de segundo.

56. ¿Por qué están encontrando loscientíficos tantas partículas

subatómicas nuevas y cuál es su

significado?

La clave de la respuesta puede resumirse en una sola frase: «más energía».

Y los físicos estudian la estructura interna del núcleo atómico de una manera muy bruta. Lo bombardean con todas sus fuerzas con partículas subatómicas, destrozan el núcleo en fragmentos y estudian luego los trozos.

Lo que ha cambiado en los últimos treinta años ha sido la energía con que esos «proyectiles» subatómicos irrumpen en el núcleo atómico. En los años treinta tenían energías de millones de electronvoltios; en los cuarenta, de cientos de millones; en los cincuenta, de miles de millones; en los sesenta, de docenas de miles de millones. Y a lo que se ve, en la presente década llegaremos a cientos de miles de millones de electronvoltios

Cuanto mayor es la energía con que se irrumpe en el núcleo, mayor es el número de partículas que salen y mayor es también su inestabilidad. Y sería lógico pensar que a medida que crece la fuerza del impacto, menores deberían ser las partículas resultantes. (Al fin y al cabo, un golpe fuerte parte una roca en dos mitades, pero uno más fuerte aún la dividirá en una docena de trozos pequeños.) En el caso de los núcleos no es así. Las partículas que se obtienen suelen ser bastante pesadas.

Y es que no hay que olvidar que la energía se puede convertir en masa. Las partículas subatómicas que aparecen en un proceso de ruptura de un núcleo no salen de allí como si hubiesen estado alojadas en él desde el principio. Se forman en el momento del impacto a partir de la energía de las partículas que intervienen en el choque. Cuanto mayor es la energía de la partícula entrante, mayor es la masa que podrá tener la partícula formada y mayor será también, por lo general, su inestabilidad.

En cierto modo cabría decir que las partículas subatómicas salen del núcleo destrozado de la misma manera que las chispas salen del acero al golpearlo con un pedernal. Las chispas no estaban desde el principio en el acero, sino que se forman a partir de la energía del golpe.

Pero entonces, ¿tienen algún significado todas estas partículas subatómicas nuevas? ¿No serán meros productos fortuitos de la energía, como las chispas?

Los físicos piensan que no, porque el orden que impera entre ellas es muy grande. Las partículas formadas tienen ciertas propiedades que obedecen determinadas reglas, bastante complicadas. Es decir, las diversas partículas pueden ser representadas mediante números, que a su vez son identificados por nombres como los de «spin isotópico», «rareza», «paridad», etc., y la naturaleza de dichos números viene dictada por limitaciones muy rígidas.

Detrás de estas limitaciones tiene que haber algo, indudablemente.

El físico americano Murray Gell-Mann ha ideado un sistema de ordenar las distintas partículas subatómicas de acuerdo con dichos números en una progresión regular, y gracias a ello ha logrado predecir partículas nuevas, hasta ahora desconocidas. En concreto predijo la existencia de una partícula omega negativa que poseía propiedades bastante improbables; pero después de investigar se comprobó que efectivamente existía y que poseía además dichas propiedades.

Gell-Mann sugiere también que los cientos de partículas que hoy se conocen quedarían ordenadas de modo muy natural de la manera que él ha demostrado, sólo con que estuviesen compuestas de unos cuantos tipos de partículas más elementales llamadas «quarks». Los físicos buscan hoy día con ahínco esos quarks. De encontrarlos, podrían ofrecernos una visión completamente nueva, y quizá muy útil, de la naturaleza fundamental de la materia.

57. ¿Qué es un quark?

La noción de los quarks tuvo su origen en el hecho de que en el último cuarto de siglo se han descubierto un centenar corrido de diferentes tipos de partículas subatómicas. Hay que decir que muy pocas de ellas viven más de una milmillonésima de segundo antes de desintegrarse, pero el mero hecho de su existencia basta para que los físicos anden de cabeza.

¿Por qué hay tantas y todas ellas diferentes? ¿No podrán agruparse en varias familias? Dentro de cada familia, las distintas partículas podrían diferir unas de otras de un modo perfectamente regular. Y entonces no sería necesario explicar la existencia de todas y cada una de las partículas, sino sólo de unas cuantas familias, poniendo así un poco de orden en lo que de otro modo parece una «jungla» subatómica.

El físico americano Murray Gell-Mann y el físico israelita Yuval Ne'emen idearon en 1961, cada uno por su lado, un sistema de organizar las partículas en tales familias. Gell-Mann llegó incluso a presentar una familia que incluía la partícula que él llamó omega negativa, que poseía propiedades muy raras y que jamás había sido observada. Sabiendo qué propiedades debía tener, los físicos sabían también dónde y cómo buscarla. En 1964 la encontraron, descubriendo que era exactamente como Gell-Mann la había descrito.

Estudiando sus familias, Gell-Mann pensó que las distintas partículas subatómicas quizá podrían estar constituidas por combinaciones de unas cuantas partículas aún más elementales, lo cual simplificaría mucho la visión del universo. Según él, postulando tres partículas subatómicas con determinadas propiedades sería posible disponerlas de diferentes modos y obtener así todas las partículas subatómicas conocidas.

La necesidad de combinar tres de estas hipotéticas partículas para construir todas las partículas conocidas le recordó a Gell-Mann un pasaje de la obra Finnegans Wake de James Joyce (libro en que el autor retuerce y distorsiona palabras con fines literarios) que dice: «Three quarks for Musther Mark.»

Y así fue como Gell-Mann llamó «quarks» a esas partículas hipotéticas.

La curioso del caso es que los quarks tendrían que tener cargas eléctricas fraccionarias. Todas las cargas conocidas son iguales o a la de un electrón (- 1), o a la de un protón (+ 1) o a un múltiplo exacto de estas dos. La carga del quark p, sin embargo, sería + 2/3, y las del quark n y quark lambda – 1/3, Un protón, por ejemplo, estaría constituido por un quark n y dos quarks p, un neutrón por dos quarks n y un quark p, etc.

Pero el quark ¿existe realmente o es pura ficción matemática?

Para aclarar la pregunta, consideremos un billete de un dólar. Un billete de un dólar podemos considerarlo igual a diez monedas de diez centavos, pero ¿se trata sólo de una ecuación matemática o es realmente posible que al romper el billete en diez trozos comprobemos que cada uno de éstos es una moneda metálica de diez centavos?

Desde que Gell-Mann propuso la existencia de los quarks, los físicos han intentado localizar indicios de su presencia, pero en vano. En 1969, ciertos informes de Australia hablaron de que entre la lluvia de partículas producida por choques de rayos cósmicos se habían detectado rastros de partículas con carga eléctrica fraccionaria. Pero las pruebas eran sumamente marginales, y la mayoría de los físicos se mostraron muy escépticos hacía el informe.

58. Se ha dicho que los protonesestán constituidos por combinaciones de

tres quarks p y también que un quark

es treinta veces más pesado que un

protón. ¿Cómo pueden ser ciertas

ambas cosas a la vez?

Ambos enunciados pueden ser ciertos. Por lo menos, no tienen por qué ser contradictorios. La clave de esta aparente contradicción está en que la masa es un aspecto de la energía.

Se puede decir que cualquier objeto posee energía cinética respecto a algún sistema de referencia apropiado. La energía cinética es igual a la mitad del producto de la masa del objeto por el cuadrado de su velocidad. Cuando aumenta su energía aumentan también la masa y la velocidad (la segunda principalmente a bajas energías, y la primera sobre todo a energías muy altas).

Señalemos a continuación que cuanto más pequeños son los objetos y más íntimamente unidos están, más fuertes son (por lo general) las fuerzas que los mantienen unidos. Los cuerpos de tamaño verdaderamente grande, como el Sol o la Tierra, mantienen su integridad gracias al campo gravitatorio, que es con mucho la fuerza más débil de las que se conocen.

Los átomos y las moléculas se mantienen unidos por el campo electromagnético, que es mucho más fuerte. Gracias a él mantienen firmemente algunas moléculas su estructura; más firmemente aún los átomos de una molécula; y todavía más, los electrones y núcleos dentro de un átomo.

Las partículas dentro de un núcleo atómico se mantienen juntas gracias a un campo nuclear que es unas cien veces más intenso que el campo electromagnético y de hecho la fuerza más intensa que se conoce. (Por eso son mucho más violentas las explosiones nucleares que las químicas.)

Si los protones (y neutrones) que se encuentran dentro del núcleo están compuestos a su vez por partículas aún más fundamentales -los quarks-, las ligaduras que mantienen unidos a los quarks tendrán que ser mucho más fuertes que las que sujetan a los protones y neutrones. Y en ese sentido puede ser que exista un nuevo campo, mucho más intenso que los que hasta ahora se conocen.

Para romper un protón o un neutrón y descomponerlo en quarks hará falta invertir energías enormes, mucho mayores que las que hacen falta para romper el conglomerado de protones y neutrones que forman el núcleo atómico.

Al desintegrarse el protón o el neutrón, los quarks que aparecen recogen la energía previamente invertida. Parte de ella se manifestaría en la forma de velocidades muy grandes, y otra parte en la forma de una gran masa. En resumen, gracias al empleo de enormes energías, el quark, que dentro del protón sólo tenía un tercio de la masa del protón, se convierte en una partícula mucho más masiva que él.

Los quarks en libertad tendrían una tendencia muy grande a volverse a unir, debido a la insólita intensidad del campo, que les hace experimentar esa atracción mutua. La reunificación liberaría grandes cantidades de energía, y la pérdida de ésta se traduciría en una pérdida de masa. La masa de los quarks se reduciría entonces lo suficiente como para hacer que la combinación de tres de ellos no tuviera una masa mayor que la de un protón.

Hoy día los físicos no disponen de la energía necesaria para dividir las partículas subatómicas en quarks, por lo cual no es fácil comprobar si la hipótesis es buena o no. Pero hay algunas partículas de los rayos cósmicos que sí tienen tales energías, y en la actualidad se están buscando los quarks en las lluvias de partículas que aquéllas producen al chocar con átomos.

59. En la bomba atómica se conviertemateria en energía. ¿Es posible hacer

lo contrario y convertir energía en

materia?

Sí que es posible convertir energía en materia, pero hacerlo en grandes cantidades resulta poco práctico. Veamos por qué.

Según la teoría especial de la relatividad de Einstein, tenemos que e = mc2, donde e representa la energía, medida en ergios, m representa la masa en gramos y c es la velocidad de la luz en centímetros por segundo.

La luz se propaga en el vacío a una velocidad muy próxima a los 30.000 millones (3 ´ 1010) de centímetros por segundo. La cantidad c2 representa el producto c ´ c, es decir, 3 ´ 1010 ´ 3 ´ 1010 ó 9 ´ 1020. Por tanto, c2 es igual a 900.000.000.000.000.000.000.

Así pues, una masa de un gramo puede convertirse, en teoría en 9 x 1020 ergios de energía.

El ergio es una unidad muy pequeña de energía. La kilocaloría, de nombre quizá mucho más conocido, es igual a unos 42.000 millones de ergios. Un gramo de materia, convertido a energía, daría 2,2 ´ 1010 (22.000 millones) de kilocalorías. Una persona puede sobrevivir cómodamente con 2.500 kilocalorías al día, obtenidas de los alimentos ingeridos. Con la energía que representa un solo gramo de materia tendríamos reservas para unos 24.110 años, que no es poco para la vida de un hombre.

O expresémoslo de otro modo: si fuese posible convertir en energía eléctrica la energía representada por un solo gramo de materia bastaría para tener luciendo continuamente una bombilla de 100 vatios durante unos 28.200 años.

O bien: la energía que representa un solo gramo de materia equivale a la que se obtendría de quemar unos 32 millones de litros de gasolina.

Nada tiene de extraño, por tanto, que las bombas nucleares, donde se convierten en energía cantidades apreciables de materia, desaten tanta destrucción.

La conversión opera en ambos sentidos. La materia se puede convertir en energía, y la energía en materia. Esto último puede hacerse en cualquier momento en el laboratorio. Una partícula muy energética de energía -un fotón de rayos gamma- puede convertirse en un electrón y un positrón sin grandes dificultades. Con ello se invierte el proceso, convirtiéndose energía en materia.

Ahora bien la materia formada se reduce a dos partículas ligerísimas, de masa casi despreciable. ¿Podrá utilizarse el mismo principio para formar una cantidad mayor de materia, lo suficiente para que resulte visible?

¡Ah! Pero la aritmética es implacable. Si un gramo de materia puede convertirse en una cantidad de energía igual a la que produce la combustión de 32 millones de litros de gasolina, entonces hará falta toda esa energía para fabricar un solo gramo de materia.

Aun cuando alguien estuviese dispuesto a hacer el experimento y correr con el gasto de reunir toda esa energía (y quizás varias veces más, a fin de cubrir pérdidas inevitables) para formar un gramo de materia, no lo conseguiría. Sería imposible producir y concentrar toda esa energía en un volumen suficientemente pequeño para producir de golpe un gramo de materia.

Así pues, la conversión es posible en teoría, pero completamente inviable en la práctica. En cuanto a la materia del universo, se supone, desde luego, que se produjo a partir de energía, pero en unas condiciones que sería imposible reproducir hoy día en el laboratorio.

60. Las antipartículas ¿producenantienergía?

A principios del siglo XX los físicos empezaron a comprender que toda la materia consistía en determinadas clases de partículas. El físico inglés Paul Dirac, que trabajaba en la teoría de esas partículas, llegó en 1930 a la conclusión de que cada clase tenía que tener su opuesta.

El electrón, por ejemplo, tiene una carga eléctrica negativa y el protón una carga eléctrica positiva exactamente igual, pero las dos partículas no son opuestas, porque la masa del protón es mucho mayor que la del electrón.

Según Dirac, tenía que haber una partícula con la misma masa que el electrón pero con carga eléctrica positiva, y otra con la misma masa que el protón pero con carga eléctrica negativa. Ambas fueron descubiertas en su día, de modo que hoy conocemos el «antielectrón» (o «positrón») y el «antiprotón».

El neutrón no tiene carga eléctrica, pero en cambio posee un campo magnético que apunta en una determinada dirección. Y existe el «antineutrón», que tampoco tiene carga eléctrica pero cuyo campo magnético apunta en la dirección opuesta.

Pues bien, lo siguiente es al parecer una ley de la naturaleza: una partícula puede convertirse en otra, pero siempre que se forma una partícula sin la existencia previa de otra, tiene que formarse simultáneamente una antipartícula.

He aquí un ejemplo. Un neutrón puede convertirse en un protón, lo cual es perfectamente admisible, porque lo único que ha sucedido es que una partícula se ha convertido en otra. Pero en esa conversión se forma también un electrón. Es decir, una partícula se ha convertido en dos. Para contrarrestar esa segunda partícula se forma una diminuta antipartícula llamada «antineutrino».

Una partícula (el neutrón) se ha convertido en otra (el protón) más un par partícula / antipartícula (el electrón y el antineutrino).

A partir de energía se pueden formar pares partícula / antipartícula, que a su vez pueden volver a convertirse en energía en cualquier número. De energía no podemos sacar una partícula sola, ni una única antipartícula, pero sí un par.

Como la propia energía está formada de «fotones», se plantea entonces el problema de si el fotón es una partícula o una antipartícula. No parece que haya ningún modo de convertir un fotón en un electrón, por lo cual no puede ser una partícula; ni tampoco de convertirlo en un antielectrón, por lo cual tampoco puede ser una antipartícula.

Sin embargo, un fotón de rayos gamma suficientemente energético sí puede convertirse en un par electrón / antielectrón. Parece, pues, que el fotón no es ni una partícula ni una antipartícula, sino un par partícula / antipartícula.

Todo fotón es a la vez un antifotón, o digámoslo así, un fotón es su propio opuesto.

También podríamos mirarlo del siguiente modo. Doblemos una hoja de papel por la mitad y escribamos los nombres de todas las partículas en un lado y los de las antipartículas en el otro. ¿Dónde pondríamos al fotón? Pues justo en el doblez.

Por eso la energía producida por un mundo de partículas consiste en fotones, igual que la producida por un mundo de antipartículas. La energía es la misma en ambos casos y, por lo que sabemos hoy día, no hay nada que podamos llamar antienergía.

61. ¿En qué difieren las propiedadesde los rayos cósmicos de las de los

neutrinos?

Los rayos cósmicos son partículas subatómicas muy rápidas, de gran masa y cargadas positivamente. Aproximadamente el 90 por 100 de las partículas son protones (núcleos de hidrógeno) y el 9 por 100, partículas alfa (núcleos de helio). El 1 por 100 restante son núcleos de átomos más complejos. Hasta se han detectado núcleos de átomos tan complejos como el hierro, cuya masa es 56 veces mayor que la del protón.

Las partículas de los rayos cósmicos, por tener tanta masa y moverse con velocidades tan grandes (casi la velocidad de la luz), son muy energéticas. De hecho son las partículas más energéticas que conocemos, algunas de ellas miles de millones de veces más que las que se pueden producir en los mayores aceleradores.

Las partículas de los rayos cósmicos, al estrellarse contra la atmósfera terrestre, rompen todos los átomos que encuentran a su paso y producen una lluvia de «radiación secundaria» que, entre otras cosas, incluye mesones y positrones. La radiación se estrella finalmente contra la Tierra propiamente dicha, penetrando parte de ella varios metros en el suelo antes de ser absorbida.

Tales partículas pueden originar cambios en los átomos que encuentran, sin excluir los del cuerpo humano. Y es posible que dichos cambios ocasionen enfermedades como la leucemia e induzcan mutaciones. Pero la probabilidad de que un individuo dado sufra una desgracia de este tipo es pequeña, porque la mayoría de las partículas de rayos cósmicos que chocan contra una persona pasan de largo sin ocasionar ningún daño.

La fuente exacta de las partículas de los rayos cósmicos y la manera en que adquieren energías tan enormes son dos temas aún debatidos.

En cualquier reacción nuclear que produzca electrones, positrones o muones se producen también neutrinos. Las reacciones nucleares que se desarrollan en el Sol, por ejemplo, producen grandes cantidades de positrones y, por tanto, también de neutrinos.

Los neutrinos, que se mueven a la velocidad de la luz, son aún más rápidos que las partículas de los rayos cósmicos pero mucho menos energéticos, porque carecen de carga eléctrica y de masa. Los neutrinos no son absorbidos por la materia a menos que choquen frontalmente contra un núcleo atómico, y este suceso es tan raro que por término medio pueden atravesar billones de kilómetros de plomo sin ser absorbidos.

Los billones de neutrinos que produce el Sol cada segundo se esparcen en todas direcciones. Los que, por casualidad, salen en dirección a la Tierra llegan hasta nosotros y pasan a través del planeta como si no estuviera allí. Los neutrinos nos bombardean día y noche durante toda nuestra vida. Pero como nos atraviesan sin ser absorbidos, no nos afectan para nada.

Claro está que, por un golpe de suerte, puede ser que un neutrino choque frontalmente contra un núcleo atómico muy cerca de nosotros. En ese caso puede ser detectado. Durante los años cincuenta, los físicos aprendieron a aprovechar estos rarísimos casos. Los neutrinos pueden servir, para proporcionarnos información acerca del interior de las estrellas (donde se forman), que de otro modo no podríamos conocer.

62. ¿Qué peligro encierran los rayoscósmicos para los hombres en el

espacio?

Allá por el año 1911, el físico austríaco Víctor F. Hess descubrió que la Tierra estaba siendo bombardeada por una radiación muy penetrante que venía del espacio exterior. En 1925, el físico americano Robert A. Millikan le puso el nombre de «rayos cósmicos», dado que venían del «cosmos», del universo.

Más tarde se descubrió que los rayos cósmicos consistían en núcleos atómicos de velocidad muy alta, dotados de una carga eléctrica positiva. Un 90 por 100 de ellos eran protones (núcleos de átomos de hidrógeno) y el 9 por 100 partículas alfa (núcleos de átomos de helio). El 1 por 100 restante son núcleos más complicados y de mayor masa, algunos tan grandes como el de hierro, que es 56 veces más pesado que el protón.

Los veloces núcleos que chocan contra la atmósfera exterior de la Tierra constituyen la «radiación primaria». Chocan con las moléculas de aire, las desintegran y producen una serie de partículas casi tan energéticas como la radiación primaria. Estas nuevas partículas, extraídas violentamente de las moléculas de aire, constituyen la «radiación secundaria».

Parte de la radiación llega hasta la superficie de la Tierra y penetra varios pies en la corteza. Algo de ella atraviesa también los cuerpos humanos, pudiendo ocasionar daños en las células: tal puede ser uno de los factores que producen mutaciones en los genes. Una cantidad suficiente de esta radiación podría dañar células bastantes para matar a una persona, pero lo cierto es que aquí abajo, en la parte inferior de la atmósfera, no llega tanta radiación como para eso. Las criaturas vivientes han sobrevivido miles de millones de años al bombardeo de los rayos cósmicos.

El origen de los rayos cósmicos es un tema muy debatido, pero se sabe que por lo menos parte de ellos se forman en las estrellas ordinarias. En 1942 se descubrió que el Sol produce rayos cósmicos benignos cuando una erupción solar abate su superficie.

La atmósfera superior absorbe gran parte del impacto de las partículas de los rayos cósmicos normales, mientras que la radiación secundaria es absorbida en parte más abajo. De la energía inicial de radiación, sólo sobrevive y llega hasta la superficie terrestre una pequeña fracción.

Pero allá fuera, en el espacio, tienen que contar con la posibilidad de enfrentarse con la furia desatada de la radiación primaria. Y aquí no sirve de nada un blindaje. Las partículas de los rayos cósmicos, al chocar con los átomos del blindaje, provocarían una radiación secundaria que saldría disparada en todas direcciones como metralla. Una elección equivocada del material de blindaje empeoraría incluso aún más las cosas.

La magnitud del peligro depende de la actividad de rayos cósmicos que haya en el espacio exterior, sobre todo del número de partículas de gran masa, que serían las más nocivas. Los Estados Unidos y la Unión Soviética han enviado numerosos satélites al espacio para comprobar las cantidades de rayos cósmicos, y parece ser que en condiciones normales son suficientemente bajas como para que haya una relativa seguridad.

La fuente más probable de peligro son los rayos cósmicos débiles del Sol. La atmósfera terrestre los detiene casi por completo, pero en el caso de los astronautas no hay nada que les preste ese servicio. Y aunque son débiles, en cantidad suficiente pueden ser peligrosos. Los rayos cósmicos del Sol sólo se presentan en abundancia cuando se produce una erupción solar. Las erupciones solares no son, por suerte, muy frecuentes, pero por desgracia es imposible predecirlas.

Así pues, lo único que podemos hacer mientras los astronautas se encuentran en la Luna es esperar que durante esa semana o par de semanas no se produzca ninguna gran erupción que lance sobre ellos partículas de rayos cósmicos.

63. Los neutrinos ¿son materia oenergía?

Los físicos del siglo xix creían que la materia y la energía eran dos cosas completamente diferentes. Materia era todo aquello que ocupaba un espacio y que poseía masa. Y al tener masa, tenía también inercia y respondía al campo gravitatorio. La energía, en cambio, no ocupaba espacio ni tenía masa, pero podía efectuar trabajo. Además se pensaba que la materia consistía en partículas (átomos), mientras que la energía se componía de ondas.

Por otro lado, los físicos del siglo xix creían que ni la materia ni la energía, cada una por su parte, podía ser creada ni destruida. La cantidad total de materia del universo era constante, igual que la cantidad total de energía. Había, pues, una ley de conservación de la energía y una ley de conservación de la materia.

Albert Einstein demostró más tarde, en 1905, que la masa es una forma muy concentrada de energía. La masa podía convertirse en energía y viceversa. Lo único que había que tener en cuenta era la ley de conservación de la energía. En ella iba incluida la materia.

Hacia los años veinte se vio además que no se podía hablar de partículas y ondas como si fuesen dos cosas diferentes. Lo que solemos considerar partículas actúa en ciertos aspectos como ondas. Y lo que normalmente consideramos como ondas, actúa en ciertos aspectos como partículas. Así pues, podemos hablar de «ondas del electrón», por ejemplo; y también de «partículas de luz», o «fotones».

Una diferencia sí que sigue habiendo. Las partículas de materia pueden hallarse en reposo respecto a un observador. Aun estando en reposo, poseen masa. Poseen una «masa en reposo», mayor que cero.

Las partículas como los fotones, por el contrario, tienen una masa en reposo nula. Estando en reposo respecto a nosotros, no podríamos medir masa alguna. Pero eso es pura teoría, porque las partículas que tienen una masa en reposo nula jamás pueden estar paradas respecto a ningún observador. Esas partículas se mueven siempre a una velocidad de 299.793 kilómetros por segundo a través del vacío. Tan pronto como nacen, empiezan a moverse a esa velocidad.

Precisamente porque los fotones se mueven siempre a 299.793 kilómetros por segundo (en el vacío) y porque la luz está compuesta de fotones, llamamos a esta velocidad la «velocidad de la luz».

¿Y los neutrinos? Los neutrinos se forman en ciertas reacciones nucleares y ningún físico atómico ha sido hasta ahora capaz de medir su masa. Es muy probable que los neutrinos, como los fotones, tengan una masa en reposo nula.

En ese caso, los neutrinos viajan siempre a 299.793 kilómetros por segundo y adquieren esa velocidad en el instante en que se forman.

Pero los neutrinos no son fotones, porque ambos tienen propiedades muy distintas. Los fotones interaccionan fácilmente con las partículas de materia, y son retardados y absorbidos (a veces muy rápidamente) al pasar por la materia. Los neutrinos, por el contrario, apenas interaccionan con las partículas de materia y pueden atravesar un espesor de años-luz de plomo sin verse afectados.

Parece claro, por tanto, que si los neutrinos tienen una masa en reposo nula, no son materia. Por otro lado, hace falta energía para formarlos, y al alejarse se llevan algo de ella consigo, de modo que son una forma de energía.

Sin embargo, atraviesan cualquier espesor de materia sin interaccionar apenas, de modo que prácticamente no efectúan trabajo. Lo cual les distingue de cualquier otra forma de energía. Lo mejor quizá sea llamarlos simplemente… neutrinos.

64. ¿Cómo funciona una cámara deburbujas?

La cámara de burbujas es un dispositivo para detectar partículas subatómicas. Fue inventada en 1952 por el físico americano Donald A. Glaser, quien en 1960, recibió por ello el Premio Nóbel de Física.

En esencia es un depósito de líquido a una temperatura superior a su punto de ebullición. El líquido se halla bajo presión, con lo cual se impide que hierva. Pero si se disminuye la presión, el líquido entra en ebullición y aparecen en él burbujas de vapor.

Imaginemos que una partícula subatómica -un protón o un mesón, por ejemplo- se zambulle en el líquido de una cámara de burbujas. Choca contra las moléculas y átomos del líquido y les transfiere una parte de su energía, formándose así una línea de átomos y moléculas de temperatura superior al resto. Si se retira la presión que actúa sobre el líquido, las burbujas de vapor se forman en primer lugar a lo largo de la línea de energía que ha dejado atrás la partícula subat6mica. El paso de las partículas queda así marcado por un trazo visible de burbujas que se puede fotografiar fácilmente.

Este rastro visible revela a los físicos muchas cosas, sobre todo si la cámara de burbujas está colocada entre los polos de un potente imán. Las partículas capaces de dejar un rastro de burbujas poseen siempre una carga eléctrica, o positiva o negativa. Si la carga es positiva, la trayectoria de la partícula se curva en una determinada dirección bajo la influencia del imán; si es negativa, se curva en la dirección contraria. Por la curvatura de la curva puede determinar el físico la velocidad de la partícula. Con esto, con el espesor de la traza y otros datos, puede determinar también la masa de la partícula.

Cuando una partícula se desintegra en dos o más partículas, la traza se ramifica. También aparecen ramales en el caso de una colisión. En las fotografías de una cámara de burbujas aparecen normalmente numerosos trazos que convergen, se separan y se ramifican. Hay veces que entre dos porciones del conjunto de trazas se observa un hueco. Lo que hay que hacer entonces es llenar ese hueco con alguna partícula sin carga; pues las partículas que carecen de carga no dejan traza alguna al pasar por una cámara de burbujas.

Para el físico nuclear, la compleja trama de trazos de una fotografía tomada en una cámara de burbujas es tan significativa como los rastros en la nieve para un cazador ducho. De la naturaleza de las trazas puede deducir el físico qué clase de partículas han intervenido o si ha encontrado un nuevo tipo de partícula.

La primera cámara de burbujas que construyó Glaser tenía sólo unas cuantas pulgadas de diámetro. Hoy día, en cambio, se construyen cámaras enormes de muchos pies de diámetro, que contienen cientos de litros de líquido.

Los líquidos que se usan en las cámaras de burbujas pueden ser de varios tipos. Algunos contienen gases nobles licuados, como el xenón o el helio. Otros, gases orgánicos licuados.

Pero el líquido más útil para las cámaras de burbujas es el hidrógeno líquido. El hidrógeno está compuesto por los átomos más elementales que se conocen. Cada átomo de hidrógeno consiste en un núcleo constituido por un único protón, alrededor del cual gira un solo electrón. Es decir, el hidrógeno líquido está formado sólo por protones y electrones aislados. Los núcleos atómicos de todos los demás líquidos son conglomerados de varios protones y neutrones.

Como consecuencia de lo anterior, los sucesos subatómicos que tienen lugar en el seno del hidrógeno líquido son especialmente simples y tanto más fáciles de deducir a partir de las trazas de burbujas.

65. ¿Qué es un reactor generador?

El uranio 235 es un combustible práctico. Es decir, los neutrones lentos son capaces de hacer que el uranio 235 se fisione (se rompa en dos), produciendo más neutrones lentos, que a su vez inducen otras fisiones atómicas, etc. El uranio 233 y el plutonio 239 son también combustibles nucleares prácticos por las mismas razones.

Por desgracia, el uranio 233 y el plutonio 239 no existen en estado natural sino en trazas mínimas, y el uranio 235, aunque existe en cantidades apreciables, no deja de ser raro. En cualquier muestra de uranio natural, sólo siete de cada mil átomos son uranio 235. El resto es uranio 238.

El uranio 238, la variedad común de uranio, no es un combustible nuclear práctico. Puede conseguirse que se fisione, pero sólo con neutrones rápidos. Los átomos de uranio 238 que se rompen en dos producen neutrones lentos, que no bastan para inducir nuevas fisiones. El uranio 238 cabría compararlo a la madera húmeda: es posible hacer que arda, pero acabará por apagarse.

Supongamos, sin embargo, que se separa el uranio 235 del uranio 238 (trabajo más bien difícil) y que se utiliza aquél para hacer funcionar un reactor nuclear. Los átomos de uranio 235 que forman el combustible del reactor se fisionan y esparcen miríadas de neutrones lentos en todas las direcciones. Si el reactor está rodeado por una capa de uranio ordinario (que en su mayor parte es uranio 238), los neutrones que van a parar allí son absorbidos por el uranio 238. Dichos neutrones no pueden hacer que el uranio 238 se fisione, pero provocarán otros cambios que, al final, producirán plutonio 239. Separando este plutonio 239 del uranio (trabajo bastante fácil), puede utilizarse como combustible nuclear práctico.

Un reactor nuclear que genera así nuevo combustible para reponer el usado es un «reactor generador». Un reactor generador de diseño adecuado produce más plutonio 239 que el uranio 235 consumido. De este modo, las reservas totales de uranio de la Tierra (y no sólo las de uranio 235) se convierten en potenciales depósitos de combustible.

El torio, tal como se da en la naturaleza, consiste todo él en torio 232, que, al igual que el uranio 238, no es un combustible nuclear práctico, porque requiere neutrones rápidos para fisionarse. Pero si se coloca torio 232 alrededor de un reactor nuclear sus átomos reabsorberán, los neutrones y, sin experimentar fisión alguna se convertirán en átomos de uranio 233. Como el uranio 233 es un combustible práctico que se puede separar fácilmente del torio, el resultado es otra variedad de reactor generador, en este caso un reactor que convierte las reservas de torio en un combustible nuclear potencial.

La cantidad total de uranio y de torio que hay en la Tierra es unas 800 veces mayor que las reservas de uranio 235, lo cual significa que el buen uso de los reactores generadores podría multiplicar por 800 la oferta potencial de energía a través de plantas de fisión nuclear.

66. ¿Cuánto y durante cuánto tiempohay que calentar el hidrógeno para

mantener una reacción de fusión?

Al calentar el hidrógeno a temperaturas cada vez más altas, pierde energía por radiación a una velocidad cada vez mayor, Si la temperatura sigue aumentando, los átomos de hidrógeno pierden sus electrones, dejando que los núcleos desnudos choquen unos contra otros y se fundan. Esta fusión produce energía, Si la temperatura sigue subiendo, la cantidad de energía producida por fusión es cada vez mayor.

La cantidad de energía producida por la fusión aumenta más deprisa con la temperatura que la pérdida de energía por radiación. Al alcanzar cierta temperatura crítica, la energía producida por la fusión llega a ser igual a la perdida por radiación. En ese momento la temperatura se estabiliza y la reacción de fusión se automantiene. Con tal de suministrar hidrógeno al sistema, éste producirá energía a un ritmo constante.

La temperatura requerida varía con el tipo de hidrógeno. El tipo más común es el hidrógeno (H) con un núcleo compuesto por un solo protón. Después está el hidrógeno pesado, o deuterio (D), con un núcleo compuesto por un protón y un neutrón, y el hidrógeno radiactivo, o tritio (T), con un núcleo de un protón y dos neutrones.

La cantidad de energía producida por fusión nuclear a una temperatura dada con D es mayor que con H y menor que con T.

Las fusiones con H producen tan poca energía que haría falta una temperatura de más de mil millones de grados para mantener la reacción en el laboratorio. Es cierto que lo que se funde en el centro del Sol, donde la temperatura alcanza sólo los 15.000.000 de grados, es H, pero a una temperatura tan baja sólo se funde una proporción diminuta del hidrógeno. Sin embargo, la cantidad de H que hay en el Sol es tan ingente que aun esa diminuta proporción basta para mantener la radiación solar.

La fusión que menos temperatura requiere para iniciarse es la de T: basta unos cuantos millones de grados. Desgraciadamente el tritio es inestable y apenas se da en la naturaleza. Habría que formarlo en el laboratorio casi por encargo, y aun así sería imposible mantener a base de tritio la cantidad de reacciones de fusión que necesita la Tierra.

La fusión del deuterio tiene una temperatura de ignición de 400.000.000º C. El deuterio es estable pero raro; sólo un átomo de cada 6.700 es de deuterio. Pero tampoco exageremos. En un litro de agua ordinaria hay suficiente deuterio para producir por fusión una energía equivalente a la combustión de 67 litros de gasolina.

Un modo de alcanzar la temperatura necesaria es mediante la adición de algo de tritio para que actúe de detonador. La fusión de deuterio con tritio puede iniciarse a los 45.000.000º C. Si se logra prender un poco de la mezcla, el resto se calentaría lo suficiente para que el deuterio pudiese arder él solo.

El tiempo que hay que mantener la temperatura depende de la densidad del hidrógeno. Cuantos más átomos por centímetro cúbico, tantas más colisiones y más rápida la ignición. Si hay 1015 átomos por centímetro cúbico (una diezmilésima del número de moléculas por centímetro cúbico del aire normal), la temperatura habría que mantenerla durante dos segundos.

Claro está que cuanto mayor es la densidad y más alta la temperatura, tanto más difícil es mantener el deuterio en su sitio, incluso durante ese tiempo tan breve. Los sistemas de fusión han ido avanzando poco a poco durante todos estos años, pero aún no se han conseguido las condiciones para la ignición.

67. ¿Cómo funciona un microscopioelectrónico?

Antes de contestar, planteemos la siguiente pregunta: ¿cómo determinamos el tamaño de un objeto?

Los rayos de luz que nos llegan desde los dos lados opuestos de un objeto forman un ángulo en la retina. Por el tamaño de ese ángulo determinamos el tamaño aparente del objeto.

Pero si esos rayos luminosos pasan por una lente convexa antes de incidir en el ojo, se doblan de tal manera que el ángulo formado en la retina se hace mayor. Por consiguiente, el objeto que vemos a través de la lente aparece agrandado, igual que cada una de sus partes. Lo que tenemos es una lupa.

Utilizando una combinación de varias lentes se puede aumentar un objeto miles de veces y ver claramente detalles que son demasiado pequeños para distinguirlos a simple vista. Lo que tenemos entonces es un «microscopio óptico», que trabaja con ondas luminosas. Con ayuda de él podemos ver objetos tan pequeños como una bacteria.

¿No podríamos apilar lente sobre lente y construir así un microscopio de tantos aumentos que nos permitiese ver objetos incluso menores que las bacterias, átomos por ejemplo?

Desgraciadamente, no. Ni siquiera utilizando lentes perfectas en perfecta combinación. La luz está compuesta de ondas de una cierta longitud (aprox. 1/20.000 de centímetro) y nuestra vista no podrá ver con claridad nada que sea más pequeño que eso. Las ondas luminosas son suficientemente grandes para «saltar» por encima de cualquier cosa menor que ellas.

Es cierto que hay formas de luz cuya longitud de onda es mucho más pequeña que la de la luz ordinaria. La de los rayos X es diez mil veces menor que la de la luz. Pero, por desgracia, los rayos X atraviesan directamente los objetos que tratamos de ver.

Mas no hay que afligirse, porque también tenemos a los electrones. Los electrones son partículas, pero también pueden comportarse como ondas, Tienen una longitud de onda más o menos igual a la de los rayos X y no atraviesan los objetos que estamos tratando de ver.

Imaginemos que lanzamos un haz de luz sobre un objeto. El objeto absorbe la luz y proyecta una sombra, y entonces vemos el objeto por el contraste de la luz con la sombra. Si se proyecta un haz de electrones sobre un objeto, éste absorberá los electrones y proyectará una «sombra electrónica». Y como sería peligroso intentar poner los ojos delante de un haz de electrones, lo que hacemos es colocar una película fotográfica. La sombra electrónica nos mostrará la forma del objeto, e incluso sus detalles, si hay partes que absorben los electrones con más intensidad que otras.

Pero ¿y si el objeto es muy pequeño? Si estuviésemos manejando haces de luz, podríamos utilizar lentes para doblar dichos haces y aumentar el tamaño del objeto. Para doblar un haz de electrones no podemos utilizar lentes ordinarias, pero sí otra cosa. Los electrones portan una carga eléctrica, y esto significa que, dentro de un campo magnético, seguirán una trayectoria curva. Utilizando un campo magnético de intensidad y forma adecuadas podemos manejar un haz de electrones de la misma manera que podemos manejar uno de luz con ayuda de una lente corriente.

Lo que tenemos, en resumen, es un «microscopio electrónico», que utiliza haces de electrones exactamente igual que un «microscopio óptico» utiliza haces luminosos.

La diferencia es que los electrones tienen una longitud de onda muchísimo más corta que la luz ordinaria, de modo que el microscopio electrónico es capaz de mostrarnos objetos del tamaño de los virus, mientras que el óptico no.

68. ¿Qué es la entropía?

La energía sólo puede ser convertida en trabajo cuando, dentro del sistema concreto que se esté utilizando, la concentración de energía no es uniforme. La energía tiende entonces a fluir desde el punto de mayor concentración al de menor concentración, hasta establecer la uniformidad. La obtención de trabajo a partir de energía consiste precisamente en aprovechar este flujo.

El agua de un río está más alta y tiene más energía gravitatoria en el manantial que en la desembocadura. Por eso fluye el agua río abajo hasta el mar. (Si no fuese por la lluvia, todas las aguas continentales fluirían montaña abajo hasta el mar y el nivel del océano subiría ligeramente. La energía gravitatoria total permanecería igual, pero estaría distribuida con mayor uniformidad.)

Una rueda hidráulica gira gracias al agua que corre ladera abajo: ese agua puede realizar un trabajo. El agua sobre una superficie horizontal no puede realizar trabajo, aunque esté sobre una meseta muy alta y posea una energía gravitatoria excepcional. El factor crucial es la diferencia en la concentración de energía y el flujo hacia la uniformidad.

Y lo mismo reza para cualquier clase de energía. En las máquinas de vapor hay un depósito de calor que convierte el agua en vapor, y otro depósito frío que vuelve a condensar el vapor en agua. El factor decisivo es esta diferencia de temperatura. Trabajando a un mismo y único nivel de temperatura no se puede extraer ningún trabajo, por muy alta que sea aquélla.

El término «entropía» lo introdujo el físico alemán Rudolf J. E. Clausius en 1850 para representar el grado de uniformidad con que está distribuida la energía, sea de la clase que sea. Cuanto más uniforme, mayor la entropía. Cuando la energía está distribuida de manera perfectamente uniforme, la entropía es máxima para el sistema en cuestión.

Clausius observó que cualquier diferencia de energía dentro de un sistema tiende siempre a igualarse por sí sola. Si se coloca un objeto caliente en contacto con otro frío, el calor fluye de manera que el primero se enfría y el segundo se calienta, hasta que ambos quedan a la misma temperatura. Si tenemos dos depósitos de agua comunicados entre sí y el nivel de uno de ellos es más alto que el del otro, la atracción gravitatoria hará que el primero baje y el segundo suba, hasta que ambos niveles se igualan y la energía gravitatoria queda distribuida uniformemente.

Clausius afirmó por tanto que en la naturaleza era regla general que las diferencias en las concentraciones de energía tendían a igualarse. O digámoslo así: que «la entropía aumenta con el tiempo».

El estudio del flujo de energía desde puntos de alta concentración a otros de baja concentración se llevó a cabo de modo especialmente complejo en relación con la energía térmica. Por eso, el estudio del flujo de energía y de los intercambios de energía y trabajo recibió el nombre de «termodinámica», que en griego significa «movimiento de calor».

Con anterioridad se había llegado ya a la conclusión de que la energía no podía ser destruida ni creada. Esta regla es tan fundamental que se la denomina «primer principio de la termodinámica».

La idea sugerida por Clausius de que la entropía aumenta con el tiempo es una regla general no menos básica, y se denomina «segundo principio de la termodinámica».

69. ¿Está degradándose el universo?

Según el «segundo principio de la termodinámica», la entropía aumenta constantemente, lo cual significa que las diferencias en la concentración de energía también van desapareciendo. Cuando todas las diferencias en la concentración de energía se han igualado por completo, no se puede extraer más trabajo, ni pueden producirse cambios.

Pensemos en un reloj. Los relojes andan gracias a una concentración de energía en su resorte o en su batería. A medida que el resorte se destensa o la reacción química de la batería avanza, se establece un flujo de energía desde el punto de alta concentración al de baja concentración, y como resultado de este flujo anda el reloj. Cuando el resorte se ha destensado por completo o la batería ha finalizado su reacción química, el nivel de energía es uniforme en todo el reloj, no hay ya flujo de energía y la maquinaria se para. Podríamos decir que el reloj se ha «degradado». Por analogía decimos que el universo se «degradará» cuando toda la energía se haya igualado.

Claro está que podemos dar otra vez cuerda al reloj o comprar una batería nueva. Para dar cuerda al reloj utilizamos nuestra fuerza muscular, «degradándonos» nosotros mismos un poco. O si compramos un batería nueva, esa batería habrá que fabricarla, y para fabricarla es preciso que la industria del hombre se «degrade» un poco.

Comiendo podemos reponer la fuerza muscular gastada, pero los alimentos provienen en último término de las plantas, que utilizan la luz solar. La industria del hombre funciona principalmente a base de carbón y petróleo, que fueron formados por plantas, en épocas remotas, a partir de la energía solar. A medida que las cosas se degradan, podemos volver a «darles cuerda» utilizando algo que, por lo general, se remonta a la energía del Sol y al modo en que éste se está «degradando».

El Sol se compone en su mayor parte de hidrógeno, elemento que contiene mucha más energía por partícula que otros átomos más complicados, como el helio, el oxígeno y el hierro. Dentro del Sol se está produciendo constantemente una gradual uniformización de la concentración de energía a medida que el hidrógeno se convierte en átomos más complicados. (En las plantas de energía atómica de la Tierra ocurre otro tanto, a medida que los átomos de uranio se convierten en átomos menos complejos. Si algún día se llegan a construir plantas de fusión de hidrógeno, lo que estaríamos haciendo sería copiar en cierto modo lo que ocurre en el Sol.)

Por lo que hoy sabemos, llegará un momento en que las concentraciones de energía del Sol se igualarán, quedando sólo átomos de tamaño intermedio. Y lo mismo reza para todas las demás estrellas del universo y en general para todo lo que hay en él.

Si es cierto el segundo principio de la termodinámica, todas las concentraciones de energía en todos los lugares del universo se están igualando, y en ese sentido el universo se está degradando. La entropía alcanzará un máximo cuando toda la energía del universo esté perfectamente igualada; a partir de entonces no ocurrirá nada, porque aunque la energía seguirá allí, no habrá ya ningún flujo que haga que las cosas ocurran.

La situación parece deprimente (si es que el segundo principio es cierto en todo tipo de condiciones), pero tampoco hace falta alarmarse antes de tiempo. El proceso tardará billones de años en llegar a su fin, y el universo, tal como hoy existe, no sólo sobrevivirá a nuestro tiempo, sino con toda probabilidad a la humanidad e incluso a la Tierra.

70. ¿Qué relación hay entre entropíay orden?

Imaginemos nueve personas ordenadas en un cuadrado: tres columnas de tres, separadas las filas y columnas uniformemente. A esta disposición podemos calificarla de ordenada, porque es simétrica, fácil de visualizar y fácil de describir.

Si los nueve dan al mismo tiempo un paso hacia adelante, permanecerán en formación y la disposición seguirá siendo ordenada. Y lo mismo ocurre si todos dan un paso hacia atrás, o un paso a la izquierda, o a la derecha.

Pero supongamos que a cada uno se le dice que tiene que dar un paso hacia adelante, hacia atrás, a la izquierda o a la derecha dejándole que elija la dirección. Puede ser que todos ellos, sin mutuo acuerdo, decidan dar un paso hacia adelante, y en ese caso se mantendrá el orden. Pero la probabilidad de que uno de ellos dé un paso hacia adelante es sólo de 1 entre 4, puesto que es libre de moverse en cuatro direcciones La probabilidad de que los nueve hombres decidan independientemente avanzar hacia adelante es de 1 entre 4´4´4´4´4´4´4´4, ó 1 entre 262.144.

Si todos ellos se mueven hacia la derecha, o hacia la izquierda, o hacia atrás, también seguirán en orden, de manera que la probabilidad total de que no se rompa la formación es de 4 entre 262.144, ó 1 entre 65.536. Como se ve, el orden tiene una probabilidad diminuta, y sabemos que en el momento que demos libertad para moverse, bastará un solo paso para romper el cuadrado y disminuir la cantidad de orden. Incluso si, por casualidad, todos se mueven en bloque, es casi seguro que el siguiente paso romperá la formación.

Todo esto para el caso de que sólo haya nueve hombres y cuatro direcciones de movimiento. En la mayoría de los procesos naturales intervienen billones y billones de átomos que se pueden mover en infinidad de direcciones. Si, por casualidad, la disposición de átomos estuviera en un principio sometida a alguna clase de orden, es casi seguro que cualquier movimiento aleatorio, cualquier cambio espontáneo, disminuiría ese orden, o por decirlo de otra manera, aumentaría el desorden.

De acuerdo con el segundo principio de la termodinámica, la entropía del universo está en constante aumento; es decir, la distribución de energía en el universo está constantemente igualándose. Puede demostrarse que cualquier proceso que iguala las concentraciones de energía aumenta también el desorden. Por consiguiente, esta tendencia a incrementar el desorden en el universo con los movimientos aleatorios libres de las partículas que lo componen no es sino otro aspecto del segundo principio, y la entropía cabe considerarla como una medida del desorden que existe en el universo.

Miradas las cosas de esta manera, es fácil ver la mano del segundo principio por doquier, porque los cambios naturales actúan claramente en la dirección del desorden; para restaurar el orden hace falta un esfuerzo especial, y su esfuerzo cae sobre nuestras espaldas. Los objetos se descolocan, las cosas se desordenan, los vestidos se ensucian… Y para tener las cosas a punto es preciso estar constantemente arreglando y limpiando el polvo y ordenando. Quizá sirva de consuelo pensar que todo esto es el resultado de una gran ley cósmica; pero, no sé por qué, a mí no me sirve.

71. ¿Qué relación hay entre laentropía y el tiempo?

Imaginemos una película del movimiento de la Tierra alrededor del Sol, tomada desde un lugar muy lejano del espacio y pasada a cámara rápida, de modo que la Tierra parezca recorrer velozmente su órbita. Supongamos que pasamos primero la película hacia adelante y luego marcha atrás. ¿Podremos distinguir entre ambos casos con sólo mirar el movimiento de la Tierra?

Habrá quien diga que la Tierra rodea al Sol en dirección contraria a las agujas del reloj cuando se mira desde encima del polo norte del Sol; y que si las revoluciones son en el sentido de las manillas del reloj, entonces es que la película se ha proyectado marcha atrás y por tanto el tiempo corre también marcha atrás.

Pero si miramos la Tierra desde encima del polo sur del Sol, la Tierra se mueve en la dirección de las manecillas del reloj. Y suponiendo que lo que vemos es ese sentido de rotación, ¿cómo sabremos si estamos encima del polo norte con el tiempo corriendo hacia atrás o encima del polo sur con el tiempo marchando hacia adelante?

No podemos. En procesos elementales en los que intervienen pocos objetos es imposible saber si el tiempo marcha hacia adelante o hacia atrás. Las leyes de la naturaleza se cumplen igual en ambos casos. Y lo mismo ocurre con las partículas subatómicas.

Un electrón curvándose en determinada dirección con el tiempo marchando hacia adelante podría ser igual de bien un positrón curvándose en la misma dirección pero con el tiempo marchando hacia atrás. Si sólo consideramos esa partícula, es imposible determinar cuál de las dos posibilidades es la correcta.

En aquellos procesos elementales en que no se puede decir si el tiempo marcha hacia atrás o hacia delante no hay cambio de entropía (o es tan pequeño que se puede ignorar). Pero en los procesos corrientes, en los que intervienen muchas partículas, la entropía siempre aumenta. Que es lo mismo que decir que el desorden siempre aumenta. Un saltador de trampolín cae en la piscina y el agua salpica hacia arriba; se cae un jarrón y se rompe; las hojas se caen de un árbol y quedan dispersas por el suelo.

Se puede demostrar que todas estas cosas, y en general todo cuanto ocurre normalmente en derredor nuestro, lleva consigo un aumento de entropía. Estamos acostumbrados a ver que la entropía aumenta y aceptamos ese aumento como señal de que todo se desarrolla normalmente y de que nos movemos hacia adelante en el tiempo. Si de pronto viésemos que la entropía disminuye, la única manera de explicarlo sería suponer que nos estábamos moviendo hacia atrás en el tiempo.

Imaginemos, por ejemplo, que estamos viendo una película sobre una serie de actividades cotidianas. De pronto vemos que las salpicaduras de agua se juntan y que el saltador asciende hasta el trampolín. O que los fragmentos de un jarrón suben por el aire y se reúnen encima de una mesa. O que las hojas convergen hacia el árbol y se adosan a él en lugares específicos. Todas estas cosas muestran una disminución de la entropía, y sabemos que esto está tan fuera del orden de las cosas, que la película no tiene más remedio que estar marchando al revés. En efecto, las cosas toman un giro tan extraño cuando el tiempo se invierte, que el verlo nos hace reír.

Por eso la entropía se denomina a veces «la flecha del tiempo», porque su constante aumento marca lo que nosotros consideramos el «avance» del tiempo. (Señalemos que si todos los átomos de los distintos objetos se movieran en la dirección adecuada, todas estas cosas invertidas podrían ocurrir; pero la probabilidad es tan pequeña, que podemos ignorarla tranquilamente.)

72. Si el universo estáconstantemente degradándose, ¿cómo fue

al principio?

La mejor respuesta a esta pregunta es que nadie lo sabe. Todos los cambios se producen en la dirección de la entropía creciente, del aumento del desorden, del aumento de la aleatoriedad, de la degradación. Pero hubo un tiempo en que el universo se hallaba en una posición desde la cual podía degradarse durante billones y billones de años. ¿Cómo llegó a esa posición?

Se me ocurren tres posibles respuestas, todas ellas meras especulaciones:

1. No conocemos todo lo que está pasando en el universo. Los cambios que efectivamente observamos ocurren todos ellos en la dirección de la entropía creciente. Pero en algún lugar puede que haya cambios, en condiciones poco usuales que aún no podemos estudiar, que se desarrollen en la dirección de la entropía decreciente. En ese caso puede ser que el universo permanezca estable en su totalidad. La parte que parece degradarse sería entonces sólo la pequeña porción que nosotros observamos, mientras que en otro lugar habría un movimiento contrario que lo compensaría.

2. Supongamos que el universo no experimente ninguna disminución de entropía en ningún lugar y que todo él se degrada. Al llegar al punto de máxima entropía, toda la energía queda esparcida uniformemente y el tiempo deja de avanzar, ya sea hacia atrás o hacía adelante. Pero toda la energía sigue estando ahí, y por tanto todos los átomos del universo poseen algo de esa energía y se mueven al azar.

En esas condiciones puede ser que por movimientos puramente aleatorios se concentre una cierta cantidad de energía en alguna parte del universo. Es decir, mediante movimientos al azar se produciría otra vez un poco de orden. Después, esa parte del universo comenzaría a degradarse de nuevo.

El máximo de entropía quizá sea la condición normal de un vasto universo infinito, y puede que muy de cuando en cuando (según las medidas normales del tiempo) ocurra que ciertas regiones limitadas adquieran cierto orden; hoy día estaríamos en una de esas regiones.

3. Es posible que la única razón de que la entropía parezca aumentar continuamente en el universo sea que el universo se está expandiendo. Puede ser entonces que en estas condiciones, y sólo en ellas, las disposiciones desordenadas sean más probables que las ordenadas.

Hay astrónomos que piensan que el universo no seguirá dilatándose para siempre. La explosión inicial lo desintegró en pedazos, pero puede ser que la atracción gravitatoria mutua de sus partes esté disminuyendo poco a poco su velocidad de expansión, hasta que finalmente los haga detenerse y volver a contraerse de nuevo. Quizá en un universo en contracción las disposiciones más ordenadas sean más probables que las menos ordenadas. Lo cual significa que habría un cambio natural en la dirección de mayor orden y por tanto una disminución continua de la entropía.

De ser así, puede que el universo se degrade mientras se expande y vuelva a regenerarse mientras se contrae, repitiendo el mismo procedimiento por los siglos de los siglos.

Cabría aún combinar las especulaciones 1 y 3 si consideramos los «agujeros negros». Los agujeros negros son regiones en las que la masa está tan concentrada y la gravedad es tan poderosa, que todo cuanto cae dentro de ellas desaparece y no vuelve jamás: ni siquiera la luz. Son algo así como muestras diminutas de un universo en contracción; puede ser que en esos agujeros negros el segundo principio de la termodinámica se invierta y que, mientras el universo se degrada en la mayor parte de los lugares, se regenere poco a poco allí.

73. Las ondas de radio y las ondasluminosas se utllizan para «ver»

cosas en el espacio. ¿Hay otras

clases de ondas con las que podamos

«ver»?

Las ondas de radio están emparentadas con las ondas luminosas. La diferencia es sobre todo una cuestión de longitud: las primeras son mucho más largas que las segundas.

Existe toda una familia de ondas de diversa longitud, denominada espectro electromagnético. Este espectro se suele dividir en siete regiones que podemos clasificar, por orden descendente de longitud, de la siguiente manera: 1) ondas de radio, 2) microondas, 3) rayos infrarrojos, 4) luz visible, 5) rayos ultravioletas, 6) rayos X y 7) rayos gamma.

La atmósfera terrestre sólo es relativamente transparente a la luz visible y a las microondas. Las demás partes del espectro electromagnético son enteramente absorbidas mucho antes de pasar el aire. Así pues, para observar los cielos desde la superficie de la Tierra sólo nos sirven la luz y las microondas.

La humanidad ha observado desde siempre los cielos porque no en balde el hombre siempre ha tenido ojos. Fue en 1931 cuando el ingeniero americano Karl Jarisky descubrió que lo que estaba detectando eran microondas emitidas por los cuerpos celestes. Y como las microondas se clasifican a veces como ondas de radio muy cortas, esta rama de la observación astronómica recibió el nombre de «radioastronomía».

Hay objetos que, aun siendo detectables por su emisión de microondas no emiten mucha luz. Es decir, hay fuentes de radio que son invisibles para la vista.

Pero en el momento en que se empieza a hacer observaciones desde fuera de la atmósfera tenemos el espectro electromagnético completo a nuestra disposición. Las observaciones desde cohetes han demostrado que los cuerpos celestes bombardean la tierra con toda clase de radiaciones. El estudio de éstas podría contribuir mucho al conocimiento del universo.

En el cielo hay regiones, por ejemplo, que emiten luz ultravioleta en grandes cantidades. La nebulosa de Orión es una fuente de ultravioletas, igual que las regiones que circundan a la estrella Spica, de primera magnitud. El origen de esas grandes cantidades de luz ultravioleta en dichas regiones no se conoce todavía.

Más misteriosa aún es la existencia de una serie de puntos en el cielo que, según se ha descubierto, son fuentes prolíficas de rayos X. Para que un objeto emita rayos X tiene que estar extraordínariamente caliente: un millón de grados o más. Ninguna estrella corriente tiene una superficie tan caliente. Pero existen estrellas de neutrones en las que la materia está tan densamente empaquetada, que toda la masa de un objeto del tamaño del Sol quedaría reducida a una pelota de unos 16 kilómetros de diámetro. Estos objetos y otros igual de extraños pueden emitir rayos X.

Es probable que los astrónomos no puedan hacer un estudio completo de los distintos tipos de radiación que nos llegan desde el espacio hasta que no sean capaces de efectuar todas sus observaciones desde fuera de la atmósfera.

La Luna, que carece de atmósfera, sería un lugar ideal para un observatorio semejante. La posibilidad de construir esos observatorios y de potenciar así nuestro conocimiento es una de las razones más atractivas para intentar colonizar la Luna.

74. Al calentar una sustancia sepone primero roja, luego naranja,

después amarilla, pero a continuación

blanca. ¿Por qué no sigue el espectro

y se pone verde?

Cualquier objeto, a cualquier energía superior al cero absoluto, radia ondas electromagnéticas. Si su temperatura es muy baja, emite sólo ondas de radio largas, muy pobres en energía. Al aumentar la temperatura, radia una cantidad mayor de ondas, pero también empieza a radiar ondas de radio más cortas (y más energéticas). Si la temperatura sigue subiendo, empiezan a radiarse microondas aún más energéticas y después radiaciones infrarrojas.

Esto no quiere decir que a una temperatura dada sólo se emitan ondas de radio largas, un poco más arriba sólo ondas de radio cortas, luego sólo microondas y después sólo infrarrojos. En realidad se emite toda la gama de radiaciones, pero siempre hay una radiación máxima, es decir una gama de longitudes de onda que son las más radiadas, flanqueadas por cantidades menores en el lado de las energías bajas y por cantidades todavía más pequeñas en el de las altas.

Cuando un objeto alcanza la temperatura del cuerpo humano (37º C) el máximo de radiación se encuentra en los infrarrojos largos. El cuerpo humano también radia ondas de radio, pero las longitudes de onda más cortas y más energéticas son siempre las más fáciles de detectar y por consiguiente las más notables.

Cuando la temperatura alcanza aproximadamente los 600º C, el máximo de radiación se halla en el infrarrojo corto. Pero a esas alturas la pequeña cantidad de radiación que se halla en el lado de las energías altas adquiere una importancia especial porque entra ya en la región de la luz visible roja. El objeto reluce entonces con un rojo intenso.

Este rojo constituye sólo un pequeño porcentaje de la radiación total, pero como da la casualidad de que nuestro ojo lo percibe, le otorgamos toda nuestra atención y decimos que el objeto está al «rojo vivo».

Si la temperatura sigue subiendo, el máximo de radiación continúa desplazándose hacia las longitudes de onda cortas y cada vez se emite más luz visible de longitudes cada vez menores. Aunque el objeto radia más luz roja, se van agregando poco a poco luz anaranjada y luz amarilla en cantidades menores pero significativas. Al llegar a los 1.000º C la mezcla de colores la percibimos como naranja, y a los 2.000º C como amarilla. Lo cual no significa que a los 1.000º C sólo se radie luz naranja o que a los 2.000º C se radie sólo amarilla. Porque si fuese así, habría efectivamente que esperar que lo siguiente fuese «calor verde». Lo que en realidad vemos son mezclas de colores.

Al llegar a los 6.000º C (la temperatura superficial del Sol) el máximo de radiación está en el amarillo visible y lo que llega a nuestros ojos son grandes cantidades de luz visible, desde el violeta hasta el rojo. La incidencia simultánea de toda la gama de luz visible sobre nuestra retina nos da la sensación de blanco, y de ahí el color del Sol.

Los objetos más calientes aún que el Sol radian todas las longitudes de onda de la luz visible y en cantidades todavía mayores. Pero el máximo de radiación se desplaza al azul, de modo que la mezcla se desequilibra y el blanco adquiere un tinte azulado.

Todo esto reza para objetos calientes que emiten «espectros continuos», es decir, que radian luz en la forma de una ancha banda de longitudes de onda. Ciertas sustancias, en condiciones adecuadas, radian sólo luz de determinadas longitudes de onda. El nitrato de bario radia luz verde cuando se calienta, y con ese fin se lo utiliza en los fuegos de artificio. «Calor verde», si así lo queréis.

75. ¿Qué es la luz polarizada?

La luz podemos imaginárnosla como si estuviese compuesta por diminutas ondas que pueden oscilar en cualquier plano. En un haz de luz cualquiera puede haber ondas que oscilen de arriba a abajo, otras que oscilen de un lado al otro y otras que lo hagan en diversas direcciones diagonales. Las direcciones de oscilación pueden estar repartidas equitativamente, sin que haya planos privilegiados que contengan una cantidad más que proporcional de ondas luminosas. La luz corriente del Sol o de una bombilla es de este tipo.

Pero supongamos ahora que la luz atraviesa un cristal transparente. El cristal está compuesto de multitud de átomos o grupos de átomos alineados en filas y capas regulares. Las ondas luminosas pasarán fácilmente a través del cristal si da la casualidad de que oscilan en un plano que les permite colarse entre dos capas de átomos. Pero si oscilan en un plano que forma un pequeño ángulo con el anterior, chocarán contra los átomos y gran parte de su energía se irá en hacerlos vibrar. En ese caso la luz sería parcial o totalmente absorbida.

(Para daros una idea de cómo funciona esto, imaginad que atáis una cuerda a un árbol del jardín del vecino, sosteniendo el otro extremo en vuestra mano. Supongamos además que la cuerda pasa entre dos palotes de una valla a mitad de camino. Si agitáis la cuerda arriba y abajo, las ondas pasarán entre los dos palotes e irán a parar al árbol. La valla sería entonces «transparente» a dichas ondas. Pero si las ondas son de derecha a izquierda, chocarán contra los palotes y no pasarán.)

Hay cristales que separan la energía de las ondas luminosas en dos rayos distintos. El plano de oscilación ya no está distribuido uniformemente. En uno de los rayos, todas las ondas oscilan en un determinado plano, mientras que en el otro oscilan en un plano perpendicular al primero. Las oscilaciones diagonales quedan totalmente excluidas.

Cuando las ondas luminosas están obligadas a oscilar en un plano determinado, se dice que la luz está «polarizada linealmente» o simplemente «polarizada». La luz ordinaria, que oscila en cualquier dirección, es luz «no polarizada».

Pero ¿por qué «polarizada»? Cuando el fenómeno recibió su nombre, allá por el año 1808, el ingeniero francés E. L. Malus, que fue quien lo bautizó, tenía una teoría equivocada acerca de la naturaleza de la luz. Malus pensaba que la luz estaba compuesta por partículas con polos, como los de un imán, y que la luz que emergía de un cristal tenía todos sus polos dirigidos en una misma dirección. La teoría resultó ser falsa, pero el nombre estaba ya demasiado afincado para cambiarlo.

Los dos rayos de luz producidos por algunos cristales, cada cual con su propio plano de polarización, tienen propiedades algo diferentes. Por ejemplo, puede que se refracten con ángulos distintos al pasar por el cristal. Aprovechando esto se pueden fabricar cristales en los cuales el primer rayo se refleje y se pierda, y sea sólo el otro el que atraviese hasta el final.

En algunos cristales sólo pasa uno de los rayos; el otro es absorbido y convertido en calor. Los cristales polaroides (que llevan diminutos cristales de este tipo incrustados en plástico) absorben gran parte de la luz por este conducto, y más aún gracias a que están coloreados. De este modo reducen los reflejos.

Cuando la luz polarizada pasa a través de una solución que contiene ciertos tipos de moléculas asimétricas, el plano de oscilación gira. La dirección y magnitud de ese giro ha permitido a los químicos deducir muchas cosas acerca de la estructura real de las moléculas, y sobre todo de las orgánicas. De ahí la enorme importancia de la luz polarizada para la teoría química.

76. La luz ¿puede ejercer fuerzasobre la materia?

Un haz de luz contiene energía. Cuando choca contra un objeto opaco y es absorbido, esa energía tiene que ir a algún lado. La mayor parte se convierte en calor, es decir, las partículas que constituyen el objeto opaco se llevan la energía luminosa y empiezan a vibrar con más rapidez.

Pero el haz de luz ¿puede ejercer una fuerza directa sobre el objeto opaco? ¿Puede comunicar su movimiento al objeto que lo absorbe? El efecto de un cuerpo sólido en movimiento sobre cualquier cosa que se cruce en su camino, es de sobra conocido. Una bola, al chocar contra un juego de bolos, los manda a paseo. Ahora bien, la luz está compuesta por partículas de masa nula. ¿Puede, pese a ello, transferir su movimiento y ejercer una fuerza sobre la materia?

Allá por el año 1873, el físico escocés J. Clerk Maxwell estudió el problema desde el ángulo teórico y demostró que la luz, aun estando compuesta por ondas sin masa, tenía que ejercer una fuerza sobre la materia. Su magnitud dependería de la energía contenida en el haz de luz por unidad de longitud. Ahí está el quid. Supongamos que encendemos una linterna durante un segundo. La luz emitida durante ese segundo contiene bastante energía, pero en ese brevísimo lapso de tiempo la primera onda de luz emitida ha avanzado ya 299.793 kilómetros. Toda la luz emitida en un segundo por ese relámpago luminoso queda repartida a lo largo de un haz de esa longitud, de modo que la energía que toca cada metro, o incluso cada kilómetro, es realmente pequeña.

Por eso, en circunstancias normales no nos percatamos de ninguna fuerza ejercida por la luz sobre la materia.

Supongamos, sin embargo, que cogemos una barra ligera, le adosamos un disco plano a cada lado y la suspendemos horizontalmente, por el punto medio, con un filo hilo de cuarzo. La más mínima fuerza sobre uno de los discos hará que la barra gire alrededor del hilo. Si hacemos incidir un haz de luz sobre uno de los discos, la barra girará siempre que el haz ejerza una fuerza sobre él.

Naturalmente, esa diminuta fuerza quedaría anulada en el momento en que hubiese un ligerísimo viento soplando en contra, de manera que el sistema entero tiene que ir encerrado en una cámara. Por otro lado, el mismo choque de las moléculas de aire contra los discos crearía fuerzas mucho mayores que la de la luz, por lo cual habrá que hacer un alto vacío en la cámara. Una vez hecho todo esto y tomadas otra serie de precauciones, se puede medir ya el pequeñísimo desplazamiento de los discos cuando sobre ellos se hace incidir un intenso haz luminoso.

Los físicos americanos Ernest F. Nichols y Gordon F. Hull realizaron en 1901 tal experimento en la Universidad Dartmouth y demostraron que la luz ejercía efectivamente una fuerza, cuya magnitud era exactamente la predicha por Maxwell veintiocho años antes. Casi al mismo tiempo, el físico ruso Peter N. Lebedev demostró lo mismo utilizando un aparato algo más complicado.

Demostrada ya la existencia de esta «presión de radiación», los astrónomos vieron en ella la explicación de un interesante fenómeno relacionado con los cometas. La cola de un cometa apunta siempre en dirección contraria a la del Sol. Cuando el cometa se va acercando a éste, la cola ondea detrás, da la vuelta en el punto de máxima aproximación y se coloca delante del cometa al alejarse éste.

«Ajajá», pensaron los astrónomos. «¡La presión de radiación!»

Durante medio siglo no dudaron de que era así, pero estaban equivocados. La presión de radiación de la luz solar no es lo bastante fuerte. Es el viento solar el que empuja la cola de los cometas en dirección contraria a la del Sol.

77. La luz roja es la menos desviadaal pasar por un prIsma, pero la que

más se desvía al pasar por una red de

difracción. ¿Por qué esa diferencia?

La luz cabe considerarla como un movimiento ondulatorio y la luz del Sol como una colección de ondas de diferentes longitudes. La luz de diferentes longitudes de onda produce efectos distintos sobre la retina, y es eso lo que nos da la sensación de los colores. De todas las formas visibles de luz, la roja es la de mayor longitud de onda; luego viene el anaranjado, el amarillo, el verde, el azul y finalmente el violeta.

Cuando la luz pasa del aire al vidrio, al agua o a otro medio transparente, disminuye su velocidad. Si el haz de luz incide sobre el trozo de vidrio con un ángulo oblicuo desde la derecha, la parte derecha del haz, que es la que primero choca contra el vidrio, es también la que pierde primero velocidad. Durante un instante, la parte derecha se mueve lentamente mientras que la izquierda sigue a toda velocidad. El resultado es que el haz cambia de dirección al entrar en el vidrio. Es lo que se llama «refracción».

Lo mismo ocurriría si una columna de soldados entrara oblicuamente desde una carretera asfaltada a un campo arado. Los soldados que se encuentran en el lado más próximo al campo entrarían primero en él y disminuirían antes el paso. Y a menos que se hiciera un esfuerzo deliberado para impedirlo, la columna cambiaría de dirección al entrar en el campo.

El efecto retardador del campo proviene de la dificultad de despegar las botas en un suelo blando. Una vez despegada y en el aire, se mueve igual de deprisa en un campo que una carretera. Lo cual significa que los soldados patilargos, al establecer menos contactos por unidad de distancia que los paticortos (gracias a su mayor zancada), sufrirán un retardo menor. Una columna de soldados patilargos cambiaría menos de dirección que otra de paticortos.

La luz roja, con su gran longitud de onda, es similar en este aspecto a un soldado patilargo. Su velocidad disminuye menos que la de la luz de cualquier otro color, y, por tanto, sufre una refracción mínima. Y la luz violeta es naturalmente la que se refracta más.

La difracción implica un principio completamente diferente. Un movimiento ondulatorio puede rodear libremente un obstáculo con tal de que éste no sea mayor que la longitud de una de sus ondas. Cuanto mayor es el obstáculo, peor podrá rodearlo.

Las longitudes de onda de la luz son tan diminutas (aproximadamente 1/20.000 de centímetro) que al tropezar con obstáculos corrientes no se desvía apenas nada, sino que prosigue en línea recta y proyecta sombras nítidas. (Las ondas sonoras, cuya naturaleza es muy distinta de las de la luz, son mucho más largas. Por eso se puede oír al otro lado de una esquina, pero no ver… al menos sin espejos.)

Una red de difracción consiste en un gran número de líneas opacas muy finas, todas ellas paralelas y trazadas sobre un fondo transparente. Las líneas opacas son lo bastante finas como para que incluso las diminutas ondas luminosas, al pasar por las regiones transparentes vecinas, puedan rodearlas un poco. Esto es lo que se denomina «difracción».

Está claro que cuanto más larga sea la longitud de onda de la luz, más pequeña será la obstrucción de las líneas opacas y tanto más podrá abarcar la luz alrededor de ellas. La luz roja, con su gran longitud de onda, es la que más puede abarcar alrededor de las líneas opacas y, por tanto, la que más se difracta. Y la luz violeta, por supuesto, la que menos.

Tanto los prismas de refracción como las redes de difracción dan un «arco iris» o espectro. Pero uno es el inverso del otro. Leyendo hacia fuera desde la dirección original de la luz, el espectro de refracción es rojo, anaranjado, amarillo, verde, azul y violeta. Y el de difracción: violeta, azul, verde, amarillo, anaranjado y rojo.

78. ¿Qué ocurre con la energíacuando dos haces de luz interfieren y

producen oscuridad?

Un haz de luz viene a estar compuesto por un tren de ondas. Cuando dos haces luminosos chocan entre sí formando un ángulo pequeño, puede ocurrir que las ondas se encuentren de tal manera que cuando las unas bajan las otras suben, y viceversa. Las dos ondas «interfieren» y se cancelan parcial o incluso totalmente. El resultado es que la combinación de dos ondas puede producir una luz menos intensa que cualquiera de ellas por separado.

Ahora bien, cada uno de los conjuntos de ondas representa una cierta cantidad de energía. Si las dos ondas se cancelan mutuamente y provocan oscuridad allí donde antes había luz, ¿es que ha desaparecido la energía?

¡Naturalmente que no! Una de las reglas fundamentales de la física es que la energía no puede desaparecer. Tal es la «ley de conservación de la energía». En el fenómeno de la interferencia hay una energía que ha dejado de existir en forma de luz. Por tanto, tiene que aparecer una cantidad exactamente igual de energía en otra forma distinta.

La forma menos organizada de energía es la del movimiento aleatorio de las partículas que componen la materia, movimiento que llamamos «calor». La energía tiende a perder organización al cambiar de forma, de manera que cuando parece que la energía desaparece, lo mejor es buscar calor, es decir, moléculas que se muevan al azar y a velocidades mayores que antes.

Esto es lo que ocurre en el caso de la interferencia luminosa. En teoría es posible disponer dos haces de luz de manera que interfieran perfectamente. Al incidir en una pantalla la dejarán perfectamente, oscura, pero aun así, la pantalla aumentará de temperatura. La energía no ha desaparecido, sólo ha cambiado de forma.

Un problema parecido es el siguiente. Supongamos que damos cuerda al resorte de un reloj. Ahora contiene más energía que cuando estaba distendido. A continuación disolvemos el resorte, todavía tenso, en un ácido. ¿Qué ocurre con la energía?

También aquí se convierte en calor. Si empezamos con dos soluciones ácidas a la misma temperatura y disolvemos en una de ellas un muelle distendido y en la otra un muelle tenso (por lo demás idénticos), la segunda solución tendrá al final una temperatura mayor que la primera.

La ley de la conservación de la energía no fue entendida del todo hasta el año 1847, cuando los físicos lograron captar en todo su sentido la naturaleza del calor. Desde entonces, la aplicación de esa ley ha permitido comprender una serie de fenómenos básicos. Las transformaciones radiactivas, pongamos por caso, producen más calor del que podían explicar los cálculos físicos decimonónicos. El problema quedó resuelto cuando, Einstein elaboró su famosa ecuación e = mc2, demostrando que la propia materia era una forma de energía.

Por otro lado, en algunas transformaciones radiactivas se producen electrones de energía demasiado pequeña. En lugar de admitir una violación de la ley de conservación de la energía, Wolfgang Pauli sugirió en 1931 que en dicha transformación se producía simultáneamente otra partícula, el neutrino, que se llevaba el resto de la energía. Y tenía razón.

79. ¿Qué es el efecto Coriolis?

Moverse por un objeto que sea estacionario o que se desplace a velocidad constante con respecto a un punto fijo no representa ningún problema. Si queremos desplazarnos desde el punto A en uno de los extremos hasta el punto B en el extremo contrario, lo podremos hacer sin experimentar ninguna dificultad.

Pero la situación cambia cuando, las distintas partes del objeto llevan una velocidad diferente. Pensemos en un tiovivo o cualquier otro objeto plano y grande que gire alrededor de su centro. El objeto entero gira de una pieza, pero lo cierto es que cualquier punto cercano al centro describe un círculo pequeño y se mueve despacio mientras que los puntos próximos al borde exterior describen círculos grandes y se mueven, por tanto, muy deprisa.

Imagina que estás en un punto próximo al centro y que quieres dirigirte a otro cerca del borde exterior, siguiendo una línea recta que arranque del centro. En el punto de salida, cerca del centro, participas de la velocidad de dicho punto y, por tanto, te mueves despacio. Sin embargo, a medida que avanzas hacia afuera el efecto de la inercia tiende a que sigas moviéndote despacio mientras que el suelo que pisas va cada vez más rápido. La combinación de tu lentitud y la rapidez del suelo hacen que te sientas empujado en la dirección opuesta a la del movimiento de giro. Si el tiovivo gira en dirección contraria a la de las manillas del reloj, comprobarás que tu trayectoria se curva cada vez, más en el sentido de las manillas del reloj a medida que avanzas.

Si empiezas en un punto próximo al borde exterior y avanzas hacia el centro, retendrás la rapidez de dicho punto al tiempo que el suelo irá moviéndose cada vez más despacio debajo de tus pies. Por consiguiente, te sentirás empujado cada vez más en la dirección de giro. Sí el tiovivo se mueve en dirección contraria a la de las manillas del reloj, tu trayectoria se curvará cada vez más en el sentido de las agujas del reloj.

Saliendo de un punto próximo al centro, desplazándote hasta un punto cercano al borde exterior y volviendo luego al centro, comprobarás -si sigues siempre el camino de menor resistencia- que has descrito una trayectoria más o menos circular.

Este fenómeno fue estudiado por primera vez con detalle en 1835 por el físico francés Gaspard de Coriolis, y en honor suyo se llama «efecto Coriolis». A veces se denomina «fuerza de Coriolis», pero en realidad no es una fuerza, sino simplemente el resultado de la inercia.

La consecuencia más importante del efecto Coriolis para los asuntos cotidianos tiene que ver con la rotación de la Tierra. Los puntos de la superficie terrestre cercanos al ecuador describen en el lapso de veinticuatro horas un gran círculo y, por tanto, se mueven muy deprisa. Cuanto más al norte (o al sur) nos movamos, menor es el círculo descrito por un punto de la superficie y más despacio se mueve.

Los vientos y corrientes oceánicas que corren hacía el norte desde los trópicos llevan desde el principio, por la misma rotación terrestre, un rápido movimiento de oeste a este. Al desplazarse hacia el norte conservan su velocidad, pero como resulta que la superficie de la Tierra se mueve cada vez más despacio, el viento o la corriente se adelanta y empieza a curvarse hacia el este. Al final acaban por moverse en grandes círculos: a derechas en el hemisferio norte y a izquierdas en el hemisferio sur.

Es, precisamente el efecto Coriolis el que inicia ese movimiento circular que, concentrado en mayor grado (y, por tanto, más energéticamente) da origen a los huracanes, y en grado aún mayor, a los tornados.

80. El sonido se mueve más deprisaen sustancias densas como el agua o

el acero que en el aire; sin embargo

se mueve más deprisa en el aire

caliente que en el frío, cuando el

aire caliente es menos denso que el

frío. ¿Es una paradoja?

Lo que nuestros oídos detectan como sonido está causado por una vibración que a su vez origina un movimiento oscilatorio en los átomos o moléculas que constituyen el medio por el que se propaga, La vibración empuja y comprime las moléculas cercanas. Las moléculas así comprimidas vuelven a separarse después y originan otra compresión en la región adyacente, de suerte que la zona de compresión parece propagarse hacia fuera a partir de la fuente sonora. La velocidad con que se mueve la onda de compresión a partir de la fuente es la velocidad del sonido en ese medio.

La velocidad del sonido depende de la velocidad natural con que se mueven las moléculas que componen cada sustancia. Comprimida una cierta sección de aire (sí éste es el medio en cuestión), las moléculas vuelven luego a disgregarse por efecto de sus movimientos aleatorios naturales. Si este movimiento aleatorio es rápido, las moléculas de la región comprimida se disgregan rápidamente y comprimen, también rápidamente, las moléculas de la región vecina. Esta, a su vez, se dilata rápidamente y comprime a la sección siguiente con igual celeridad. En resumidas cuentas: la onda de compresión se propaga rápidamente y la velocidad del sonido en ese medio es alta.

Cualquier factor que aumente (o disminuya) la velocidad natural de las moléculas del aire, aumenta (o disminuye) la velocidad del sonido en el aire.

Pues bien, las moléculas del aire se mueven más deprisa a temperaturas altas que a bajas. Y por eso el sonido se propaga más rápidamente a través. del aire caliente que del frío. Lo cual no tiene nada que ver con la densidad.

A 0º C, el punto de congelación del agua, el sonido, se propaga a 1.195 kilómetros por hora. Esta velocidad aumenta a razón de 2,2 kilómetros por hora con cada grado adicional de temperatura.

Los gases compuestos por moléculas más ligeras que las del aire son, por lo general, menos densos que éste. Las moléculas más ligeras se mueven también con mayor rapidez. La velocidad del sonido en esos gases ligeros es mayor que en el aire, pero no por efecto de la menor densidad, sino por la mayor rapidez de las moléculas. En hidrógeno a 0º el sonido se propaga a 4.667 kilómetros por hora.

Al pasar a los líquidos y sólidos la situación es completamente diferente ala de los gases. En éstos, las moléculas están muy distanciadas y apenas interfieren entre sí. Las moléculas, después de comprimirlas, sólo se separan por efecto de sus movimientos aleatorios. Por el contrario, las moléculas y átomos de los líquidos y sólidos se mantienen en contacto. Al comprimirlas, se separan de nuevo rápidamente debido a su repulsión mutua.

Lo anterior se aplica especialmente a los sólidos, donde los átomos y moléculas se mantienen más o menos rígidamente fijos en su sitio. Cuanto más rígida sea esta atadura, más rápidamente recuperarán su posición al comprimirlos. Por eso el sonido se propaga más deprisa en líquidos que en gases, más deprisa aún en sólidos, y aún más en sólidos rígidos. La densidad no es el factor principal.

Así, el sonido se propaga en el agua a unos 5.311 kilómetros por hora, y en el acero, a unos 17.700 kilómetros por hora.

81. ¿Se hunden los barcos hasta elfondo del mar o llega un momento en

que la presión les impide seguir

bajando?

Un objeto se hunde en el agua si es más denso que ella. La densidad del agua es de un gramo por centímetro cúbico, y las sustancias como la piedra o los metales son mucho más densos que eso. Los barcos, aunque están construidos de grandes masas de acero, flotan porque en su interior encierran grandes espacios de aire. La densidad media del acero y demás materiales de construcción más el volumen de aire dentro del barco es menor que la del agua. Si por accidente, entra agua en el barco, la densidad media de los materiales de construcción más el agua del interior es mayor que la del agua, y el barco se hunde.

A medida que se hunde, va experimentando presiones cada vez mayores. En la superficie del océano, la presión (debida a la atmósfera) es de1.034 gramos por centímetro cuadrado de superficie. Diez metros más abajo, el peso de esa columna de agua añade otros 1.034 gramos por centímetro cuadrado a la presión, y lo mismo para cada uno de los diez metros siguientes. La presión en el fondo del lugar más profundo del océano que se conoce es de mil cien veces la presión atmosférica, lo que equivale a más de una tonelada por centímetro cuadrado.

Tales presiones no tienen, sin embargo, ningún efecto sobre el empuje hacia arriba que experimenta un objeto al hundirse. La presión actúa en todas las direcciones por igual, hacia abajo, hacia arriba y lateralmente, de manera que el objeto sigue hundiéndose, sin hacer ningún caso del aumento de presión.

Pero hay otro factor. La presión comprime el agua y aumenta así su densidad. ¿No podría ser que, como consecuencia de ese aumento de presión, el agua se hiciese tan densa que el objeto dejara de hundirse y quedará flotando en las profundidades del mar?

¡No! El efecto de compresión es muy pequeño. Incluso a una presión de 1 tonelada por centímetro cuadrado, la densidad del agua aumenta sólo de 1 a unos 1,05 gramos por centímetro cúbico. Un sólido que tuviera una densidad de 1,02 gramos por centímetro cúbico se hundiría, efectivamente en el agua, pero quedaría flotando a unos cinco kilómetros de profundidad. Los materiales de construcción ordinarios, sin embargo, tienen densidades muy superiores a 1,05. La densidad del aluminio es 2,7 y la del acero 7,8 gramos por centímetro cúbico. Los barcos metálicos se hundirían hasta el fondo de los abismos más profundos sin la menor posibilidad de flotar.

Pero supongamos que el océano fuese más profundo aún. ¿Llegaría un momento en que una barra de aluminio por poner un ejemplo, alcanzase una profundidad máxima? La respuesta sigue siendo, ¡no!

Si los océanos tuviesen una profundidad de 68 kilómetros (en lugar de unos 11 como máximo), la presión en el fondo alcanzaría unas 7 toneladas por centímetro cuadrado y la densidad del agua 1,3 gramos por centímetro cúbico. Pero para entonces el agua ya no sería líquida, sino que se convertiría en una sustancia sólida llamada «hielo VI». (El hielo VI es más denso que el agua, mientras que el hielo I -el hielo ordinario- es menos denso.)

Por consiguiente, el aluminio o cualquier otra sustancia de densidad mayor que 1,3 gramos por centímetro cúbico descenderían hasta cualquier profundidad oceánica mientras el agua siguiese siendo líquida, y en último término iría a posarse sobre una superficie sólida que podría ser el fondo marino o ese hielo VI. El agua ordinaria nunca puede hacerse suficientemente densa para hacer flotar al aluminio y mucho menos al acero.

82. ¿Cuáles son los elementosquímicos más activos y por qué?

Los electrones rodean al núcleo atómico en esferas concéntricas llamadas «capas». Para cada elemento hay un número fijo de electrones en cada capa. La distribución es especialmente estable cuando hay ocho electrones en la capa más exterior.

Supongamos, sin embargo, que un elemento tiene tantos electrones, que después de acomodar ocho de ellos en una de las capas exteriores quedan varios por alojar en una capa aún más externa. Estos pocos electrones, los más exteriores y -como todos- cargados negativamente, son atraídos muy débilmente por el núcleo atómico, cargado positivamente y situado en el centro. Esos electrones exteriores son cedidos con gran facilidad a otros átomos. Lo que quede ahora del átomo es esa disposición estable de ocho electrones en la capa más externa.

Las reacciones químicas implican la transferencia de electrones, por lo cual un elemento que pueda perder fácilmente uno o más participará ávidamente en tales reacciones y será «químicamente activo». Por lo general, cuantos menos sean los electrones que excedan de ocho, tanto más fácilmente son transferidos y más activo es el elemento. Los elementos más activos con los que tienen un único electrón por encima de los ocho: aquellos en los que hay un electrón solitario en las capas exteriores.

Ejemplos de tales elementos son el sodio, con una distribución electrónica en tres capas (2, 8, 1), y el potasio, en cuatro capas (2, 8, 8, 1).

Las capas electrónicas interiores tienden a aislar a ese solitario electrón exterior del núcleo, positivamente cargado. Cuantas más capas haya entremedias, tanto más débil es la atracción del núcleo sobre el electrón exterior y tanto más fácil es que el átomo lo transfiera. Por eso el potasio es más activo que el sodio, y el cesio (2, 8, 18, 18, 8, 1) más aún que el potasio.

Todavía más activo sería el francio (2, 8, 18, 32, 18, 8, 1), pero tiene el inconveniente de que sólo se pueden estudiar unos cuantos átomos cada vez. Su isótopo más estable tiene una vida media de sólo veintiún minutos. El cesio es, por tanto, el elemento metálico estable más activo.

Supongamos ahora que a un elemento le faltan algunos electrones para completar una capa exterior de ocho. Tales átomos muestran cierta tendencia a aceptar ciertos electrones hasta completar la cifra de ocho. Por consiguiente, intervienen ávidamente en reacciones químicas y son activos.

En general, cuanto menor es el número de electrones que faltan para completar los ocho, mayor es la tendencia a aceptar electrones. Por eso los elementos los elementos más activos de esta clase son aquellos cuyos átomos contienen siete electrones en la capa exterior, necesitando sólo uno para completar los ocho.

Ejemplos de tales elementos son el cloro, cuya distribución de electrones es (2, 8, 7), y el bromo, con (2, 8, 18, 7).

En estos elementos ocurre que cuanto mayor es la atracción del núcleo, mayor es la tendencia a robar el electrón que falta. A menor número de capas internas de electrones, menor aislamiento alrededor del núcleo, mayor la atracción de éste y más activo el elemento.

De los elementos de esta clase, el que menos capas de electrones tiene es el flúor, con una disposición electrónica (2, 7). El flúor es, por tanto, el elemento no metálico más activo.

83. ¿Qué tienen de noble los gasesnobles?

Los elementos que reaccionan difícilmente o que no reaccionan en absoluto con otros elementos se denominan «inertes». El nitrógeno y el platino son ejemplos de elementos inertes.

En la última década del siglo pasado se descubrieron en la atmósfera una serie de gases que no parecían intervenir en ninguna reacción química. Estos nuevos gases -helio, neón, argón, criptón, xenón y radón- son más inertes que cualquier otro elemento y se agrupan bajo el nombre de «gases inertes».

Los elementos inertes reciben a veces el calificativo de «nobles» porque esa resistencia a reaccionar con otros elementos recordaba un poco a la altanería de la aristocracia. El oro y el platino son ejemplo de «metales nobles», y por la misma razón se llamaba a veces «gases nobles» a los gases inertes. Hasta 1962 el nombre más común era el de «gases inertes», quizá porque lo de nobles parecía poco apropiado en sociedades democráticas.

La razón de que los gases inertes sean inertes es que el conjunto de electrones de cada uno de sus átomos está distribuido en capas especialmente estables. La más exterior, en concreto, tiene ocho electrones. Así la distribución electrónica del neón es (2, 8) y la del argón (2, 8, 8). Como la adición o sustracción de electrones rompe esta distribución estable, no pueden producirse cambios electrónicos. Lo cual significa que no se pueden producir reacciones químicas y que esos elementos son inertes.

Ahora bien, el grado de inercia depende de la fuerza con que el núcleo, cargado positivamente y situado en el centro del átomo, sujeta a los ocho electrones de la capa exterior. Cuantas más capas electrónicas haya entre la exterior y el centro, más débil será la atracción del núcleo central.

Quiere esto decir que el gas inerte más complejo es también el menos inerte. El gas inerte de estructura atómica más complicada es el radón. Sus átomos tienen una distribución electrónica de (2, 8, 18, 32, 18, 8). El radón, sin embargo, está sólo constituido por, isótopos radiactivos y es un elemento con el que difícilmente se pueden hacer experimentos químicos. El siguiente en orden de complejidad es el xenón, que es estable. Sus átomos tienen una distribución electrónica de (2, 8, 18, 18, 8).

Los electrones más exteriores de los átomos de xenón y radón están bastante alejados del núcleo y, por consiguiente, muy sueltos. En presencia de átomos que tienen una gran apetencia de electrones, son cedidos rápidamente. El átomo con mayor apetencia de electrones es el flúor, y así fue como en 1962 el químico canadiense Neil Bartlett consiguió formar compuestos de xenón y flúor.

Desde entonces se ha conseguido formar también compuestos de radón y criptón. Por eso los químicos rehuyen el nombre de «gases inertes», porque, a fin de cuentas, esos átomos no son completamente inertes. Hoy día se ha impuesto la denominación de «gases nobles» y existe toda una rama de la química que se ocupa de los «compuestos de gases nobles».

Naturalmente, cuanto más pequeño es el átomo de un gas noble, más inerte es, y. no se ha encontrado nada que sea capaz de arrancarles algún electrón. El argón, cuya distribución electrónica es (2, 8, 8), y el neón, con (2, 8), siguen siendo completamente inertes. Y el más inerte de todos es el helio, cuyos átomos contienen una sola capa electrónica con dos electrones (que es lo máximo que puede alojar esa primera capa).

84. ¿Por qué se forman los cristalesy por qué lo hacen siempre en ciertas

formas?

En condiciones ordinarias existen tres estados de la materia: gaseoso, líquido y sólido. En los gases, la energía de los átomos o (lo que es más corriente) de las moléculas que los componen es tan grande y/ o la atracción entre las distintas moléculas es tan pequeña, que éstas se mueven independientes de un lado para otro.

Si la energía disminuye hasta un cierto punto, las moléculas ya no pueden conservar su independencia, y tienen que permanecer en contacto unas con otras. Sin embargo, hay todavía suficiente energía para que las moléculas se muevan un poco, deslizándose unas sobre otras. Lo que tenemos entonces es un líquido.

Si la energía disminuye aún más, las moléculas ya no podrán resbalar y deslizarse, sino que tienen que permanecer fijas en una orientación determinada (aunque, pueden vibrar, y de hecho vibran, de un lado a otro alrededor de esa posición fija). La sustancia es ahora un sólido.

Dos moléculas vecinas (o átomos, o iones) de un sólido no pueden ocupar una posición cualquiera, sino que adoptan una ordenación regular que depende de la proporción de partículas diferentes que haya, de las diferencias, de tamaño que puedan existir, de la presión exterior, etc. En el cloruro sódico, los iones de sodio y los de cloruro están en igualdad de número y difieren en tamaño. En el fluoruro de cesio, los iones de cesio y los de fluoruro están en igualdad numérica, pero los primeros son mucho mayores que los segundos. En el cloruro de magnesio, los iones de magnesio y los de cloruro no difieren apenas en tamaño, pero hay el doble de los segundos que de los primeros. Esto hace que cada compuesto empaquete sus iones de manera diferente.

Cualquier trozo visible de materia compuesta de átomos, iones o moléculas dispuestos de manera ordenada mostrará superficies lisas que se cortan según ángulos fijos. (Es lo mismo que una formación militar vista desde el aire. Quizá no podamos ver uno a uno a cada soldado, pero si van bien formados veremos que la formación es un rectángulo, por ejemplo.) La forma del trozo visible de materia (o «cristal») depende de la ordenación atómica. Para cualquier sustancia dada, y con un conjunto específico de condiciones, sólo hay una distribución atómica posible. De ahí que los cristales tengan siempre una forma dada.

Las sustancias sólidas son casi siempre de naturaleza cristalina, aunque no lo parezca. Para formar un cristal perfecto, lo mejor es empezar con una sustancia pura en disolución (para que no se cuelen átomos extraños que pueden perturbar la ordenación). Luego hay que enfriarla lentamente, para que los átomos tengan tiempo de irse colocando cada uno en su lugar. Lo que predomina en la naturaleza son mezclas de sustancias, y por eso lo que resulta al final es una yuxtaposición de diferentes tipos de cristales que se entrecruzan. Además, si el enfriamiento es muy rápido, se empiezan a formar tantos cristales que ninguno de ellos tiene la oportunidad de pasar del tamaño microscópico, con lo cual cada uno se orienta por su lado y no dan una forma determinada.

Por eso es muy raro ver cristales grandes y limpios en la naturaleza. Lo que solemos encontrar son trozos irregulares de material compuesto por cristales microscópicos que no vemos.

Hay sustancias sólidas que no son cristalinas y que, por tanto, no son realmente sólidas. El vidrio, por ejemplo. El vidrio líquido es muy viscoso, y eso impide que los iones se muevan con soltura y se ordenen adecuadamente. Al enfriarse el vidrio, los iones se van moviendo cada vez más despacio hasta que se detienen del todo, conservando en adelante la posición que tenían en ese momento.

En tales condiciones no hay ordenación ninguna, de modo que el vidrio «sólido» es realmente un «líquido subenfriado». El vidrio es ciertamente duro y parece sólido, pero no tiene estructura cristalina ni tampoco (lo cual es decisivo) un punto de fusión definido. Por eso el vidrio «sólido» se va ablandando poco a poco al calentarlo.

85. ¿Se puede comprimir el agua?

La contestación más sencilla es que cualquier cosa se puede comprimir.

Lo cierto es que es mucho más fácil comprimir materia en forma gaseosa que en cualquier otra modalidad. Y es porque los gases están compuestos de moléculas muy separadas entre sí. En el aire normal, pongamos por caso, las moléculas ocupan algo así como una décima parte del volumen total.

Parta comprimir un gas basta con apretujar las moléculas un poco contra la tendencia expansiva de su propio movimiento aleatorio y eliminar algo del espacio vacío que existe entre ellas. Es un trabajo para el cual basta la fuerza muscular del hombre. Cuando hinchamos un globo estamos comprimiendo aire.

En el caso de los líquidos y sólidos, los átomos y moléculas que los componen están más o menos en contacto. Si no se acercan aún más es por la repulsión mutua de los electrones que existen en las regiones, exteriores de los átomos. Esta repulsión es una resistencia mucho más fuerte a la compresión que el movimiento molecular en un gas. Quiérese decir que los músculos humanos no bastan ya para realizar este trabajo, al menos para que sea perceptible.

Pensemos por un momento que vertimos cierta cantidad de agua en un recipiente rígido abierto por arriba y que ajustamos un pistón en la abertura hasta tocar al agua. Si empujamos el pistón hacia abajo con todas nuestras fuerzas, veremos que apenas cederá. Por eso se dice a menudo que el agua es «incompresible» y que no se puede apretujar en un volumen más pequeño.

Nada de eso. Al empujar el pistón sí que comprimimos el agua, pero no lo suficiente para medirlo. Si la presión aplicada es mucho mayor que la que pueden ejercer los músculos humanos, la disminución del volumen de agua, o de cualquier otro líquido o sólido, llega a ser medible. Por ejemplo, si comprimimos 100 litros de agua con una fuerza de 1.050 kilogramos por centímetro cuadrado, su volumen se contraerá a 96 litros. Si la presión aumenta aún más, el volumen Seguirá disminuyendo. Bajo tal compresión, los electrones son, empujados, por así decir, cada vez más cerca del núcleo.

Si la presión se hace suficientemente grande -digamos que por el peso acumulado de muchos miles de kilómetros de materia bajo una gran fuerza gravitatoria-, la repulsión electrostática se viene abajo. Los electrones ya no se pueden mantener en órbita alrededor del núcleo y son desplazados. La materia se reduce entonces a núcleos atómicos desnudos y electrones volando de acá para allá en movimientos alocados.

Los núcleos son mucho más diminutos que los átomos, de manera que esta «materia degenerada» sigue siendo en su mayor parte espacio vacío. La presión en el centro de la Tierra o incluso de Júpiter no es suficiente para formar materia degenerada, pero en cambio sí la hay en el centro del Sol.

Una estrella compuesta por entero de materia degenerada puede tener la misma masa que el Sol y aun así poseer un volumen no mayor que el de la Tierra. Es lo que se llama una «enana blanca». Bajo su propia gravedad puede comprimirse aún más, hasta quedar compuesta de neutrones en contacto mutuo. Tales «estrellas de neutrones» pueden albergar la masa entera del Sol en una esfera de trece kilómetros.

E incluso eso puede comprimirse, piensan los astrónomos, hasta el volumen cero de un «agujero negro».

86. ¿Que es el hidrógeno metálico?¿Cómo puede ser el hidrógeno un

metal?

Todo el mundo reconoce un metal al verlo, porque los metales tienen propiedades muy características. En superficies lisas reflejan la luz con gran eficacia, que es lo que les confiere su «brillo metálico», mientras que los no metales son muy poco reflectantes y poseen una tonalidad opaca. Los metales son fácilmente deformables, se dejan extender en láminas y estirar en hilos, mientras que los no metales son quebradizos y se rompen o se pulverizan al golpearlos. Los metales conducen el calor y la electricidad fácilmente; los no metales, no.

¿De dónde viene la diferencia?

En la mayoría de los compuestos corrientes, como los que vemos a nuestro alrededor en el mar y en la tierra, las moléculas están compuestas por átomos firmemente unidos por electrones compartidos. Cada electrón está ligado firmemente a un átomo determinado. En estos casos la sustancia exhibe propiedades no metálicas.

Según este criterio, el hidrógeno es un no metal. El hidrógeno ordinario está compuesto de moléculas compuesto de moléculas constituidas por dos átomos de hidrógeno. Cada átomo de hidrógeno tiene un sólo electrón, y los dos átomos que componen una molécula comparten los dos electrones a partes iguales. No sobra ningún electrón.

¿Qué ocurre cuando hay electrones que no están firmemente ligados? Consideremos, por ejemplo, el elemento potasio. Cada átomo de potasio tiene diecinueve electrones distribuidos en cuatro capas. Los únicos electrones que se pueden compartir son los de la capa, exterior, de modo que en el caso del potasio cada átomo sólo puede compartir un electrón con su vecino. Además, este electrón exterior esta especialmente suelto porque entre él y el núcleo atómico central que lo atrae se interponen otras capas de electrones. Estas capas intermedias aíslan al electrón exterior de la atracción central.

Los átomos del potasio sólido están empaquetados muy juntos, como esas pirámides de naranjas que se ven a veces en las fruterías. Cada átomo de potasio tiene ocho vecinos. Con tantos vecinos y tan cerca, y estando tan suelto el electrón exterior, es muy fácil que cualquiera de éstos salte de un vecino a otro.

Son estos electrones sueltos y móviles los que permiten a los átomos de potasio empaquetarse tan densamente, conducir fácilmente el calor y la electricidad y deformarse. En resumen, estos electrones sueltos y móviles son los que hacen que el potasio (y otros elementos y mezclas que los poseen) sea metálico.

Pues bien, recordemos que el hidrógeno, al igual que el potasio, tiene un solo electrón para compartir con vecinos. Pero hay una diferencia. Entre ese único electrón del hidrógeno y el núcleo central no hay electrones aislantes. Por consiguiente, el electrón está demasiado sujeto para ser suficientemente móvil y poder convertir el hidrógeno en un metal o hacer que sus átomos se empaqueten densamente.

Pero ¿y si se le da al hidrógeno una pequeña ayuda? ¿Qué ocurre si se le obliga a empaquetarse densamente, no por su propia constitución electrónica, sino por presión exterior? Supongamos que la presión aplicada es suficiente para estrujar los átomos de hidrógeno y hacer que cada átomo quede rodeado por ocho, diez o incluso doce vecinos más próximos. Podría ser entonces que el electrón de cada átomo, a pesar de la fortísima atracción del núcleo, empezara a deslizarse de un vecino a otro. Lo que tendríamos sería «hidrógeno metálico».

Para conseguir que el hidrógeno se empaquete tan densamente, tiene que hallarse en estado casi puro (la presencia de otros átomos, estorbaría) y a una temperatura no demasiado alta. De lo contrario, se expandiría). Por otro lado tiene que hallarse bajo enormes presiones. Uno de los lugares del sistema solar donde las condiciones son casi perfectas es el centro de Júpiter, y hay quienes creen que el interior de este planeta está compuesto por hidrógeno metálico.

87. ¿Qué es la «poliagua»? Sisigue siendo H2O, ¿cuál es la

diferencia?

Al describir la molécula de agua suele decirse que está compuesta por dos átomos de hidrógeno y uno de oxígeno: H2O. Sí la cosa acabara ahí, sería una molécula pequeña con bajo punto de ebullición. El sulfuro de hidrógeno (H2S), que tiene una molécula parecida, pero más pesada (porque el S es más pesado que el O), es un gas que no se lícúa hasta los -61,8º C. Si el agua no, fuese más que H2O, se licuaría a una temperatura todavía más baja, quizá alrededor de los -80º C.

Pero consideremos la forma de las moléculas de agua Los tres átomos forman un ángulo casi recto, con el de oxígeno en el vértice. El oxígeno comparte dos electrones con cada uno de los átomos de hidrógeno, pero el reparto no es equitativo. El oxígeno ejerce una mayor atracción sobre los electrones, de modo que éstos, con su carga eléctrica negativa, están muy del lado del oxígeno. Por eso, aunque la molécula de agua es eléctricamente neutra en su conjunto, la parte del oxígeno tiene una pequeña carga negativa, mientras que los dos átomos de hidrógeno tienen pequeñas cargas positivas que contrarrestan a aquélla.

Las cargas de signo opuesto se atraen. Hay, pues, una tendencia a que dos moléculas del agua se alineen de manera que el extremo negativo (el del oxígeno) de una de ellas quede adyacente al positivo (el del hidrógeno) de la siguiente. Esto constituye un «enlace de hidrógeno» que es veinte veces más débil que los enlaces normales que unen al hidrógeno y al oxígeno dentro de la molécula. Sin embargo, basta para que las moléculas de agua sean «pegajosas».

Debido a esta pegajosidad, las molécula de agua se unen con más facilidad y se separan con más dificultad que si no fuese así. Para superar esa fuerza pegajosa y hacer que hierva el agua, hace falta calentarla a 100º C. Cuando la temperatura baja hasta 0ª C, la prevalencia de enlaces de hidrógeno es tal, que las moléculas de agua quedan fijas en su sitio, formándose hielo. De no ser por los enlaces de hidrógeno la temperatura tendría que ser mucho más baja para que esto ocurriera.

En una molécula como la del H2S no sucede la mismo, porque el átomo de azufre y el de hidrógeno tienen una apetencia de electrones aproximadamente igual. No hay acumulación de cargas ni a un lado ni al otro y, por consiguiente, la molécula no es «pegajosa».

Supongamos ahora que tenemos moléculas de agua en un espacio muy limitado, un tubo de vidrio muy fino, pongamos por caso. En estas condiciones tendrán que apelotonarse unas contra otras más de lo normal. El átomo de oxígeno de una de las moléculas se verá empujado muy cerca del átomo de hidrógeno del vecino, tanto, que el enlace de hidrógeno se hará tan fuerte como un enlace ordinario. Las dos moléculas se convierten en una, y a esta doble molécula se podrá enganchar otra, y luego otra, etc.

Al final habrá multitud de moléculas fuertemente, unidas entre sí, con todos los hidrógenos y oxígenos formando hexágonos regulares. La sustancia múltiple resultante es un ejemplo de «polímero». Es «agua polimerizada», o «poliagua» en abreviatura. Para poder romper esta sustancia (anunciada por vez primera por químicos soviéticos en 1965) en moléculas H2O de vapor de agua, hay que calentarla hasta 500º C. Y debido también a que las moléculas están aquí mucho más apelotonadas que en el agua ordinaria, la poliagua tiene una densidad 1,5 veces superior a la del agua normal.

Sin embargo, la noción de poliagua no ha sido aceptada universalmente. Muchos químicos. piensan que lo que se ha llamado poliagua es en realidad agua que ha cogido impurezas o que ha disuelto un poco de vidrio. En este caso puede ser que la poliagua ni siquiera exista.

88. ¿Por qué se dilata el agua alcongelarse?

Primero cabria preguntar: ¿por qué son sólidos los sólidos? ¿Y por qué son líquidos los líquidos?

Entre las moléculas de una sustancia sólida hay una cierta atracción que las mantiene firmemente unidas en una posición fija. Es difícil separarlas y, por consiguiente la sustancia es sólida.

Sin embargo, las moléculas contienen energía de movimiento y vibran alrededor de esas posiciones fijas. Al subir la temperatura, van ganando cada vez más energía y vibrando con mayor violencia. En último término adquieren tanta energía que la atracción de las demás moléculas no basta ya para retenerlas. Rompen entonces las ligaduras y empiezan a moverse por su cuenta, resbalando y deslizándose sobre sus compañeras. El sólido se ha licuado: se ha convertido en un líquido.

La mayoría de los sólidos son cristalinos. Es decir, las moléculas no sólo permanecen fijas en su sitio, sino que están ordenadas en formaciones regulares, en filas y columnas. Esta regularidad se rompe, cuando las moléculas adquieren suficiente energía para salirse de la formación, y entonces el sólido se funde.

La disposición regular de las moléculas en un sólido cristalino suele darse en una especie de orden compacto. Las moléculas se apiñan unas contra otras, con muy poco espacio entre medías. Pero al fundirse la sustancia, las moléculas, al deslizarse unas sobre otras, se empujan y desplazan. El efecto general de estos empujones es que las moléculas se separan un poco más. La sustancia se expande y su densidad aumenta. Así pues, en general los líquidos son menos densos que los sólidos.

O digámoslo así: los sólidos se expanden al fundirse y los líquidos se contraen al congelarse.

Sin embargo, mucho depende de cómo estén situadas las moléculas en la forma sólida. En el hielo, por ejemplo, las moléculas de agua están dispuestas en una formación especialmente laxa, en una formación tridimensional que en realidad deja muchos «huecos».

Al aumentar la temperatura, las moléculas quedan sueltas y empiezan a moverse cada una por su lado, con los empujones y empellones de rigor. Lo cual las separaría, si no fuese porque de esta manera muchas de ellas pasan a rellenar esos huecos. Y al rellenarlos, el agua líquida ocupa menos espacio que el hielo sólido, a pesar de los empujones moleculares. Al fundirse 1 centímetro cúbico de hielo sólo se forman 0,9 centímetros cúbicos de agua.

Como el hielo es menos denso que el agua, flota sobre ella. Un centímetro cúbico de hielo se hunde en el agua hasta que quedan 0,9 centímetros cúbicos por debajo de la superficie. Estos 0,9 cm3 desplazan 0,9 cm3 de agua líquida, que pesan tanto como el centímetro cúbico entero de hielo. El hielo es sostenido entonces por el empuje del agua, quedando 0,1 centímetros cúbicos por encima de la superficie. Todo esto es válido para el hielo en general. Cualquier trozo de hielo flota en el agua, con una décima parte por encima de la superficie y nueve décimas por debajo.

Esta circunstancia resulta muy afortunada para la vida en general, pues tal como son las cosas, cualquier hielo que se forme en una masa de agua, flota en la superficie. Aísla las capas más profundas y reduce la cantidad de calor que escapa de abajo. Gracias a ello las aguas profundas no suelen congelarse, ni siquiera en los climas más gélidos. En cambio, en épocas más calurosas el hielo flotante recibe el pleno efecto del Sol y se funde rápidamente.

Si el hielo fuese más denso que el agua, se hundiría al fondo a medida que fuese formándose, dejando al aire libre otra capa de agua, que a su vez se congelaría también. Además el hielo del fondo, no tendría posibilidad ninguna de recoger el calor del Sol y fundirse. Si el hielo fuese más denso que el agua, las reservas acuáticas del planeta estarían casi todas ellas congeladas, aunque la Tierra no estuviese más lejos del Sol que ahora.

89. ¿Qué son las pilas decombustible? ¿Qué ventajas presentan

en la generación de electrIcidad?

Una pila de combustible es un dispositivo para generar electricidad. Para entender su valor consideremos las palabras, «combustible» y «pila» por separado.

Para generar electricidad a partir de un combustible como el carbón o el petróleo hay que quemarlos primero. La energía producida por su combustión convierte agua en vapor, que se utiliza a su vez para hacer girar una turbina colocada en un campo magnético. De esta manera se genera una corriente eléctrica. Es decir, convertimos la energía química del combustible en energía térmica para luego convertir ésta en energía eléctrica.

En el transcurso de esta doble conversión se pierde gran parte de la energía química primitiva Pero el combustible es tan barato que esa pérdida no impide producir grandes cantidades de electricidad sin un gasto excesivo.

También es posible convertir directamente energía química en energía eléctrica sin pasar por el calor. Para ello hay que usar una pila eléctrica. Esta pila consiste en una o más soluciones de ciertos productos químicos en las que se sumergen dos barras metálicas llamadas electrodos. En cada uno de los electrodos se produce una reacción química en la que o se absorben o se liberan electrones. La presión de electrones en uno es mayor que en el otro, de modo que si los dos están conectados mediante un cable, los electrones pasarán por él de un electrodo a otro.

Ese flujo de electrones es una corriente eléctrica, que persistirá mientras las reacciones químicas persistan en la célula. Las baterías de linterna son un ejemplo de tales pilas.

Hay casos en los que si se hace pasar una corriente eléctrica a través de una pila agotada, las reacciones químicas que tienen lugar en su interior se desarrollan, al revés, con lo cual la célula puede volver a almacenar energía química y producir otra vez una corriente eléctrica. Las baterías de los coches son un ejemplo de pilas reversibles.

La energía química que se pierde en una pila es mucho menor, puesto que se convierte en electricidad en un solo paso. Como contrapartida, los productos químicos que utilizan las pilas son todos ellos muy caros. Las pilas de linternas utilizan cinc, por ejemplo, y plomo las baterías de los coches. El coste de los metales necesarios para abastecer de electricidad a una ciudad entera por este procedimiento sería de miles de millones de dólares diarios.

La pila de combustible sería un dispositivo que combinase ambas ideas: la de combustible y la de la pila eléctrica. Es una célula cuyas reacciones químicas no implican el uso de metales caros, sino de combustibles baratos. La energía química de dichos combustibles se convierte en energía eléctrica en un solo paso, con una pérdida mucho, menor que en el procedimiento normal de dos etapas. De este modo se puede multiplicar sustancialmente la cantidad de electricidad disponible por el hombre.

La pega es que es difícil fabricar una pila de combustible, que realmente funcione con garantías. Se han fabricado algunas en las que la energía eléctrica se extrae de la combustión de hidrógeno con oxígeno, pero, el hidrógeno sigue siendo bastante caro. En lugar del hidrógeno se ha utilizado también monóxido de carbono, que es algo más barato. Recientemente se han fabricado también pilas que funcionan a base de combinar desperdicios con oxígeno bajo la influencia de la acción bacteriana. No cabe duda de que la idea de convertir desperdicios en electricidad es interesantísima, porque resolvería dos problemas: la obtención de energía barata y la eliminación de desperdicios.

Aún queda mucho por hacer antes de que las pilas de combustible sean realmente prácticas, pero, con todo, representan una de las grandes esperanzas del futuro.

90. ¿Qué son las vitaminas y porqué las necesitamos?

Para entender lo que son las vitaminas tenemos que empezar por las enzimas. Las enzimas son moléculas que sirven para acelerar ciertos cambios químicos en el cuerpo. Las enzimas se presentan en miles de variedades, porque cada cambio químico está gobernado por una enzima particular.

Para controlar un cambio químico no hace falta más que una cantidad minúscula de enzima, pero esa cantidad minúscula es imprescindible. La maquinaria química del cuerpo está interconectada de un modo muy intrincado, de manera que el retardo de una sola transformación química por culpa de la falta de una enzima puede resultar en una enfermedad grave o incluso en la muerte.

La mayor parte de las enzimas las puede fabricar el cuerpo con las sustancias que se hallan presentes en casi todos los alimentos. No hay peligro de que nos quedemos sin ellas, salvo que nos estemos muriendo materialmente de hambre. Pero hay un pero.

Algunas enzimas contienen, como parte de su estructura, ciertas combinaciones atómicas poco usuales. Estas combinaciones de átomos no suelen encontrarse más que en las enzimas y, por tanto, sólo se necesitan en cantidades ínfimas, porque las propias enzimas sólo se necesitan en esas proporciones.

Pero el cuerpo tiene que tenerlas. Y si una de estas combinaciones de átomos escasea, las distintas enzimas que las necesitan dejarán de funcionar. Ciertos cambios químicos empezarán a desarrollarse mal y como consecuencia de ello sobrevendrá la enfermedad y finalmente, la muerte.

El peligro estriba en que, a pesar de que la mayoría de las moléculas enzimáticas las puede fabricar el cuerpo, estas combinaciones particulares de átomos, no. Tienen que ser absorbidas, intactas, de los alimentos. El cuerpo humano se muere si la comida que ingiere no contiene cantidades minúsculas de estas singulares combinaciones de átomos.

Cuando se descubrió esto a principios del siglo xx, no se conocía la naturaleza química de dichas naciones. Se pensaba que algunas de ellas al menos, pertenecían a una clase de sustancias llamadas «aminas». Por eso se les dio el nombre de «vitaminas» («aminas de la vida»).

Las plantas son la fuente básica de las vitaminas. Fabrican todas las sustancias de sus tejidos a partir de productos químicos elementales, como son el anhídrido carbónico, el agua, los nitratos, etc. Si no fuesen capaces de fabricar todas y cada una de vitaminas a partir de cero, no podrían sobrevivir.

Los animales, en cambio, pueden comer plantas y utilizar las vitaminas que se hallan ya presentes en los tejidos vegetales, sin tener que fabricarlas por su cuenta. Los animales almacenan las vitaminas que absorben allí donde los mecanismos enzimáticos más los necesitan: en los músculos, el hígado, los riñones, la leche, etcétera. Los animales carnívoros obtienen las vitaminas de las reservas que poco a poco han ido acumulando sus presas herbívoras.

El no tener que fabricarse sus propias vitaminas tiene ciertas ventajas, porque su fabricación exige la presencia de una maquinaria química muy respetable en cada célula. Eliminando esta función queda más espacio, por decirlo así, para desarrollar la maquinaria que requieren las muchas cosas que las plantas no tienen que hacer: acción nerviosa, contracción muscular, filtración renal, etc.

El precio que se paga, sin embargo, es la posibilidad de una falta de vitaminas. Los seres humanos que viven con una dieta muy pobre (sea porque les guste o porque no tengan otra cosa) pueden caer víctimas de enfermedades como el beriberi, el escorbuto, la pelagra o el raquitismo; todas ellas son el resultado de una química del cuerpo, que va parándose poco a poco debido al mal funcionamiento de ciertas enzimas por falta de una vitamina.

91. ¿Cómo empezó la vida?

Una respuesta clara y rotunda no la hay, porque cuando empezó la vida no había nadie allí que sirviese de testigo. Pero se pueden hacer análisis lógicos del problema.

Los astrónomos han llegado a ciertas conclusiones acerca de la composición general del universo. Han encontrado, por ejemplo, que un 90 por 100 de él es hidrógeno y un 9 por 100 helio. El otro 1 por 100 está constituido principalmente por oxígeno, nitrógeno, neón, argón, carbono, azufre, silicio y hierro.

Partiendo de ahí y sabiendo de qué manera es probable que se combinen tales elementos, es lógico concluir que la Tierra tenía al principio una atmósfera muy rica en ciertos compuestos de hidrógeno: vapor de agua, amoníaco, metano, sulfuro de hidrógeno, cianuro de hidrógeno, etc. Y también habría un océano de agua líquida con gases atmosféricos disueltos en ella.

Para que se iniciase la vida en un mundo como éste es preciso que las moléculas elementales que existían, al principio se combinaran entre sí para formar moléculas complejas. En general, la construcción de moléculas complicadas de muchos átomos a base de moléculas elementales de pocos átomos requiere un aporte de energía. La luz del: Sol (sobre todo su contenido ultravioleta), al incidir sobre el océano, podía suministrar la energía necesaria para obligar a las moléculas pequeñas a formar otras mayores.

Pero ¿cuáles eran esas moléculas mayores?

El químico americano Stanley L. Miller decidió en 1952 averiguarlo. Preparó una mezcla de sustancias parecida a la que, según se cree, existió en la primitiva atmósfera terrestre, y se cercioró de que era completamente estéril. Luego la expuso durante varias semanas a una descarga eléctrica que servía como fuente de energía. Al final comprobó que la mezcla contenía moléculas algo más complicadas que aquéllas con las que había comenzado. Todas ellas eran moléculas del tipo que se encuentran en los tejidos vivos y entre ellas había algunos de los aminoácidos que son los bloques fundamentales de unos importantes compuestos: las proteínas.

Desde 1952 ha habido muchos investigadores, de diversos países, que han repetido el experimento, añadiendo detalles y refinamientos. Han construido diversas moléculas por métodos muy distintos y las han utilizado luego como punto de partida de otras construcciones.

Se ha comprobado que las sustancias así formadas apuntan directamente hacia las complejas sustancias de la vida: las proteínas y los ácidos nucleicos. No se ha hallado ninguna sustancia que difiera radicalmente de las que son características de los tejidos vivos.

Aún no se ha conseguido nada que ni por un máximo esfuerzo de imaginación pudiera llamarse viviente, pero hay que tener en cuenta que los científicos están trabajando con unos cuantos decilitros de líquido, durante unas cuantas semanas cada vez. En los orígenes de la Tierra, lo que estaba expuesto al Sol era un océano entero de líquido durante miles de millones de años.

Bajo el azote de la luz solar, las moléculas del océano fueron haciéndose cada vez más complejas, hasta que en último término surgió una que era capaz de inducir la organización de moléculas elementales en otra molécula igual que ella. Con ello comenzó y continuó la vida, evolucionando gradualmente hasta el presente. Las formas primitivas de «vida» tuvieron que ser mucho menos complejas que las formas más simples de vida en la actualidad, pero de todos modos ya eran bastante complejas. Hoy día los científicos tratan de averiguar cómo se formó esa singular molécula que acabamos, de mencionar.

Parece bastante seguro que la vida se desarrolló, no como un milagro, sino debido a la combinación de moléculas según una trayectoria de mínima resistencia. Dadas las condiciones de la Tierra primitiva, la vida no tuvo por menos de formarse, igual que el hierro no tiene por menos que oxidarse en el aire húmedo. Cualquier otro planeta que se parezca física y químicamente a la Tierra desarrollaría inevitablemente vida, aunque no necesariamente inteligente.

92. ¿Es posible una vida desilicio?

Todos los seres vivientes, desde la célula más simple hasta la sequoia más grande, contienen agua, y además como la molécula más abundante, con mucho. Inmersas en el agua hay moléculas muy complejas, llamadas proteínas y ácidos nucleicos que al parecer son características de todo lo que conocemos por el nombre de vida. Estas moléculas complejas tienen una estructura básica compuesta en cadenas y anillos de átomos de carbono. A casi todos los carbonos van unidos uno o más átomos de hidrógeno. A una minoría, en cambio, van ligadas combinaciones de átomos como los de oxígeno nitrógeno, azufre y fósforo.

Expresándolo con la máxima sencillez podemos decir que la vida, tal como la conocemos, está compuesta de derivados de hidrocarburos en agua.

¿Puede la vida estar compuesta de otra cosa? ¿Existen otros tipos de moléculas que proporcionen la complejidad y versatilidad de la vida, algo distinto del agua que proporcione, sin embargo, las propiedades poco usuales, pero necesarias, que sirven como trasfondo de la vida?

¿Es posible concebir algo parecido al agua que pudiera sustituirla? Las propiedades del, amoníaco líquido son las más afines o las del agua. En un planeta más frío que la Tierra, por ejemplo, Júpiter, donde el amoníaco abunda en estado líquido mientras que el agua está solidificada, puede que sea concebible una vida basada en el amoníaco.

Por otro lado, hay que decir que si el hidrógeno va unido a tantos puntos de la cadena del carbono, es porque es un átomo muy pequeño que se acopla en cualquier lugar. El átomo de flúor es parecido al de hidrógeno en algunos aspectos y casi tan pequeño como él. Así, pues, igual que tenemos una química de los hidrocarburos podemos tener una química de los fluorcarburos, con la única salvedad de que éstos son mucho más estables que aquellos. Quizá en un planeta más caliente que la Tierra podría concebirse una vida a base de fluorcarburos.

Pero ¿y en cuanto al átomo de carbono? ¿Existe algún sustituto? El carbono puede unirse a un máximo de cuatro átomos diferentes (que pueden ser también de carbono) en cuatro direcciones distintas, y es tan pequeño que los átomos de carbono vecinos se hallan suficientemente próximos para formar un enlace muy fuerte. Esta característica es la que hace que las cadenas y anillos de carbono sean estables.

El silicio se parece mucho al carbono y también puede unirse a un máximo de cuatro átomos diferentes en cuatro direcciones distintas. El átomo de silicio, sin embargo, es mayor que el de carbono con lo cual las combinaciones silicio-silicio son menos estables que las de carbono-carbono. La existencia de largas cadenas y anillos de átomos de silicio es mucho más improbable que en el caso del carbono.

Lo que sí es posible son largas y complicadas cadenas de átomos en las que alternen el silicio con el oxígeno. Cada átomo de silicio puede unirse a otros dos átomos o grupos de átomos, y este tipo de moléculas se denominan «siliconas».

A la molécula de silicona pueden ir unidos grupos de hidrocarburos o de fluorcarburos, y estas combinaciones podrían resultar en moléculas suficientemente grandes, delicadas y versátiles como para formar la base de la vida En ese sentido sí que es concebible una vida a base de silicio.

Pero ¿existen realmente esas otras formas de vida en algún lugar del universo? ¿O serán formas de vida basadas en una química completamente extraña, sin ningún punto de semejanza con la nuestra? Quizá nunca lo sepamos.

93. ¿Por qué se extinguieron losdinosaurios?

Durante ciento cincuenta millones de años las criaturas más difundidas de la Tierra fueron ciertos grandes reptiles conocidos vulgarmente por el nombre de «dinosaurios». Los más grandes de entre los reptiles terrestres de esta especie puede que pesaran hasta 85 toneladas. Los grandes ictiosauros y plesiosauros dominaban el mar mientras que los pterosaurios surcaban los aires con gigantescas alas de hasta 20 pies de envergadura.

Más tarde, hace unos setenta millones de años, se extinguieron todas esas monstruosas criaturas. No de la noche a la mañana, pero sí en un tiempo bastante breve: digamos que un millón de años. Otras formas de vida animal como los peces y los mamíferos y aves primitivos salieron indemnes, igual que la vida vegetal.

Acerca de esta extinción se han hecho diversas conjeturas… pero son sólo eso, conjeturas. A ciencia cierta nadie lo sabe.

Hay quien piensa que se debió a un cambio del clima. Donde antes había un mundo suave y apacible, con pantanos y mares poco profundos, surgieron ahora montañas. El continente se secó, los mares se hicieron profundos y las estaciones adquirieron un carácter áspero y riguroso. Pero es difícil de creer que no quedaran regiones de clima apropiado. Y, por otro lado, los mares no tenían por qué verse afectados.

Otros sugieren que quizá los mamíferos primitivos empezaron a alimentarse de los huevos de dinosaurio acabando así con ellos. (Los reptiles marinos, en cambio eran vivíparos.) O que quizá la Tierra se cubrió de nuevas especies de hierbas que desplazaron la antigua vegetación, más blanda y jugosa. Puede ser que los dinosaurios vegetarianos no tuvieran el tipo de dentadura necesaria para triturar esta nueva especie de hierba más dura y que, al extinguirse aquellos, los dinosaurios carnívoros, al no encontrar alimento, se extinguieran también.

Otra posibilidad es que los dinosaurios a experimentar de pronto gran cantidad de mutaciones. Como la mayoría de las mutaciones son para mal, es posible que el excesivo número de dinosaurios tarados trajese consigo la extinción de la especie.

Esta explicación ha despertado gran interés, pero ¿Por qué un aumento repentino en el número de mutaciones?

Una de las causas de las mutaciones es la radiación muy energética. La Tierra está constantemente bombardeada por los rayos cósmicos, que podrían ser la causa de las mutaciones que constantemente aparecen en organismos hoy día. La tasa actual de mutación no es demasiado alta, pero imaginemos los que ocurriría si, de cuando en cuando incidiese sobre la Tierra un chorro muy potente de radiación.

K. D. Terry, de la Universidad de Kansas, y W. H. Tucker, de la Universidad Rice, han señalado que si explotase una supernova más o menos cerca del sistema solar, la Tierra podría verse inundada de rayos cósmicos. Terry y Tucker estimaron la frecuencia y distancia de estas explosiones y calcularon que cada diez millones de años (por término medio) la Tierra podría recibir una dosis de rayos cósmicos siete mil veces mayor que la actual. Puede ser que hace setenta millones de años la Tierra sufriese una tal andanada de rayos cósmicos.

Pero en este caso ¿por qué afectó sólo a los dinosaurios y no a otras criaturas? Quizá sí que las afectó, sólo que los dinosaurios estaban tan especializados que eran mucho más vulnerables a las mutaciones que las demás criaturas.

¿Y qué tipo de mutación pudo ser la decisiva? H. K. Erben, de la Universidad de Bonn, ha señalado recientemente que en los últimos períodos de existencia de los dinosaurios, los huevos que ponían eran de cáscara muy gruesa. Puede que esta anomalía fuese consecuencia de una mutación. Al ser cada vez más difícil romper el cascarón, fue reduciéndose cada vez más la tasa de natalidad. Entre esta mutación y otras similares se extinguió toda esta especie de magníficas criaturas.

94. ¿Cuál es la diferencia entre uncerebro y un computador? ¿Pueden

pensar los computadores?

La diferencia entre un cerebro y un computador puede expresarse en una sola palabra: complejidad.

El cerebro de los grandes mamíferos es, para su tamaño, la cosa más complicada que conocemos. El cerebro humano pesa unos 1.350 gramos, pero en ese kilo y medio corto hay diez mil millones de neuronas y cientos de miles de millones de otras células menores. Estos miles y miles de millones de células están conectadas entre sí en una red enormemente compleja que sólo ahora estamos empezando a desenmarañar.

Ni siquiera el computador más complicado construido hasta ahora por el hombre puede compararse en complejidad con el cerebro. Las conexiones y componentes de los computadores ascienden a miles, no a miles de millones. Es más, los conmutadores de un computador son sólo dispositivos on-off, mientras que las células cerebrales poseen ya de por sí una estructura interna enormemente compleja.

¿Pueden pensar los computadores? Depende de lo que entendamos por «pensar». Si resolver un problema matemático es «p.ensar», entonces los computadores «piensan», y además mucho más deprisa que el hombre. Claro está que la mayoría de los problemas matemáticos se pueden resolver de manera bastante mecánica, repitiendo una y otra vez ciertos procesos elementales. Y eso lo pueden hacer incluso los computadores más sencillos que existen hoy día.

A menudo se ha dicho que los computadores sólo resuelven problemas porque están «programados» para resolverlos. Que sólo pueden hacer lo que el hombre quiere que hagan. Pero hay que recordar que los seres humanos tampoco pueden hacer otra cosa que aquello para lo que están «programados». Nuestros genes nos «programan» en el momento en que se forma el huevo fertilizado, quedando limitadas nuestras potencialidades por ese «programa».

Ahora bien, nuestro programa es de una complejidad tan superior, que quizá prefiramos definir la palabra «pensar» en función de la creatividad que hace falta para escribir una gran comedia o componer una gran sinfonía, concebir una brillante teoría científica o un juicio ético profundo. En ese sentido, los computadores no pensar, ni tampoco la mayoría de los mortales.

Está claro, sin embargo, que un computador al que se le dotase de suficiente complejidad podría ser tan creativo como el hombre. Si se consiguiera que fuese igual de complejo que el cerebro humano, podría ser el equivalente de éste y hacer exactamente lo mismo.

Suponer lo contrario sería suponer que el cerebro humano es algo más que la materia que lo compone. El cerebro está compuesto de células en un cierto orden, y las células están constituidas por átomos y moléculas en una determinada disposición. Si hay algo más, jamás se han detectado signos de su presencia. Duplicar la complejidad material del cerebro es, por consiguiente, duplicar todo cuanto hay en él.

¿Pero hasta cuándo habrá que esperar para construir un computador suficientemente complejo como para reproducir el cerebro humano? Quizá no tanto como algunos piensan. Puede que, mucho antes de llegar a un computador igual de complejo que el cerebro, consigamos construir otro lo bastante complejo como para que diseñe un segundo más complejo que él. Este segundo computador podría diseñar otro aún más complejo, y así sucesivamente.

Dicho con otras palabras, una vez superado cierto punto los computadores toman las riendas en sus manos y se produce una «explosión de complejidad». Al cabo de muy poco podrían existir computadores que no sólo igualasen al cerebro humano, sino que lo superaran. Y luego ¿qué? El caso es que la humanidad no está distinguiéndose demasiado en la administración de los asuntos terrestres. Puede que llegue el día en que tengamos que hacernos humildemente a un lado y dejar las cosas en manos de quien las sepa llevar mejor. Y si no nos hacemos a un lado, es posible que llegue el Supercomputador y nos aparte por las malas.

95. ¿Cuál es la velocidad delpensamiento?

Depende de lo que entendamos por «pensamiento».

Puede que queramos decir «imaginación». Uno puede imaginarse que está, aquí en la Tierra, y un segundo más tarde que está en Marte o en Alpha Centauri o cerca de un lejano quasar. Si es eso lo que entendemos por «pensamiento», entonces puede tener cualquier velocidad hasta el infinito.

Sí, pero uno no recorre realmente esa distancia ¿verdad? Aunque yo me imagine que estoy presenciando la formación de la Tierra no quiere decir, que haya hecho un viaje a través del tiempo. Y aunque me imagine en el centro del Sol no quiere decir que pueda realmente existir en esas condiciones.

Para que la pregunta tenga algún significado científico es preciso definir «pensamiento» de manera que su velocidad pueda realmente medirse por métodos físicos. A este respecto recordemos que si podemos pensar es porque hay unos impulsos que pasan de célula nerviosa a célula nerviosa. Cualquier acción que dependa del sistema nervioso depende de esos impulsos. Al tocar un objeto caliente retiramos la mano, pero no lo podremos hacer hasta que la sensación de calor pase de la mano al sistema nervioso central y luego otro impulso nervioso pase del sistema nervioso central a los músculos.

El «pensamiento» inconsciente que implica todo esto -«noto algo caliente, y más me vale quitar la mano porque si no me la quemaré»- no puede ser más rápido que el tiempo que tarda el impulso nervioso en recorrer el trayecto de ida y vuelta. Por consiguiente, hemos de entender que la «velocidad del pensamiento» es la: «velocidad del impulso nervioso», porque si no, no hay respuesta.

Allá por el año 1846, el gran fisiólogo alemán Johannes Müller decidió, en un rapto de pesimismo, que la velocidad del impulso nervioso jamás podría medirse. Seis años más tarde, en 1852, consiguió medirlo uno de sus mejores discípulos, Hermann von Helmholtz, trabajando con un músculo todavía inervado. Helmholtz estimuló el nervio en diversos puntos y midió el tiempo que tardaba el músculo en contraerse. Al estimular el nervio en un punto más alejado del músculo, la contracción se retrasaba. A partir del tiempo de retardo logró calcular el tiempo que había tardado el impulso nervioso en recorrer esa distancia adicional.

La velocidad del impulso nervioso depende del grosor del nervio. Cuanto más grueso es el nervio, mayor es la velocidad. La velocidad depende también de si el nervio está o no aislado por una vaina de material graso. Los nervios aislados conducen más rápidamente los impulsos nerviosos que los no aislados.

Los nervios de los mamíferos son los más eficaces de todo el reino animal: los de mejor calidad conducen los impulsos nerviosos a una velocidad de 362 kilómetros por hora.

Esto quizá parezca decepcionante, porque al fin y al cabo la velocidad del pensamiento no es mayor que la de los viejos aeroplanos de hélice. Pero pensemos que un impulso nervioso puede ir desde cualquier punto del cuerpo humano hasta cualquier otro y volver en menos de 1/25 de segundo (omitiendo los retrasos debidos al procesamiento en el sistema nervioso central). El nervio más largo en los mamíferos pertenece a la ballena azul, que mide unos 100 pies de longitud, e incluso en ese caso cualquier posible viaje de ida y vuelta dentro del cuerpo lo puede realizar el impulso nervioso en poco más de medio segundo. Lo cual es bastante rápido.

96. ¿Qué son los «relojesbioIógicos» y cómo funcionan?

Hay veces que uno no necesita mirar el reloj. Cuando tenemos hambre, sabemos que es la hora de almorzar. Cuando tenemos sueño, sabemos que es hora de irse a la cama. Naturalmente, si hemos almorzado copiosamente, es muy probable que se nos pase la hora de la cena sin que sintamos hambre. Y si hemos dormido hasta muy tarde o nos encontramos en una fiesta muy animada, es posible que se nos pase la hora de acostarnos sin que sintamos sueño. Pero en condiciones normales se puede hacer un cálculo muy exacto.

Dentro de cada uno de nosotros hay un cambio cíclico que nos hace sentir hambre cada tanto tiempo y sueño, cada cuanto. Estos cambios son bastante regulares, con lo cual es posible medir el tiempo (aproximadamente) por estos ciclos. Tales ciclos son un ejemplo de «relojes biológicos».

En el mundo exterior de los organismos se producen ciclos regulares. El más notable es la alternancia de la luz del día con la oscuridad de la noche, pero también está el ritmo bidiario de las mareas, cuya amplitud varía con el cambio mensual de fase de la luna, y el ciclo de temperatura, que varía con el período día-noche y con el período anual de las estaciones.

Para el organismo resulta útil responder a estos cambios. Si tiene que procurarse el alimento de noche o sólo en la estación cálida, lo mejor que puede hacer es dormir durante el día o hibernar durante el invierno. Si pone sus huevos en la orilla, lo mejor y más prudente es que lo haga cuando hay marea alta y luna llena. Incluso las plantas responden a estos ritmos: las hojas se abarquillan al ponerse el sol, las flores y los frutos llegan en determinadas estaciones del año, etc.

Lo que no podemos suponer es que los organismos vivientes hacen todo esto conscientemente. No dicen: «Es de noche, me voy a dormir», o «los días se está acortando, voy a dejar caer mis hojas», sino que dentro del organismo hay ciclos automáticos que coinciden con los ciclos astronómicos. Esta coincidencia o sincronización es producto de la selección natural. Los animales y plantas que disfrutan de una buena sincronización se desenvuelven mejor y tienen mayores oportunidades de engendrar una prole más numerosa que los que no la disfrutan, de manera que es un factor que mejora de generación en generación.

Los ciclos internos existen incluso en el nivel molecular. La temperatura del cuerpo sube y baja regularmente, igual que la concentración de ciertos constituyentes de la sangre, la susceptibilidad del cuerpo hacia ciertas drogas, etc. La mayoría de estos ciclos tardan aproximadamente un día en completar un movimiento de sube y baja: es lo que se denomina «ritmos circadianos».

El ciclo interno ¿está controlado por los ritmos del medio ambiente? No del todo. Si colocamos un animal o una planta en un medio artificial en el cual se ha eliminado el ciclo externo -donde, por ejemplo, la luz es constante o la temperatura no varía-, los ritmos prosiguen sin embargo su curso. Puede que sean menos marcados o que difieran un poco del ciclo estricto de veinticuatro horas, pero están ahí. Los ritmos del medio ambiente no actúan más que como un «control fino».

Cuando viajamos en avión a un país muy lejano, con una diferencia de horas muy grande, nuestros ritmos internos dejan de estar sincronizados con el período día-noche. Esto da lugar a síntomas muy desagradables, hasta que logramos poner de nuevo en hora el reloj biológico.

¿Qué cómo funciona el reloj biológico? Lo puedo describir en tres palabras: ¡Nadie lo sabe!

¿Será una especie de reacción química periódica en nuestro cuerpo? En ese caso el reloj debería variar con la temperatura o con las drogas, y no es verdad. ¿O será que está sincronizado con sutilísimos ritmos del mundo exterior que persisten aun cuando eliminemos las variaciones de luz y temperatura? Puede ser, pero en ese caso desconocemos todavía la naturaleza de esos ritmos.

97. ¿Cuál es la diferencia entrebacterias, microbios, gérmenes y

virus?

Las bacterias son un grupo de organismos unicelulares reunidos por los biólogos bajo el nombre de «esquizomicetos». La célula de la bacteria tiene una pared muy parecida a la de las células vegetales normales, pero carece de clorofila. Por eso las bacterias se clasifican a veces junto con otras plantas carentes de clorofila y se denominan «hongos».

Las bacterias se distinguen de otras células vegetales en que son muy pequeñas. En efecto, son las células más pequeñas que existen. Además, no poseen un núcleo diferenciado, sino que el material nuclear está disperso por toda la célula. Por eso se clasifican a veces junto con ciertas células vegetales llamadas «algas verde-azules», cuyo material nuclear también está disperso, pero que además tienen clorofila.

Cada vez es más usual agrupar las bacterias junto con otras criaturas unicelulares, formando una clase de seres que no están considerados ni como plantas ni como animales: constituyen un tercer reino de vida, los «protistos». Hay bacterias que son patógenas es decir, que causan enfermedades. Pero la mayoría de ellas no lo son, e incluso hay muchas que son muy beneficiosas. La fertilidad del suelo, por ejemplo, depende en gran medida de la actividad de las bacterias nitrogenantes.

Un «microbio» es, en rigor, cualquier forma de vida microscópica, porque el término viene de dos palabras griegas que significan «vida pequeña». El término «germen» es aún más general, pues significa cualquier fragmento pequeño de vida, aunque sea parte de un organismo mayor. Por ejemplo, la sección de la semilla que contiene la verdadera porción viviente es el germen; así hablamos del «germen del trigo», por ejemplo. Por otro lado, el óvulo y el espermatozoide, que portan las diminutas chispas de vida que en su día florecen en un organismo acabado, se llaman «células germinales».

En el lenguaje corriente, sin embargo, se utilizan las palabras microbio y germen como sinónimos de bacteria, en especial de bacteria patógena.

La palabra «virus» viene del latín y significa «veneno». Esta etimología viene de los tiempos en que los biólogos no sabían exactamente qué eran los virus, pero sí que ciertas preparaciones contenían algo que ocasionaba enfermedad.

Los virus difieren de las bacterias y de todos los demás organismos en que no están compuestos de células. Son mucho más pequeños que las células y su tamaño viene a ser el de una gran molécula. Están formados, por un arrollamiento de ácido nucleico, rodeado de un recubrimiento de proteína. En esto se parecen a los cromosomas de una célula, de modo que cabría casi considerarlos como «cromosomas sueltos».

Los cromosomas controlan la química de la célula; los virus, cuando se introducen en una célula, establecen un contracontrol por su cuenta. Por lo general son capaces de someter toda la química de la célula a sus propios fines, poniendo toda la maquinaria celular al servicio de la formación de nuevos virus. La célula suele morir en el proceso.

Los virus, a diferencia de las bacterias, no son capaces de llevar una vida independiente. Sólo se pueden multiplicar dentro de las células. Todos ellos son parásitos. El daño que ocasionan pasa a veces inadvertido, pero en otros casos producen graves enfermedades.

98. ¿Cómo se descubrieron los virus?

Hacia los años sesenta del siglo pasado* el químico francés Louis Pasteur propuso la «teoría germinal de las enfermedades», según la cual todas las enfermedades eran causadas y propagadas por alguna forma diminuta de vida que se multiplicaba en el organismo enfermo, pasaba de ese organismo a otro sano, lo hacía enfermar, etc.

Pasteur, sin embargo, estaba trabajando a la sazón con una enfermedad mortal, la rabia (también llamada hidrofobia), y descubrió que aunque la enfermedad era contagiosa y podía contraerse por el mordisco de un animal rabioso, no se veía el germen por ningún lado. Pasteur concluyó que el germen sí que estaba allí, pero que era demasiado pequeño para verlo con el microscopio con que trabajaba.

Otras enfermedades también parecían carecer de germen, quizá por la misma razón. Un ejemplo era la «enfermedad del mosaico del tabaco», que atacaba a las plantas del tabaco y producía como síntoma un dibujo en forma de mosaico sobre las hojas. Triturando éstas, se podía extraer un jugo que ocasionaba esa enfermedad en plantas sanas, pero el jugo no contenía ningún germen que fuese visible al microscopio.

¿Hasta qué punto se podía fiar uno de los microscopios en el límite mismo de la visibilidad? El bacteriólogo ruso Dimitri Ivanovski abordó en 1892 el problema por un camino diferente: utilizó un filtro de porcelana sin vidriar que retenía cualquier organismo suficientemente grande para poder verlo con los microscopios de aquella época. Hizo pasar el extracto infeccioso de plantas del tabaco enfermas a través del filtro y comprobó que el producto filtrado seguía infectando a las plantas sanas. Ivanovski pensó que quizá el filtro fuese defectuoso y no se atrevió a afirmar que había gérmenes demasiado pequeños para verlos al microscopio.

En 1898, y de manera independiente, el botánico holandés Martinus Beijerinck hizo el mismo experimento y obtuvo igual resultado. Aceptó la validez del experimento y decidió que, fuese cual fuese la causa de la enfermedad del mosaico del tabaco, tenía que consistir en partículas tan pequeñas que pudiesen pasar por el filtro.

Beijerinck llamó al líquido patógeno «virus», de una palabra latina que significa «veneno». Como el líquido era capaz de pasar por un filtro sin perder su calidad venenosa, le dio el nombre de«virus filtrable». El término fue aplicado más tarde, no al líquido, sino a las partículas patógenas que contenía. Luego se eliminó el adjetivo, llamando simplemente virus a dichas partículas.

¿Pero qué tamaño tenían las partículas de los virus? Beijerínck pensó que quizá no fueran mucho mayores que las moléculas de agua, de modo que cualquier sustancia que dejara pasar el agua dejaría pasar también a los virus.

Esto es lo que decidió comprobar en 1931 el bacteriólogo inglés William Elford. Para ello utilizó membranas de colodión con orificios microscópicos de diversos tamaños. Filtrando líquidos víricos a través de membranas, encontró que una de ellas tenía unos orificios tan pequeños, que aunque las moléculas de agua pasaban, las partículas. de virus quedaban retenidas. Elford vio que mientras que el líquido no filtrado transmitía la enfermedad, lo que pasaba por el filtro ya no la transmitía.

De esa manera se logró averiguar el tamaño de las partículas de virus. Eran más pequeñas que las células más pequeñas que se conocían; tanto, que quizá sólo consistieran en unas cuantas moléculas. Esas moléculas, sin embargo, eran moléculas gigantes.

99. ¿Por qué las células de lasangre se reponen cada pocos meses,

mientras que la mayoría de las células

del cerebro duran toda la vida?

La maquinaria de la división celular es extraordinariamente complicada. El proceso consta de numerosos pasos en los que la membrana nuclear desaparece, el centrosoma se divide, los cromosomas forman réplicas de sí mismos, son capturados en la red en la red formada por los centrosomas divididos y se reparten en lados opuestos de la célula. Luego se forma una nueva membrana nuclear a ambos lados, mientras la célula se constriñe por el medio y se divide en dos.

Los cambios químicos involucrados en todo esto son, sin duda alguna, mucho más complicados todavía. Sólo en los últimos años se han empezado a vislumbrar algunos de ellos. No tenemos ni la más ligera idea, por ejemplo de cuál es el cambio químico que hace que la célula deje de dividirse cuando ya no hace falta que lo haga. Si supiéramos la respuesta, podríamos resolver el problema del cáncer, que es un desorden en el crecimiento celular, una incapacidad de las células para dejar de dividirse.

Una criatura tan compleja como el hombre tiene (y debe tener) células extraordinariamente especializadas. Las células pueden realizar ciertas funciones que todas las demás pueden también realizar, pero es que en cada caso llevan su cometido hasta el extremo. Las células musculares han desarrollado una eficacia extrema en contraerse, las nerviosas en conducir impulsos eléctricos, las renales en permitir que pasen sólo ciertas sustancias y no otras. La maquinaria dedicada en esas células a la función especializada es tanta, que no hay lugar para los mecanismos de la división celular. Estas células, y todas las que poseen un cierto grado de especialización, tienen que prescindir de la división.

En términos generales podemos decir que una vez que un organismo ha alcanzado pleno desarrollo, ya no hay necesidad de un mayor tamaño, ni necesidad, por tanto, de más células.

Sin embargo, hay algunas que están sometidas a un continuo desgaste. Las células de la piel están en constante contacto con el mundo exterior, las de la membrana intestinal rozan con los alimentos y los glóbulos rojos chocan contra las paredes de los capilares. En todos estos casos, el rozamiento y demás avatares se cobran su tributo. En el caso de la piel y de las membranas intestinales, las células de las capas más profundas tienen que seguir siendo capaces de dividirse, a fin de reponer las células desprendidas por otras nuevas. De hecho, las células superficiales de la piel mueren antes de desprenderse, de modo que la capa exterior constituye una película muerta de protección, dura y resistente. Allí donde el rozamiento es especialmente grande, la capa muerta llega a formar callo.

Los glóbulos rojos de la sangre carecen de núcleo y, por consiguiente, de esa maquinaria, de división celular que está invariablemente concentrada en los núcleos. Pero en muchos lugares del cuerpo, sobre todo en la médula de ciertos huesos, hay células provistas de núcleos que se pueden dividir y formar células hijas; éstas a su vez, pierden gradualmente el núcleo y se convierten glóbulos rojos.

Algunas células que normalmente no se dividen una vez alcanzado el pleno desarrollo pueden, sin embargo hacerlo cuando hace falta una reparación. Un hueso, por ejemplo, que hace mucho que ha dejado de crecer, puede volver a hacerlo si sufre una rotura; crecerá lo justo para reparar la fractura y luego parará. (¡Qué lástima que las células nerviosas no puedan hacer lo propio!)

La vida de una célula concreta hasta ser reemplazada depende normalmente de la naturaleza y la intensidad de las tensiones a que está expuesta, por lo cual es muy difícil dar datos exactos. (Se ha comprobado qué la piel exterior de la planta de una rata queda completamente reemplazada, en ciertas condiciones, en dos semanas.) Una excepción son los glóbulos rojos, que están sometidos a un desgaste continuo e invariable. Los glóbulos rojos del hombre tienen una vida predecible de unos ciento veinticinco días.

100. ¿Qué fin tiene el envejecer?

Parece una pena tener que envejecer y morir, pero evidentemente es inevitable. Los organismos como el nuestro están efectivamente diseñados para envejecer y morir, porque nuestras células están «programadas» por sus genes para que vayan experimentando gradualmente esos cambios que denominamos envejecer.

¿Qué propósito puede tener el envejecimiento? ¿Puede ser beneficioso?

Veamos. La propiedad más sorprendente de la vida, dejando aparte su propia existencia, es su versatilidad. Hay criaturas vivientes en la tierra, en el mar y en el aire, en los géiseres, en los desiertos, en los desiertos, en la jungla, en los desiertos polares… en todas partes. Incluso es posible inventar un medio corno los que creemos que existen en Marte o en Júpiter y encontrar formas elementales de vida que lograrían sobrevivir en esas condiciones.

Para conseguir esa versatilidad tienen que producirse constantes cambios en las combinaciones de genes y en su propia naturaleza.

Al dividirse un organismo unicelular, cada una de las dos células hijas tiene los mismos genes que la célula original. Si los genes se transmitieran como copias perfectas, la naturaleza de la célula original jamás cambiaría por mucho que se dividiera, y redividiera. Pero la copia no siempre es perfecta; de vez en cuando hay cambios fortuitos («mutaciones»), de modo que de una misma célula van surgiendo poco a poco distintas razas, variedades y, finalmente, especies («evolución»). Algunas de estas especies se desenvuelven mucho mejor, en un medio dado que otras, y así es como las distintas especies van llenando los diversos nichos ecológicos de la Tierra.

Hay veces que los organismos unicelulares intercambian entre sí porciones de cromosomas. Esta primitiva versión del sexo origina cambios de las combinaciones de genes, acelerando aún más los cambios evolutivos. En los animales pluricelulares fue adquiriendo cada vez más importancia la reproducción sexual, que implica la cooperación de dos organismos. La constante producción de descendientes, cuyos genes son una mezcla aleatoria de algunos del padre y otros de la madre, introdujo una variedad superior a lo que permitían las mutaciones por sí solas. Como resultado de ello se aceleró considerablemente el ritmo de evolución; las distintas especies podías ahora extenderse más fácilmente y con mayor rapidez dentro de nuevos nichos ecológicos o adaptarse mejor a los ya existentes a fin de explotarlos con mejor rendimiento.

Vemos, pues, que la clave de todo esto fue la producción de descendientes, con sus nuevas combinaciones de genes. Algunas de las nuevas combinaciones eran seguramente muy deficientes, pero no durarían mucho. De entre las nuevas combinaciones, las más útiles fueron las que «llegaron a la meta» y engrosaron la competencia. Pero para que este sistema funcione bien es preciso que la vieja generación, con sus combinaciones «no mejoradas» de genes, desaparezca de la escena. No cabe duda de que los viejos morirían tarde o temprano en accidente o debido al desgaste general de la vida, pero es mucho más eficaz que el proceso venga acelerado por otro lado.

Aquellas especies en las que las generaciones antiguas poseyeran células diseñadas para envejecer serían mucho más eficientes a la hora de deshacerse de los vejestorios y dejar el terreno expedito para los jóvenes. De este modo evolucionarían más rápido y tendrían más éxito. La desventaja de la longevidad está a la vista. Las sequoias y los pinos están casi extinguidos. El longevo elefante no tiene ni de lejos el éxito de la efímera rata; y lo mismo diríamos de la vetusta tortuga comparada con el lagarto.

Para bien de las especies (incluida la humana) lo mejor es que los viejos se mueran para que los jóvenes puedan vivir.

¡Y perdonen!

* El XIX, no nos despistemos (Nota de Dom)

13/06/2008
